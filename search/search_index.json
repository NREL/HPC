{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"NREL HPC Resources Intro These docs are driven by the NREL HPC community. They are currently under active development. If you would like to contribute or recommend a topic to be covered please open an issue or pull request in the repository. Docs repository Workshops The HPC community also hosts workshops covering various topics. Check the training calendar below, or view material from recent workshops. Recent workshops Additional NREL resources User Basics Eagle Slurm partitions Calendar Calendar of training events and office hours for NREL's HPC.","title":"Home"},{"location":"#nrel-hpc-resources","text":"","title":"NREL HPC Resources"},{"location":"#intro","text":"These docs are driven by the NREL HPC community. They are currently under active development. If you would like to contribute or recommend a topic to be covered please open an issue or pull request in the repository. Docs repository","title":"Intro"},{"location":"#workshops","text":"The HPC community also hosts workshops covering various topics. Check the training calendar below, or view material from recent workshops. Recent workshops","title":"Workshops"},{"location":"#additional-nrel-resources","text":"User Basics Eagle Slurm partitions","title":"Additional NREL resources"},{"location":"#calendar","text":"Calendar of training events and office hours for NREL's HPC.","title":"Calendar"},{"location":"Announcements/2020-11-03-announcement/","text":"ESIF-HPC-3 Project Update The ESIF-HPC-3 project has begun! The effort to acquire Eagle\u2019s successor involves ongoing engagement with stakeholders (EERE, Lab Program Management, and the HPC user community), tracking industry trends, analysis of Eagle\u2019s workload over its life to date, external design review, and a carefully managed Request for Proposals targeted for release later this year. We are currently in the process of completing the draft technical specifications for the ESIF-HPC-3 system, and are targeting the start of FY23 for general production access. If you would like to weigh in on how your work would benefit from existing or new features you could envision, please feel free to send a note to hpc-help@nrel.gov. We will open a discussion on the draft design in its current form if there is sufficient interest. Lustre Quotas Effective with the new Fiscal Year 2021 Project allocations for Eagle, quotas for approved storage allocations' capacities have been implemented on /projects and MSS on Eagle. This was to encourage users to manage their /projects data usage and usage of /scratch for jobs. HPC Operations is developing reporting capabilities of usage, but in the mean time, users may request help from the HPC Help Desk, or utilize these procedures from an Eagle Login node: To view project quotas and usage: Get the ProjectID for your /projects directory: lfs project -d /projects/csc000 110255 P /projects/csc000 Get the usage and quota in kbytes: lfs quota -p 110255 /projects/csc000 Disk quotas for prj 110255 (pid 110255): Filesystem kbytes quota limit grace files quota limit grace /projects/csc000 3165308* 3072 4096 - 48 1073741824 2147483648 - An * means you have exceeded your soft quota of 3072kb, the hard limit of 4096kb reached means no more writes are allowed. Grace period is set to default of 7 days but will show time until writes are suspended. \"files\" indicate the number of inodes used and soft and hard limits. We encourage users to run their jobs in Eagle /scratch and copy results and other necessary project files to /projects, possibly using tar and zip to conserve space (tar czvf tar-file-name.tgz source-directory-files-to-tar-and zip). If you are over your project quota, we recommend removing unneeded files and directories or moving them to your /scratch directory until no longer needed. Remember /scratch files are purged regularly per NREL\u2019s HPC storage retention policies. Changes to Eagle Mass Storage System (MSS) What: NREL HPC Operations is in the process of retiring the on-premise MSS capability and has started using cloud-based data storage capability. Why: The vendor has announced end-of-life for the technology previously used to provide MSS. In exploring alternatives, a cloud-based solution leverages expertise of the Advanced Computing Operations team and is significantly more cost effective. Adequate bandwith is available to/from Cloud (10x more bandwith than to/from the previous on-premise MSS). Also a very small percentage of data written to MSS is ever read again, thus prompting a change to a fixed 15-month retention from when data is written to MSS. When/How: On December 1st, all new writes to MSS will be to cloud-based storage. Reading data from the existing on-premise MSS capability will be supported through March 31, 2021. Active data (written or last read within the last 15 months prior to December 1st, 2020) will be migrated from the on-premise MSS to the new cloud-based storage. What stays the same: MSS is an additional location available to projects active on Eagle to keep and protect important data in addition to the Eagle high-performance storage (/projects, /shared-projects, /datasets) What changes: Data will be retained for 15 months from the date written. This differs from the current retention policy of minimum 15 months with deletion if needed. Restore requests of MSS data that are cloud-based will initially require a request to the HPC Help Desk, and may require 48 hours to be able to recover. Also a reminder: Project data (/projects) for FY20 projects not continuing into FY21 will have until December 31, 2020 to move data off Eagle to MSS or other long-term storage, before it is purged from Eagle on January 1st, 2021.","title":"November 2020 Monthly Update"},{"location":"Announcements/2020-11-03-announcement/#esif-hpc-3-project-update","text":"The ESIF-HPC-3 project has begun! The effort to acquire Eagle\u2019s successor involves ongoing engagement with stakeholders (EERE, Lab Program Management, and the HPC user community), tracking industry trends, analysis of Eagle\u2019s workload over its life to date, external design review, and a carefully managed Request for Proposals targeted for release later this year. We are currently in the process of completing the draft technical specifications for the ESIF-HPC-3 system, and are targeting the start of FY23 for general production access. If you would like to weigh in on how your work would benefit from existing or new features you could envision, please feel free to send a note to hpc-help@nrel.gov. We will open a discussion on the draft design in its current form if there is sufficient interest.","title":"ESIF-HPC-3 Project Update"},{"location":"Announcements/2020-11-03-announcement/#lustre-quotas","text":"Effective with the new Fiscal Year 2021 Project allocations for Eagle, quotas for approved storage allocations' capacities have been implemented on /projects and MSS on Eagle. This was to encourage users to manage their /projects data usage and usage of /scratch for jobs. HPC Operations is developing reporting capabilities of usage, but in the mean time, users may request help from the HPC Help Desk, or utilize these procedures from an Eagle Login node: To view project quotas and usage: Get the ProjectID for your /projects directory: lfs project -d /projects/csc000 110255 P /projects/csc000 Get the usage and quota in kbytes: lfs quota -p 110255 /projects/csc000 Disk quotas for prj 110255 (pid 110255): Filesystem kbytes quota limit grace files quota limit grace /projects/csc000 3165308* 3072 4096 - 48 1073741824 2147483648 - An * means you have exceeded your soft quota of 3072kb, the hard limit of 4096kb reached means no more writes are allowed. Grace period is set to default of 7 days but will show time until writes are suspended. \"files\" indicate the number of inodes used and soft and hard limits. We encourage users to run their jobs in Eagle /scratch and copy results and other necessary project files to /projects, possibly using tar and zip to conserve space (tar czvf tar-file-name.tgz source-directory-files-to-tar-and zip). If you are over your project quota, we recommend removing unneeded files and directories or moving them to your /scratch directory until no longer needed. Remember /scratch files are purged regularly per NREL\u2019s HPC storage retention policies.","title":"Lustre Quotas"},{"location":"Announcements/2020-11-03-announcement/#changes-to-eagle-mass-storage-system-mss","text":"What: NREL HPC Operations is in the process of retiring the on-premise MSS capability and has started using cloud-based data storage capability. Why: The vendor has announced end-of-life for the technology previously used to provide MSS. In exploring alternatives, a cloud-based solution leverages expertise of the Advanced Computing Operations team and is significantly more cost effective. Adequate bandwith is available to/from Cloud (10x more bandwith than to/from the previous on-premise MSS). Also a very small percentage of data written to MSS is ever read again, thus prompting a change to a fixed 15-month retention from when data is written to MSS. When/How: On December 1st, all new writes to MSS will be to cloud-based storage. Reading data from the existing on-premise MSS capability will be supported through March 31, 2021. Active data (written or last read within the last 15 months prior to December 1st, 2020) will be migrated from the on-premise MSS to the new cloud-based storage. What stays the same: MSS is an additional location available to projects active on Eagle to keep and protect important data in addition to the Eagle high-performance storage (/projects, /shared-projects, /datasets) What changes: Data will be retained for 15 months from the date written. This differs from the current retention policy of minimum 15 months with deletion if needed. Restore requests of MSS data that are cloud-based will initially require a request to the HPC Help Desk, and may require 48 hours to be able to recover. Also a reminder: Project data (/projects) for FY20 projects not continuing into FY21 will have until December 31, 2020 to move data off Eagle to MSS or other long-term storage, before it is purged from Eagle on January 1st, 2021.","title":"Changes to Eagle Mass Storage System (MSS)"},{"location":"Announcements/2020-12-03-announcement/","text":"FY20 Expired Projects' Data Reminder that FY20 expired Projects' data will be removed from Eagle on January 1st, 2021. Any data needed needs to either be copied to the new AWS MSS or other arrangements made outside of HPC. Due to vendor ending support for the old MSS equipment, the new HPC Mass Storage System (MSS) environment will reside on Amazon Web Services. The old Gyrfalcon MSS data will be made read-only on December 1st, 2020. Any data 15 months old or less will be migrated to AWS MSS. Changes to Eagle Mass Storage System (MSS) What: NREL HPC Operations is in the process of retiring the on-premise MSS capability and has started using cloud-based data storage capability. Why: The vendor has announced end-of-life for the technology previously used to provide MSS. In exploring alternatives, a cloud-based solution leverages expertise of the Advanced Computing Operations team and is significantly more cost effective. Adequate bandwith is available to/from Cloud (10x more bandwith than to/from the previous on-premise MSS). Also a very small percentage of data written to MSS is ever read again, thus prompting a change to a fixed 15-month retention from when data is written to MSS. When/How: On December 1st, all new writes to MSS will be to cloud-based storage. Reading data from the existing on-premise MSS capability will be supported through March 31, 2021. Active data (written or last read within the last 15 months prior to December 1st, 2020) will be migrated from the on-premise MSS to the new cloud-based storage. What stays the same: MSS is an additional location available to projects active on Eagle to keep and protect important data in addition to the Eagle high-performance storage (/projects, /shared-projects, /datasets) What changes: Data will be retained for 15 months from the date written. This differs from the current retention policy of minimum 15 months with deletion if needed. Restore requests of MSS data that are cloud-based will initially require a request to the HPC Help Desk, and may require 48 hours to be able to recover. ESIF-HPC-3 Project Update The ESIF-HPC-3 project moves on. The Request for Proposals and its many pieces (including the technical specifications, benchmark suite and specifications, and workload analysis) should be live by the time you read this. At this time, our hope is that vendors are busy designing the next-generation computing and storage systems that will serve EERE research starting in FY23, and preparing proposals for review by a cross-Directorate NREL Source Evaluation Team starting in mid-January 2021.","title":"December 2020 Monthly Update"},{"location":"Announcements/2020-12-03-announcement/#fy20-expired-projects-data","text":"Reminder that FY20 expired Projects' data will be removed from Eagle on January 1st, 2021. Any data needed needs to either be copied to the new AWS MSS or other arrangements made outside of HPC. Due to vendor ending support for the old MSS equipment, the new HPC Mass Storage System (MSS) environment will reside on Amazon Web Services. The old Gyrfalcon MSS data will be made read-only on December 1st, 2020. Any data 15 months old or less will be migrated to AWS MSS.","title":"FY20 Expired Projects' Data"},{"location":"Announcements/2020-12-03-announcement/#changes-to-eagle-mass-storage-system-mss","text":"What: NREL HPC Operations is in the process of retiring the on-premise MSS capability and has started using cloud-based data storage capability. Why: The vendor has announced end-of-life for the technology previously used to provide MSS. In exploring alternatives, a cloud-based solution leverages expertise of the Advanced Computing Operations team and is significantly more cost effective. Adequate bandwith is available to/from Cloud (10x more bandwith than to/from the previous on-premise MSS). Also a very small percentage of data written to MSS is ever read again, thus prompting a change to a fixed 15-month retention from when data is written to MSS. When/How: On December 1st, all new writes to MSS will be to cloud-based storage. Reading data from the existing on-premise MSS capability will be supported through March 31, 2021. Active data (written or last read within the last 15 months prior to December 1st, 2020) will be migrated from the on-premise MSS to the new cloud-based storage. What stays the same: MSS is an additional location available to projects active on Eagle to keep and protect important data in addition to the Eagle high-performance storage (/projects, /shared-projects, /datasets) What changes: Data will be retained for 15 months from the date written. This differs from the current retention policy of minimum 15 months with deletion if needed. Restore requests of MSS data that are cloud-based will initially require a request to the HPC Help Desk, and may require 48 hours to be able to recover.","title":"Changes to Eagle Mass Storage System (MSS)"},{"location":"Announcements/2020-12-03-announcement/#esif-hpc-3-project-update","text":"The ESIF-HPC-3 project moves on. The Request for Proposals and its many pieces (including the technical specifications, benchmark suite and specifications, and workload analysis) should be live by the time you read this. At this time, our hope is that vendors are busy designing the next-generation computing and storage systems that will serve EERE research starting in FY23, and preparing proposals for review by a cross-Directorate NREL Source Evaluation Team starting in mid-January 2021.","title":"ESIF-HPC-3 Project Update"},{"location":"Announcements/2021-01-04-announcement/","text":"Arbiter2 On Tuesday, January 12, we will be upgrading the Arbiter2 software on the Eagle login nodes. The upgrade improves stability of the program, as well as fixes some broken features. Arbiter2 limits individual resources on these shared resources within a range. Certain processes (for example, those related to code compilation) are exempt from these limits. For other processes, consistently high processor utilization leads to resource throttling in order to equalize the net amount of resource users have access to over time. As usage returns below a level consistent with the smooth operation of the shared login node, the throttling is relaxed. Users exceeding per-user resource limits on login nodes (\"in violation\") will receive emails when they trigger a violation, and when their usage returns below limits. From users' perspective the upgrade will not change limits or throttling behavior, it will just turn on notifications. ESIF-HPC-3 Project Update The ESIF-HPC-3 Request for Proposals is live! For those interested, the content can be found on SAM.gov. Application Updates Q-Chem has been upgraded to version 5.3.2. See changes here. Star-CCM version 15.06.008 is available on Eagle. ARM Forge version 20.2 is available on Eagle. We are working on acquiring a Maintenance license for VASP 6. Once we have this in place, users will need to have an upgraded VASP 6 Research workgroup license in order to use our VASP 6 builds on Eagle.","title":"January 2021 Monthly Update"},{"location":"Announcements/2021-01-04-announcement/#arbiter2","text":"On Tuesday, January 12, we will be upgrading the Arbiter2 software on the Eagle login nodes. The upgrade improves stability of the program, as well as fixes some broken features. Arbiter2 limits individual resources on these shared resources within a range. Certain processes (for example, those related to code compilation) are exempt from these limits. For other processes, consistently high processor utilization leads to resource throttling in order to equalize the net amount of resource users have access to over time. As usage returns below a level consistent with the smooth operation of the shared login node, the throttling is relaxed. Users exceeding per-user resource limits on login nodes (\"in violation\") will receive emails when they trigger a violation, and when their usage returns below limits. From users' perspective the upgrade will not change limits or throttling behavior, it will just turn on notifications.","title":"Arbiter2"},{"location":"Announcements/2021-01-04-announcement/#esif-hpc-3-project-update","text":"The ESIF-HPC-3 Request for Proposals is live! For those interested, the content can be found on SAM.gov.","title":"ESIF-HPC-3 Project Update"},{"location":"Announcements/2021-01-04-announcement/#application-updates","text":"Q-Chem has been upgraded to version 5.3.2. See changes here. Star-CCM version 15.06.008 is available on Eagle. ARM Forge version 20.2 is available on Eagle. We are working on acquiring a Maintenance license for VASP 6. Once we have this in place, users will need to have an upgraded VASP 6 Research workgroup license in order to use our VASP 6 builds on Eagle.","title":"Application Updates"},{"location":"Announcements/2021-02-02-announcement/","text":"Eagle Outage Eagle will be taking an outage on March 2nd. We will be making time critical hardware repairs to the Lustre file system and doing some security patching. Small tweaks are being made to slurm to reduce the amount of advertised memory and reduce the number of E-cells jobs run on. Both should make nodes and jobs more reliable and increase performance for larger jobs. Jupyterhub Documentation We have written up a how-to guide for using the Europa Jupyterhub server, including setting up custom Python, Julia, and R kernels and interacting with Eagle. See https://www.nrel.gov/hpc/jupyterhub.html for more. ESIF-HPC-3 Project Update We have pushed the proposal deadline out to February 18th and expect to have reviews completed sometime in mid-April. We have been fielding questions from various interested offerors, and are looking forward to seeing what they've got!","title":"February 2021 Monthly Update"},{"location":"Announcements/2021-02-02-announcement/#eagle-outage","text":"Eagle will be taking an outage on March 2nd. We will be making time critical hardware repairs to the Lustre file system and doing some security patching. Small tweaks are being made to slurm to reduce the amount of advertised memory and reduce the number of E-cells jobs run on. Both should make nodes and jobs more reliable and increase performance for larger jobs.","title":"Eagle Outage"},{"location":"Announcements/2021-02-02-announcement/#jupyterhub-documentation","text":"We have written up a how-to guide for using the Europa Jupyterhub server, including setting up custom Python, Julia, and R kernels and interacting with Eagle. See https://www.nrel.gov/hpc/jupyterhub.html for more.","title":"Jupyterhub Documentation"},{"location":"Announcements/2021-02-02-announcement/#esif-hpc-3-project-update","text":"We have pushed the proposal deadline out to February 18th and expect to have reviews completed sometime in mid-April. We have been fielding questions from various interested offerors, and are looking forward to seeing what they've got!","title":"ESIF-HPC-3 Project Update"},{"location":"Announcements/2021-03-03-announcement/","text":"Elevate Your Work With New Tracking for Advanced Computing in the NREL Publishing Tracker There is a new question on the User Facilities & Program Areas page when you enter a publication into the Pub Tracker \u2013 \"The High Performance Computing Facility was used to produce results or data used in this publication.\" Please be sure to check Yes on this question for your work that made use of the HPC User Facility or other systems in the ESIF HPC Data Center. In addition, there are three new Program Areas to use to tag your publication under the Advanced Computing heading: Cloud, HPC and Visualization & Insight Center. Making use of these metadata will enable us to elevate your work through communications highlights, feature stories, and reporting to EERE. More information about the NREL Publishing Tracker can be found by visiting the Access and Use the NREL Publishing Tracker page on the Source. Fiscal Year 2021 Quarterly Allocation Reductions You may have noticed that NREL did not make any reductions to allocations that were under-using their AUs during Q1. This is for two reasons. First, we were in the process of putting together a new, more transparent, fairer allocation reduction policy. Second, we are aware that many users were inconvenienced by the fact that allocation decisions were issued on October 1. We realize allocation reductions for low use are not popular with our users. However, they are physically necessary. AUs are an \"expiring resource.\" If an AU is not used in Q1, it cannot be stored and saved for use in Q4. Because of this, we have to remove some percentage of the unused AUs every quarter. Otherwise we can hit a situation where we have many more AUs available to users than the machine can physically provide as the year progresses. This creates long queue times that make Eagle physically unusable. Our new allocation policy is given below. We had an informal discussion with users across multiple centers before designing this policy. Users emphasized the need for a policy to be clear enough so they could see what they might lose at the end of the quarter. Users have also increasingly requested allocations that had different usages in different quarters to deal with their project needs, and we wanted a policy that treated these fairly. Shortly after Q1, Q2, and Q3 ends, allocations will be automatically adjusted to account for low utilization against planned usage. At the end of each quarter, the total allocation units used for the year to date will be compared to the total planned usage for the year to date for each allocation. At the end of Q1, the total used for the year to date is compared to the Q1 planned usage; at the end of Q2, the total used for the year to date is compared to the Q1+Q2 planned usage, and at the end of Q3, the toal used for the year to date is compared to the Q1+Q2+Q3 planned usage. Allocation units are then removed based on the table below. Note that allocation reductions are meant to be cumulative over the course of a year, and not compounding. If, for instance, a project was reduced by 10,000 AUs for low usage in Q1, and the reduction table suggests the project should be reduced by 25,000 AUs at the end of Q2, 15,000 (25,000-15,000) AUs should be removed at the end of Q2 to make the total removal for the year to date equal to 25,000 AUs. Percentage of planned AUs used to date Percentage of planned to date AUs removed More than 70% 0% (No reduction) Less than 70% but greater than 55% 20% Less than 55% but greater than 40% 35% Less than 40% but greater than 20% 55% Less than 20% 80% To understand how this process would work, we consider the following two 100,000 AU allocations, one with a uniform distribution of planned AUs throughout the year, and one with a distribution designed to enable development in Q1 and production runs in Q2 through Q4. The two allocations are described in the table below. Quarter Allocation \"Renewables\" Allocation \"Efficiency\" Q1 25,000 10,000 Q2 25,000 30,000 Q3 25,000 30,000 Q4 25,000 30,000 If \"Renewables\" uses 9,000 AUs in Q1, 20,000 AUs in Q2, and 25,000 AUs in Q3, it would be reduced in the following manner: After Q1, the project will have used 36% (9,000/25,000) if its allocation, leading to 13,750 (55% x 25,000) AUs being removed. After Q2, the project will have used 58% [(9,000+20,000)/(25,000+25,000)] of its allocation, leading to 10,000 AUs (20% x 50,000) being potentially removed. However, because 12,500 AUs were removed in Q1, no removal is performed. Note that AUs are not restored in this case. After Q3, the project will have used 72 % [(9,000+20,000+25,000)/(25,000+25,000+25,000)] of its allocation. No reduction would be made. The project would then begin Q4 with 33,500 AUs (100,000-9,000-20,000-25,000-12,500). If \"Efficiency\" uses AUs under the same schedule, after Q1, the project will have used 90% (9,000/10,000) of its allocation and will not be penalized. After Q2, the project will have used 72.5% [(9,000+20,000)/(10,000+30,000)] of its allocation and will not be penalized. After Q3, the project will have used 77% [(9,000+20,000+25,000)/(10,000+30,000+30,000)] of its allocation, and will not be penalized. The project would then begin Q4 with 46,000 AUs. Note that \"Renewables\" and \"Efficiency\" have the same total (100,000 AUs) but lose very different amounts of AUs over the course of the year. This is because the allocation request \"Efficiency\" is more closely tuned to the user\u2019s actual us of HPC resources. NREL allows users to tune their allocation request through the use of \"profiles\" in the allocation request to avoid this sorts of reductions. This information can also be found by visiting the Fiscal Year 2021 Quarterly Allocation Reductions page on our website.","title":"March 2021 Monthly Update"},{"location":"Announcements/2021-03-03-announcement/#elevate-your-work-with-new-tracking-for-advanced-computing-in-the-nrel-publishing-tracker","text":"There is a new question on the User Facilities & Program Areas page when you enter a publication into the Pub Tracker \u2013 \"The High Performance Computing Facility was used to produce results or data used in this publication.\" Please be sure to check Yes on this question for your work that made use of the HPC User Facility or other systems in the ESIF HPC Data Center. In addition, there are three new Program Areas to use to tag your publication under the Advanced Computing heading: Cloud, HPC and Visualization & Insight Center. Making use of these metadata will enable us to elevate your work through communications highlights, feature stories, and reporting to EERE. More information about the NREL Publishing Tracker can be found by visiting the Access and Use the NREL Publishing Tracker page on the Source.","title":"Elevate Your Work With New Tracking for Advanced Computing in the NREL Publishing Tracker"},{"location":"Announcements/2021-03-03-announcement/#fiscal-year-2021-quarterly-allocation-reductions","text":"You may have noticed that NREL did not make any reductions to allocations that were under-using their AUs during Q1. This is for two reasons. First, we were in the process of putting together a new, more transparent, fairer allocation reduction policy. Second, we are aware that many users were inconvenienced by the fact that allocation decisions were issued on October 1. We realize allocation reductions for low use are not popular with our users. However, they are physically necessary. AUs are an \"expiring resource.\" If an AU is not used in Q1, it cannot be stored and saved for use in Q4. Because of this, we have to remove some percentage of the unused AUs every quarter. Otherwise we can hit a situation where we have many more AUs available to users than the machine can physically provide as the year progresses. This creates long queue times that make Eagle physically unusable. Our new allocation policy is given below. We had an informal discussion with users across multiple centers before designing this policy. Users emphasized the need for a policy to be clear enough so they could see what they might lose at the end of the quarter. Users have also increasingly requested allocations that had different usages in different quarters to deal with their project needs, and we wanted a policy that treated these fairly. Shortly after Q1, Q2, and Q3 ends, allocations will be automatically adjusted to account for low utilization against planned usage. At the end of each quarter, the total allocation units used for the year to date will be compared to the total planned usage for the year to date for each allocation. At the end of Q1, the total used for the year to date is compared to the Q1 planned usage; at the end of Q2, the total used for the year to date is compared to the Q1+Q2 planned usage, and at the end of Q3, the toal used for the year to date is compared to the Q1+Q2+Q3 planned usage. Allocation units are then removed based on the table below. Note that allocation reductions are meant to be cumulative over the course of a year, and not compounding. If, for instance, a project was reduced by 10,000 AUs for low usage in Q1, and the reduction table suggests the project should be reduced by 25,000 AUs at the end of Q2, 15,000 (25,000-15,000) AUs should be removed at the end of Q2 to make the total removal for the year to date equal to 25,000 AUs. Percentage of planned AUs used to date Percentage of planned to date AUs removed More than 70% 0% (No reduction) Less than 70% but greater than 55% 20% Less than 55% but greater than 40% 35% Less than 40% but greater than 20% 55% Less than 20% 80% To understand how this process would work, we consider the following two 100,000 AU allocations, one with a uniform distribution of planned AUs throughout the year, and one with a distribution designed to enable development in Q1 and production runs in Q2 through Q4. The two allocations are described in the table below. Quarter Allocation \"Renewables\" Allocation \"Efficiency\" Q1 25,000 10,000 Q2 25,000 30,000 Q3 25,000 30,000 Q4 25,000 30,000 If \"Renewables\" uses 9,000 AUs in Q1, 20,000 AUs in Q2, and 25,000 AUs in Q3, it would be reduced in the following manner: After Q1, the project will have used 36% (9,000/25,000) if its allocation, leading to 13,750 (55% x 25,000) AUs being removed. After Q2, the project will have used 58% [(9,000+20,000)/(25,000+25,000)] of its allocation, leading to 10,000 AUs (20% x 50,000) being potentially removed. However, because 12,500 AUs were removed in Q1, no removal is performed. Note that AUs are not restored in this case. After Q3, the project will have used 72 % [(9,000+20,000+25,000)/(25,000+25,000+25,000)] of its allocation. No reduction would be made. The project would then begin Q4 with 33,500 AUs (100,000-9,000-20,000-25,000-12,500). If \"Efficiency\" uses AUs under the same schedule, after Q1, the project will have used 90% (9,000/10,000) of its allocation and will not be penalized. After Q2, the project will have used 72.5% [(9,000+20,000)/(10,000+30,000)] of its allocation and will not be penalized. After Q3, the project will have used 77% [(9,000+20,000+25,000)/(10,000+30,000+30,000)] of its allocation, and will not be penalized. The project would then begin Q4 with 46,000 AUs. Note that \"Renewables\" and \"Efficiency\" have the same total (100,000 AUs) but lose very different amounts of AUs over the course of the year. This is because the allocation request \"Efficiency\" is more closely tuned to the user\u2019s actual us of HPC resources. NREL allows users to tune their allocation request through the use of \"profiles\" in the allocation request to avoid this sorts of reductions. This information can also be found by visiting the Fiscal Year 2021 Quarterly Allocation Reductions page on our website.","title":"Fiscal Year 2021 Quarterly Allocation Reductions"},{"location":"Announcements/2021-04-06-announcement/","text":"Eagle File System Usage The Lustre file systems that hosts /projects and /scratch works most efficiently when it is under 80% full. Please do your part to keep the file system under 80% by cleaning up your /projects and /scratch spaces. Eagle System Time Eagle will have a system time for a work week starting May 3rd. There will be a power outage in the data center at this time, and we will need to do some work on Eagle as well. FY22 HPC Allocation Process The Eagle allocation process for FY22 will open up in May, with applications due in June (the exact dates are still to be decided.) The application process will be very similar to FY21. HPC Operations will again host a seminar to explain the application process. Watch this space for announcements. FY22 Cloud Allocation Process HPC and Cloud are both supported by Advanced Computing Operations (ACO) in the Computational Sciences Center. We are aligning the request process for both computing and cloud resources. The Cloud allocation process for FY22 will open up in May, with applications due in June (the exact dates are still to be decided.) The annual allocation process for cloud resources will be updated this year, and a new web interface will be provided for submitting your request. Cloud allocations have a different funding model from high performance computing: baseline services and administration of the cloud environment is funded by NREL, and project-specific computing and services is funded directly by the project. This means it\u2019s important to work with the cloud team to determine your needs in the cloud and to generate cost estimates for your project. Requests for cloud resources are reviewed and approved by the IACAC. Watch for the Call for Requests notification, and attend the upcoming Cloud Allocation Request webinar in May to learn more! Coming Soon: New Cloud User Website We are transitioning from the CSC Cloud Team Wiki to a website modeled after https://hpc.nrel.gov . The website will have a similar structure and look and feel that the user community is accustomed to.","title":"April 2021 Monthly Update"},{"location":"Announcements/2021-04-06-announcement/#eagle-file-system-usage","text":"The Lustre file systems that hosts /projects and /scratch works most efficiently when it is under 80% full. Please do your part to keep the file system under 80% by cleaning up your /projects and /scratch spaces.","title":"Eagle File System Usage"},{"location":"Announcements/2021-04-06-announcement/#eagle-system-time","text":"Eagle will have a system time for a work week starting May 3rd. There will be a power outage in the data center at this time, and we will need to do some work on Eagle as well.","title":"Eagle System Time"},{"location":"Announcements/2021-04-06-announcement/#fy22-hpc-allocation-process","text":"The Eagle allocation process for FY22 will open up in May, with applications due in June (the exact dates are still to be decided.) The application process will be very similar to FY21. HPC Operations will again host a seminar to explain the application process. Watch this space for announcements.","title":"FY22 HPC Allocation Process"},{"location":"Announcements/2021-04-06-announcement/#fy22-cloud-allocation-process","text":"HPC and Cloud are both supported by Advanced Computing Operations (ACO) in the Computational Sciences Center. We are aligning the request process for both computing and cloud resources. The Cloud allocation process for FY22 will open up in May, with applications due in June (the exact dates are still to be decided.) The annual allocation process for cloud resources will be updated this year, and a new web interface will be provided for submitting your request. Cloud allocations have a different funding model from high performance computing: baseline services and administration of the cloud environment is funded by NREL, and project-specific computing and services is funded directly by the project. This means it\u2019s important to work with the cloud team to determine your needs in the cloud and to generate cost estimates for your project. Requests for cloud resources are reviewed and approved by the IACAC. Watch for the Call for Requests notification, and attend the upcoming Cloud Allocation Request webinar in May to learn more!","title":"FY22 Cloud Allocation Process"},{"location":"Announcements/2021-04-06-announcement/#coming-soon-new-cloud-user-website","text":"We are transitioning from the CSC Cloud Team Wiki to a website modeled after https://hpc.nrel.gov . The website will have a similar structure and look and feel that the user community is accustomed to.","title":"Coming Soon: New Cloud User Website"},{"location":"Announcements/2021-05-05-announcement/","text":"Slurm Fairshare Refresher FY21 saw the introduction of the \"fairshare\" priority algorithm in Eagle's job scheduler, Slurm. Queue times have been high during the Q2-Q3 rush and we've received some questions, so here's a quick refresher on Fairshare and what it means in regards to job scheduling. The fairshare algorithm is a part of the Slurm \"multi-factor priority\" plugin that determines when a job should run. This algorithm is designed to help moderate queue usage by promoting jobs from under-utilized allocations, while over-utilized allocations get shifted towards CPU time that would otherwise be idle. The base fairshare value for an allocation is determined by the number of AUs allocated to a project, and is currently re-calculated on a quarterly basis. Every job that runs will affect the fairshare value, reducing the priority of future jobs. Larger jobs will have a larger impact, running smaller jobs will have less of an impact. The effects of any job on fairshare value will reduce by half every two weeks. And most importantly, fairshare only accounts for about half of job priority calculations--the rest relies on other factors, including the job's size, QOS setting, and partition. Queue Times The allocation year transitioned from Q2 to Q3 on April 1st. The job queue leading up to the end of Q2 saw a very large spike in jobs submitted, and queue depth (job wait time) rose accordingly. A few projects saw some effect of fairshare, but much of the pressure came from over a third of all jobs being submitted as qos=high. Because of the large surge in jobs submitted, interactions with fairshare and a few projects that have used up their allocation we have been analyzing the scheduling algorithms. Based on some recommendations from SchedMD and internal analysis we have made a few adjustments to the slurm configuration. Those changes thus far appear to have alleviated some of the pressure on the queues as well as a reduction in the number of jobs submitted with qos=high. Advanced Jupyter workshop (10am May 13th, 2021) Beyond the basics: this advanced Jupyter workshop will survey topics which enable you to get more out of your interactive notebooks. It will build on the recent Intro to Jupyter workshop and introduce additional Magic commands. Interacting with Slurm from a notebook will also be covered, and how this can be used to achieve multi-node parallelism. Additional topics include utilizing GPUs from a notebook, and parameterized notebook execution with Papermill.","title":"May 2021 Monthly Update"},{"location":"Announcements/2021-05-05-announcement/#slurm-fairshare-refresher","text":"FY21 saw the introduction of the \"fairshare\" priority algorithm in Eagle's job scheduler, Slurm. Queue times have been high during the Q2-Q3 rush and we've received some questions, so here's a quick refresher on Fairshare and what it means in regards to job scheduling. The fairshare algorithm is a part of the Slurm \"multi-factor priority\" plugin that determines when a job should run. This algorithm is designed to help moderate queue usage by promoting jobs from under-utilized allocations, while over-utilized allocations get shifted towards CPU time that would otherwise be idle. The base fairshare value for an allocation is determined by the number of AUs allocated to a project, and is currently re-calculated on a quarterly basis. Every job that runs will affect the fairshare value, reducing the priority of future jobs. Larger jobs will have a larger impact, running smaller jobs will have less of an impact. The effects of any job on fairshare value will reduce by half every two weeks. And most importantly, fairshare only accounts for about half of job priority calculations--the rest relies on other factors, including the job's size, QOS setting, and partition.","title":"Slurm Fairshare Refresher"},{"location":"Announcements/2021-05-05-announcement/#queue-times","text":"The allocation year transitioned from Q2 to Q3 on April 1st. The job queue leading up to the end of Q2 saw a very large spike in jobs submitted, and queue depth (job wait time) rose accordingly. A few projects saw some effect of fairshare, but much of the pressure came from over a third of all jobs being submitted as qos=high. Because of the large surge in jobs submitted, interactions with fairshare and a few projects that have used up their allocation we have been analyzing the scheduling algorithms. Based on some recommendations from SchedMD and internal analysis we have made a few adjustments to the slurm configuration. Those changes thus far appear to have alleviated some of the pressure on the queues as well as a reduction in the number of jobs submitted with qos=high.","title":"Queue Times"},{"location":"Announcements/2021-05-05-announcement/#advanced-jupyter-workshop-10am-may-13th-2021","text":"Beyond the basics: this advanced Jupyter workshop will survey topics which enable you to get more out of your interactive notebooks. It will build on the recent Intro to Jupyter workshop and introduce additional Magic commands. Interacting with Slurm from a notebook will also be covered, and how this can be used to achieve multi-node parallelism. Additional topics include utilizing GPUs from a notebook, and parameterized notebook execution with Papermill.","title":"Advanced Jupyter workshop (10am May 13th, 2021)"},{"location":"Documentation/Data-and-File-Systems/File-Systems/filesystemsio/","text":"File systems Eagle has three primary file systems available for compute nodes. Understanding the usage of these is important for achieving the best performance. NREL file systems Home file system Quota of 50 GB Used to hold scripts, source code, executables Lustre parallel file system : Accessiblle across all nodes. When using this file system please familiarize yourself with the best practices section /scratch/username /projects /shared-projects /datasets Node file system : The local drive on each node, these are accessible only on a given node. /tmp/scratch For more information on the file systems available on Eagle please see: Eagle System Configuration","title":"File Systems"},{"location":"Documentation/Data-and-File-Systems/File-Systems/filesystemsio/#file-systems","text":"Eagle has three primary file systems available for compute nodes. Understanding the usage of these is important for achieving the best performance.","title":"File systems"},{"location":"Documentation/Data-and-File-Systems/File-Systems/filesystemsio/#nrel-file-systems","text":"Home file system Quota of 50 GB Used to hold scripts, source code, executables Lustre parallel file system : Accessiblle across all nodes. When using this file system please familiarize yourself with the best practices section /scratch/username /projects /shared-projects /datasets Node file system : The local drive on each node, these are accessible only on a given node. /tmp/scratch For more information on the file systems available on Eagle please see: Eagle System Configuration","title":"NREL file systems"},{"location":"Documentation/Data-and-File-Systems/File-Systems/Lustre/lustrebestpractices/","text":"Lustre best practices In some cases special care must be taken while using Lustre so as not to affect the performance of the filesystem for yourself and other users. The below Do's and Don'ts are provided as guidance. Do Use the lfs find e.g. lfs find /scratch/username -type f -name \"*.py\" Break up directories with many files into more directories if possible Store small files and directories of small files on a single OST. Limit the number of processes accessing a file. It may be better to read in a file once and broadcast necessary information to other processes. Change your stripecount based on the filesize Write many files to the node filesystem /tmp/scratch/ this is not a Lustre filesystem. The files can then be added to a tar archive and transferred to the /project/project_name Don't Use ls -l Have a file accessed by multiple processes In Python avoid using os.walk or os.scandir List files instead of using wildcards e.g. don't use cp * dir/ If you need to tar/rm/cp a large number of files use xargs or similar. lfs find /scratch/username/old_data/ -t f -print0 | xargs -0 rm Have many small files in a single directory Run binary executables from the Lustre filesystem e.g. don't keep libraries or programs in /scratch/username Useful Lustre commands Check your storage usage: lfs quota -h -u <username> /scratch See which MDT a directory is located on lfs getstripe --mdt-index /scratch/<username> This will return an index 0-2 indicating the MDT Create a folder on a specific MDT (admin only) lfs mkdir \u2013i <mdt_index> /dir_path Striping Lustre provides a way to stripe files, this spreads them across multiple OSTs. Striping a large file being accessed by many processes can greatly improve the performace. See Lustre file striping for more details. lfs setstripe <file> -c <count> -s <size> * The stripecount determines how many OST the data is spread across * The stripe size is how large each of the stripes are in KB, MB, GB References Lustre manual CU Boulder - Lustre Do's and Don'ts NASA - Lustre Best Practices NASA - Lustre basics UMBC - Lustre Best Practices NICS - I/O and Lustre Usage NERSC - Lustre","title":"Lustre Best Practices"},{"location":"Documentation/Data-and-File-Systems/File-Systems/Lustre/lustrebestpractices/#lustre-best-practices","text":"In some cases special care must be taken while using Lustre so as not to affect the performance of the filesystem for yourself and other users. The below Do's and Don'ts are provided as guidance. Do Use the lfs find e.g. lfs find /scratch/username -type f -name \"*.py\" Break up directories with many files into more directories if possible Store small files and directories of small files on a single OST. Limit the number of processes accessing a file. It may be better to read in a file once and broadcast necessary information to other processes. Change your stripecount based on the filesize Write many files to the node filesystem /tmp/scratch/ this is not a Lustre filesystem. The files can then be added to a tar archive and transferred to the /project/project_name Don't Use ls -l Have a file accessed by multiple processes In Python avoid using os.walk or os.scandir List files instead of using wildcards e.g. don't use cp * dir/ If you need to tar/rm/cp a large number of files use xargs or similar. lfs find /scratch/username/old_data/ -t f -print0 | xargs -0 rm Have many small files in a single directory Run binary executables from the Lustre filesystem e.g. don't keep libraries or programs in /scratch/username","title":"Lustre best practices"},{"location":"Documentation/Data-and-File-Systems/File-Systems/Lustre/lustrebestpractices/#useful-lustre-commands","text":"Check your storage usage: lfs quota -h -u <username> /scratch See which MDT a directory is located on lfs getstripe --mdt-index /scratch/<username> This will return an index 0-2 indicating the MDT Create a folder on a specific MDT (admin only) lfs mkdir \u2013i <mdt_index> /dir_path","title":"Useful Lustre commands"},{"location":"Documentation/Data-and-File-Systems/File-Systems/Lustre/lustrebestpractices/#striping","text":"Lustre provides a way to stripe files, this spreads them across multiple OSTs. Striping a large file being accessed by many processes can greatly improve the performace. See Lustre file striping for more details. lfs setstripe <file> -c <count> -s <size> * The stripecount determines how many OST the data is spread across * The stripe size is how large each of the stripes are in KB, MB, GB","title":"Striping"},{"location":"Documentation/Data-and-File-Systems/File-Systems/Lustre/lustrebestpractices/#references","text":"Lustre manual CU Boulder - Lustre Do's and Don'ts NASA - Lustre Best Practices NASA - Lustre basics UMBC - Lustre Best Practices NICS - I/O and Lustre Usage NERSC - Lustre","title":"References"},{"location":"Documentation/Data-and-File-Systems/Transferring-Data/FileZilla/","text":"Transferring files using FileZilla FileZilla can be used to securely transfer files between your local computer running Windows, Linux or MacOS to a remote computer running Linux. Setting Up FileZilla Download and install FileZilla . Connecting to a Host Decide which host you wish to connect to such as, eagle.hpc.nrel.gov Enter your username in the Username field. Enter your password or Password+OTP Token in the Password field. Use 22 as the Port. Click the 'Quickconnect' button. Transferring Files You may use FileZilla to transfer individual files or directories from the Local Directory to the Remote Directory or vice versa. Transfer files by dragging them from the Local Directory (left pane) to the Remote Directory (right pane) or vice versa. Once the transfer is complete the selected file will be visible in the pane it was transferred to.","title":"FileZilla"},{"location":"Documentation/Data-and-File-Systems/Transferring-Data/FileZilla/#transferring-files-using-filezilla","text":"FileZilla can be used to securely transfer files between your local computer running Windows, Linux or MacOS to a remote computer running Linux.","title":"Transferring files using FileZilla"},{"location":"Documentation/Data-and-File-Systems/Transferring-Data/FileZilla/#setting-up-filezilla","text":"Download and install FileZilla .","title":"Setting Up FileZilla"},{"location":"Documentation/Data-and-File-Systems/Transferring-Data/FileZilla/#connecting-to-a-host","text":"Decide which host you wish to connect to such as, eagle.hpc.nrel.gov Enter your username in the Username field. Enter your password or Password+OTP Token in the Password field. Use 22 as the Port. Click the 'Quickconnect' button.","title":"Connecting to a Host"},{"location":"Documentation/Data-and-File-Systems/Transferring-Data/FileZilla/#transferring-files","text":"You may use FileZilla to transfer individual files or directories from the Local Directory to the Remote Directory or vice versa. Transfer files by dragging them from the Local Directory (left pane) to the Remote Directory (right pane) or vice versa. Once the transfer is complete the selected file will be visible in the pane it was transferred to.","title":"Transferring Files"},{"location":"Documentation/Data-and-File-Systems/Transferring-Data/file-transfers/","text":"Transferring files Learn how to transfer data within, to and from NREL's high-performance computing (HPC) systems. A supported set of instructions for data transfer using NREL HPC systems is provided on the HPC NREL Website . Checking Usage and Quota The below command is used to check your quota from an Eagle login node. hours_report will display your usage and quota for each filesystem. $ hours_report Best Practices for Transferring Files File Transfers Between Filesystems on the NREL network rsync is the recommended tool for transferring data between NREL systems. It allows you to easily restart transfers if they fail, and also provides more consistency when dealing with symbolic links, hard links, and sparse files than either scp or cp. It is recommended you do not use compression for transfers within NREL systems. An example command is: $ rsync -aP --no-g /scratch/username/dataset1/ /mss/users/username/dataset1/ Mass Storage has quotas that limit the number of individual files you can store. If you are copying hundreds of thousands of files then it is best to archive these files prior to copying to Mass Storage. See the guide on how to archive files . Mass Storage quotas rely on the group of the file and not the directory path. It is best to use the --no-g option when rsyncing to MSS so you use the destination group rather than the group permissions of your source. You can also chgrp your files to the appropriate group prior to rsyncing to MSS. Small Transfers (<100GB) outside of the NREL network rsync , scp , and curl will be your best option for small transfers (<100GB) outside of the NREL network. If your rsync/scp/curl transfers are taking hours to complete then you should consider using Globus . If you're transferring many files then you should use rsync: $ rsync -azP --no-g /mss/users/username/dataset1/ user@desthost:/home/username/dataset1/ If you're transferring an individual file then use scp: $ scp /home/username/example.tar.gz user@desthost:/home/username/ You can use curl or wget to download individual files: $ curl -O https://URL $ wget https://URL Large Transfers (>100GB) outside of the NREL network Globus is optimized for file transfers between data centers and anything outside of the NREL network. It will be several times faster than any other tools you will have available. Documentation about requesting a HPC Globus account is available on the Globus Services page on the HPC website . See Transfering files using Globus for instructions on transfering files with Globus. Transfering files using Windows For Windows you will need to download WinSCP to transfer files to and from HPC systems over SCP. See Transfering using WinSCP . Archiving files and directories Learn various techniques to combine and compress multiple files or directories into a single file to reduce storage footprint or simplify sharing. tar tar , along with zip , is one of the basic commands to combine multiple individual files into a single file (called a \"tarball\"). tar requires at least one command line option. A typical usage would be: $ tar -cf newArchiveName.tar file1 file2 file3 # or $ tar -cf newArchiveName.tar /path/to/folder/ The -c flag denotes c reating an archive, and -f denotes that the next argument given will be the archive name\u2014in this case it means the name you would prefer for the resulting archive file. To extract files from a tar, it's recommended to use: $ tar -xvf existingArchiveName.tar -x is for ex tracting, -v uses v erbose mode which will print the name of each file as it is extracted from the archive. Compressing tar can also generate compressed tarballs which reduce the size of the resulting archive. This can be done with the -z flag (which just calls gzip on the resulting archive automatically, resulting in a .tar.gz extension) or -j (which uses bzip2 , creating a .tar.bz2 ). For example: # gzip $ tar -czvf newArchive.tar.gz file1 file2 file3 $ tar -xvzf newArchive.tar.gz # bzip2 $ tar -czjf newArchive.tar.bz2 file1 file2 file3 $ tar -xvjf newArchive.tar.bz2","title":"File Transfers"},{"location":"Documentation/Data-and-File-Systems/Transferring-Data/file-transfers/#transferring-files","text":"Learn how to transfer data within, to and from NREL's high-performance computing (HPC) systems. A supported set of instructions for data transfer using NREL HPC systems is provided on the HPC NREL Website .","title":"Transferring files"},{"location":"Documentation/Data-and-File-Systems/Transferring-Data/file-transfers/#checking-usage-and-quota","text":"The below command is used to check your quota from an Eagle login node. hours_report will display your usage and quota for each filesystem. $ hours_report","title":"Checking Usage and Quota"},{"location":"Documentation/Data-and-File-Systems/Transferring-Data/file-transfers/#best-practices-for-transferring-files","text":"","title":"Best Practices for Transferring Files"},{"location":"Documentation/Data-and-File-Systems/Transferring-Data/file-transfers/#file-transfers-between-filesystems-on-the-nrel-network","text":"rsync is the recommended tool for transferring data between NREL systems. It allows you to easily restart transfers if they fail, and also provides more consistency when dealing with symbolic links, hard links, and sparse files than either scp or cp. It is recommended you do not use compression for transfers within NREL systems. An example command is: $ rsync -aP --no-g /scratch/username/dataset1/ /mss/users/username/dataset1/ Mass Storage has quotas that limit the number of individual files you can store. If you are copying hundreds of thousands of files then it is best to archive these files prior to copying to Mass Storage. See the guide on how to archive files . Mass Storage quotas rely on the group of the file and not the directory path. It is best to use the --no-g option when rsyncing to MSS so you use the destination group rather than the group permissions of your source. You can also chgrp your files to the appropriate group prior to rsyncing to MSS.","title":"File Transfers Between Filesystems on the NREL network"},{"location":"Documentation/Data-and-File-Systems/Transferring-Data/file-transfers/#small-transfers-100gb-outside-of-the-nrel-network","text":"rsync , scp , and curl will be your best option for small transfers (<100GB) outside of the NREL network. If your rsync/scp/curl transfers are taking hours to complete then you should consider using Globus . If you're transferring many files then you should use rsync: $ rsync -azP --no-g /mss/users/username/dataset1/ user@desthost:/home/username/dataset1/ If you're transferring an individual file then use scp: $ scp /home/username/example.tar.gz user@desthost:/home/username/ You can use curl or wget to download individual files: $ curl -O https://URL $ wget https://URL","title":"Small Transfers (&lt;100GB) outside of the NREL network"},{"location":"Documentation/Data-and-File-Systems/Transferring-Data/file-transfers/#large-transfers-100gb-outside-of-the-nrel-network","text":"Globus is optimized for file transfers between data centers and anything outside of the NREL network. It will be several times faster than any other tools you will have available. Documentation about requesting a HPC Globus account is available on the Globus Services page on the HPC website . See Transfering files using Globus for instructions on transfering files with Globus.","title":"Large Transfers (&gt;100GB) outside of the NREL network"},{"location":"Documentation/Data-and-File-Systems/Transferring-Data/file-transfers/#transfering-files-using-windows","text":"For Windows you will need to download WinSCP to transfer files to and from HPC systems over SCP. See Transfering using WinSCP .","title":"Transfering files using Windows"},{"location":"Documentation/Data-and-File-Systems/Transferring-Data/file-transfers/#archiving-files-and-directories","text":"Learn various techniques to combine and compress multiple files or directories into a single file to reduce storage footprint or simplify sharing.","title":"Archiving files and directories"},{"location":"Documentation/Data-and-File-Systems/Transferring-Data/file-transfers/#tar","text":"tar , along with zip , is one of the basic commands to combine multiple individual files into a single file (called a \"tarball\"). tar requires at least one command line option. A typical usage would be: $ tar -cf newArchiveName.tar file1 file2 file3 # or $ tar -cf newArchiveName.tar /path/to/folder/ The -c flag denotes c reating an archive, and -f denotes that the next argument given will be the archive name\u2014in this case it means the name you would prefer for the resulting archive file. To extract files from a tar, it's recommended to use: $ tar -xvf existingArchiveName.tar -x is for ex tracting, -v uses v erbose mode which will print the name of each file as it is extracted from the archive.","title":"tar"},{"location":"Documentation/Data-and-File-Systems/Transferring-Data/file-transfers/#compressing","text":"tar can also generate compressed tarballs which reduce the size of the resulting archive. This can be done with the -z flag (which just calls gzip on the resulting archive automatically, resulting in a .tar.gz extension) or -j (which uses bzip2 , creating a .tar.bz2 ). For example: # gzip $ tar -czvf newArchive.tar.gz file1 file2 file3 $ tar -xvzf newArchive.tar.gz # bzip2 $ tar -czjf newArchive.tar.bz2 file1 file2 file3 $ tar -xvjf newArchive.tar.bz2","title":"Compressing"},{"location":"Documentation/Data-and-File-Systems/Transferring-Data/globus/","text":"Transfering Files with Globus For large data transfers between NREL\u2019s high-performance computing (HPC) systems and another data center, or even a laptop off-site, we recommend using Globus. A supported set of instructions for requesting a HPC Globus account and data transfer using Globus is available on the HPC NREL Website What Is Globus? Globus provides services for research data management, including file transfer. It enables you to quickly, securely and reliably move your data to and from locations you have access to . Globus transfers files using GridFTP . GridFTP is a high-performance data transfer protocol which is optimized for high-bandwidth wide-area networks. It provides more reliable high performance file transfer and synchronization than scp or rsync. It automatically tunes parameters to maximize bandwidth while providing automatic fault recovery and notification of completion or problems. Globus Personal endpoints You can set up a \"Globus Connect Personal EndPoint\", which turns your personal computer into an endpoint, by downloading and installing the Globus Connect Personal application on your system. We use a personal endpoint to demonstrate how to transfer files to and from Eagle. Set Up a Personal EndPoint Login to the Globus website. From the Manage Data drop down menu, select Transfer Files. Then click Get Globus Connect Personal. Pick a name for your personal endpoint and select Generate Startup Key. Follow the instructions on the web page to save your key. Download and install the Globus Connect Personal software on your personal system. Copy the startup key from the Globus web page to this application. Configure Permissions on a Personal EndPoint Once Globus Connect Personal is installed on your system, set up the permissions for reading or writing files to your local system. If you are using a Mac, click on the \"g\" icon on the upper right portion of your screen to access the Globus Connect Personal application. Select Preferences. To allow Globus to copy files to your local system, make sure that the directory (folder) they will go in is Writable. Transferring Files You can transfer files with Globus through the Globus Online website or via a CLI (command line interface). Globus Online Globus Online is a hosted service that allows you to use a browser to transfer files between trusted sites called \"endpoints\". To use it, the Globus software must be installed on the systems at both ends of the data transfer. The NREL endpoint is nrel#globus. Click Login on the Globus web site . On the login page select \"Globus ID\" as the login method and click continue. Use the Globus credentials you used to register your Globus.org account. Go to the Transfer Files page, the link is located under the Manage Data tab at the top of the page. Select nrel#globus as the endpoint on one right side. In the box asking for authentication, enter your Eagle (NREL HPC) username and password . Do not use your globus.org username and password when authenticating with the nrel#globus endpoint. Select another Globus endpoint, such as a personal endpoint or an endpoint at another institution that you have access to. To use your personal endpoint, first start the Globus Connect Personal application. Then enter \"USERNAME#ENDPOINT\" on the left side or use the drop down menu to find it. Click \"go\". To transfer files Select the files you want to transfer someplace else from the system from the dialog box on the left. Select the destination location (a folder or directory) from the dialog box on right right. Click the large blue button at the top of the screen to begin to transfer the files. When your transfer is complete, you will be notified by email. Globus CLI (Command line Interface) Configuring your Globus.org account to allow ssh CLI access To use the CLI you must have a Globus account with ssh access enabled. To enable your account for ssh access you must add your ssh public key to your Globus account by visiting the Manage Identities page and clicking \"manage SSH and X.509 keys\" and then \"Add a New Key\". If you do not have an ssh key, follow the directions here to create one. Globus.org CLI examples $ ssh <globus username>@cli.globusonline.org <command> <options> <params> $ ssh <globus username>@cli.globusonline.org help A one-liner can be used to integrate globus.org CLI commands into shell scripts $ ssh <globus username>@cli.globusonline.org scp nrel#globus:/globusro/file1.txt myuser#laptop:/tmp/myfile.txt The globus.org CLI can be used interactively $ ssh <globus username>@cli.globusonline.org Welcome to globusonline.org, <globus username>. Type 'help' for help. $ help $ scp nrel#globus:/globusro/file1.txt myuser#laptop:/tmp/myfile.txt $ exit You can find more information on the Globus CLI from the official Globus CLI documentation .","title":"Globus"},{"location":"Documentation/Data-and-File-Systems/Transferring-Data/globus/#transfering-files-with-globus","text":"For large data transfers between NREL\u2019s high-performance computing (HPC) systems and another data center, or even a laptop off-site, we recommend using Globus. A supported set of instructions for requesting a HPC Globus account and data transfer using Globus is available on the HPC NREL Website","title":"Transfering Files with Globus"},{"location":"Documentation/Data-and-File-Systems/Transferring-Data/globus/#what-is-globus","text":"Globus provides services for research data management, including file transfer. It enables you to quickly, securely and reliably move your data to and from locations you have access to . Globus transfers files using GridFTP . GridFTP is a high-performance data transfer protocol which is optimized for high-bandwidth wide-area networks. It provides more reliable high performance file transfer and synchronization than scp or rsync. It automatically tunes parameters to maximize bandwidth while providing automatic fault recovery and notification of completion or problems.","title":"What Is Globus?"},{"location":"Documentation/Data-and-File-Systems/Transferring-Data/globus/#globus-personal-endpoints","text":"You can set up a \"Globus Connect Personal EndPoint\", which turns your personal computer into an endpoint, by downloading and installing the Globus Connect Personal application on your system. We use a personal endpoint to demonstrate how to transfer files to and from Eagle.","title":"Globus Personal endpoints"},{"location":"Documentation/Data-and-File-Systems/Transferring-Data/globus/#set-up-a-personal-endpoint","text":"Login to the Globus website. From the Manage Data drop down menu, select Transfer Files. Then click Get Globus Connect Personal. Pick a name for your personal endpoint and select Generate Startup Key. Follow the instructions on the web page to save your key. Download and install the Globus Connect Personal software on your personal system. Copy the startup key from the Globus web page to this application.","title":"Set Up a Personal EndPoint"},{"location":"Documentation/Data-and-File-Systems/Transferring-Data/globus/#configure-permissions-on-a-personal-endpoint","text":"Once Globus Connect Personal is installed on your system, set up the permissions for reading or writing files to your local system. If you are using a Mac, click on the \"g\" icon on the upper right portion of your screen to access the Globus Connect Personal application. Select Preferences. To allow Globus to copy files to your local system, make sure that the directory (folder) they will go in is Writable.","title":"Configure Permissions on a Personal EndPoint"},{"location":"Documentation/Data-and-File-Systems/Transferring-Data/globus/#transferring-files","text":"You can transfer files with Globus through the Globus Online website or via a CLI (command line interface).","title":"Transferring Files"},{"location":"Documentation/Data-and-File-Systems/Transferring-Data/globus/#globus-online","text":"Globus Online is a hosted service that allows you to use a browser to transfer files between trusted sites called \"endpoints\". To use it, the Globus software must be installed on the systems at both ends of the data transfer. The NREL endpoint is nrel#globus. Click Login on the Globus web site . On the login page select \"Globus ID\" as the login method and click continue. Use the Globus credentials you used to register your Globus.org account. Go to the Transfer Files page, the link is located under the Manage Data tab at the top of the page. Select nrel#globus as the endpoint on one right side. In the box asking for authentication, enter your Eagle (NREL HPC) username and password . Do not use your globus.org username and password when authenticating with the nrel#globus endpoint. Select another Globus endpoint, such as a personal endpoint or an endpoint at another institution that you have access to. To use your personal endpoint, first start the Globus Connect Personal application. Then enter \"USERNAME#ENDPOINT\" on the left side or use the drop down menu to find it. Click \"go\". To transfer files Select the files you want to transfer someplace else from the system from the dialog box on the left. Select the destination location (a folder or directory) from the dialog box on right right. Click the large blue button at the top of the screen to begin to transfer the files. When your transfer is complete, you will be notified by email.","title":"Globus Online"},{"location":"Documentation/Data-and-File-Systems/Transferring-Data/globus/#globus-cli-command-line-interface","text":"Configuring your Globus.org account to allow ssh CLI access To use the CLI you must have a Globus account with ssh access enabled. To enable your account for ssh access you must add your ssh public key to your Globus account by visiting the Manage Identities page and clicking \"manage SSH and X.509 keys\" and then \"Add a New Key\". If you do not have an ssh key, follow the directions here to create one. Globus.org CLI examples $ ssh <globus username>@cli.globusonline.org <command> <options> <params> $ ssh <globus username>@cli.globusonline.org help A one-liner can be used to integrate globus.org CLI commands into shell scripts $ ssh <globus username>@cli.globusonline.org scp nrel#globus:/globusro/file1.txt myuser#laptop:/tmp/myfile.txt The globus.org CLI can be used interactively $ ssh <globus username>@cli.globusonline.org Welcome to globusonline.org, <globus username>. Type 'help' for help. $ help $ scp nrel#globus:/globusro/file1.txt myuser#laptop:/tmp/myfile.txt $ exit You can find more information on the Globus CLI from the official Globus CLI documentation .","title":"Globus CLI (Command line Interface)"},{"location":"Documentation/Environments/conda/","text":"Why Conda? Conda is a package manager which allows you to easily create and switch betwen different software environments in different languages for different purposes. With Conda, it's easy to: Manage different (potentially conflicting) versions of the same software without complication Quickly stand up even complicated dependencies for stacks of software Share your specific programming environment with others for reproducible results Creating Environments by Name To create a basic Conda environment, we'll start by running conda create --name mypy python where the --name option (or the shortened -n ) means the environment will be specified by name and myenv will be the name of the created environment. Any arguments following the environment name are the packages to be installed. To specify a specific version of a package, simply add the version number after the \"=\" sign conda create --name mypy37 python=3.7 You can specify multiple packages for installation during environment creation conda create --name mynumpy python=3.7 numpy Conda ensures dependencies are satisfied when installing packages, so the version of the numpy package installed will be consistent with Python 3.7 (and any other packages specified). Tip: It\u2019s recommended to install all the packages you want to include in an environment at the same time to help avoid dependency conflicts. Environment Navigation To see a list of all existing environments (useful to confirm the successful creation of a new environment): conda env list To activate your new environment: conda activate mypy Your usual command prompt should now be prefixed with (mypy) , which helps keep track of which environment is currently activated. To see which packages are installed from within a currently active environment : conda list When finished with this programming session, deactivate your environment with: conda deactivate Creating Environments by Location Creating environments by location is especially helpful when working on the Eagle HPC, as the default location is your /home/<username>/ directory, which is limited to 50 GB. To create a Conda environment somewhere besides the default location, use the --prefix flag (or the shortened -p ) instead of --name when creating. conda create --prefix /path/to/mypy python=3.7 numpy This re-creates the python+numpy environment from earlier, but with all downloaded packages stored in the specified location. Warning: Keep in mind that scratch on Eagle is temporary in that files are purged after 28 days of inactivity. Unfortunately, placing environments outside of the default /env folder means that it needs to be activated with the full path ( conda activate /path/to/mypy ) and will show the full path rather than the environment name at the command prompt. To fix the cumbersome command prompt, simply modify the env_prompt setting in your .condarc file: conda config --set env_prompt '({name}) ' Note that '({name})' is not a placeholder for your desired environment name but text to be copied literally. This will edit your .condarc file if you already have one or create a .condarc file if you do not. For more on modifying your .condarc file, check out the User Guide . Once you've completed this step, the command prompt will show the shortened name (mypy, in the previous example). Managing Conda Environments Over time, it may become necessary to add additional packages to your environments. New packages can be installed in the currently active environment with: conda install pandas Conda will ensure that all dependencies are satisfied which may include upgrades to existing packages in this repository. To install packages from other sources, specify the channel option: conda install --channel conda-forge fenics To add a pip-installable package to your environment: conda install pip pip <pip_subcommand> A note on mixing Conda and Pip: Issues may arise when using pip and conda together. When combining conda and pip, it is best to use an isolated conda environment. Only after conda has been used to install as many packages as possible should pip be used to install any remaining software . If modifications are needed to the environment, it is best to create a new environment rather than running conda after pip. When appropriate, conda and pip requirements should be stored in text files. For more information on this point, check out the User Guide We can use conda list to see which packages are currently installed, but for a more version-control-flavored approach: conda list --revisions which shows changes to the environment over time. To revert back to a previous environemnt conda install --revision 1 To remove packages from the currently activated environment: conda remove pkg1 To completely remove an environment and all installed packages: conda remove --name mypy --all Conda environments can become large quickly due to the liberal creation of cached files. To remove these files and free up space you can use conda clean --all or to simply preview the potential changes before doing any actual deletion conda clean --all --dry-run Sharing Conda Environments To create a file with the the exact \"recipe\" used to create the current environment: conda env export > environment.yaml In practice, this recipe may be overly-specific to the point of creating problems on different hardware. To save an abbreviated version of the recipe with only the packages you explicitly requested : conda env export --from-history > environment.yaml To create a new environment with the recipe specified in the .yaml file: conda env create --name mypyeagle --file environment.yaml If a name or prefix isn't specified, the environment will be given the same name as the original environment the recipe was exported from (which may be desirable if you're moving to a different computer). Eagle Considerations Interacting with your Conda environments on Eagle should feel exactly the same as working on your desktop. An example desktop-to-HPC workflow might go: Create the environment locally Verify that environment works on a minimal working example Export local environment file and copy to Eagle Duplicate local environment on Eagle Execute production-level runs on Eagle #!/bin/bash #SBATCH --ntasks=4 #SBATCH --nodes=1 #SBATCH --time=5 #SBATCH --account=<project_handle> module purge module load conda conda activate mypy srun -n 8 python my_main.py Cheat Sheet of Common Commands Task ... outside environment ... inside environment Create by name conda create -n mypy pkg1 pkg2 N/A Create by path conda create -p path/to/mypy pkg1 pkg2 N/A Create by file conda env create -f environment.yml N/A Show environments conda env list N/A Activate conda activate mypy N/A Deactivate N/A conda deactivate Install New Package conda install -n mypy pkg1 pkg2 conda install pkg1 pkg2 List All Packages conda list -n mypy conda list Revision Listing conda list --revisions -n mypy conda list --revisions Export Environment conda env export -n mypy > environment.yaml conda env export > environment.yaml Remove Package conda remove -n mypy pkg1 pkg2 conda remove pkg1 pkg2","title":"Conda"},{"location":"Documentation/Environments/conda/#why-conda","text":"Conda is a package manager which allows you to easily create and switch betwen different software environments in different languages for different purposes. With Conda, it's easy to: Manage different (potentially conflicting) versions of the same software without complication Quickly stand up even complicated dependencies for stacks of software Share your specific programming environment with others for reproducible results","title":"Why Conda?"},{"location":"Documentation/Environments/conda/#creating-environments-by-name","text":"To create a basic Conda environment, we'll start by running conda create --name mypy python where the --name option (or the shortened -n ) means the environment will be specified by name and myenv will be the name of the created environment. Any arguments following the environment name are the packages to be installed. To specify a specific version of a package, simply add the version number after the \"=\" sign conda create --name mypy37 python=3.7 You can specify multiple packages for installation during environment creation conda create --name mynumpy python=3.7 numpy Conda ensures dependencies are satisfied when installing packages, so the version of the numpy package installed will be consistent with Python 3.7 (and any other packages specified). Tip: It\u2019s recommended to install all the packages you want to include in an environment at the same time to help avoid dependency conflicts.","title":"Creating Environments by Name"},{"location":"Documentation/Environments/conda/#environment-navigation","text":"To see a list of all existing environments (useful to confirm the successful creation of a new environment): conda env list To activate your new environment: conda activate mypy Your usual command prompt should now be prefixed with (mypy) , which helps keep track of which environment is currently activated. To see which packages are installed from within a currently active environment : conda list When finished with this programming session, deactivate your environment with: conda deactivate","title":"Environment Navigation"},{"location":"Documentation/Environments/conda/#creating-environments-by-location","text":"Creating environments by location is especially helpful when working on the Eagle HPC, as the default location is your /home/<username>/ directory, which is limited to 50 GB. To create a Conda environment somewhere besides the default location, use the --prefix flag (or the shortened -p ) instead of --name when creating. conda create --prefix /path/to/mypy python=3.7 numpy This re-creates the python+numpy environment from earlier, but with all downloaded packages stored in the specified location. Warning: Keep in mind that scratch on Eagle is temporary in that files are purged after 28 days of inactivity. Unfortunately, placing environments outside of the default /env folder means that it needs to be activated with the full path ( conda activate /path/to/mypy ) and will show the full path rather than the environment name at the command prompt. To fix the cumbersome command prompt, simply modify the env_prompt setting in your .condarc file: conda config --set env_prompt '({name}) ' Note that '({name})' is not a placeholder for your desired environment name but text to be copied literally. This will edit your .condarc file if you already have one or create a .condarc file if you do not. For more on modifying your .condarc file, check out the User Guide . Once you've completed this step, the command prompt will show the shortened name (mypy, in the previous example).","title":"Creating Environments by Location"},{"location":"Documentation/Environments/conda/#managing-conda-environments","text":"Over time, it may become necessary to add additional packages to your environments. New packages can be installed in the currently active environment with: conda install pandas Conda will ensure that all dependencies are satisfied which may include upgrades to existing packages in this repository. To install packages from other sources, specify the channel option: conda install --channel conda-forge fenics To add a pip-installable package to your environment: conda install pip pip <pip_subcommand> A note on mixing Conda and Pip: Issues may arise when using pip and conda together. When combining conda and pip, it is best to use an isolated conda environment. Only after conda has been used to install as many packages as possible should pip be used to install any remaining software . If modifications are needed to the environment, it is best to create a new environment rather than running conda after pip. When appropriate, conda and pip requirements should be stored in text files. For more information on this point, check out the User Guide We can use conda list to see which packages are currently installed, but for a more version-control-flavored approach: conda list --revisions which shows changes to the environment over time. To revert back to a previous environemnt conda install --revision 1 To remove packages from the currently activated environment: conda remove pkg1 To completely remove an environment and all installed packages: conda remove --name mypy --all Conda environments can become large quickly due to the liberal creation of cached files. To remove these files and free up space you can use conda clean --all or to simply preview the potential changes before doing any actual deletion conda clean --all --dry-run","title":"Managing Conda Environments"},{"location":"Documentation/Environments/conda/#sharing-conda-environments","text":"To create a file with the the exact \"recipe\" used to create the current environment: conda env export > environment.yaml In practice, this recipe may be overly-specific to the point of creating problems on different hardware. To save an abbreviated version of the recipe with only the packages you explicitly requested : conda env export --from-history > environment.yaml To create a new environment with the recipe specified in the .yaml file: conda env create --name mypyeagle --file environment.yaml If a name or prefix isn't specified, the environment will be given the same name as the original environment the recipe was exported from (which may be desirable if you're moving to a different computer).","title":"Sharing Conda Environments"},{"location":"Documentation/Environments/conda/#eagle-considerations","text":"Interacting with your Conda environments on Eagle should feel exactly the same as working on your desktop. An example desktop-to-HPC workflow might go: Create the environment locally Verify that environment works on a minimal working example Export local environment file and copy to Eagle Duplicate local environment on Eagle Execute production-level runs on Eagle #!/bin/bash #SBATCH --ntasks=4 #SBATCH --nodes=1 #SBATCH --time=5 #SBATCH --account=<project_handle> module purge module load conda conda activate mypy srun -n 8 python my_main.py","title":"Eagle Considerations"},{"location":"Documentation/Environments/conda/#cheat-sheet-of-common-commands","text":"Task ... outside environment ... inside environment Create by name conda create -n mypy pkg1 pkg2 N/A Create by path conda create -p path/to/mypy pkg1 pkg2 N/A Create by file conda env create -f environment.yml N/A Show environments conda env list N/A Activate conda activate mypy N/A Deactivate N/A conda deactivate Install New Package conda install -n mypy pkg1 pkg2 conda install pkg1 pkg2 List All Packages conda list -n mypy conda list Revision Listing conda list --revisions -n mypy conda list --revisions Export Environment conda env export -n mypy > environment.yaml conda env export > environment.yaml Remove Package conda remove -n mypy pkg1 pkg2 conda remove pkg1 pkg2","title":"Cheat Sheet of Common Commands"},{"location":"Documentation/Environments/Containers/intro/","text":"Introduction to containers What are containers? Containers provide a method of packaging your code so that it can be run anywhere you have a container runtime. This enables you to create a container on your local laptop and then run it on Eagle or other computing resources. Containers provide an alternative way of isolating and packaging your code from solutions such as Conda environments. Docker vs. Singularity The most common container runtime outside of HPC is Docker. Docker is not suited for the HPC environment on Eagle and is therefore not available on the system currently. Singularity is an alternative container tool which is provided. Compatibility Singularity is able to run most Docker images, but Docker is unable to run Singularity images. A key consideration when deciding to containerize an application is which container engine to build with. A suggested best practice is to build images with Docker when possible, as this provides more flexibility. Sometimes this is not possible though, and you may have to build with Singularity or maintain separate builds for each container engine. Container advantages Portability : containers can be run on HPC, locally, and on cloud infrastructure used at NREL. Reproducibility : Containers are one option to ensure reproducible research by packaging all necessary software to reproduce an analysis. Containers are also easily versioned using a hash. Workflow integration : Workflow management systems such as Airflow, Nextflow, Luigi, and others provide built in integration with container engines. HPC hardware Both Singularity and Docker provide the ability to use hardware based features of Eagle. A common usage for containers is packaging of GPU enabled tools such as TensorFlow. Singularity enables access to the GPU and driver on the host. Likewise the MPI installations available on Eagle can be accessed from correctly configured containers. Building Containers are built from a container specification file, Dockerfiles for Docker or Singularity Definition File in Singularity. These files specify the steps necessary to create the desired package and the additional software packages to install and configure in this environment. FROM ubuntu:20.04 RUN apt-get -y update && apt-get install -y python3 The above Dockerfile illustrates the build steps to create a simple image. Images are normally built from a base image indicated by FROM , in this case Ubuntu. The ability to use a different base image provides a way to use packages which may work more easily on one Linux Distribution. For example the Linux distribution on Eagle is CentOS, building the above image would allow the user to install packages from Ubuntu repositories. The RUN portion of the above Dockerfile indicates the command to run, in this example it installs the Python 3 package. Additional commands such as COPY , ENV , and others enable the customization of your image to suit your compute environment requirements.","title":"Containers Intro"},{"location":"Documentation/Environments/Containers/intro/#introduction-to-containers","text":"","title":"Introduction to containers"},{"location":"Documentation/Environments/Containers/intro/#what-are-containers","text":"Containers provide a method of packaging your code so that it can be run anywhere you have a container runtime. This enables you to create a container on your local laptop and then run it on Eagle or other computing resources. Containers provide an alternative way of isolating and packaging your code from solutions such as Conda environments.","title":"What are containers?"},{"location":"Documentation/Environments/Containers/intro/#docker-vs-singularity","text":"The most common container runtime outside of HPC is Docker. Docker is not suited for the HPC environment on Eagle and is therefore not available on the system currently. Singularity is an alternative container tool which is provided.","title":"Docker vs. Singularity"},{"location":"Documentation/Environments/Containers/intro/#compatibility","text":"Singularity is able to run most Docker images, but Docker is unable to run Singularity images. A key consideration when deciding to containerize an application is which container engine to build with. A suggested best practice is to build images with Docker when possible, as this provides more flexibility. Sometimes this is not possible though, and you may have to build with Singularity or maintain separate builds for each container engine.","title":"Compatibility"},{"location":"Documentation/Environments/Containers/intro/#container-advantages","text":"Portability : containers can be run on HPC, locally, and on cloud infrastructure used at NREL. Reproducibility : Containers are one option to ensure reproducible research by packaging all necessary software to reproduce an analysis. Containers are also easily versioned using a hash. Workflow integration : Workflow management systems such as Airflow, Nextflow, Luigi, and others provide built in integration with container engines.","title":"Container advantages"},{"location":"Documentation/Environments/Containers/intro/#hpc-hardware","text":"Both Singularity and Docker provide the ability to use hardware based features of Eagle. A common usage for containers is packaging of GPU enabled tools such as TensorFlow. Singularity enables access to the GPU and driver on the host. Likewise the MPI installations available on Eagle can be accessed from correctly configured containers.","title":"HPC hardware"},{"location":"Documentation/Environments/Containers/intro/#building","text":"Containers are built from a container specification file, Dockerfiles for Docker or Singularity Definition File in Singularity. These files specify the steps necessary to create the desired package and the additional software packages to install and configure in this environment. FROM ubuntu:20.04 RUN apt-get -y update && apt-get install -y python3 The above Dockerfile illustrates the build steps to create a simple image. Images are normally built from a base image indicated by FROM , in this case Ubuntu. The ability to use a different base image provides a way to use packages which may work more easily on one Linux Distribution. For example the Linux distribution on Eagle is CentOS, building the above image would allow the user to install packages from Ubuntu repositories. The RUN portion of the above Dockerfile indicates the command to run, in this example it installs the Python 3 package. Additional commands such as COPY , ENV , and others enable the customization of your image to suit your compute environment requirements.","title":"Building"},{"location":"Documentation/Environments/Containers/singularity/","text":"Singularity is installed on Eagle's compute nodes as a module named singularity-container. Images can be copied to eagle and run, or can be generated from a recipe (definition file) . Note: Input commands in the following examples are preceded by a $ . Run hello-world ubuntu image Step 1 : Log into compute node, checking it is running CentOS 7 $ ssh eagle.hpc.nrel.gov [ $USER @el1 ~ ] $ srun -A MYALLOCATION -t 60 -N 1 --pty $SHELL [ $USER @r1i3n18 ~ ] $ cat /etc/redhat-release CentOS Linux release 7 .7.1908 ( Core ) Step 2 : Load the singularity-container module [ $USER @r1i3n18 ~ ] $ module purge [ $USER @r1i3n18 ~ ] $ module load singularity-container Step 3 : Retrieve hello-world image. Be sure to use /scratch as images are typically large [ $USER @r1i3n18 ~ ] $ cd /scratch/ $USER [ $USER @r1i3n18 $USER ] $ mkdir -p singularity-images [ $USER @r1i3n18 $USER ] $ cd singularity-images [ $USER @r1i3n18 singularity-images ] $ singularity pull --name hello-world.simg shub://vsoch/hello-world Progress | =================================== | 100 .0% Done. Container is at: /lustre/eaglefs/scratch/ $USER /singularity-images/hello-world.simg Step 4 : Explore image details [ $USER @r1i3n18 singularity-images ] $ singularity inspect hello-world.simg # Shows labels { \"org.label-schema.usage.singularity.deffile.bootstrap\" : \"docker\" , \"MAINTAINER\" : \"vanessasaur\" , \"org.label-schema.usage.singularity.deffile\" : \"Singularity\" , \"org.label-schema.schema-version\" : \"1.0\" , \"WHATAMI\" : \"dinosaur\" , \"org.label-schema.usage.singularity.deffile.from\" : \"ubuntu:14.04\" , \"org.label-schema.build-date\" : \"2017-10-15T12:52:56+00:00\" , \"org.label-schema.usage.singularity.version\" : \"2.4-feature-squashbuild-secbuild.g780c84d\" , \"org.label-schema.build-size\" : \"333MB\" } [ $USER @r1i3n18 singularity-images ] $ singularity inspect -r hello-world.simg # Shows the script run #!/bin/sh exec /bin/bash /rawr.sh Step 5 : Run image default script [ $USER @r1i3n18 singularity-images ] $ singularity run hello-world.simg RaawwWWWWWRRRR!! Avocado. Step 6 : Run in singularity bash shell [ $USER @r1i3n18 singularity-images ] $ cat /etc/redhat-release CentOS Linux release 7 .7.1908 ( Core ) [ $USER @r1i3n18 singularity-images ] $ cat /etc/lsb-release cat: /etc/lsb-release: No such file or directory [ $USER @r1i3n18 singularity-images ] $ singularity shell hello-world.simg Singularity: Invoking an interactive shell within container... Singularity hello-world.simg:~> cat /etc/lsb-release DISTRIB_ID = Ubuntu DISTRIB_RELEASE = 14 .04 DISTRIB_CODENAME = trusty DISTRIB_DESCRIPTION = \"Ubuntu 14.04.5 LTS\" Singularity hello-world.simg:~> cat /etc/redhat-release cat: /etc/redhat-release: No such file or directory Create a CentOS 7 EPEL image with MPI support This example shows how to create a CentOS 7 singularity image with openmpi installed. It requires root/admin privileges to create the image so must be run on a user's computer with singularity installed. After creation, the image can be copied to Eagle and run. Step 1 : Create a new recipe based on singularityhub/centos:latest echo \"Bootstrap: shub From: singularityhub/centos:latest \" > centos-mpi.recipe Step 2 : Install development tools and enable epel repository after bootstrap is created echo \"%post yum -y groupinstall \" Development Tools \" yum -y install https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm \" >> centos-mpi.recipe Step 3 : Download, compile and install openmpi 2.1 echo \" curl -O https://download.open-mpi.org/release/open-mpi/v2.1/openmpi-2.1.2.tar.bz2 tar jxf openmpi-2.1.2.tar.bz2 cd openmpi-2.1.2 ./configure --prefix=/usr/local make make install \" >> centos-mpi.recipe Step 4 : Compile and install example mpi application echo \" mpicc examples/ring_c.c -o ring cp ring /usr/bin/ \" >> centos-mpi.recipe Step 5 : Install a package found in EPEL, in this example R echo \" yum -y install R \" >> centos-mpi.recipe Step 6 : Set default script to run ring echo \"%runscript /usr/bin/ring \" >> centos-mpi.recipe Step 7 : Build image sudo $( type -p singularity ) build centos-mpi.simg centos-mpi.recipe Step 8 : Test image $ mpirun -np 20 singularity exec centos-mpi.simg /usr/bin/ring $ singularity run centos-epel-r.simg --version R version 3 .4.4 ( 2018 -03-15 ) -- \"Someone to Lean On\" Copyright ( C ) 2018 The R Foundation for Statistical Computing Platform: x86_64-redhat-linux-gnu ( 64 -bit ) R is free software and comes with ABSOLUTELY NO WARRANTY. You are welcome to redistribute it under the terms of the GNU General Public License versions 2 or 3 . For more information about these matters see http://www.gnu.org/licenses/. $ singularity exec centos-mpi.simg Rscript -e \"a <- 42; A <- a*2; print(A)\" [ 1 ] 84","title":"Singularity on Eagle"},{"location":"Documentation/Environments/Containers/singularity/#run-hello-world-ubuntu-image","text":"Step 1 : Log into compute node, checking it is running CentOS 7 $ ssh eagle.hpc.nrel.gov [ $USER @el1 ~ ] $ srun -A MYALLOCATION -t 60 -N 1 --pty $SHELL [ $USER @r1i3n18 ~ ] $ cat /etc/redhat-release CentOS Linux release 7 .7.1908 ( Core ) Step 2 : Load the singularity-container module [ $USER @r1i3n18 ~ ] $ module purge [ $USER @r1i3n18 ~ ] $ module load singularity-container Step 3 : Retrieve hello-world image. Be sure to use /scratch as images are typically large [ $USER @r1i3n18 ~ ] $ cd /scratch/ $USER [ $USER @r1i3n18 $USER ] $ mkdir -p singularity-images [ $USER @r1i3n18 $USER ] $ cd singularity-images [ $USER @r1i3n18 singularity-images ] $ singularity pull --name hello-world.simg shub://vsoch/hello-world Progress | =================================== | 100 .0% Done. Container is at: /lustre/eaglefs/scratch/ $USER /singularity-images/hello-world.simg Step 4 : Explore image details [ $USER @r1i3n18 singularity-images ] $ singularity inspect hello-world.simg # Shows labels { \"org.label-schema.usage.singularity.deffile.bootstrap\" : \"docker\" , \"MAINTAINER\" : \"vanessasaur\" , \"org.label-schema.usage.singularity.deffile\" : \"Singularity\" , \"org.label-schema.schema-version\" : \"1.0\" , \"WHATAMI\" : \"dinosaur\" , \"org.label-schema.usage.singularity.deffile.from\" : \"ubuntu:14.04\" , \"org.label-schema.build-date\" : \"2017-10-15T12:52:56+00:00\" , \"org.label-schema.usage.singularity.version\" : \"2.4-feature-squashbuild-secbuild.g780c84d\" , \"org.label-schema.build-size\" : \"333MB\" } [ $USER @r1i3n18 singularity-images ] $ singularity inspect -r hello-world.simg # Shows the script run #!/bin/sh exec /bin/bash /rawr.sh Step 5 : Run image default script [ $USER @r1i3n18 singularity-images ] $ singularity run hello-world.simg RaawwWWWWWRRRR!! Avocado. Step 6 : Run in singularity bash shell [ $USER @r1i3n18 singularity-images ] $ cat /etc/redhat-release CentOS Linux release 7 .7.1908 ( Core ) [ $USER @r1i3n18 singularity-images ] $ cat /etc/lsb-release cat: /etc/lsb-release: No such file or directory [ $USER @r1i3n18 singularity-images ] $ singularity shell hello-world.simg Singularity: Invoking an interactive shell within container... Singularity hello-world.simg:~> cat /etc/lsb-release DISTRIB_ID = Ubuntu DISTRIB_RELEASE = 14 .04 DISTRIB_CODENAME = trusty DISTRIB_DESCRIPTION = \"Ubuntu 14.04.5 LTS\" Singularity hello-world.simg:~> cat /etc/redhat-release cat: /etc/redhat-release: No such file or directory","title":"Run hello-world ubuntu image"},{"location":"Documentation/Environments/Containers/singularity/#create-a-centos-7-epel-image-with-mpi-support","text":"This example shows how to create a CentOS 7 singularity image with openmpi installed. It requires root/admin privileges to create the image so must be run on a user's computer with singularity installed. After creation, the image can be copied to Eagle and run. Step 1 : Create a new recipe based on singularityhub/centos:latest echo \"Bootstrap: shub From: singularityhub/centos:latest \" > centos-mpi.recipe Step 2 : Install development tools and enable epel repository after bootstrap is created echo \"%post yum -y groupinstall \" Development Tools \" yum -y install https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm \" >> centos-mpi.recipe Step 3 : Download, compile and install openmpi 2.1 echo \" curl -O https://download.open-mpi.org/release/open-mpi/v2.1/openmpi-2.1.2.tar.bz2 tar jxf openmpi-2.1.2.tar.bz2 cd openmpi-2.1.2 ./configure --prefix=/usr/local make make install \" >> centos-mpi.recipe Step 4 : Compile and install example mpi application echo \" mpicc examples/ring_c.c -o ring cp ring /usr/bin/ \" >> centos-mpi.recipe Step 5 : Install a package found in EPEL, in this example R echo \" yum -y install R \" >> centos-mpi.recipe Step 6 : Set default script to run ring echo \"%runscript /usr/bin/ring \" >> centos-mpi.recipe Step 7 : Build image sudo $( type -p singularity ) build centos-mpi.simg centos-mpi.recipe Step 8 : Test image $ mpirun -np 20 singularity exec centos-mpi.simg /usr/bin/ring $ singularity run centos-epel-r.simg --version R version 3 .4.4 ( 2018 -03-15 ) -- \"Someone to Lean On\" Copyright ( C ) 2018 The R Foundation for Statistical Computing Platform: x86_64-redhat-linux-gnu ( 64 -bit ) R is free software and comes with ABSOLUTELY NO WARRANTY. You are welcome to redistribute it under the terms of the GNU General Public License versions 2 or 3 . For more information about these matters see http://www.gnu.org/licenses/. $ singularity exec centos-mpi.simg Rscript -e \"a <- 42; A <- a*2; print(A)\" [ 1 ] 84","title":"Create a CentOS 7 EPEL image with MPI support"},{"location":"Documentation/Environments/Containers/tensorflow/","text":"TensorFlow with GPU support singularity container This Singularity container supplies TensorFlow 2.3.0 optimized for use with GPU nodes. It also has opencv, numpy, pandas, seaborn, scikit-learn python libraries. Quickstart # Get allocation salloc --gres = gpu:2 -N 1 -A hpcapps -t 0 :10:00 -p debug # Run singularity in srun environment module load singularity-container unset LD_PRELOAD srun --gpus = 2 --pty singularity shell --nv /nopt/nrel/apps/singularity/images/tensorflow_gpu_extras_2.3.0.sif Building a custom image based on TensorFlow In order to build a custom Singularity image based on this one, docker must be installed on your local computer. Docker documentation shows how to install docker. Update Dockerfile shown below to represent the changes desired and save to working directory. FROM tensorflow/tensorflow:2.3.0-gpu-jupyter RUN apt-get update RUN DEBIAN_FRONTEND=\"noninteractive\" apt-get -y install python3-opencv RUN mkdir /custom_env COPY requirements.txt /custom_env RUN pip install -r /custom_env/requirements.txt Update requirements.txt shown below for changing the python library list and save to working directory. seaborn pandas numpy scikit-learn git+https://github.com/tensorflow/docs Build new docker image docker build -t tensorflow-custom-tag-name . Create Singularity image file. Note the ./images directory must be created before running this command. docker run -v /var/run/docker.sock:/var/run/docker.sock \\ -v $( PWD ) /images:/output \\ --privileged -t --rm \\ quay.io/singularity/docker2singularity --name tensorflow_custom.sif \\ tensorflow-custom-tag-name Transfer image file to Eagle. For this example I created a directory named /scratch/$(USER)/tensorflow on eagle rsync -v images/tensorflow_custom.sif eagle.hpc.nrel.gov:/scratch/ $( USER ) /tensorflow/","title":"Containerized TensorFlow"},{"location":"Documentation/Environments/Containers/tensorflow/#tensorflow-with-gpu-support-singularity-container","text":"This Singularity container supplies TensorFlow 2.3.0 optimized for use with GPU nodes. It also has opencv, numpy, pandas, seaborn, scikit-learn python libraries.","title":"TensorFlow with GPU support singularity container"},{"location":"Documentation/Environments/Containers/tensorflow/#quickstart","text":"# Get allocation salloc --gres = gpu:2 -N 1 -A hpcapps -t 0 :10:00 -p debug # Run singularity in srun environment module load singularity-container unset LD_PRELOAD srun --gpus = 2 --pty singularity shell --nv /nopt/nrel/apps/singularity/images/tensorflow_gpu_extras_2.3.0.sif","title":"Quickstart"},{"location":"Documentation/Environments/Containers/tensorflow/#building-a-custom-image-based-on-tensorflow","text":"In order to build a custom Singularity image based on this one, docker must be installed on your local computer. Docker documentation shows how to install docker. Update Dockerfile shown below to represent the changes desired and save to working directory. FROM tensorflow/tensorflow:2.3.0-gpu-jupyter RUN apt-get update RUN DEBIAN_FRONTEND=\"noninteractive\" apt-get -y install python3-opencv RUN mkdir /custom_env COPY requirements.txt /custom_env RUN pip install -r /custom_env/requirements.txt Update requirements.txt shown below for changing the python library list and save to working directory. seaborn pandas numpy scikit-learn git+https://github.com/tensorflow/docs Build new docker image docker build -t tensorflow-custom-tag-name . Create Singularity image file. Note the ./images directory must be created before running this command. docker run -v /var/run/docker.sock:/var/run/docker.sock \\ -v $( PWD ) /images:/output \\ --privileged -t --rm \\ quay.io/singularity/docker2singularity --name tensorflow_custom.sif \\ tensorflow-custom-tag-name Transfer image file to Eagle. For this example I created a directory named /scratch/$(USER)/tensorflow on eagle rsync -v images/tensorflow_custom.sif eagle.hpc.nrel.gov:/scratch/ $( USER ) /tensorflow/","title":"Building a custom image based on TensorFlow"},{"location":"Documentation/Environments/building-packages/a-building-packages/","text":"Building packages on Eagle for individual or project use. This training module will walk through how to build a reasonably complex package, OpenMPI, and deploy it for use by yourself or members of a project. Acquire the package and set up for build Configure, build, and install the package Setting up your own environment module Why build your own application? Sometimes, the package version that you need, or the capabilities you want, are only available as source code. Other times, a package has dependencies on other ones with application programming interfaces that change rapidly. A source code build might have code to adapt to the (older, newer) libraries you have available, whereas a binary distribution will likely not. In other cases, a binary distribution may be associated with a particular Linux distribution and version different from Eagle's. One example is a package for Linux version X+1 (with a shiny new libc). If you try to run this on Linux version X, you will almost certainly get errors associated with the GLIBC version required. If you build the application against your own, older libc version, those dependencies are not created. Performance; for example, if a more performant numerical library is available, you may be able to link against it. A pre-built binary may have been built against a more universally available but lower performance library. The same holds for optimizing compilers. Curiosity to know more about the tools you use. Pride of building one's tools oneself. For the sheer thrill of building packages. Next - Acquire the package and set up for build","title":"Building Packages"},{"location":"Documentation/Environments/building-packages/a-building-packages/#building-packages-on-eagle-for-individual-or-project-use","text":"This training module will walk through how to build a reasonably complex package, OpenMPI, and deploy it for use by yourself or members of a project. Acquire the package and set up for build Configure, build, and install the package Setting up your own environment module","title":"Building packages on Eagle for individual or project use."},{"location":"Documentation/Environments/building-packages/a-building-packages/#why-build-your-own-application","text":"Sometimes, the package version that you need, or the capabilities you want, are only available as source code. Other times, a package has dependencies on other ones with application programming interfaces that change rapidly. A source code build might have code to adapt to the (older, newer) libraries you have available, whereas a binary distribution will likely not. In other cases, a binary distribution may be associated with a particular Linux distribution and version different from Eagle's. One example is a package for Linux version X+1 (with a shiny new libc). If you try to run this on Linux version X, you will almost certainly get errors associated with the GLIBC version required. If you build the application against your own, older libc version, those dependencies are not created. Performance; for example, if a more performant numerical library is available, you may be able to link against it. A pre-built binary may have been built against a more universally available but lower performance library. The same holds for optimizing compilers. Curiosity to know more about the tools you use. Pride of building one's tools oneself. For the sheer thrill of building packages. Next - Acquire the package and set up for build","title":"Why build your own application?"},{"location":"Documentation/Environments/building-packages/b-acquire/","text":"Getting the package Change working directory to the location where you'll build the package. A convenient location is /scratch/$USER , which we'll use for this example. cd /scratch/$USER OpenMPI can be found at https://www.open-mpi.org/software/ompi/ . This will automatically redirect you to the latest version, but older releases can be seen in the left menu bar. For this, choose version 4.1. There are several packaging options. Here, we'll get the bzipped tarball openmpi-4.1.0.tar.bz2 . You can either download it to a local machine (laptop) and then scp the file over to Eagle, or get it directly on Eagle with wget. wget https://download.open-mpi.org/release/open-mpi/v4.1/openmpi-4.1.0.tar.bz2 You should now have a compressed tarball in your scratch directory. 4, List the contents of the tarball before unpacking. This is very useful to avoid inadvertently filling a directory with gobs of files and directories when the tarball has them at the top of the file structure), tar -tf openmpi-4.1.0.tar.bz2 Unpack it via tar -xjf openmpi-4.1.0.tar.bz2 If you're curious to see what's in the file as it unpacks, add the -v option. You should now have an openmpi-4.1.0 directory. cd openmpi-4.1.0 , at which point you are in the top level of the package distribution. You can now proceed to configuring, making, and installing. Previous - Building Packages on Eagle | Next - Configure, build, and install the package","title":"Acquire"},{"location":"Documentation/Environments/building-packages/b-acquire/#getting-the-package","text":"Change working directory to the location where you'll build the package. A convenient location is /scratch/$USER , which we'll use for this example. cd /scratch/$USER OpenMPI can be found at https://www.open-mpi.org/software/ompi/ . This will automatically redirect you to the latest version, but older releases can be seen in the left menu bar. For this, choose version 4.1. There are several packaging options. Here, we'll get the bzipped tarball openmpi-4.1.0.tar.bz2 . You can either download it to a local machine (laptop) and then scp the file over to Eagle, or get it directly on Eagle with wget. wget https://download.open-mpi.org/release/open-mpi/v4.1/openmpi-4.1.0.tar.bz2 You should now have a compressed tarball in your scratch directory. 4, List the contents of the tarball before unpacking. This is very useful to avoid inadvertently filling a directory with gobs of files and directories when the tarball has them at the top of the file structure), tar -tf openmpi-4.1.0.tar.bz2 Unpack it via tar -xjf openmpi-4.1.0.tar.bz2 If you're curious to see what's in the file as it unpacks, add the -v option. You should now have an openmpi-4.1.0 directory. cd openmpi-4.1.0 , at which point you are in the top level of the package distribution. You can now proceed to configuring, making, and installing. Previous - Building Packages on Eagle | Next - Configure, build, and install the package","title":"Getting the package"},{"location":"Documentation/Environments/building-packages/c-config_make_install/","text":"Configuring your build We will illustrate a package build that relies on the popular autotools system. Colloquially, this is the configure; make; make install process that is often encountered first by those new to package builds on Linux. Other build systems like CMake (which differ primarily in the configuration steps) won't be covered. If you need to build a package that relies on CMake, please contact hpc-help@nrel.gov for assistance. We'll use GCC version 8.4.0 for this illustration, so load the associated module first ( i.e. , gcc/8.4.0 ). Now that you've acquired and unpacked the package tarball and changed into the top-level directory of the package, you should see a script named \"configure\". In order to see all available options to an autotools configure script, use ./configure -h (don't forget to include the ./ explicit path, otherwise the script will not be found in the default Linux search paths, or worse, a different script will be found). We will build with the following command: ./configure --prefix=/scratch/$USER/openmpi/4.1.0-gcc-8.4.0 --with-slurm --with-pmi=/nopt/slurm/current --with-gnu-ld --with-lustre --with-zlib --without-psm --without-psm2 --with-ucx --without-verbs --with-hwloc=external --with-hwloc-libdir=/nopt/nrel/apps/base/2020-05-12/spack/opt/spack/linux-centos7-x86_64/gcc-8.4.0/hwloc-1.11.11-mb5lwdajmllvrdtwltwe3r732aca76ny/lib --enable-cxx-exceptions --enable-mpi-cxx --enable-mpi-fortran --enable-static LDFLAGS=\"-L/nopt/nrel/apps/base/2020-05-12/spack/opt/spack/linux-centos7-x86_64/gcc-8.4.0/11.0.2-4x2ws7fkooqbrerbsnfbzs6wyr5xutdk/lib64 -L/nopt/nrel/apps/base/2020-05-12/spack/opt/spack/linux-centos7-x86_64/gcc-8.4.0/cuda-11.0.2-4x2ws7fkooqbrerbsnfbzs6wyr5xutdk/lib64 -Wl,-rpath=/nopt/nrel/apps/base/2020-05-12/spack/opt/spack/linux-centos7-x86_64/gcc-8.4.0/hwloc-1.11.11-mb5lwdajmllvrdtwltwe3r732aca76ny/lib -Wl,-rpath=/nopt/nrel/apps/base/2020-05-12/spack/opt/spack/linux-centos7-x86_64/gcc-8.4.0/cuda-11.0.2-4x2ws7fkooqbrerbsnfbzs6wyr5xutdk/lib64\" CPPFLAGS=-I/nopt/nrel/apps/base/2020-05-12/spack/opt/spack/linux-centos7-x86_64/gcc-8.4.0/hwloc-1.11.11-mb5lwdajmllvrdtwltwe3r732aca76ny/include These options are given for the following reasons. --prefix= : This sets the location that \"make install\" will ultimately populate. If this isn't given, generally the default is to install into /usr or /usr/local, both of which require privileged access. We'll set up the environment using environment modules to point to this custom location. --with-slurm : Enables the interface with the Slurm resource manager --with-pmi= : Point to the Process Management Interface, the abstraction layer for MPI options --with-gnu-ld : Letting the build system know that linking will be done with GNU's linker, rather than a commercial or alternative open one. --with-lustre : Enable Lustre features --with-zlib : Enable compression library --without-psm[2] : Explicitly turn off interfaces to Intel's Performance Scaled Messaging for the now-defunct Omni-Path network --with-ucx= : Point to UCX, an intermediate layer between the network drivers and MPI --without-verbs= : For newer MPIs, communications go through UCX and/or libfabric, not directly to the Verbs layer --with-hwloc[-libdir]= : Point to a separately built hardware localization library for process pinning --enable-cxx-exceptions , --enable-mpi-cxx : Build the C++ interface for the libraries --enable-mpi-fortran : Build the Fortran interface for the libraries --enable-static : Build the .a archive files for static linking of applications LDFLAGS : -L options point to non-standard library locations. -Wl,-rpath options embed paths into the binaries, so that having LD_LIBRARY_PATH set correctly is not necessary (i.e., no separate module for these components). CPPFLAGS : Point to header files in non-standard locations. NOTE: The CUDA paths are not needed for CUDA function per se, but the resulting MPI errors out without setting them. There appears to be a lack of modularity that sets up a seemingly unneeded dependency. After lots of messages scroll by, you should be returned to a prompt following a summary of options. It's not a bad idea to glance through these, and make sure everything makes sense and is what you intended. Now that the build is configured, you can \"make\" it. For packages that are well integrated with automake, you can speed the build up by parallelizing it over multiple processes with the -j # option. If you're building this on a compute node, feel free to set this option to the total number of cores available. On the other hand, if you're using a login node, be a good citizen and leave cores available for other users ( i.e. , don't use more than 4; Arbiter should limit access at any rate regardless of this setting). make -j 4 Try a make check and/or a make test . Not every package enables these tests, but if they do, it's a great idea to run these sanity checks to find if your build is perfect, maybe-good-enough, or totally wrong before building lots of other software on top of it. Assuming checks passed if present, it's now time for make install . Assuming that completes without errors, you can move onto creating an environment module to use your new MPI library. Previous - Acquire the package and set up for build | Next - Setting up your own environment module","title":"Config Make Install"},{"location":"Documentation/Environments/building-packages/c-config_make_install/#configuring-your-build","text":"We will illustrate a package build that relies on the popular autotools system. Colloquially, this is the configure; make; make install process that is often encountered first by those new to package builds on Linux. Other build systems like CMake (which differ primarily in the configuration steps) won't be covered. If you need to build a package that relies on CMake, please contact hpc-help@nrel.gov for assistance. We'll use GCC version 8.4.0 for this illustration, so load the associated module first ( i.e. , gcc/8.4.0 ). Now that you've acquired and unpacked the package tarball and changed into the top-level directory of the package, you should see a script named \"configure\". In order to see all available options to an autotools configure script, use ./configure -h (don't forget to include the ./ explicit path, otherwise the script will not be found in the default Linux search paths, or worse, a different script will be found). We will build with the following command: ./configure --prefix=/scratch/$USER/openmpi/4.1.0-gcc-8.4.0 --with-slurm --with-pmi=/nopt/slurm/current --with-gnu-ld --with-lustre --with-zlib --without-psm --without-psm2 --with-ucx --without-verbs --with-hwloc=external --with-hwloc-libdir=/nopt/nrel/apps/base/2020-05-12/spack/opt/spack/linux-centos7-x86_64/gcc-8.4.0/hwloc-1.11.11-mb5lwdajmllvrdtwltwe3r732aca76ny/lib --enable-cxx-exceptions --enable-mpi-cxx --enable-mpi-fortran --enable-static LDFLAGS=\"-L/nopt/nrel/apps/base/2020-05-12/spack/opt/spack/linux-centos7-x86_64/gcc-8.4.0/11.0.2-4x2ws7fkooqbrerbsnfbzs6wyr5xutdk/lib64 -L/nopt/nrel/apps/base/2020-05-12/spack/opt/spack/linux-centos7-x86_64/gcc-8.4.0/cuda-11.0.2-4x2ws7fkooqbrerbsnfbzs6wyr5xutdk/lib64 -Wl,-rpath=/nopt/nrel/apps/base/2020-05-12/spack/opt/spack/linux-centos7-x86_64/gcc-8.4.0/hwloc-1.11.11-mb5lwdajmllvrdtwltwe3r732aca76ny/lib -Wl,-rpath=/nopt/nrel/apps/base/2020-05-12/spack/opt/spack/linux-centos7-x86_64/gcc-8.4.0/cuda-11.0.2-4x2ws7fkooqbrerbsnfbzs6wyr5xutdk/lib64\" CPPFLAGS=-I/nopt/nrel/apps/base/2020-05-12/spack/opt/spack/linux-centos7-x86_64/gcc-8.4.0/hwloc-1.11.11-mb5lwdajmllvrdtwltwe3r732aca76ny/include These options are given for the following reasons. --prefix= : This sets the location that \"make install\" will ultimately populate. If this isn't given, generally the default is to install into /usr or /usr/local, both of which require privileged access. We'll set up the environment using environment modules to point to this custom location. --with-slurm : Enables the interface with the Slurm resource manager --with-pmi= : Point to the Process Management Interface, the abstraction layer for MPI options --with-gnu-ld : Letting the build system know that linking will be done with GNU's linker, rather than a commercial or alternative open one. --with-lustre : Enable Lustre features --with-zlib : Enable compression library --without-psm[2] : Explicitly turn off interfaces to Intel's Performance Scaled Messaging for the now-defunct Omni-Path network --with-ucx= : Point to UCX, an intermediate layer between the network drivers and MPI --without-verbs= : For newer MPIs, communications go through UCX and/or libfabric, not directly to the Verbs layer --with-hwloc[-libdir]= : Point to a separately built hardware localization library for process pinning --enable-cxx-exceptions , --enable-mpi-cxx : Build the C++ interface for the libraries --enable-mpi-fortran : Build the Fortran interface for the libraries --enable-static : Build the .a archive files for static linking of applications LDFLAGS : -L options point to non-standard library locations. -Wl,-rpath options embed paths into the binaries, so that having LD_LIBRARY_PATH set correctly is not necessary (i.e., no separate module for these components). CPPFLAGS : Point to header files in non-standard locations. NOTE: The CUDA paths are not needed for CUDA function per se, but the resulting MPI errors out without setting them. There appears to be a lack of modularity that sets up a seemingly unneeded dependency. After lots of messages scroll by, you should be returned to a prompt following a summary of options. It's not a bad idea to glance through these, and make sure everything makes sense and is what you intended. Now that the build is configured, you can \"make\" it. For packages that are well integrated with automake, you can speed the build up by parallelizing it over multiple processes with the -j # option. If you're building this on a compute node, feel free to set this option to the total number of cores available. On the other hand, if you're using a login node, be a good citizen and leave cores available for other users ( i.e. , don't use more than 4; Arbiter should limit access at any rate regardless of this setting). make -j 4 Try a make check and/or a make test . Not every package enables these tests, but if they do, it's a great idea to run these sanity checks to find if your build is perfect, maybe-good-enough, or totally wrong before building lots of other software on top of it. Assuming checks passed if present, it's now time for make install . Assuming that completes without errors, you can move onto creating an environment module to use your new MPI library. Previous - Acquire the package and set up for build | Next - Setting up your own environment module","title":"Configuring your build"},{"location":"Documentation/Environments/building-packages/d-modules/","text":"Setting up your module Now that the package has been installed to your preferred location, we can set up an environment module. a. If this is your first package, then you probably need to create a place to collect modulefiles. For example, mkdir -p /scratch/$USER/modules/default/modulefiles . b. You can look at the systems module collection(s), e.g. , /nopt/nrel/apps/modules/default/modulefiles , to see how modules are organized from a filesystem perspective. In short, each library, application, or framework has its own directory in the modulefiles directory, and the modulefile itself sits either in this directory, or one level lower to accomodate additional versioning. In this example, there is the MPI version (4.1.0), as well as the compiler type and version (GCC 8.4.0) to keep track of. So, we'll make a /scratch/$USER/modules/default/modulefiles/openmpi/4.1.0 directory, and name the file by the compiler version used to build (gcc-8.4.0). You're free to modify this scheme to suit your own intentions. c. In the openmpi/4.1.0/gcc840 directory you just made, or whatever directory name you chose, goes the actual modulefile. It's much easier to copy an example from the system collection than to write one de novo , so you can do cp /nopt/nrel/apps/modules/default/modulefiles/openmpi/4.0.4/gcc-8.4.0.lua /scratch/$USER/modules/default/modulefiles/openmpi/4.1.0/. The Lmod modules system uses the Lua language natively for module code. It is not necessary for you to know the language to modify our examples. Tcl modules will also work under Lmod, but don't offer quite as much flexibility. d. For this example, (a) the OpenMPI version we're building is 4.1.0 instead of 4.0.4, and (b) the location is in /scratch/$USER , rather than /nopt/nrel/apps . So, edit /scratch/$USER/modules/default/modulefiles/openmpi/4.1.0/gcc-8.4.0.lua to make the required changes. Most of these changes only need to be made at the top of the file; variable definitions take care of the rest. e. Now you need to make a one-time change in order to see modules that you put in this collection ( /scratch/$USER/modules/default/modulefiles ). In your $HOME/.bash_profile , add the following line near the top: module use /scratch/$USER/modules/default/modulefiles Obviously, if you've built packages before and enabled them this way, you don't have to do this again! Now logout, log back in, and you should see your personal modules collection with a brand new module. [cchang@el1 ~]$ module avail ---------------------------------- /scratch/cchang/modules/default/modulefiles ----------------------------------- openmpi/4.1.0/gcc-8.4.0 Notice that the \".lua\" extension does not appear--the converse is also true, if the extension is missing it will not appear via module commands! As a sanity check, it's a good idea to load the module, and check that an executable file you know exists there is in fact on your PATH: [cchang@el1 ~]$ module load openmpi/4.1.0/gcc-8.4.0 [cchang@el1 ~]$ which mpirun /scratch/cchang/openmpi/4.1.0-gcc-8.4.0/bin/mpirun Previous - Configure, build, and install the package |","title":"Modules"},{"location":"Documentation/Environments/building-packages/d-modules/#setting-up-your-module","text":"Now that the package has been installed to your preferred location, we can set up an environment module. a. If this is your first package, then you probably need to create a place to collect modulefiles. For example, mkdir -p /scratch/$USER/modules/default/modulefiles . b. You can look at the systems module collection(s), e.g. , /nopt/nrel/apps/modules/default/modulefiles , to see how modules are organized from a filesystem perspective. In short, each library, application, or framework has its own directory in the modulefiles directory, and the modulefile itself sits either in this directory, or one level lower to accomodate additional versioning. In this example, there is the MPI version (4.1.0), as well as the compiler type and version (GCC 8.4.0) to keep track of. So, we'll make a /scratch/$USER/modules/default/modulefiles/openmpi/4.1.0 directory, and name the file by the compiler version used to build (gcc-8.4.0). You're free to modify this scheme to suit your own intentions. c. In the openmpi/4.1.0/gcc840 directory you just made, or whatever directory name you chose, goes the actual modulefile. It's much easier to copy an example from the system collection than to write one de novo , so you can do cp /nopt/nrel/apps/modules/default/modulefiles/openmpi/4.0.4/gcc-8.4.0.lua /scratch/$USER/modules/default/modulefiles/openmpi/4.1.0/. The Lmod modules system uses the Lua language natively for module code. It is not necessary for you to know the language to modify our examples. Tcl modules will also work under Lmod, but don't offer quite as much flexibility. d. For this example, (a) the OpenMPI version we're building is 4.1.0 instead of 4.0.4, and (b) the location is in /scratch/$USER , rather than /nopt/nrel/apps . So, edit /scratch/$USER/modules/default/modulefiles/openmpi/4.1.0/gcc-8.4.0.lua to make the required changes. Most of these changes only need to be made at the top of the file; variable definitions take care of the rest. e. Now you need to make a one-time change in order to see modules that you put in this collection ( /scratch/$USER/modules/default/modulefiles ). In your $HOME/.bash_profile , add the following line near the top: module use /scratch/$USER/modules/default/modulefiles Obviously, if you've built packages before and enabled them this way, you don't have to do this again! Now logout, log back in, and you should see your personal modules collection with a brand new module. [cchang@el1 ~]$ module avail ---------------------------------- /scratch/cchang/modules/default/modulefiles ----------------------------------- openmpi/4.1.0/gcc-8.4.0 Notice that the \".lua\" extension does not appear--the converse is also true, if the extension is missing it will not appear via module commands! As a sanity check, it's a good idea to load the module, and check that an executable file you know exists there is in fact on your PATH: [cchang@el1 ~]$ module load openmpi/4.1.0/gcc-8.4.0 [cchang@el1 ~]$ which mpirun /scratch/cchang/openmpi/4.1.0-gcc-8.4.0/bin/mpirun Previous - Configure, build, and install the package |","title":"Setting up your module"},{"location":"Documentation/Jupyter/jupyterhub/","text":"JupyterHub Prior to using Jupyterhub, you will have had to have logged into Eagle via the command line at least once. Given that, to start using Jupyterhub on Eagle, go to Europa in your local machine's browser, and log in with your Eagle username and password. You should land in your home directory, and see everything there via the standard Jupyter file listing. From the \"New\" pulldown on the right hand side, you can start a notebook, open a terminal, or create a file or folder. The default installation is Python version 3, and a variety of Conda modules are installed already. You can start a Python3 notebook right away, and access the Python modules that are already present. To see what's installed, from a notebook you can use the following command: !conda list Alternatively, you can start a Terminal, and use the usual conda commands from the shell. Creating a custom environment to access from the notebook Start a Terminal session, and follow the instructions on the HPC website to create an environment. Now, to make this environment visible from your future notebooks, run the following command: source activate <myenv> python -m ipykernel install --user --name <myenv> --display-name \"How-you-want-your-custom-kernel-to-appear-in-the-notebook-pulldown (<myenv>)\" where <myenv> is the argument to -n you used in your conda create command. After running this command, when you open a new notebook, you should see as an option your new environment, and once loaded be able to access all Python modules therein. Using Jupyterhub from Eagle To use inside Eagle, the Jupyterhub server exists on the internal network @ https://europa-int/. Customizing JupyterHub provides the ability to use custom kernels including ones for other popular programming languages such as Julia and R. NREL's custom kernels documentation provides more information on how to setup JupyterHub with other languages.","title":"JupyterHub"},{"location":"Documentation/Jupyter/jupyterhub/#jupyterhub","text":"Prior to using Jupyterhub, you will have had to have logged into Eagle via the command line at least once. Given that, to start using Jupyterhub on Eagle, go to Europa in your local machine's browser, and log in with your Eagle username and password. You should land in your home directory, and see everything there via the standard Jupyter file listing. From the \"New\" pulldown on the right hand side, you can start a notebook, open a terminal, or create a file or folder. The default installation is Python version 3, and a variety of Conda modules are installed already. You can start a Python3 notebook right away, and access the Python modules that are already present. To see what's installed, from a notebook you can use the following command: !conda list Alternatively, you can start a Terminal, and use the usual conda commands from the shell.","title":"JupyterHub"},{"location":"Documentation/Jupyter/jupyterhub/#creating-a-custom-environment-to-access-from-the-notebook","text":"Start a Terminal session, and follow the instructions on the HPC website to create an environment. Now, to make this environment visible from your future notebooks, run the following command: source activate <myenv> python -m ipykernel install --user --name <myenv> --display-name \"How-you-want-your-custom-kernel-to-appear-in-the-notebook-pulldown (<myenv>)\" where <myenv> is the argument to -n you used in your conda create command. After running this command, when you open a new notebook, you should see as an option your new environment, and once loaded be able to access all Python modules therein.","title":"Creating a custom environment to access from the notebook"},{"location":"Documentation/Jupyter/jupyterhub/#using-jupyterhub-from-eagle","text":"To use inside Eagle, the Jupyterhub server exists on the internal network @ https://europa-int/.","title":"Using Jupyterhub from Eagle"},{"location":"Documentation/Jupyter/jupyterhub/#customizing","text":"JupyterHub provides the ability to use custom kernels including ones for other popular programming languages such as Julia and R. NREL's custom kernels documentation provides more information on how to setup JupyterHub with other languages.","title":"Customizing"},{"location":"Documentation/Systems/systems/","text":"NREL Systems NREL operates two on-premises systems for computational work. System configurations Name Eagle Swift OS CentOS Rocky Linux CPU Dual Intel Xeon Gold Skylake 6154 Dual AMD EPYC 7532 Rome CPU Interconnect InfiniBand InfiniBand HPC scheduler Slurm Slurm Network Storage 17PB Lustre FS 480TB GPU Dual NVIDIA Tesla V100 Single NVIDIA V100 Memory 96GB, 192GB, 768GB 256GB Number of Nodes 2618 40","title":"Systems"},{"location":"Documentation/Systems/systems/#nrel-systems","text":"NREL operates two on-premises systems for computational work.","title":"NREL Systems"},{"location":"Documentation/Systems/systems/#system-configurations","text":"Name Eagle Swift OS CentOS Rocky Linux CPU Dual Intel Xeon Gold Skylake 6154 Dual AMD EPYC 7532 Rome CPU Interconnect InfiniBand InfiniBand HPC scheduler Slurm Slurm Network Storage 17PB Lustre FS 480TB GPU Dual NVIDIA Tesla V100 Single NVIDIA V100 Memory 96GB, 192GB, 768GB 256GB Number of Nodes 2618 40","title":"System configurations"},{"location":"Documentation/Systems/Swift/applications/","text":"Swift applications Some optimized versions of common applications are provided for the Swift cluster. Below is a list of how to utilize these applications and the optimizations for Swift. Modules Many are available as part of the Modules setup. TensorFlow TensorFlow has been built for the AMD architecture on Swift. This was done by using the following two build flags. -march=znver2 -mtune=znver2 This version of TensorFlow can be installed from a wheel file: pip install --upgrade --no-deps --force-reinstall /nopt/nrel/apps/wheels/tensorflow-2.4.2-cp38-cp38-linux_x86_64-cpu.whl Currently, this wheel is not built with NVIDIA CUDA support for running on GPU. TensorFlow installed on Swift with Conda may be significantly slower than the optimized version","title":"Applications"},{"location":"Documentation/Systems/Swift/applications/#swift-applications","text":"Some optimized versions of common applications are provided for the Swift cluster. Below is a list of how to utilize these applications and the optimizations for Swift.","title":"Swift applications"},{"location":"Documentation/Systems/Swift/applications/#modules","text":"Many are available as part of the Modules setup.","title":"Modules"},{"location":"Documentation/Systems/Swift/applications/#tensorflow","text":"TensorFlow has been built for the AMD architecture on Swift. This was done by using the following two build flags. -march=znver2 -mtune=znver2 This version of TensorFlow can be installed from a wheel file: pip install --upgrade --no-deps --force-reinstall /nopt/nrel/apps/wheels/tensorflow-2.4.2-cp38-cp38-linux_x86_64-cpu.whl Currently, this wheel is not built with NVIDIA CUDA support for running on GPU. TensorFlow installed on Swift with Conda may be significantly slower than the optimized version","title":"TensorFlow"},{"location":"Documentation/Systems/Swift/modules/","text":"Swift modules This describes how to activate and use the modules available on Swift. There are currently a number of known issues on Swift pleace check Known issues for a complete list Source Environments are provided with a number of commonly used modules including compilers, common build tools, specific AMD optimized libraries, and some analysis tools. The environments are in date stamped subdirectory under in the directory /nopt/nrel/apps. Each environemnt directory has a file myenv.*. Sourcing that file will enable the environment. For example to enable the environment /nopt/nrel/apps/210728a source the provided environment file. source /nopt/nrel/apps/210728a/myenv.2107290127 You will now have access to the modules provided. These can be listed using the following: ml avail If you want to build applications you can then module load compilers and the like; for example ml gcc openmpi will load gnu 9.4 and openmpi. Software is installed using a spack hierarchy. It is possible to add software to the hierarchy. This should be only done by people responsible for installing software for all users. It is also possible to do a spack install creating a new level of the hierarchy in your personal space. These procedures are documented in https://github.nrel.gov/tkaiser2/spackit.git in the file Notes03.md under the sections Building on the hierarchy and Building outside the hierarchy . If you want to try this please contact Tim Kaiser tkaiser2@nrel.gov to walk through the procedure.","title":"Modules"},{"location":"Documentation/Systems/Swift/modules/#swift-modules","text":"This describes how to activate and use the modules available on Swift. There are currently a number of known issues on Swift pleace check Known issues for a complete list","title":"Swift modules"},{"location":"Documentation/Systems/Swift/modules/#source","text":"Environments are provided with a number of commonly used modules including compilers, common build tools, specific AMD optimized libraries, and some analysis tools. The environments are in date stamped subdirectory under in the directory /nopt/nrel/apps. Each environemnt directory has a file myenv.*. Sourcing that file will enable the environment. For example to enable the environment /nopt/nrel/apps/210728a source the provided environment file. source /nopt/nrel/apps/210728a/myenv.2107290127 You will now have access to the modules provided. These can be listed using the following: ml avail If you want to build applications you can then module load compilers and the like; for example ml gcc openmpi will load gnu 9.4 and openmpi. Software is installed using a spack hierarchy. It is possible to add software to the hierarchy. This should be only done by people responsible for installing software for all users. It is also possible to do a spack install creating a new level of the hierarchy in your personal space. These procedures are documented in https://github.nrel.gov/tkaiser2/spackit.git in the file Notes03.md under the sections Building on the hierarchy and Building outside the hierarchy . If you want to try this please contact Tim Kaiser tkaiser2@nrel.gov to walk through the procedure.","title":"Source"},{"location":"Documentation/Systems/Swift/running/","text":"Running on Swift Please see the Modules page for information about setting up your environment and loading modules. There are currently a number of known issues on Swift please check Known issues for a complete list Slurm and Partitions As more of Swift is brought on line different partitions will be created. Initially the only partition avalible (08/01/21) is test . This is what is used to run the examples shown below. A list of partitions can be returned by sunning the sinfo command. If the command sinfo is not found then first ensure that slurm is in your path by running: source /nopt/nrel/apps/210728a/myenv.2107290127 module load slurm Example Environments are provided with a number of commonly used modules including compilers, common build tools, specific AMD optimized libraries, and some analysis tools. The environments are in date stamped subdirectory under in the directory /nopt/nrel/apps. Each environment directory has a file myenv.*. Sourcing that file will enable the environment. In the directory for an environment you will see a subdirectory example . This contains a makefile for a simple hello world program written in both Fortran and C. The README.md file contains additional information, most of which is replicated here. It is suggested you cp -r example ~/example cd ~/example Simple batch script Here is a sample batch script for running the hello world examples runopenmpi . #!/bin/bash #SBATCH --job-name=\"install\" #SBATCH --nodes=1 #SBATCH --exclusive #SBATCH --partition=test #SBATCH --time=00:01:00 cat $0 source /nopt/nrel/apps/210728a/myenv.2107290127 module load slurm ml gcc openmpi export OMP_NUM_THREADS = 2 srun -n 2 ./fhostone.I -F srun -n 2 ./phostone.I -F To run this you must first ensure that slurm is in your path by running: source /nopt/nrel/apps/210728a/myenv.2107290127 module load slurm Then sbatch --partition=test runopenmpi Building hello world first Obviously for the script given above to work you must first build the application. You need to: Load the environment Load the modules make Loading the environment Loading the environment is just a matter of sourcing the file source /nopt/nrel/apps/210728a/myenv.2107290127 Note that 210728 is a date stamp showing when the environment was built. You may have a different value as environments evolve. Loading the modules. We are going to use gnu compilers with OpenMPI. ml gcc openmpi Run make make Full procedure [ tkaiser2@eaglet 210728a ] $ cd /nopt/nrel/apps/210728a [ tkaiser2@eaglet 210728a ] $ cp -r example ~/example [ tkaiser2@eaglet 210728a ] $ cd ~/example [ tkaiser2@eaglet example ] $ cat runopenmpi #!/bin/bash #SBATCH --job-name=\"install\" #SBATCH --nodes=1 #SBATCH --exclusive #SBATCH --partition=debug #SBATCH --time=00:01:00 cat $0 source /nopt/nrel/apps/210728a/myenv.2107290127 module load slurm ml gcc openmpi export OMP_NUM_THREADS = 2 mpirun -n 2 ./fhostone -F mpirun -n 2 ./phostone -F [ tkaiser2@eaglet example ] $ PATH = /nopt/nrel/slurm/bin: $PATH [ tkaiser2@eaglet example ] $ source /nopt/nrel/apps/210728a/myenv* [ tkaiser2@eaglet example ] $ ml gcc openmpi [ tkaiser2@eaglet example ] $ make mpif90 -fopenmp fhostone.f90 -o fhostone rm getit.mod mympi.mod numz.mod mpicc -fopenmp phostone.c -o phostone [ tkaiser2@eaglet example ] $ sbatch --partition = test runopenmpi Submitted batch job 187 [ tkaiser2@eaglet example ] $ Results [ tkaiser2@eaglet example ] $ cat slurm-187.out #!/bin/bash #SBATCH --job-name=\"install\" #SBATCH --nodes=1 #SBATCH --exclusive #SBATCH --partition=debug #SBATCH --time=00:01:00 cat $0 PATH = /nopt/nrel/slurm/bin: $PATH source /nopt/nrel/apps/210728a/myenv* ml gcc openmpi export OMP_NUM_THREADS = 2 mpirun -n 2 ./fhostone -F mpirun -n 2 ./phostone -F MPI Version:Open MPI v4.1.1, package: Open MPI tkaiser2@c1-32 Distribution, ident: 4 .1.1, repo rev: v4.1.1, Apr 24 , 2021 task thread node name first task # on node core 0000 0000 c1-32 0000 0000 000 0000 0001 c1-32 0000 0000 064 0001 0000 c1-32 0000 0001 065 0001 0001 c1-32 0000 0001 001 MPI VERSION Open MPI v4.1.1, package: Open MPI tkaiser2@c1-32 Distribution, ident: 4 .1.1, repo rev: v4.1.1, Apr 24 , 2021 task thread node name first task # on node core 0000 0000 c1-32 0000 0000 0000 0000 0001 c1-32 0000 0000 0064 0001 0001 c1-32 0000 0001 0065 0001 0000 c1-32 0000 0001 0001 [ tkaiser2@eaglet example ] $ Building with Intel Fortran or Intel C and OpenMPI You can build parallel programs using OpenMPI and the Intel Fortran ifort and Intel C icc compilers. We have the example programs build with gnu compilers and OpenMP using the lines: [tkaiser2@eaglet example]$ mpif90 -fopenmp fhostone.f90 -o fhostone [tkaiser2@eaglet example]$ mpicc -fopenmp phostone.c -o phostone This gives us: [tkaiser2@eaglet example]$ ls -l fhostone -rwxrwxr-x. 1 tkaiser2 tkaiser2 36880 Jul 30 13:36 fhostone [tkaiser2@eaglet example]$ ls -l phostone -rwxrwxr-x. 1 tkaiser2 tkaiser2 27536 Jul 30 13:36 phostone Note the size of the executable files. If you want to use the Intel compilers you first do a module load. ml intel-oneapi-compilers Then we can set the variables OMPI_FC=ifort and OMPI_CC=icc . Then recompile. [ tkaiser2@eaglet example ] $ export OMPI_FC = ifort [ tkaiser2@eaglet example ] $ export OMPI_CC = icc [ tkaiser2@eaglet example ] $ mpif90 -fopenmp fhostone.f90 -o fhostone [ tkaiser2@eaglet example ] $ mpicc -fopenmp phostone.c -o phostone [ tkaiser2@eaglet example ] $ ls -lt fhostone -rwxrwxr-x. 1 tkaiser2 tkaiser2 951448 Jul 30 13 :37 fhostone [ tkaiser2@eaglet example ] $ ls -lt phostone -rwxrwxr-x. 1 tkaiser2 tkaiser2 155856 Jul 30 13 :37 phostone [ tkaiser2@eaglet example ] $ Note the size of the executable files have changed. You can also see the difference by running the commands nm fhostone | grep intel | wc nm phostone | grep intel | wc on the two versions of the program. It will show how many calls to Intel routines are in each, 51 and 36 compared to 0 for the gnu versions. Building and Running with Intel MPI We can build with the Intel versions of MPI. We assume we will want to build with icc and ifort as the backend compilers. We load the modules: ml gcc ml intel-oneapi-compilers ml intel-oneapi-mpi Then, building and running the same example as above: make clean make PFC=mpiifort PCC=mpiicc Giving us: [tkaiser2@swift-login-1 example]$ ls -lt fhostone phostone -rwxrwxr-x. 1 tkaiser2 hpcapps 155696 Aug 5 16:14 phostone -rwxrwxr-x. 1 tkaiser2 hpcapps 947112 Aug 5 16:14 fhostone [tkaiser2@swift-login-1 example]$ We need to make some changes to our batch script. Add the lines: ml intel-oneapi-compilers ml intel-oneapi-mpi export I_MPI_PMI_LIBRARY=/nopt/nrel/apps/210728a/level01/gcc-9.4.0/slurm-20-11-5-1/lib/libpmi2.so <<<<<<< HEAD ======= export UCX_TLS=all >>>>>>> auxsys Launch with the srun command: <<<<<<< HEAD srun --mpi=pmi2 ./a.out -F ======= srun ./a.out -F >>>>>>> auxsys Our IntelMPI batch script is: #!/bin/bash #SBATCH --job-name=\"install\" #SBATCH --nodes=1 #SBATCH --exclusive #SBATCH --partition=test #SBATCH --time=00:01:00 cat $0 PATH = /nopt/nrel/slurm/bin: $PATH source /nopt/nrel/apps/210728a/myenv* ml intel-oneapi-mpi intel-oneapi-compilers gcc export I_MPI_PMI_LIBRARY = /nopt/nrel/apps/210728a/level01/gcc-9.4.0/slurm-20-11-5-1/lib/libpmi2.so <<<<<< < HEAD export OMP_NUM_THREADS = 2 srun --mpi = pmi2 -n 2 ./fhostone -F srun --mpi = pmi2 -n 2 ./phostone -F ======= export UCX_TLS = all export OMP_NUM_THREADS = 2 srun -n 2 ./fhostone -F srun -n 2 ./phostone -F >>>>>>> auxsys With output MPI Version:Intel(R) MPI Library 2021.3 for Linux* OS task thread node name first task # on node core 0000 0000 c1-32 0000 0000 127 0000 0001 c1-32 0000 0000 097 0001 0000 c1-32 0000 0001 062 0001 0001 c1-32 0000 0001 099 MPI VERSION Intel(R) MPI Library 2021.3 for Linux* OS task thread node name first task # on node core 0000 0000 c1-32 0000 0000 0127 0000 0001 c1-32 0000 0000 0097 0001 0000 c1-32 0000 0001 0127 0001 0001 c1-32 0000 0001 0099 Running VASP The batch script given above can be modified to run VASP. You need to add ml vasp This will give you: [ tkaiser2@eaglet example ] $ which vasp_gam /nopt/nrel/apps/210728a/level02/gcc-9.4.0/vasp-6.1.1/bin/vasp_gam [ tkaiser2@eaglet example ] $ which vasp_ncl /nopt/nrel/apps/210728a/level02/gcc-9.4.0/vasp-6.1.1/bin/vasp_ncl [ tkaiser2@eaglet example ] $ which vasp_std /nopt/nrel/apps/210728a/level02/gcc-9.4.0/vasp-6.1.1/bin/vasp_std [ tkaiser2@eaglet example ] $ Note the directory might be different. Then you need to add calls in your script to set up / point do your data files. So your final script will look something like the following. Here we download data from NREL's benchmark repository. #!/bin/bash #SBATCH --job-name=b2_4 #SBATCH --nodes=1 #SBATCH --time=8:00:00 ##SBATCH --error=std.err ##SBATCH --output=std.out #SBATCH --partition=test #SBATCH --exclusive cat $0 hostname source /nopt/nrel/apps/210728a/myenv.* module purge ml openmpi gcc ml vasp #### wget is needed to download data ml wget #### get input and set it up #### This is from an old benchmark test #### see https://github.nrel.gov/ESIF-Benchmarks/VASP/tree/master/bench2 <<<<<< < HEAD mkdir input wget https://github.nrel.gov/raw/ESIF-Benchmarks/VASP/master/bench2/input/INCAR?token = AAAALJZRV4QFFTS7RC6LLGLBBV67M -q -O INCAR wget https://github.nrel.gov/raw/ESIF-Benchmarks/VASP/master/bench2/input/POTCAR?token = AAAALJ6E7KHVTGWQMR4RKYTBBV7SC -q -O POTCAR wget https://github.nrel.gov/raw/ESIF-Benchmarks/VASP/master/bench2/input/POSCAR?token = AAAALJ5WKM2QKC3D44SXIQTBBV7P2 -q -O POSCAR wget https://github.nrel.gov/raw/ESIF-Benchmarks/VASP/master/bench2/input/KPOINTS?token = AAAALJ5YTSCJFDHUUZMZY63BBV7NU -q -O KPOINTS ======= mkdir $SLURM_JOB_ID cp input/* $SLURM_JOB_ID cd $SLURM_JOB_ID >>>>>>> auxsys export OMP_NUM_THREADS = 4 <<<<<< < HEAD srun --mpi = pmi2 -n 16 vasp_std ======= srun -n 16 vasp_std >>>>>>> auxsys","title":"Running on Swift"},{"location":"Documentation/Systems/Swift/running/#running-on-swift","text":"Please see the Modules page for information about setting up your environment and loading modules. There are currently a number of known issues on Swift please check Known issues for a complete list","title":"Running on Swift"},{"location":"Documentation/Systems/Swift/running/#slurm-and-partitions","text":"As more of Swift is brought on line different partitions will be created. Initially the only partition avalible (08/01/21) is test . This is what is used to run the examples shown below. A list of partitions can be returned by sunning the sinfo command. If the command sinfo is not found then first ensure that slurm is in your path by running: source /nopt/nrel/apps/210728a/myenv.2107290127 module load slurm","title":"Slurm and Partitions"},{"location":"Documentation/Systems/Swift/running/#example","text":"Environments are provided with a number of commonly used modules including compilers, common build tools, specific AMD optimized libraries, and some analysis tools. The environments are in date stamped subdirectory under in the directory /nopt/nrel/apps. Each environment directory has a file myenv.*. Sourcing that file will enable the environment. In the directory for an environment you will see a subdirectory example . This contains a makefile for a simple hello world program written in both Fortran and C. The README.md file contains additional information, most of which is replicated here. It is suggested you cp -r example ~/example cd ~/example","title":"Example"},{"location":"Documentation/Systems/Swift/running/#simple-batch-script","text":"Here is a sample batch script for running the hello world examples runopenmpi . #!/bin/bash #SBATCH --job-name=\"install\" #SBATCH --nodes=1 #SBATCH --exclusive #SBATCH --partition=test #SBATCH --time=00:01:00 cat $0 source /nopt/nrel/apps/210728a/myenv.2107290127 module load slurm ml gcc openmpi export OMP_NUM_THREADS = 2 srun -n 2 ./fhostone.I -F srun -n 2 ./phostone.I -F To run this you must first ensure that slurm is in your path by running: source /nopt/nrel/apps/210728a/myenv.2107290127 module load slurm Then sbatch --partition=test runopenmpi","title":"Simple batch script"},{"location":"Documentation/Systems/Swift/running/#building-hello-world-first","text":"Obviously for the script given above to work you must first build the application. You need to: Load the environment Load the modules make","title":"Building hello world first"},{"location":"Documentation/Systems/Swift/running/#loading-the-environment","text":"Loading the environment is just a matter of sourcing the file source /nopt/nrel/apps/210728a/myenv.2107290127 Note that 210728 is a date stamp showing when the environment was built. You may have a different value as environments evolve.","title":"Loading the environment"},{"location":"Documentation/Systems/Swift/running/#loading-the-modules","text":"We are going to use gnu compilers with OpenMPI. ml gcc openmpi","title":"Loading the modules."},{"location":"Documentation/Systems/Swift/running/#run-make","text":"make","title":"Run make"},{"location":"Documentation/Systems/Swift/running/#full-procedure","text":"[ tkaiser2@eaglet 210728a ] $ cd /nopt/nrel/apps/210728a [ tkaiser2@eaglet 210728a ] $ cp -r example ~/example [ tkaiser2@eaglet 210728a ] $ cd ~/example [ tkaiser2@eaglet example ] $ cat runopenmpi #!/bin/bash #SBATCH --job-name=\"install\" #SBATCH --nodes=1 #SBATCH --exclusive #SBATCH --partition=debug #SBATCH --time=00:01:00 cat $0 source /nopt/nrel/apps/210728a/myenv.2107290127 module load slurm ml gcc openmpi export OMP_NUM_THREADS = 2 mpirun -n 2 ./fhostone -F mpirun -n 2 ./phostone -F [ tkaiser2@eaglet example ] $ PATH = /nopt/nrel/slurm/bin: $PATH [ tkaiser2@eaglet example ] $ source /nopt/nrel/apps/210728a/myenv* [ tkaiser2@eaglet example ] $ ml gcc openmpi [ tkaiser2@eaglet example ] $ make mpif90 -fopenmp fhostone.f90 -o fhostone rm getit.mod mympi.mod numz.mod mpicc -fopenmp phostone.c -o phostone [ tkaiser2@eaglet example ] $ sbatch --partition = test runopenmpi Submitted batch job 187 [ tkaiser2@eaglet example ] $","title":"Full procedure"},{"location":"Documentation/Systems/Swift/running/#results","text":"[ tkaiser2@eaglet example ] $ cat slurm-187.out #!/bin/bash #SBATCH --job-name=\"install\" #SBATCH --nodes=1 #SBATCH --exclusive #SBATCH --partition=debug #SBATCH --time=00:01:00 cat $0 PATH = /nopt/nrel/slurm/bin: $PATH source /nopt/nrel/apps/210728a/myenv* ml gcc openmpi export OMP_NUM_THREADS = 2 mpirun -n 2 ./fhostone -F mpirun -n 2 ./phostone -F MPI Version:Open MPI v4.1.1, package: Open MPI tkaiser2@c1-32 Distribution, ident: 4 .1.1, repo rev: v4.1.1, Apr 24 , 2021 task thread node name first task # on node core 0000 0000 c1-32 0000 0000 000 0000 0001 c1-32 0000 0000 064 0001 0000 c1-32 0000 0001 065 0001 0001 c1-32 0000 0001 001 MPI VERSION Open MPI v4.1.1, package: Open MPI tkaiser2@c1-32 Distribution, ident: 4 .1.1, repo rev: v4.1.1, Apr 24 , 2021 task thread node name first task # on node core 0000 0000 c1-32 0000 0000 0000 0000 0001 c1-32 0000 0000 0064 0001 0001 c1-32 0000 0001 0065 0001 0000 c1-32 0000 0001 0001 [ tkaiser2@eaglet example ] $","title":"Results"},{"location":"Documentation/Systems/Swift/running/#building-with-intel-fortran-or-intel-c-and-openmpi","text":"You can build parallel programs using OpenMPI and the Intel Fortran ifort and Intel C icc compilers. We have the example programs build with gnu compilers and OpenMP using the lines: [tkaiser2@eaglet example]$ mpif90 -fopenmp fhostone.f90 -o fhostone [tkaiser2@eaglet example]$ mpicc -fopenmp phostone.c -o phostone This gives us: [tkaiser2@eaglet example]$ ls -l fhostone -rwxrwxr-x. 1 tkaiser2 tkaiser2 36880 Jul 30 13:36 fhostone [tkaiser2@eaglet example]$ ls -l phostone -rwxrwxr-x. 1 tkaiser2 tkaiser2 27536 Jul 30 13:36 phostone Note the size of the executable files. If you want to use the Intel compilers you first do a module load. ml intel-oneapi-compilers Then we can set the variables OMPI_FC=ifort and OMPI_CC=icc . Then recompile. [ tkaiser2@eaglet example ] $ export OMPI_FC = ifort [ tkaiser2@eaglet example ] $ export OMPI_CC = icc [ tkaiser2@eaglet example ] $ mpif90 -fopenmp fhostone.f90 -o fhostone [ tkaiser2@eaglet example ] $ mpicc -fopenmp phostone.c -o phostone [ tkaiser2@eaglet example ] $ ls -lt fhostone -rwxrwxr-x. 1 tkaiser2 tkaiser2 951448 Jul 30 13 :37 fhostone [ tkaiser2@eaglet example ] $ ls -lt phostone -rwxrwxr-x. 1 tkaiser2 tkaiser2 155856 Jul 30 13 :37 phostone [ tkaiser2@eaglet example ] $ Note the size of the executable files have changed. You can also see the difference by running the commands nm fhostone | grep intel | wc nm phostone | grep intel | wc on the two versions of the program. It will show how many calls to Intel routines are in each, 51 and 36 compared to 0 for the gnu versions.","title":"Building with Intel Fortran or Intel C and OpenMPI"},{"location":"Documentation/Systems/Swift/running/#building-and-running-with-intel-mpi","text":"We can build with the Intel versions of MPI. We assume we will want to build with icc and ifort as the backend compilers. We load the modules: ml gcc ml intel-oneapi-compilers ml intel-oneapi-mpi Then, building and running the same example as above: make clean make PFC=mpiifort PCC=mpiicc Giving us: [tkaiser2@swift-login-1 example]$ ls -lt fhostone phostone -rwxrwxr-x. 1 tkaiser2 hpcapps 155696 Aug 5 16:14 phostone -rwxrwxr-x. 1 tkaiser2 hpcapps 947112 Aug 5 16:14 fhostone [tkaiser2@swift-login-1 example]$ We need to make some changes to our batch script. Add the lines: ml intel-oneapi-compilers ml intel-oneapi-mpi export I_MPI_PMI_LIBRARY=/nopt/nrel/apps/210728a/level01/gcc-9.4.0/slurm-20-11-5-1/lib/libpmi2.so <<<<<<< HEAD ======= export UCX_TLS=all >>>>>>> auxsys Launch with the srun command: <<<<<<< HEAD srun --mpi=pmi2 ./a.out -F ======= srun ./a.out -F >>>>>>> auxsys Our IntelMPI batch script is: #!/bin/bash #SBATCH --job-name=\"install\" #SBATCH --nodes=1 #SBATCH --exclusive #SBATCH --partition=test #SBATCH --time=00:01:00 cat $0 PATH = /nopt/nrel/slurm/bin: $PATH source /nopt/nrel/apps/210728a/myenv* ml intel-oneapi-mpi intel-oneapi-compilers gcc export I_MPI_PMI_LIBRARY = /nopt/nrel/apps/210728a/level01/gcc-9.4.0/slurm-20-11-5-1/lib/libpmi2.so <<<<<< < HEAD export OMP_NUM_THREADS = 2 srun --mpi = pmi2 -n 2 ./fhostone -F srun --mpi = pmi2 -n 2 ./phostone -F ======= export UCX_TLS = all export OMP_NUM_THREADS = 2 srun -n 2 ./fhostone -F srun -n 2 ./phostone -F >>>>>>> auxsys With output MPI Version:Intel(R) MPI Library 2021.3 for Linux* OS task thread node name first task # on node core 0000 0000 c1-32 0000 0000 127 0000 0001 c1-32 0000 0000 097 0001 0000 c1-32 0000 0001 062 0001 0001 c1-32 0000 0001 099 MPI VERSION Intel(R) MPI Library 2021.3 for Linux* OS task thread node name first task # on node core 0000 0000 c1-32 0000 0000 0127 0000 0001 c1-32 0000 0000 0097 0001 0000 c1-32 0000 0001 0127 0001 0001 c1-32 0000 0001 0099","title":"Building and Running with Intel MPI"},{"location":"Documentation/Systems/Swift/running/#running-vasp","text":"The batch script given above can be modified to run VASP. You need to add ml vasp This will give you: [ tkaiser2@eaglet example ] $ which vasp_gam /nopt/nrel/apps/210728a/level02/gcc-9.4.0/vasp-6.1.1/bin/vasp_gam [ tkaiser2@eaglet example ] $ which vasp_ncl /nopt/nrel/apps/210728a/level02/gcc-9.4.0/vasp-6.1.1/bin/vasp_ncl [ tkaiser2@eaglet example ] $ which vasp_std /nopt/nrel/apps/210728a/level02/gcc-9.4.0/vasp-6.1.1/bin/vasp_std [ tkaiser2@eaglet example ] $ Note the directory might be different. Then you need to add calls in your script to set up / point do your data files. So your final script will look something like the following. Here we download data from NREL's benchmark repository. #!/bin/bash #SBATCH --job-name=b2_4 #SBATCH --nodes=1 #SBATCH --time=8:00:00 ##SBATCH --error=std.err ##SBATCH --output=std.out #SBATCH --partition=test #SBATCH --exclusive cat $0 hostname source /nopt/nrel/apps/210728a/myenv.* module purge ml openmpi gcc ml vasp #### wget is needed to download data ml wget #### get input and set it up #### This is from an old benchmark test #### see https://github.nrel.gov/ESIF-Benchmarks/VASP/tree/master/bench2 <<<<<< < HEAD mkdir input wget https://github.nrel.gov/raw/ESIF-Benchmarks/VASP/master/bench2/input/INCAR?token = AAAALJZRV4QFFTS7RC6LLGLBBV67M -q -O INCAR wget https://github.nrel.gov/raw/ESIF-Benchmarks/VASP/master/bench2/input/POTCAR?token = AAAALJ6E7KHVTGWQMR4RKYTBBV7SC -q -O POTCAR wget https://github.nrel.gov/raw/ESIF-Benchmarks/VASP/master/bench2/input/POSCAR?token = AAAALJ5WKM2QKC3D44SXIQTBBV7P2 -q -O POSCAR wget https://github.nrel.gov/raw/ESIF-Benchmarks/VASP/master/bench2/input/KPOINTS?token = AAAALJ5YTSCJFDHUUZMZY63BBV7NU -q -O KPOINTS ======= mkdir $SLURM_JOB_ID cp input/* $SLURM_JOB_ID cd $SLURM_JOB_ID >>>>>>> auxsys export OMP_NUM_THREADS = 4 <<<<<< < HEAD srun --mpi = pmi2 -n 16 vasp_std ======= srun -n 16 vasp_std >>>>>>> auxsys","title":"Running VASP"},{"location":"Documentation/Systems/Swift/swift/","text":"Swift Swift is an AMD based cluster. Known issues and solutions To run IntelMPI programs must export I_MPI_PMI_LIBRARY=/nopt/nrel/apps/210728a/level01/gcc-9.4.0/slurm-20-11-5-1/lib/libpmi2.so export UCX_TLS=all OpenMPI appears to be working properly There are some example slrum scripts in the example directory. There is a very basic version of conda in the \"anaconda\" directory in each /nopt/nrel/apps/YYMMDDa directory. However, there is a more complete environment pointed to by the module under /nopt/nrel/apps/modules. This is set up like Eagle. See: https://www.nrel.gov/hpc/eagle-software-python.html There are no GPU nodes currently available on Swift. Use the following commands to enable slurm: source /nopt/nrel/apps/210728a/myenv.2107290127 module load slurm","title":"Swift"},{"location":"Documentation/Systems/Swift/swift/#swift","text":"Swift is an AMD based cluster.","title":"Swift"},{"location":"Documentation/Systems/Swift/swift/#known-issues-and-solutions","text":"To run IntelMPI programs must export I_MPI_PMI_LIBRARY=/nopt/nrel/apps/210728a/level01/gcc-9.4.0/slurm-20-11-5-1/lib/libpmi2.so export UCX_TLS=all OpenMPI appears to be working properly There are some example slrum scripts in the example directory. There is a very basic version of conda in the \"anaconda\" directory in each /nopt/nrel/apps/YYMMDDa directory. However, there is a more complete environment pointed to by the module under /nopt/nrel/apps/modules. This is set up like Eagle. See: https://www.nrel.gov/hpc/eagle-software-python.html There are no GPU nodes currently available on Swift. Use the following commands to enable slurm: source /nopt/nrel/apps/210728a/myenv.2107290127 module load slurm","title":"Known issues and solutions"},{"location":"Documentation/languages/fortran/f90/","text":"Advanced Fortran 90 This document is derived from an HTML page written at the San Diego Supercomper Center many years ago. Its purpose is to Introduce Fortran 90 concepts to Fortran 77 programers. It does this by presenting an example program and introducing concepts as various routines of the program are presented. The original web page has been used over the years and has been translated into several languages. Format for our presentation We will \"develop\" an application Incorporate f90 features Show source code Explain what and why as we do it Application is a genetic algorithm Easy to understand and program Offers rich opportunities for enhancement We also provide an summary of F90 syntax, key words, operators, constants, and functions What was in mind of the language writers? What were they thinking? Enable portable codes Same precision Include many common extensions More reliable programs Getting away from underlying hardware Move toward parallel programming Run old programs Ease of programming Writing Maintaining Understanding Reading Recover C and C++ users Why Fortran? Famous Quote: \"I don't know what the technical characteristics of the standard language for scientific and engineering computation in the year 2000 will be... but I know it will be called Fortran.\" John Backus.* ### Note: He claimed that he never said this. Language of choice for Scientific programming Large installed user base. Fortran 90 has most of the features of C . . . and then some The compilers produce better programs Justification of topics Enhance performance Enhance portability Enhance reliability Enhance maintainability Classification of topics New useful features Old tricks Power features Overview of F90 Listing_of_topics_covered Listing of topics covered What is a Genetic Algorithm Simple algorithm for a GA Our example problem Start of real Fortran 90 discussion Comparing a FORTRAN 77 routine to a Fortran 90 routine Obsolescent features New source Form and related things New data declaration method Kind facility Modules Module functions and subroutines Allocatable arrays (the basics) Passing arrays to subroutines Interface for passing arrays Optional arguments and intent Derived data types Using defined types User defined operators Recursive functions introduction Fortran 90 recursive functions Pointers Function and subroutine overloading Fortran Minval and Minloc routines Pointer assignment More pointer usage, association and nullify Pointer usage to reference an array Data assignment with structures Using the user defined operator Passing arrays with a given arbitrary lower bounds Using pointers to access sections of arrays Allocating an array inside a subroutine Our fitness function Linked lists Linked list usage Our map representation Date and time functions Non advancing and character IO Internal IO Inquire function Namelist Vector valued functions Complete source for recent discussions Some array specific intrinsic functions The rest of our GA Compiler Information Summary Overview of F90 Fortran 95 References What is a Genetic Algorithm A \"suboptimization\" system Find good, but maybe not optimal, solutions to difficult problems Often used on NP-Hard or combinatorial optimization problems Requirements Solution(s) to the problem represented as a string A fitness function Takes as input the solution string Output the desirability of the solution A method of combining solution strings to generate new solutions Find solutions to problems by Darwinian evolution Potential solutions ar though of as living entities in a population The strings are the genetic codes for the individuals Fittest individuals are allowed to survive to reproduce Simple algorithm for a GA Generate a initial population, a collection of strings do for some time evaluate each individual (string) of the population using the fitness function sort the population with fittest coming to the top allow the fittest individuals to \"sexually\" reproduce replacing the old population allow for mutation end do Our example problem Instance:Given a map of the N states or countries and a fixed number of colors Find a coloring of the map, if it exists, such that no two states that share a boarder have the same color Notes - In general, for a fixed number of colors and an arbitrary map the only known way to find if there is a valid coloring is a brute force search with the number of combinations = (NUMBER_OF_COLORS)**(NSTATES) The strings of our population are integer vectors represent the coloring Our fitness function returns the number of boarder violations The GA searches for a mapping with few, hopefully 0 violations This problem is related to several important NP_HARD problems in computer science Processor scheduling Communication and grid allocation for parallel computing Routing Start of real Fortran 90 discussion Comparing a FORTRAN 77 routine to a Fortran 90 routine The routine is one of the random number generators from: Numerical Recipes, The Art of Scientific Computing. Press, Teukolsky, Vetterling and Flannery. Cambridge University Press 1986. Changes correct bugs increase functionality aid portability Original function ran1 ( idum ) real ran1 integer idum real r ( 97 ) parameter ( m1 = 259200 , ia1 = 7141 , ic1 = 54773 ) parameter ( m2 = 134456 , ia2 = 8121 , ic2 = 28411 ) parameter ( m3 = 243000 , ia3 = 4561 , ic3 = 51349 ) integer j integer iff , ix1 , ix2 , ix3 data iff / 0 / if ( idum . lt . 0. or . iff . eq . 0 ) then rm1 = 1.0 / m1 rm2 = 1.0 / m2 iff = 1 ix1 = mod ( ic1 - idum , m1 ) ix1 = mod ( ia1 * ix1 + ic1 , m1 ) ix2 = mod ( ix1 , m2 ) ix1 = mod ( ia1 * ix1 + ic1 , m1 ) ix3 = mod ( ix1 , m3 ) do 11 j = 1 , 97 ix1 = mod ( ia1 * ix1 + ic1 , m1 ) ix2 = mod ( ia2 * ix2 + ic2 , m2 ) r ( j ) = ( real ( ix1 ) + real ( ix2 ) * rm2 ) * rm1 11 continue idum = 1 endif ix1 = mod ( ia1 * ix1 + ic1 , m1 ) ix2 = mod ( ia2 * ix2 + ic2 , m2 ) ix3 = mod ( ia3 * ix3 + ic3 , m3 ) j = 1 + ( 97 * ix3 ) / m3 if ( j . gt . 9 7. or . j . lt . 1 ) then write ( * , * ) ' error in ran1 j=' , j stop endif ran1 = r ( j ) r ( j ) = ( real ( ix1 ) + real ( ix2 ) * rm2 ) * rm1 return end Fortran 90 module ran_mod contains function ran1 ( idum ) use numz implicit none !note after use statement real ( b8 ) ran1 integer , intent ( inout ), optional :: idum real ( b8 ) r ( 97 ), rm1 , rm2 integer , parameter :: m1 = 259200 , ia1 = 7141 , ic1 = 54773 integer , parameter :: m2 = 134456 , ia2 = 8121 , ic2 = 28411 integer , parameter :: m3 = 243000 , ia3 = 4561 , ic3 = 51349 integer j integer iff , ix1 , ix2 , ix3 data iff / 0 / save ! corrects a bug in the original routine if ( present ( idum )) then if ( idum . lt . 0. or . iff . eq . 0 ) then rm1 = 1.0_b8 m1 rm2 = 1.0_b8 m2 iff = 1 ix1 = mod ( ic1 - idum , m1 ) ix1 = mod ( ia1 * ix1 + ic1 , m1 ) ix2 = mod ( ix1 , m2 ) ix1 = mod ( ia1 * ix1 + ic1 , m1 ) ix3 = mod ( ix1 , m3 ) do j = 1 , 97 ix1 = mod ( ia1 * ix1 + ic1 , m1 ) ix2 = mod ( ia2 * ix2 + ic2 , m2 ) r ( j ) = ( real ( ix1 , b8 ) + real ( ix2 , b8 ) * rm2 ) * rm1 enddo idum = 1 endif endif ix1 = mod ( ia1 * ix1 + ic1 , m1 ) ix2 = mod ( ia2 * ix2 + ic2 , m2 ) ix3 = mod ( ia3 * ix3 + ic3 , m3 ) j = 1 + ( 97 * ix3 ) / m3 if ( j . gt . 9 7. or . j . lt . 1 ) then write ( * , * ) ' error in ran1 j=' , j stop endif ran1 = r ( j ) r ( j ) = ( real ( ix1 , b8 ) + real ( ix2 , b8 ) * rm2 ) * rm1 return end function ran1 Comments Modules are a way of encapsulating functions an data. More below. The use numz line is similar to an include file. In this case it defines our real data type. real (b8) is a new way to specify percision for data types in a portable way. integer , intent(inout), optional :: idum we are saying idum is an optional input parameter integer , parameter :: just a different syntax The save statement is needed for program correctness present(idum) is a function to determine if ran1 was called with the optional parameter Obsolescent features The following are available in Fortran 90. On the other hand, the concept of \"obsolescence\" is introduced. This means that some constructs may be removed in the future. - Arithmetic IF-statement - Control variables in a DO-loop which are floating point or double-precision floating-point - Terminating several DO-loops on the same statement - Terminating the DO-loop in some other way than with CONTINUE or END DO - Alternate return - Jump to END IF from an outer block - PAUSE - ASSIGN and assigned GOTO and assigned FORMAT , that is the whole \"statement number variable\" concept. - Hollerith editing in FORMAT. New source form and related things Summary ! now indicates the start of a comment & indicates the next line is a continuation Lines can be longer than 72 characters Statements can start in any column Use ; to put multiple statements on one line New forms for the do loop Many functions are generic 32 character names Many new array assignment techniques Features Flexibility can aid in program readability Readability decreases errors Got ya! Can no longer use C to start a comment Character in column 5 no longer is continue Tab is not a valid character (may produce a warning) Characters past 72 now count program darwin real a ( 10 ), b ( 10 ), c ( 10 ), d ( 10 ), e ( 10 ), x , y integer odd ( 5 ), even ( 5 ) ! this line is continued by using \"&\" write ( * , * ) \"starting \" ,& \"darwin\" ! this line in a continued from above ! multiple statement per line --rarely a good idea x = 1 ; y = 2 ; write ( * , * ) x , y do i = 1 , 10 ! statement lable is not required for do e ( i ) = i enddo odd = ( / 1 , 3 , 5 , 7 , 9 / ) ! array assignment even = ( / 2 , 4 , 6 , 8 , 10 / ) ! array assignment a = 1 ! array assignment, every element of a = 1 b = 2 c = a + b + e ! element by element assignment c ( odd ) = c ( even ) - 1 ! can use arrays of indices on both sides d = sin ( c ) ! element by element application of intrinsics write ( * , * ) d write ( * , * ) abs ( d ) ! many intrinsic functions are generic a_do_loop : do i = 1 , 10 write ( * , * ) i , c ( i ), d ( i ) enddo a_do_loop do if ( c ( 10 ) . lt . 0.0 ) exit c ( 10 ) = c ( 10 ) - 1 enddo write ( * , * ) c ( 10 ) do while ( c ( 9 ) . gt . 0 ) c ( 9 ) = c ( 9 ) - 1 enddo write ( * , * ) c ( 9 ) end program New data declaration method Motivation Variables can now have attributes such as - Parameter - Save - Dimension Attributes are assigned in the variable declaration statement One variable can have several attributes Requires Fortran 90 to have a new statement form integer , parameter :: in2 = 14 real , parameter :: pi = 3.141592653589793239 real , save , dimension ( 10 ) :: cpu_times , wall_times !**** the old way of doing the same ****! !**** real cpu_times(10),wall_times(10) ****! !**** save cpu_times, wall_times ****! - Other Attributes - allocatable - public - private - target - pointer - intent - optional Kind facility Motivation Assume we have a program that we want to run on two different machines We want the same representation of reals on both machines (same number of significant digits) Problem: different machines have different representations for reals Digits of precision for some (old) machines and data type Machine Real Double Precision IBM (SP) 6 15 Cray (T90) 15 33 Cray (T3E) 15 15 * or * We may want to run with at least 6 digits today and at least 14 digits tomorrow Use the Select_Real_Kind(P) function to create a data type with P digits of precision program darwin ! e has at least 4 significant digits real ( selected_real_kind ( 4 )) e ! b8 will be used to define reals with 14 digits integer , parameter :: b8 = selected_real_kind ( 14 ) real ( b8 ), parameter :: pi = 3.141592653589793239_b8 ! note usage of _b8 ! with a constant ! to force precision e = 2.71828182845904523536 write ( * , * ) \"starting \" ,& ! this line is continued by using \"&\" \"darwin\" ! this line in a continued from above write ( * , * ) \"pi has \" , precision ( pi ), \" digits precision \" , pi write ( * , * ) \"e has \" , precision ( e ), \" digits precision \" , e end program Example output sp001 % darwin starting darwin pi has 15 digits precision 3.14159265358979312 e has 6 digits precision 2.718281746 sp001 % Can convert to/from given precision for all variables created using \"b8\" by changing definition of \"b8\" Use the Select_Real_Kind(P,R) function to create a data type with P digits of precision and exponent range of R Modules Motivation: Common block usage is prone to error Provide most of capability of common blocks but safer Provide capabilities beyond common blocks Modules can contain: Data definitions Data to be shared much like using a labeled common Functions and subroutines Interfaces (more on this later) You \"include\" a module with a \"use\" statement module numz integer , parameter :: b8 = selected_real_kind ( 14 ) real ( b8 ), parameter :: pi = 3.141592653589793239_b8 integergene_size end module program darwin use numz implicit none ! now part of the standard, put it after the use statements write ( * , * ) \"pi has \" , precision ( pi ), \" digits precision \" , pi call set_size () write ( * , * ) \"gene_size=\" , gene_size end program subroutine set_size use numz gene_size = 10 end subroutine An example run pi has 15 digits precision 3.14159265358979312 gene_size=10 Module functions and subroutines Motivation: Encapsulate related functions and subroutines Can \"USE\" these functions in a program or subroutine Can be provided as a library Only routines that contain the use statement can see the routines Example is a random number package: module ran_mod ! module contains three functions ! ran1 returns a uniform random number between 0-1 ! spread returns random number between min - max ! normal returns a normal distribution contains function ran1 () !returns random number between 0 - 1 use numz implicit none real ( b8 ) ran1 , x call random_number ( x ) ! built in fortran 90 random number function ran1 = x end function ran1 function spread ( min , max ) !returns random # between min/max use numz implicit none real ( b8 ) spread real ( b8 ) min , max spread = ( max - min ) * ran1 () + min end function spread function normal ( mean , sigma ) !returns a normal distribution use numz implicit none real ( b8 ) normal , tmp real ( b8 ) mean , sigma integer flag real ( b8 ) fac , gsave , rsq , r1 , r2 save flag , gsave data flag / 0 / if ( flag . eq . 0 ) then rsq = 2.0_b8 do while ( rsq . ge . 1.0_b8 . or . rsq . eq . 0.0_b8 ) ! new from for do r1 = 2.0_b8 * ran1 () - 1.0_b8 r2 = 2.0_b8 * ran1 () - 1.0_b8 rsq = r1 * r1 + r2 * r2 enddo fac = sqrt ( - 2.0_b8 * log ( rsq ) / rsq ) gsave = r1 * fac tmp = r2 * fac flag = 1 else tmp = gsave flag = 0 endif normal = tmp * sigma + mean return end function normal end module ran_mod Exersize 1: Write a program that returns 10 uniform random numbers. Allocatable arrays (the basics) Motivation: At compile time we may not know the size an array needs to be We may want to change problem size without recompiling Allocatable arrays allow us to set the size at run time We set the size of the array using the allocate statement We may want to change the lower bound for an array A simple example: module numz integer , parameter :: b8 = selected_real_kind ( 14 ) integer gene_size , num_genes integer , allocatable :: a_gene (:), many_genes (:,:) end module program darwin use numz implicit none integer ierr call set_size () allocate ( a_gene ( gene_size ), stat = ierr ) !stat= allows for an error code return if ( ierr /= 0 ) write ( * , * ) \"allocation error\" ! /= is .ne. allocate ( many_genes ( gene_size , num_genes ), stat = ierr ) !2d array if ( ierr /= 0 ) write ( * , * ) \"allocation error\" write ( * , * ) lbound ( a_gene ), ubound ( a_gene ) ! get lower and upper bound ! for the array write ( * , * ) size ( many_genes ), size ( many_genes , 1 ) !get total size and size !along 1st dimension deallocate ( many_genes ) ! free the space for the array and matrix deallocate ( a_gene ) allocate ( a_gene ( 0 : gene_size )) ! now allocate starting at 0 instead of 1 write ( * , * ) allocated ( many_genes ), allocated ( a_gene ) ! shows if allocated write ( * , * ) lbound ( a_gene ), ubound ( a_gene ) end program subroutine set_size use numz write ( * , * ) 'enter gene size:' read ( * , * ) gene_size write ( * , * ) 'enter number of genes:' read ( * , * ) num_genes end subroutine set_size Example run enter gene size: 10 enter number of genes: 20 1 10 200 10 F T 0 10 Passing arrays to subroutines There are several ways to specify arrays for subroutines Explicit shape integer, dimension(8,8)::an_explicit_shape_array Assumed size integer, dimension(i,*)::an_assumed_size_array Assumed Shape integer, dimension(:,:)::an_assumed_shape_array Example subroutine arrays ( an_explicit_shape_array ,& i ,& !note we pass all bounds except the last an_assumed_size_array ,& an_assumed_shape_array ) ! Explicit shape integer , dimension ( 8 , 8 ) :: an_explicit_shape_array ! Assumed size integer , dimension ( i , * ) :: an_assumed_size_array ! Assumed Shape integer , dimension (:,:) :: an_assumed_shape_array write ( * , * ) sum ( an_explicit_shape_array ) write ( * , * ) lbound ( an_assumed_size_array ) ! why does sum not work here? write ( * , * ) sum ( an_assumed_shape_array ) end subroutine Interface for passing arrays !!!!Warning!!!! When passing assumed shape arrays as arguments you must provide an interface Similar to C prototypes but much more versatile The interface is a copy of the invocation line and the argument definitions Modules are a good place for interfaces If a procedure is part of a \"contains\" section in a module an interface is not required !!!!Warning!!!! The compiler may not tell you that you need an interface module numz integer , parameter :: b8 = selected_real_kind ( 14 ) integer , allocatable :: a_gene (:), many_genes (:,:) end module module face interface fitness function fitness ( vector ) use numz implicit none real ( b8 ) fitness integer , dimension (:) :: vector end function fitness end interface end module program darwin use numz use face implicit none integer i integer vect ( 10 ) ! just a regular array allocate ( a_gene ( 10 )); allocate ( many_genes ( 3 , 10 )) a_gene = 1 !sets every element of a_gene to 1 write ( * , * ) fitness ( a_gene ) vect = 8 write ( * , * ) fitness ( vect ) ! also works with regular arrays many_genes = 3 !sets every element to 3 many_genes ( 1 ,:) = a_gene !sets column 1 to a_gene many_genes ( 2 ,:) = 2 * many_genes ( 1 ,:) do i = 1 , 3 write ( * , * ) fitness ( many_genes ( i ,:)) enddo write ( * , * ) fitness ( many_genes (:, 1 )) !go along other dimension !!!!write(*,*)fitness(many_genes)!!!!does not work end program function fitness ( vector ) use numz implicit none real ( b8 ) fitness integer , dimension (:) :: vector ! must match interface fitness = sum ( vector ) end function Exersize 2: Run this program using the \"does not work line\". Why? Using intrinsic functions make it work? Exersize 3: Prove that f90 does not \"pass by address\". Optional arguments and intent Motivation: We may have a function or subroutine that we may not want to always pass all arguments Initialization Two examples Seeding the intrinsic random number generator requires keyword arguments To define an optional argument in our own function we use the optional attribute integer :: my_seed becomes integer, optional :: my_seed Used like this: ! ran1 returns a uniform random number between 0-1 ! the seed is optional and used to reset the generator contains function ran1 ( my_seed ) use numz implicit none real ( b8 ) ran1 , r integer , optional , intent ( in ) :: my_seed ! optional argument not changed in the routine integer , allocatable :: seed (:) integer the_size , j if ( present ( my_seed )) then ! use the seed if present call random_seed ( size = the_size ) ! how big is the intrisic seed? allocate ( seed ( the_size )) ! allocate space for seed do j = 1 , the_size ! create the seed seed ( j ) = abs ( my_seed ) + ( j - 1 ) ! abs is generic enddo call random_seed ( put = seed ) ! assign the seed deallocate ( seed ) ! deallocate space endif call random_number ( r ) ran1 = r end function ran1 end module program darwin use numz use ran_mod ! interface required if we have ! optional or intent arguments real ( b8 ) x , y x = ran1 ( my_seed = 12345 ) ! we can specify the name of the argument y = ran1 () write ( * , * ) x , y x = ran1 ( 12345 ) ! with only one optional argument we don't need to y = ran1 () write ( * , * ) x , y end program Intent is a hint to the compiler to enable optimization intent(in) We will not change this value in our subroutine intent(out) We will define this value in our routine intent(inout) The normal situation Derived data types Motivation: Derived data types can be used to group different types of data together (integers, reals, character, complex) Can not be done in F77 although people have \"faked\" it Example In our GA we define a collection of genes as a 2d array We call the fitness function for every member of the collection We want to sort the collection of genes based on result of fitness function Define a data type that holds the fitness value and an index into the 2d array Create an array of this data type, 1 for each member of the collection Call fitness function with the result being placed into the new data type along with a pointer into the array Again modules are a good place for data type definitions module galapagos use numz type thefit !the name of the type sequence ! sequence forces the data elements ! to be next to each other in memory ! where might this be useful? real ( b8 ) val ! our result from the fitness function integer index ! the index into our collection of genes end type thefit end module Using defined types Use the % to reference various components of the derived data type program darwin use numz use galapagos ! the module that contains the type definition use face ! contains various interfaces implicit none ! define an allocatable array of the data type ! than contains an index and a real value type ( thefit ), allocatable , target :: results (:) ! create a single instance of the data type type ( thefit ) best integer , allocatable :: genes (:,:) ! our genes for the genetic algorithm integer j integer num_genes , gene_size num_genes = 10 gene_size = 10 allocate ( results ( num_genes )) ! allocate the data type ! to hold fitness and index allocate ( genes ( num_genes , gene_size )) ! allocate our collection of genes call init_genes ( genes ) ! starting data write ( * , '(\"input\")' ) ! we can put format in write statement do j = 1 , num_genes results ( j )% index = j results ( j )% val = fitness ( genes ( j ,:)) ! just a dummy routine for now write ( * , \"(f10.8,i4)\" ) results ( j )% val , results ( j )% index enddo end program User defined operators Motivation With derived data types we may want (need) to define operations (Assignment is predefined) Example: .lt. .gt. == not defined for our data types - We want to find the minimum of our fitness values so we need < operator - In our sort routine we want to do <, >, == - In C++ terms the operators are overloaded We are free to define new operators Two step process to define operators Define a special interface Define the function that performs the operation module sort_mod !defining the interfaces interface operator (. lt .) ! overloads standard .lt. module procedure theless ! the function that does it end interface interface operator (. gt .) ! overloads standard .gt. module procedure thegreat ! the function that does it end interface interface operator (. ge .) ! overloads standard .ge. module procedure thetest ! the function that does it end interface interface operator (. converged .) ! new operator module procedure index_test ! the function that does it end interface contains ! our module will contain ! the required functions function theless ( a , b ) ! overloads .lt. for the type (thefit) use galapagos implicit none type ( thefit ), intent ( in ) :: a , b logical theless ! what we return if ( a % val . lt . b % val ) then ! this is where we do the test theless = . true . else theless = . false . endif return end function theless function thegreat ( a , b ) ! overloads .gt. for the type (thefit) use galapagos implicit none type ( thefit ), intent ( in ) :: a , b logical thegreat if ( a % val . gt . b % val ) then thegreat = . true . else thegreat = . false . endif return end function thegreat function thetest ( a , b ) ! overloads .gt.= for the type (thefit) use galapagos implicit none type ( thefit ), intent ( in ) :: a , b logical thetest if ( a % val >= b % val ) then thetest = . true . else thetest = . false . endif return end function thetest function index_test ( a , b ) ! defines a new operation for the type (thefit) use galapagos implicit none type ( thefit ), intent ( in ) :: a , b logical index_test if ( a % index . gt . b % index ) then ! check the index value for a difference index_test = . true . else index_test = . false . endif return end function index_test Recursive functions introduction Notes Recursive function is one that calls itself Anything that can be done with a do loop can be done using a recursive function Motivation Sometimes it is easier to think recursively Divide an conquer algorithms are recursive by nature - Fast FFTs - Searching - Sorting Algorithm of searching for minimum of an array function findmin ( array ) is size of array 1 ? min in the array is first element else find minimum in left half of array using findmin function find minimum in right half of array using findmin function global minimum is min of left and right half end function Fortran 90 recursive functions Recursive functions should have an interface The result and recursive keywords are required as part of the function definition Example is a function finds the minimum value for an array recursive function realmin ( ain ) result ( themin ) ! recursive and result are required for recursive functions use numz implicit none real ( b8 ) themin , t1 , t2 integer n , right real ( b8 ) , dimension (:) :: ain n = size ( ain ) if ( n == 1 ) then themin = ain ( 1 ) ! if the size is 1 return value return else right = n / 2 t1 = realmin ( ain ( 1 : right )) ! find min in left half t2 = realmin ( ain ( right + 1 : n )) ! find min in right half themin = min ( t1 , t2 ) ! find min of the two sides endif end function Example 2 is the same except the input data is our derived data type !this routine works with the data structure thefit not reals recursive function typemin ( ain ) result ( themin ) use numz use sort_mod use galapagos implicit none real ( b8 ) themin , t1 , t2 integer n , right type ( thefit ) , dimension (:) :: ain ! this line is different n = size ( ain ) if ( n == 1 ) then themin = ain ( 1 )% val ! this line is different return else right = n / 2 t1 = typemin ( ain ( 1 : right )) t2 = typemin ( ain ( right + 1 : n )) themin = min ( t1 , t2 ) endif end function Pointers Motivation Can increase performance Can improve readability Required for some derived data types (linked lists and trees) Useful for allocating \"arrays\" within subroutines Useful for referencing sections of arrays Notes Pointers can be thought of as an alias to another variable In some cases can be used in place of an array To assign a pointer use => instead of just = Unlike C and C++, pointer arithmetic is not allowed First pointer example Similar to the last findmin routine Return a pointer to the minimum recursive function pntmin ( ain ) result ( themin ) ! return a pointer use numz use galapagos use sort_mod ! contains the .lt. operator for thefit type implicit none type ( thefit ), pointer :: themin , t1 , t2 integer n , right type ( thefit ) , dimension (:), target :: ain n = size ( ain ) if ( n == 1 ) then themin => ain ( 1 ) !this is how we do pointer assignment return else right = n / 2 t1 => pntmin ( ain ( 1 : right )) t2 => pntmin ( ain ( right + 1 : n )) if ( t1 . lt . t2 ) then ; themin => t1 ; else ; themin => t2 ; endif endif end function Exercise 4: Carefully write a recursive N! program. Function and subroutine overloading Motivation Allows us to call functions or subroutine with the same name with different argument types Increases readability Notes: Similar in concept to operator overloading Requires an interface Syntax for subroutines is same as for functions Many intrinsic functions have this capability - abs (reals,complex,integer) - sin,cos,tan,exp(reals, complex) - array functions(reals, complex,integer) Example - Recall we had two functions that did the same thing but with different argument types recursive function realmin(ain) result (themin) real(b8) ,dimension(:) :: ain recursive function typemin(ain) result (themin) type (thefit) ,dimension(:) :: ain - We can define a generic interface for these two functions and call them using the same name ! note we have two functions within the same interface ! this is how we indicate function overloading ! both functions are called \"findmin\" in the main program interface findmin ! the first is called with an array of reals as input recursive function realmin ( ain ) result ( themin ) use numz real ( b8 ) themin real ( b8 ) , dimension (:) :: ain end function ! the second is called with a array of data structures as input recursive function typemin ( ain ) result ( themin ) use numz use galapagos real ( b8 ) themin type ( thefit ) , dimension (:) :: ain end function end interface Example usage program darwin use numz use ran_mod use galapagos ! the module that contains the type definition use face ! contains various interfaces use sort_mod ! more about this later it ! contains our sorting routine ! and a few other tricks implicit none ! create an allocatable array of the data type ! than contains an index and a real value type ( thefit ), allocatable , target :: results (:) ! create a single instance of the data type type ( thefit ) best ! pointers to our type type ( thefit ) , pointer :: worst , tmp integer , allocatable :: genes (:,:) ! our genes for the ga integer j integer num_genes , gene_size real ( b8 ) x real ( b8 ), allocatable :: z (:) real ( b8 ), pointer :: xyz (:) ! we'll talk about this next num_genes = 10 gene_size = 10 allocate ( results ( num_genes )) ! allocate the data type to allocate ( genes ( num_genes , gene_size )) ! hold our collection of genes call init_genes ( genes ) ! starting data write ( * , '(\"input\")' ) do j = 1 , num_genes results ( j )% index = j results ( j )% val = fitness ( genes ( j ,:)) ! just a dummy routine write ( * , \"(f10.8,i4)\" ) results ( j )% val , results ( j )% index enddo allocate ( z ( size ( results ))) z = results (:)% val ! copy our results to a real array ! use a recursive subroutine operating on the real array write ( * , * ) \"the lowest fitness: \" , findmin ( z ) ! use a recursive subroutine operating on the data structure write ( * , * ) \"the lowest fitness: \" , findmin ( results ) end program Fortran Minval and Minloc routines Fortran has routines for finding minimum and maximum values in arrays and the locations minval maxval minloc (returns an array) maxloc (returns an array) ! we show two other methods of getting the minimum fitness ! use the built in f90 routines on a real array write ( * , * ) \"the lowest fitness: \" , minval ( z ), minloc ( z ) Pointer assignment This is how we use the pointer function defined above worst is a pointer to our data type note the use of => ! use a recursive subroutine operating on the data ! structure and returning a pointer to the result worst=>pntmin(results) ! note pointer assignment ! what will this line write? write(*,*)\"the lowest fitness: \",worst More pointer usage, association and nullify Motivation Need to find if pointers point to anything Need to find if two pointers point to the same thing Need to deallocate and nullify when they are no longer used Usage We can use associated() to tell if a pointer has been set We can use associated() to compare pointers We use nullify to zero a pointer ! This code will print \"true\" when we find a match, ! that is the pointers point to the same object do j = 1 , num_genes tmp => results ( j ) write ( * , \"(f10.8,i4,l3)\" ) results ( j )% val , & results ( j )% index , & associated ( tmp , worst ) enddo nullify ( tmp ) Notes: If a pointer is nullified the object to which it points is not deallocated. In general, pointers as well as allocatable arrays become undefined on leaving a subroutine This can cause a memory leak Pointer usage to reference an array without copying Motivation Our sort routine calls a recursive sorting routine It is messy and inefficient to pass the array to the recursive routine Solution We define a \"global\" pointer in a module We point the pointer to our input array module Merge_mod_types use galapagos type ( thefit ), allocatable :: work (:) ! a \"global\" work array type ( thefit ), pointer :: a_pntr (:) ! this will be the pointer to our input array end module Merge_mod_types subroutine Sort ( ain , n ) use Merge_mod_types implicit none integer n type ( thefit ), target :: ain ( n ) allocate ( work ( n )) nullify ( a_pntr ) a_pntr => ain ! we assign the pointer to our array ! in RecMergeSort we reference it just like an array call RecMergeSort ( 1 , n ) ! very similar to the findmin functions deallocate ( work ) return end subroutine Sort In our main program sort is called like this: ! our sort routine is also recursive but ! also shows a new usage for pointers call sort ( results , num_genes ) do j = 1 , num_genes write ( * , \"(f10.8,i4)\" ) results ( j )% val , & results ( j )% index enddo Data assignment with structures ! we can copy a whole structure ! with a single assignment best = results ( 1 ) write ( * , * ) \"best result \" , best Using the user defined operator ! using the user defined operator to see if best is worst ! recall that the operator .converged. checks to see if %index matches worst => pntmin ( results ) write ( * , * ) \"worst result \" , worst write ( * , * ) \"converged=\" ,( best . converged . worst ) Passing arrays with a given arbitrary lower bounds Motivation Default lower bound within a subroutine is 1 May want to use a different lower bound if ( allocated ( z )) deallocate ( z ) allocate ( z ( - 10 : 10 )) ! a 21 element array do j =- 10 , 10 z ( j ) = j enddo ! pass z and its lower bound ! in this routine we give the array a specific lower ! bound and show how to use a pointer to reference ! different parts of an array using different indices call boink1 ( z , lbound ( z , 1 )) ! why not just lbound(z) instead of lbound(z,1)? ! lbound(z) returns a rank 1 array subroutine boink1 ( a , n ) use numz implicit none integer , intent ( in ) :: n real ( b8 ), dimension ( n :) :: a ! this is how we set lower bounds in a subroutine write ( * , * ) lbound ( a ), ubound ( a ) end subroutine Warning: because we are using an assumed shape array we need an interface Using pointers to access sections of arrays Motivation Can increase efficiency Can increase readability call boink2 ( z , lbound ( z , 1 )) subroutine boink2 ( a , n ) use numz implicit none integer , intent ( in ) :: n real ( b8 ), dimension ( n :), target :: a real ( b8 ), dimension (:), pointer :: b b => a ( n :) ! b(1) \"points\" to a(-10) write ( * , * ) \"a(-10) =\" , a ( - 10 ), \"b(1) =\" , b ( 1 ) b => a ( 0 :) ! b(1) \"points\" to a(0) write ( * , * ) \"a(-6) =\" , a ( - 6 ), \"b(-5) =\" , b ( - 5 ) end subroutine Allocating an array inside a subroutine and passing it back Motivation Size of arrays are calculated in the subroutine module numz integer , parameter :: b8 = selected_real_kind ( 14 ) end module program bla use numz real ( b8 ), dimension (:) , pointer :: xyz interface boink subroutine boink ( a ) use numz implicit none real ( b8 ), dimension (:), pointer :: a end subroutine end interface nullify ( xyz ) ! nullify sets a pointer to null write ( * , '(l5)' ) associated ( xyz ) ! is a pointer null, should be call boink ( xyz ) write ( * , '(l5)' , advance = \"no\" ) associated ( xyz ) if ( associated ( xyz )) write ( * , '(i5)' ) size ( xyz ) end program subroutine boink ( a ) use numz implicit none real ( b8 ), dimension (:), pointer :: a if ( associated ( a )) deallocate ( a ) allocate ( a ( 10 )) end subroutine An example run F T 10 Our fitness function Given a fixed number of colors, M, and a description of a map of a collection of N states. Find a coloring of the map such that no two states that share a boarder have the same coloring. Example input is a sorted list of 22 western states 22 ar ok tx la mo xx az ca nm ut nv xx ca az nv or xx co nm ut wy ne ks xx ia mo ne sd mn xx id wa or nv ut wy mt xx ks ne co ok mo xx la tx ar xx mn ia sd nd xx mo ar ok ks ne ia xx mt wy id nd xx nd mt sd wy xx ne sd wy co ks mo ia xx nm az co ok tx mn xx nv ca or id ut az xx ok ks nm tx ar mo xx or ca wa id xx sd nd wy ne ia mn xx tx ok nm la ar xx ut nv az co wy id xx wa id or mt xx wy co mt id ut nd sd ne xx Our fitness function takes a potential coloring, that is, an integer vector of length N and a returns the number of boarders that have states of the same coloring How do we represent the map in memory? One way would be to use an array but it would be very sparse Linked lists are often a better way Linked lists Motivation We have a collection of states and for each state a list of adjoining states. (Do not count a boarder twice.) Problem is that you do not know the length of the list until runtime. List of adjoining states will be different lengths for different states Solution - Linked list are a good way to handle such situations Linked lists use a derived data type with at least two components Data Pointer to next element module list_stuff type llist integer index ! data type ( llist ), pointer :: next ! pointer to the ! next element end type llist end module Linked list usage One way to fill a linked list is to use a recursive function `fortran recursive subroutine insert (item, root) use list_stuff implicit none type(llist), pointer :: root integer item if (.not. associated(root)) then allocate(root) nullify(root%next) root%index = item else call insert(item,root%next) endif end subroutine - - - - - - ## Our map representation - An array of the derived data type states - State is name of a state - Linked list containing boarders ```fortran type states character(len=2)name type(llist),pointer:: list end type states - Notes: - We have an array of linked lists - This data structure is often used to represent sparse arrays - We could have a linked list of linked lists - State name is not really required Date and time functions Motivation May want to know the date and time of your program Two functions ! all arguments are optional call date_and_time ( date = c_date , & ! character(len=8) ccyymmdd time = c_time , & ! character(len=10) hhmmss.sss zone = c_zone , & ! character(len=10) +/-hhmm (time zone) values = ivalues ) ! integer ivalues(8) all of the above call system_clock ( count = ic , & ! count of system clock (clicks) count_rate = icr , & ! clicks / second count_max = max_c ) ! max value for count Non advancing and character IO Motivation We read the states using the two character identification One line per state and do not know how many boarder states per line Note: Our list of states is presorted character ( len = 2 ) a ! we have a character variable of length 2 read ( 12 , * ) nstates ! read the number of states allocate ( map ( nstates )) ! and allocate our map do i = 1 , nstates read ( 12 , \"(a2)\" , advance = \"no\" ) map ( i )% name ! read the name !write(*,*)\"state:\",map(i)%name nullify ( map ( i )% list ) ! \"zero out\" our list do read ( 12 , \"(1x,a2)\" , advance = \"no\" ) a ! read list of states ! without going to the ! next line if ( lge ( a , \"xx\" ) . and . lle ( a , \"xx\" )) then ! if state == xx backspace ( 12 ) ! go to the next line read ( 12 , \"(1x,a2)\" , end = 1 ) a ! go to the next line exit endif 1 continue if ( llt ( a , map ( i )% name )) then ! we only add a state to ! our list if its name ! is before ours thus we ! only count boarders 1 time ! what we want put into our linked list is an index ! into our map where we find the bordering state ! thus we do the search here ! any ideas on a better way of doing this search? found =- 1 do j = 1 , i - 1 if ( lge ( a , map ( j )% name ) . and . lle ( a , map ( j )% name )) then !write(*,*)a found = j exit endif enddo if ( found == - 1 ) then write ( * , * ) \"error\" stop endif ! found the index of the boarding state insert it into our list ! note we do the insert into the linked list for a particular state call insert ( found , map ( i )% list ) endif enddo enddo Internal IO Motivation May need to create strings on the fly May need to convert from strings to reals and integers Similar to sprintf and sscanf How it works Create a string Do a normal write except write to the string instead of file number Example 1: creating a date and time stamped file name character ( len = 12 ) tmpstr write ( tmpstr , \"(a12)\" )( c_date ( 5 : 8 ) // c_time ( 1 : 4 ) // \".dat\" ) ! // does string concatination write ( * , * ) \"name of file= \" , tmpstr open ( 14 , file = tmpstr ) name of file = 0327111 4. dat Example 2: Creating a format statement at run time (array of integers and a real) ! test_vect is an array that we do not know its length until run time nstate = 9 ! the size of the array write ( fstr , '(\"(\",i4,\"i1,1x,f10.5)\")' ) nstates write ( * , * ) \"format= \" , fstr write ( * , fstr ) test_vect , fstr format = ( 9 i1 , 1 x , f10 . 5 ) Any other ideas for writing an array when you do not know its length? Example 3: Reading from a string integer ht , minut , sec read ( c_time , \"(3i2)\" ) hr , minut , sec Inquire function Motivation Need to get information about I/O Inquire statement has two forms Information about files (23 different requests can be done) Information about space required for binary output of a value Example: find the size of your real relative to the \"standard\" real Useful for inter language programming Useful for determining data types in MPI (MPI_REAL or MPI_DOUBLE_PRECISION) inquire ( iolength = len_real ) 1.0 inquire ( iolength = len_b8 ) 1.0_b8 write ( * , * ) \"len_b8 \" , len_b8 write ( * , * ) \"len_real\" , len_real iratio = len_b8 / len_real select case ( iratio ) case ( 1 ) my_mpi_type = mpi_real case ( 2 ) my_mpi_type = mpi_double_precision case default write ( * , * ) \"type undefined\" my_mpi_type = 0 end select An example run len_b8 2 len_real 1 Namelist Now part of the standard Motivation A convenient method of doing I/O Good for cases where you have similar runs but change one or two variables Good for formatted output Notes: A little flaky No options for overloading format Example: integer ncolor logical force namelist / the_input / ncolor , force ncolor = 4 force = . true . read ( 13 , the_input ) write ( * , the_input ) On input: & THE_INPUT NCOLOR=4,FORCE = F / Output is &THE_INPUT NCOLOR = 4, FORCE = F / Vector valued functions Motivation May want a function that returns a vector Notes Again requires an interface Use explicit or assumed size array Do not return a pointer to a vector unless you really want a pointer Example: Take an integer input vector which represents an integer in some base and add 1 Could be used in our program to find a \"brute force\" solution function add1 ( vector , max ) result ( rtn ) integer , dimension (:), intent ( in ) :: vector integer , dimension ( size ( vector )) :: rtn integer max integer len logical carry len = size ( vector ) rtn = vector i = 0 carry = . true . do while ( carry ) ! just continue until we do not do a carry i = i + 1 rtn ( i ) = rtn ( i ) + 1 if ( rtn ( i ) . gt . max ) then if ( i == len ) then ! role over set everything back to 0 rtn = 0 else rtn ( i ) = 0 endif else carry = . false . endif enddo end function Usage test_vect = 0 do test_vect = add1 ( test_vect , 3 ) result = fitness ( test_vect ) if ( result . lt . 1.0_b8 ) then write ( * , * ) test_vect stop endif enddo Complete source for recent discussions recent.f90 fort.13 Exersize 5 Modify the program to use the random number generator given earlier. Some array specific intrinsic functions ALL True if all values are true (LOGICAL) ANY True if any value is true (LOGICAL) COUNT Number of true elements in an array (LOGICAL) DOT_PRODUCT Dot product of two rank one arrays MATMUL Matrix multiplication MAXLOC Location of a maximum value in an array MAXVAL Maximum value in an array MINLOC Location of a minimum value in an array MINVAL Minimum value in an array PACK Pack an array into an array of rank one PRODUCT Product of array elements RESHAPE Reshape an array SPREAD Replicates array by adding a dimension SUM Sum of array elements TRANSPOSE Transpose an array of rank two UNPACK Unpack an array of rank one into an array under a mask Examples program matrix real w ( 10 ), x ( 10 ), mat ( 10 , 10 ) call random_number ( w ) call random_number ( mat ) x = matmul ( w , mat ) ! regular matrix multiply USE IT write ( * , '(\"dot(x,x)=\",f10.5)' ), dot_product ( x , x ) end program program allit character ( len = 10 ) :: f1 = \"(3l1)\" character ( len = 10 ) :: f2 = \"(3i2)\" integer b ( 2 , 3 ), c ( 2 , 3 ), one_d ( 6 ) logical l ( 2 , 3 ) one_d = ( / 1 , 3 , 5 , 2 , 4 , 6 / ) b = transpose ( reshape (( / 1 , 3 , 5 , 2 , 4 , 6 / ), shape = ( / 3 , 2 / ))) C = transpose ( reshape (( / 0 , 3 , 5 , 7 , 4 , 8 / ), shape = ( / 3 , 2 / ))) l = ( b . ne . c ) write ( * , f2 )(( b ( i , j ), j = 1 , 3 ), i = 1 , 2 ) write ( * , * ) write ( * , f2 )(( c ( i , j ), j = 1 , 3 ), i = 1 , 2 ) write ( * , * ) write ( * , f1 )(( l ( i , j ), j = 1 , 3 ), i = 1 , 2 ) write ( * , * ) write ( * , f1 ) all ( b . ne . C ) !is .false. write ( * , f1 ) all ( b . ne . C , DIM = 1 ) !is [.true., .false., .false.] write ( * , f1 ) all ( b . ne . C , DIM = 2 ) !is [.false., .false.] end The output is: 1 3 5 2 4 6 0 3 5 7 4 8 TFF TFT F TFF FF The rest of our GA Includes Reproduction Mutation Nothing new in either of these files Source and makefile \"git\" Source and makefile \"*tgz\" Compiler Information gfortran .f, .for, .ftn .f77 fixed-format Fortran source; compile .f90, .f95 free-format Fortran source; compile -fbacktrace Add debug information for runtime traceback -ffree-form -ffixed-form source form -O0, -O1, -O2, -O3 optimization level .fpp, .FPP, .F, .FOR, .FTN, .F90, .F95, .F03 or .F08 Fortran source file with preprocessor directives -fopenmp turn on OpenMP Intel .f, .for, .ftn fixed-format Fortran source; compile .f90, .f95 free-format Fortran source; compile -O0, -O1, -O2, -O3, -O4 optimization level .fpp, .F, .FOR, .FTN, .FPP, .F90 Fortran source file with preprocessor directives -g compile for debug * -traceback -notraceback (default) Add debug information for runtime traceback -nofree, -free Source is fixed or free format -fopenmp turn on OpenMP Portland Group (x86) .f, .for, .ftn fixed-format Fortran source; compile .f90, .f95, .f03 free-format Fortran source; compile .cuf free-format CUDA Fortran source; compile .CUF free-format CUDA Fortran source; preprocess, compile -O0, -O1, -O2, -O3, -O4 optimization level -g compile for debug * -traceback (default) -notraceback Add debug information for runtime traceback -Mfixed, -Mfree Source is fixed or free format -qmp turn on OpenMP IBM xlf xlf, xlf_r, f77, fort77 Compile FORTRAN 77 source files. _r = thread safe xlf90, xlf90_r, f90 Compile Fortran 90 source files. _r = thread safe xlf95, xlf95_r, f95 Compile Fortran 95 source files. _r = thread safe xlf2003, xlf2003_r,f2003 * Compile Fortran 2003 source files. _r = thread safe xlf2008, xlf2008_r, f2008 * Compile Fortran 2008 source files. .f, .f77, .f90, .f95, .f03, .f08 Fortran source file .F, .F77, .F90, .F95, .F03, .F08 Fortran source file with preprocessor directives -qtbtable=full Add debug information for runtime traceback -qsmp=omp turn on OpenMP -O0, -O1, -O2, -O3, -O4, O5 optimization level -g , g0, g1,...g9 compile for debug Summary Fortran 90 has features to: Enhance performance Enhance portability Enhance reliability Enhance maintainability Fortran 90 has new language elements Source form Derived data types Dynamic memory allocation functions Kind facility for portability and easy modification Many new intrinsic function Array assignments Examples Help show how things work Reference for future use Overview of F90 Introduction to Fortran Language Meta language used in this compact summary Structure of files that can be compiled Executable Statements and Constructs Declarations Key words - other than I/O Key words related to I/O Operators Constants Input/Output Statements Formats Intrinsic Functions Introduction to Fortran Language Brought to you by ANSI committee X3J3 and ISO-IEC/JTC1/SC22/WG5 (Fortran) This is neither complete nor precisely accurate, but hopefully, after a small investment of time it is easy to read and very useful. This is the free form version of Fortran, no statement numbers, no C in column 1, start in column 1 (not column 7), typically indent 2, 3, or 4 spaces per each structure. The typical extension is .f90 . Continue a statement on the next line by ending the previous line with an ampersand &amp; . Start the continuation with &amp; for strings. The rest of any line is a comment starting with an exclamation mark ! . Put more than one statement per line by separating statements with a semicolon ; . Null statements are OK, so lines can end with semicolons. Separate words with space or any form of \"white space\" or punctuation. Meta language used in this compact summary <xxx> means fill in something appropriate for xxx and do not type the \"<\" or \">\" . ... ellipsis means the usual, fill in something, one or more lines [stuff] means supply nothing or at most one copy of \"stuff\" [stuff1 [stuff2]] means if \"stuff1\" is included, supply nothing or at most one copy of stuff2. \"old\" means it is in the language, like almost every feature of past Fortran standards, but should not be used to write new programs. Structure of files that can be compiled program <name> usually file name is <name>.f90 use <module_name> bring in any needed modules implicit none good for error detection <declarations> <executable statements> order is important, no more declarations end program <name> block data <name> old <declarations> common, dimension, equivalence now obsolete end block data <name> module <name> bring back in with use <name> implicit none good for error detection <declarations> can have private and public and interface end module <name> subroutine <name> use: call <name> to execute implicit none good for error detection <declarations> <executable statements> end subroutine <name> subroutine <name>(par1, par2, ...) use: call <name>(arg1, arg2,... ) to execute implicit none optional, good for error detection <declarations> par1, par2, ... are defined in declarations and can be specified in, inout, pointer, etc. <executable statements> return optional, end causes automatic return entry <name> (par...) old, optional other entries end subroutine <name> function <name>(par1, par2, ...) result(<rslt>) use: <name>(arg1, arg2, ... argn) as variable implicit none optional, good for error detection <declarations> rslt, par1, ... are defined in declarations <executable statements> <rslt> = <expression> required somewhere in execution [return] optional, end causes automatic return end function <name> old <type> function(...) <name> use: <name>(arg1, arg2, ... argn) as variable <declarations> <executable statements> <name> = <expression> required somewhere in execution [return] optional, end causes automatic return end function <name> Executable Statements and Constructs <statement> will mean exactly one statement in this section a construct is multiple lines <label> : <statement> any statement can have a label (a name) <variable> = <expression> assignment statement <pointer> >= <variable> the pointer is now an alias for the variable <pointer1> >= <pointer2> pointer1 now points same place as pointer2 stop can be in any executable statement group, stop <integer> terminates execution of the program, stop <string> can have optional integer or string return exit from subroutine or function do <variable>=<from>,<to> [,<increment&gt] optional: <label> : do ... <statements> exit \\_optional or exit <label&gt if (<boolean expression>) exit / exit the loop cycle \\_optional or cycle <label> if (<boolean expression>) cycle / continue with next loop iteration end do optional: end do <name> do while (<boolean expression>) ... optional exit and cycle allowed end do do ... exit required to end the loop optional cycle can be used end do if ( <boolean expression> ) <statement> execute the statement if the boolean expression is true if ( <boolean expression1> ) then ... execute if expression1 is true else if ( <boolean expression2> ) then ... execute if expression2 is true else if ( <boolean expression3> ) then ... execute if expression3 is true else ... execute if none above are true end if select case (<expression>) optional <name> : select case ... case (<value>) <statements> execute if expression == value case (<value1>:<value2>) <statements> execute if value1 &le; expression &le; value2 ... case default <statements> execute if no values above match end select optional end select <name> real, dimension(10,12) :: A, R a sample declaration for use with \"where\" ... where (A /= 0.0) conditional assignment, only assignment allowed R = 1.0/A elsewhere R = 1.0 elements of R set to 1.0 where A == 0.0 end where go to <statement number> old go to (<statement number list>), <expression> old for I/O statements, see: section 10.0 Input/Output Statements many old forms of statements are not listed Declarations There are five (5) basic types: integer, real, complex, character and logical. There may be any number of user derived types. A modern (not old) declaration starts with a type, has attributes, then ::, then variable(s) names integer i, pivot, query old integer, intent (inout) :: arg1 integer (selected_int_kind (5)) :: i1, i2 integer, parameter :: m = 7 integer, dimension(0:4, -5:5, 10:100) :: A3D double precision x old real (selected_real_kind(15,300) :: x complex :: z logical, parameter :: what_if = .true. character, parameter :: me = \"Jon Squire\" type <name> a new user type, derived type declarations end type <name> type (<name>) :: stuff declaring stuff to be of derived type <name> real, dimension(:,:), allocatable, target :: A real, dimension(:,:), pointer :: P Attributes may be: allocatable no memory used here, allocate later dimension vector or multi dimensional array external will be defined outside this compilation intent argument may be in, inout or out intrinsic declaring function to be an intrinsic optional argument is optional parameter declaring a constant, can not be changed later pointer declaring a pointer private in a module, a private declaration public in a module, a public declaration save keep value from one call to the next, static target can be pointed to by a pointer Note: not all combinations of attributes are legal Key words (other than I/O) note: \"statement\" means key word that starts a statement, one line unless there is a continuation \"&amp;\" \"construct\" means multiple lines, usually ending with \"end ...\" \"attribute\" means it is used in a statement to further define \"old\" means it should not be used in new code allocatable attribute, no space allocated here, later allocate allocate statement, allocate memory space now for variable assign statement, old, assigned go to assignment attribute, means subroutine is assignment (=) block data construct, old, compilation unit, replaced by module call statement, call a subroutine case statement, used in select case structure character statement, basic type, intrinsic data type common statement, old, allowed overlaying of storage complex statement, basic type, intrinsic data type contains statement, internal subroutines and functions follow continue statement, old, a place to put a statement number cycle statement, continue the next iteration of a do loop data statement, old, initialized variables and arrays deallocate statement, free up storage used by specified variable default statement, in a select case structure, all others do construct, start a do loop double precision statement, old, replaced by selected_real_kind(15,300) else construct, part of if else if else end if else if construct, part of if else if else end if elsewhere construct, part of where elsewhere end where end block data construct, old, ends block data end do construct, ends do end function construct, ends function end if construct, ends if end interface construct, ends interface end module construct, ends module end program construct, ends program end select construct, ends select case end subroutine construct, ends subroutine end type construct, ends type end where construct, ends where entry statement, old, another entry point in a procedure equivalence statement, old, overlaid storage exit statement, continue execution outside of a do loop external attribute, old statement, means defines else where function construct, starts the definition of a function go to statement, old, requires fixed form statement number if statement and construct, if(...) statement implicit statement, \"none\" is preferred to help find errors in a keyword for intent, the argument is read only inout a keyword for intent, the argument is read/write integer statement, basic type, intrinsic data type intent attribute, intent(in) or intent(out) or intent(inout) interface construct, begins an interface definition intrinsic statement, says that following names are intrinsic kind attribute, sets the kind of the following variables len attribute, sets the length of a character string logical statement, basic type, intrinsic data type module construct, beginning of a module definition namelist statement, defines a namelist of input/output nullify statement, nullify(some_pointer) now points nowhere only attribute, restrict what comes from a module operator attribute, indicates function is an operator, like + optional attribute, a parameter or argument is optional out a keyword for intent, the argument will be written parameter attribute, old statement, makes variable real only pause old, replaced by stop pointer attribute, defined the variable as a pointer alias private statement and attribute, in a module, visible inside program construct, start of a main program public statement and attribute, in a module, visible outside real statement, basic type, intrinsic data type recursive attribute, allows functions and derived type recursion result attribute, allows naming of function result result(Y) return statement, returns from, exits, subroutine or function save attribute, old statement, keep value between calls select case construct, start of a case construct stop statement, terminate execution of the main procedure subroutine construct, start of a subroutine definition target attribute, allows a variable to take a pointer alias then part of if construct type construct, start of user defined type type ( ) statement, declaration of a variable for a users type use statement, brings in a module where construct, conditional assignment while construct, a while form of a do loop Key words related to I/O backspace statement, back up one record close statement, close a file endfile statement, mark the end of a file format statement, old, defines a format inquire statement, get the status of a unit open statement, open or create a file print statement, performs output to screen read statement, performs input rewind statement, move read or write position to beginning write statement, performs output Operators ** exponentiation * multiplication / division + addition - subtraction // concatenation == .eq. equality /= .ne. not equal < .lt. less than > .gt. greater than <= .le. less than or equal >= .ge. greater than or equal .not. complement, negation .and. logical and .or. logical or .eqv. logical equivalence .neqv. logical not equivalence, exclusive or .eq. == equality, old .ne. /= not equal. old .lt. < less than, old .gt. > greater than, old .le. <= less than or equal, old .ge. >= greater than or equal, old Other punctuation: / ... / used in data, common, namelist and other statements (/ ... /) array constructor, data is separated by commas 6*1.0 in some contexts, 6 copies of 1.0 (i:j:k) in some contexts, a list i, i+k, i+2k, i+3k, ... i+nk&le;j (:j) j and all below (i:) i and all above (:) undefined or all in range Constants Logical constants: .true. True .false. False Integer constants: 0 1 -1 123456789 Real constants: 0.0 1.0 -1.0 123.456 7.1E+10 -52.715E-30 Complex constants: (0.0, 0.0) (-123.456E+30, 987.654E-29) Character constants: \"ABC\" \"a\" \"123'abc$%#@!\" \" a quote \"\" \" 'ABC' 'a' '123\"abc$%#@!' ' a apostrophe '' ' Derived type values: type name character (len=30) :: last character (len=30) :: first character (len=30) :: middle end type name type address character (len=40) :: street character (len=40) :: more character (len=20) :: city character (len=2) :: state integer (selected_int_kind(5)) :: zip_code integer (selected_int_kind(4)) :: route_code end type address type person type (name) lfm type (address) snail_mail end type person type (person) :: a_person = person( name(\"Squire\",\"Jon\",\"S.\"), &amp; address(\"106 Regency Circle\", \"\", \"Linthicum\", \"MD\", 21090, 1936)) a_person%snail_mail%route_code == 1936 Input/Output Statements open (<unit number>) open (unit=<unit number>, file=<file name>, iostat=<variable>) open (unit=<unit number>, ... many more, see below ) close (<unit number>) close (unit=<unit number>, iostat=<variable>, err=<statement number>, status=\"KEEP\") read (<unit number>) <input list> read (unit=<unit number>, fmt=<format>, iostat=<variable>, end=<statement number>, err=<statement number>) <input list> read (unit=<unit number>, rec=<record number>) <input list> write (<unit number>) <output list> write (unit=<unit number>, fmt=<format>, iostat=<variable>, err=<statement number>) <output list> write (unit=<unit number>, rec=<record number>) <output list> print *, <output list> print \"(<your format here, use apostrophe, not quote>)\", <output list> rewind <unit number> rewind (<unit number>, err=<statement number>) backspace <unit number> backspace (<unit number>, iostat=<variable>) endfile <unit number> endfile (<unit number>, err=<statement number>, iostat=<variable>) inquire ( <unit number>, exists = <variable>) inquire ( file=<\"name\">, opened = <variable1>, access = <variable2> ) inquire ( iolength = <variable> ) x, y, A ! gives \"recl\" for \"open\" namelist /<name>/ <variable list> defines a name list read(*,nml=<name>) reads some/all variables in namelist write(*,nml=<name>) writes all variables in namelist &amp;<name> <variable>=<value> ... <variable=value> / data for namelist read Input / Output specifiers access one of \"sequential\" \"direct\" \"undefined\" action one of \"read\" \"write\" \"readwrite\" advance one of \"yes\" \"no\" blank one of \"null\" \"zero\" delim one of \"apostrophe\" \"quote\" \"none\" end = <integer statement number> old eor = <integer statement number> old err = <integer statement number> old exist = <logical variable> file = <\"file name\"> fmt = <\"(format)\"> or <character variable> format form one of \"formatted\" \"unformatted\" \"undefined\" iolength = <integer variable, size of unformatted record> iostat = <integer variable> 0==good, negative==eof, positive==bad name = <character variable for file name> named = <logical variable> nml = <namelist name> nextrec = <integer variable> one greater than written number = <integer variable unit number> opened = <logical variable> pad one of \"yes\" \"no\" position one of \"asis\" \"rewind\" \"append\" rec = <integer record number> recl = <integer unformatted record size> size = <integer variable> number of characters read before eor status one of \"old\" \"new\" \"unknown\" \"replace\" \"scratch\" \"keep\" unit = <integer unit number> Individual questions direct = <character variable> \"yes\" \"no\" \"unknown\" formatted = <character variable> \"yes\" \"no\" \"unknown\" read = <character variable> \"yes\" \"no\" \"unknown\" readwrite = <character variable> \"yes\" \"no\" \"unknown\" sequential = <character variable> \"yes\" \"no\" \"unknown\" unformatted = <character variable> \"yes\" \"no\" \"unknown\" write = <character variable> \"yes\" \"no\" \"unknown\" Formats format an explicit format can replace * in any I/O statement. Include the format in apostrophes or quotes and keep the parenthesis. examples: print \"(3I5,/(2X,3F7.2/))\", <output list> write(6, '(a,E15.6E3/a,G15.2)' ) <output list> read(unit=11, fmt=\"(i4, 4(f3.0,TR1))\" ) <input list> A format includes the opening and closing parenthesis. A format consists of format items and format control items separated by comma. A format may contain grouping parenthesis with an optional repeat count. Format Items, data edit descriptors: key: w is the total width of the field (filled with *** if overflow) m is the least number of digits in the (sub)field (optional) d is the number of decimal digits in the field e is the number of decimal digits in the exponent subfield c is the repeat count for the format item n is number of columns cAw data of type character (w is optional) cBw.m data of type integer with binary base cDw.d data of type real -- same as E, old double precision cEw.d or Ew.dEe data of type real cENw.d or ENw.dEe data of type real -- exponent a multiple of 3 cESw.d or ESw.dEe data of type real -- first digit non zero cFw.d data of type real -- no exponent printed cGw.d or Gw.dEe data of type real -- auto format to F or E nH n characters follow the H, no list item cIw.m data of type integer cLw data of type logical -- .true. or .false. cOw.m data of type integer with octal base cZw.m data of type integer with hexadecimal base \"<string>\" literal characters to output, no list item '<string>' literal characters to output, no list item Format Control Items, control edit descriptors: BN ignore non leading blanks in numeric fields BZ treat nonleading blanks in numeric fields as zeros nP apply scale factor to real format items old S printing of optional plus signs is processor dependent SP print optional plus signs SS do not print optional plus signs Tn tab to specified column TLn tab left n columns TRn tab right n columns nX tab right n columns / end of record (implied / at end of all format statements) : stop format processing if no more list items <input list> can be: a variable an array name an implied do ((A(i,j),j=1,n) ,i=1,m) parenthesis and commas as shown note: when there are more items in the input list than format items, the repeat rules for formats applies. <output list> can be: a constant a variable an expression an array name an implied do ((A(i,j),j=1,n) ,i=1,m) parenthesis and commas as shown note: when there are more items in the output list than format items, the repeat rules for formats applies. Repeat Rules for Formats: Each format item is used with a list item. They are used in order. When there are more list items than format items, then the following rule applies: There is an implied end of record, /, at the closing parenthesis of the format, this is processed. Scan the format backwards to the first left parenthesis. Use the repeat count, if any, in front of this parenthesis, continue to process format items and list items. Note: an infinite loop is possible print \"(3I5/(1X/))\", I, J, K, L may never stop Intrinsic Functions Intrinsic Functions are presented in alphabetical order and then grouped by topic. The function name appears first. The argument(s) and result give an indication of the type(s) of argument(s) and results. [,dim=] indicates an optional argument \"dim\". \"mask\" must be logical and usually conformable. \"character\" and \"string\" are used interchangeably. A brief description or additional information may appear. Intrinsic Functions (alphabetical): abs(integer_real_complex) result(integer_real_complex) achar(integer) result(character) integer to character acos(real) result(real) arccosine |real| &le; 1.0 0&le;result&le;Pi adjustl(character) result(character) left adjust, blanks go to back adjustr(character) result(character) right adjust, blanks to front aimag(complex) result(real) imaginary part aint(real [,kind=]) result(real) truncate to integer toward zero all(mask [,dim]) result(logical) true if all elements of mask are true allocated(array) result(logical) true if array is allocated in memory anint(real [,kind=]) result(real) round to nearest integer any(mask [,dim=}) result(logical) true if any elements of mask are true asin(real) result(real) arcsine |real| &le; 1.0 -Pi/2&le;result&le;Pi/2 associated(pointer [,target=]) result(logical) true if pointing atan(real) result(real) arctangent -Pi/2&le;result&le;Pi/2 atan2(y=real,x=real) result(real) arctangent -Pi&le;result&le;Pi bit_size(integer) result(integer) size in bits in model of argument btest(i=integer,pos=integer) result(logical) true if pos has a 1, pos=0.. ceiling(real) result(real) truncate to integer toward infinity char(integer [,kind=]) result(character) integer to character [of kind] cmplx(x=real [,y=real] [kind=]) result(complex) x+iy conjg(complex) result(complex) reverse the sign of the imaginary part cos(real_complex) result(real_complex) cosine cosh(real) result(real) hyperbolic cosine count(mask [,dim=]) result(integer) count of true entries in mask cshift(array,shift [,dim=]) circular shift elements of array, + is right date_and_time([date=] [,time=] [,zone=] [,values=]) y,m,d,utc,h,m,s,milli dble(integer_real_complex) result(real_kind_double) convert to double digits(integer_real) result(integer) number of bits to represent model dim(x=integer_real,y=integer_real) result(integer_real) proper subtraction dot_product(vector_a,vector_b) result(integer_real_complex) inner product dprod(x=real,y=real) result(x_times_y_double) double precision product eoshift(array,shift [,boundary=] [,dim=]) end-off shift using boundary epsilon(real) result(real) smallest positive number added to 1.0 /= 1.0 exp(real_complex) result(real_complex) e raised to a power exponent(real) result(integer) the model exponent of the argument floor(real) result(real) truncate to integer towards negative infinity fraction(real) result(real) the model fractional part of the argument huge(integer_real) result(integer_real) the largest model number iachar(character) result(integer) position of character in ASCII sequence iand(integer,integer) result(integer) bit by bit logical and ibclr(integer,pos) result(integer) argument with pos bit cleared to zero ibits(integer,pos,len) result(integer) extract len bits starting at pos ibset(integer,pos) result(integer) argument with pos bit set to one ichar(character) result(integer) pos in collating sequence of character ieor(integer,integer) result(integer) bit by bit logical exclusive or index(string,substring [,back=]) result(integer) pos of substring int(integer_real_complex) result(integer) convert to integer ior(integer,integer) result(integer) bit by bit logical or ishft(integer,shift) result(integer) shift bits in argument by shift ishftc(integer, shift) result(integer) shift circular bits in argument kind(any_intrinsic_type) result(integer) value of the kind lbound(array,dim) result(integer) smallest subscript of dim in array len(character) result(integer) number of characters that can be in argument len_trim(character) result(integer) length without trailing blanks lge(string_a,string_b) result(logical) string_a &ge; string_b lgt(string_a,string_b) result(logical) string_a > string_b lle(string_a,string_b) result(logical) string_a &le; string_b llt(string_a,string_b) result(logical) string_a < string_b log(real_complex) result(real_complex) natural logarithm log10(real) result(real) logarithm base 10 logical(logical [,kind=]) convert to logical matmul(matrix,matrix) result(vector_matrix) on integer_real_complex_logical max(a1,a2,a3,...) result(integer_real) maximum of list of values maxexponent(real) result(integer) maximum exponent of model type maxloc(array [,mask=]) result(integer_vector) indices in array of maximum maxval(array [,dim=] [,mask=]) result(array_element) maximum value merge(true_source,false_source,mask) result(source_type) choose by mask min(a1,a2,a3,...) result(integer-real) minimum of list of values minexponent(real) result(integer) minimum(negative) exponent of model type minloc(array [,mask=]) result(integer_vector) indices in array of minimum minval(array [,dim=] [,mask=]) result(array_element) minimum value mod(a=integer_real,p) result(integer_real) a modulo p modulo(a=integer_real,p) result(integer_real) a modulo p mvbits(from,frompos,len,to,topos) result(integer) move bits nearest(real,direction) result(real) nearest value toward direction nint(real [,kind=]) result(real) round to nearest integer value not(integer) result(integer) bit by bit logical complement pack(array,mask [,vector=]) result(vector) vector of elements from array present(argument) result(logical) true if optional argument is supplied product(array [,dim=] [,mask=]) result(integer_real_complex) product radix(integer_real) result(integer) radix of integer or real model, 2 random_number(harvest=real_out) subroutine, uniform random number 0 to 1 random_seed([size=] [,put=] [,get=]) subroutine to set random number seed range(integer_real_complex) result(integer_real) decimal exponent of model real(integer_real_complex [,kind=]) result(real) convert to real repeat(string,ncopies) result(string) concatenate n copies of string reshape(source,shape,pad,order) result(array) reshape source to array rrspacing(real) result(real) reciprocal of relative spacing of model scale(real,integer) result(real) multiply by 2**integer scan(string,set [,back]) result(integer) position of first of set in string selected_int_kind(integer) result(integer) kind number to represent digits selected_real_kind(integer,integer) result(integer) kind of digits, exp set_exponent(real,integer) result(real) put integer as exponent of real shape(array) result(integer_vector) vector of dimension sizes sign(integer_real,integer_real) result(integer_real) sign of second on first sin(real_complex) result(real_complex) sine of angle in radians sinh(real) result(real) hyperbolic sine of argument size(array [,dim=]) result(integer) number of elements in dimension spacing(real) result(real) spacing of model numbers near argument spread(source,dim,ncopies) result(array) expand dimension of source by 1 sqrt(real_complex) result(real_complex) square root of argument sum(array [,dim=] [,mask=]) result(integer_real_complex) sum of elements system_clock([count=] [,count_rate=] [,count_max=]) subroutine, all out tan(real) result(real) tangent of angle in radians tanh(real) result(real) hyperbolic tangent of angle in radians tiny(real) result(real) smallest positive model representation transfer(source,mold [,size]) result(mold_type) same bits, new type transpose(matrix) result(matrix) the transpose of a matrix trim(string) result(string) trailing blanks are removed ubound(array,dim) result(integer) largest subscript of dim in array unpack(vector,mask,field) result(v_type,mask_shape) field when not mask verify(string,set [,back]) result(integer) pos in string not in set Intrinsic Functions (grouped by topic): Intrinsic Functions (Numeric) abs(integer_real_complex) result(integer_real_complex) acos(real) result(real) arccosine |real| &le; 1.0 0&le;result&le;Pi aimag(complex) result(real) imaginary part aint(real [,kind=]) result(real) truncate to integer toward zero anint(real [,kind=]) result(real) round to nearest integer asin(real) result(real) arcsine |real| &le; 1.0 -Pi/2&le;result&le;Pi/2 atan(real) result(real) arctangent -Pi/2&le;result&le;Pi/2 atan2(y=real,x=real) result(real) arctangent -Pi&le;result&le;Pi ceiling(real) result(real) truncate to integer toward infinity cmplx(x=real [,y=real] [kind=]) result(complex) x+iy conjg(complex) result(complex) reverse the sign of the imaginary part cos(real_complex) result(real_complex) cosine cosh(real) result(real) hyperbolic cosine dble(integer_real_complex) result(real_kind_double) convert to double digits(integer_real) result(integer) number of bits to represent model dim(x=integer_real,y=integer_real) result(integer_real) proper subtraction dot_product(vector_a,vector_b) result(integer_real_complex) inner product dprod(x=real,y=real) result(x_times_y_double) double precision product epsilon(real) result(real) smallest positive number added to 1.0 /= 1.0 exp(real_complex) result(real_complex) e raised to a power exponent(real) result(integer) the model exponent of the argument floor(real) result(real) truncate to integer towards negative infinity fraction(real) result(real) the model fractional part of the argument huge(integer_real) result(integer_real) the largest model number int(integer_real_complex) result(integer) convert to integer log(real_complex) result(real_complex) natural logarithm log10(real) result(real) logarithm base 10 matmul(matrix,matrix) result(vector_matrix) on integer_real_complex_logical max(a1,a2,a3,...) result(integer_real) maximum of list of values maxexponent(real) result(integer) maximum exponent of model type maxloc(array [,mask=]) result(integer_vector) indices in array of maximum maxval(array [,dim=] [,mask=]) result(array_element) maximum value min(a1,a2,a3,...) result(integer-real) minimum of list of values minexponent(real) result(integer) minimum(negative) exponent of model type minloc(array [,mask=]) result(integer_vector) indices in array of minimum minval(array [,dim=] [,mask=]) result(array_element) minimum value mod(a=integer_real,p) result(integer_real) a modulo p modulo(a=integer_real,p) result(integer_real) a modulo p nearest(real,direction) result(real) nearest value toward direction nint(real [,kind=]) result(real) round to nearest integer value product(array [,dim=] [,mask=]) result(integer_real_complex) product radix(integer_real) result(integer) radix of integer or real model, 2 random_number(harvest=real_out) subroutine, uniform random number 0 to 1 random_seed([size=] [,put=] [,get=]) subroutine to set random number seed range(integer_real_complex) result(integer_real) decimal exponent of model real(integer_real_complex [,kind=]) result(real) convert to real rrspacing(real) result(real) reciprocal of relative spacing of model scale(real,integer) result(real) multiply by 2**integer set_exponent(real,integer) result(real) put integer as exponent of real sign(integer_real,integer_real) result(integer_real) sign of second on first sin(real_complex) result(real_complex) sine of angle in radians sinh(real) result(real) hyperbolic sine of argument spacing(real) result(real) spacing of model numbers near argument sqrt(real_complex) result(real_complex) square root of argument sum(array [,dim=] [,mask=]) result(integer_real_complex) sum of elements tan(real) result(real) tangent of angle in radians tanh(real) result(real) hyperbolic tangent of angle in radians tiny(real) result(real) smallest positive model representation transpose(matrix) result(matrix) the transpose of a matrix Intrinsic Functions (Logical and bit) all(mask [,dim]) result(logical) true if all elements of mask are true any(mask [,dim=}) result(logical) true if any elements of mask are true bit_size(integer) result(integer) size in bits in model of argument btest(i=integer,pos=integer) result(logical) true if pos has a 1, pos=0.. count(mask [,dim=]) result(integer) count of true entries in mask iand(integer,integer) result(integer) bit by bit logical and ibclr(integer,pos) result(integer) argument with pos bit cleared to zero ibits(integer,pos,len) result(integer) extract len bits starting at pos ibset(integer,pos) result(integer) argument with pos bit set to one ieor(integer,integer) result(integer) bit by bit logical exclusive or ior(integer,integer) result(integer) bit by bit logical or ishft(integer,shift) result(integer) shift bits in argument by shift ishftc(integer, shift) result(integer) shift circular bits in argument logical(logical [,kind=]) convert to logical matmul(matrix,matrix) result(vector_matrix) on integer_real_complex_logical merge(true_source,false_source,mask) result(source_type) choose by mask mvbits(from,frompos,len,to,topos) result(integer) move bits not(integer) result(integer) bit by bit logical complement transfer(source,mold [,size]) result(mold_type) same bits, new type Intrinsic Functions (Character or string) achar(integer) result(character) integer to character adjustl(character) result(character) left adjust, blanks go to back adjustr(character) result(character) right adjust, blanks to front char(integer [,kind=]) result(character) integer to character [of kind] iachar(character) result(integer) position of character in ASCII sequence ichar(character) result(integer) pos in collating sequence of character index(string,substring [,back=]) result(integer) pos of substring len(character) result(integer) number of characters that can be in argument len_trim(character) result(integer) length without trailing blanks lge(string_a,string_b) result(logical) string_a &ge; string_b lgt(string_a,string_b) result(logical) string_a > string_b lle(string_a,string_b) result(logical) string_a &le; string_b llt(string_a,string_b) result(logical) string_a < string_b repeat(string,ncopies) result(string) concatenate n copies of string scan(string,set [,back]) result(integer) position of first of set in string trim(string) result(string) trailing blanks are removed verify(string,set [,back]) result(integer) pos in string not in set Fortran 95 New Features The statement FORALL as an alternative to the DO-statement Partial nesting of FORALL and WHERE statements Masked ELSEWHERE Pure procedures Elemental procedures Pure procedures in specification expressions Revised MINLOC and MAXLOC Extensions to CEILING and FLOOR with the KIND keyword argument Pointer initialization Default initialization of derived type objects Increased compatibility with IEEE arithmetic A CPU_TIME intrinsic subroutine A function NULL to nullify a pointer Automatic deallocation of allocatable arrays at exit of scoping unit Comments in NAMELIST at input Minimal field at input Complete version of END INTERFACE Deleted Features real and double precision DO loop index variables branching to END IF from an outer block PAUSE statements ASSIGN statements and assigned GO TO statements and the use of an assigned integer as a FORMAT specification Hollerith editing in FORMAT See http://www.nsc.liu.se/~boein/f77to90/f95.html#17.5 References http://www.fortran.com/fortran/ Pointer to everything Fortran http://meteora.ucsd.edu/~pierce/fxdr_home_page.html Subroutines to do unformatted I/O across platforms, provided by David Pierce at UCSD http://www.nsc.liu.se/~boein/f77to90/a5.html A good reference for intrinsic functions https://wg5-fortran.org/N1551-N1600/N1579.pdf New Features of Fortran 2003 https://wg5-fortran.org/N1701-N1750/N1729.pdf New Features of Fortran 2008 http://www.nsc.liu.se/~boein/f77to90/ Fortran 90 for the Fortran 77 Programmer Fortran 90 Handbook Complete ANSI/ISO Reference . Jeanne Adams, Walt Brainerd, Jeanne Martin, Brian Smith, Jerrold Wagener Fortran 90 Programming . T. Ellis, Ivor Philips, Thomas Lahey https://github.com/llvm/llvm-project/blob/master/flang/docs/FortranForCProgrammers.md FFT stuff Fortran 95 and beyond","title":"Fortran 90 for Fortran 77 programmers"},{"location":"Documentation/languages/fortran/f90/#advanced-fortran-90","text":"This document is derived from an HTML page written at the San Diego Supercomper Center many years ago. Its purpose is to Introduce Fortran 90 concepts to Fortran 77 programers. It does this by presenting an example program and introducing concepts as various routines of the program are presented. The original web page has been used over the years and has been translated into several languages.","title":"Advanced Fortran 90"},{"location":"Documentation/languages/fortran/f90/#format-for-our-presentation","text":"We will \"develop\" an application Incorporate f90 features Show source code Explain what and why as we do it Application is a genetic algorithm Easy to understand and program Offers rich opportunities for enhancement We also provide an summary of F90 syntax, key words, operators, constants, and functions","title":"Format for our presentation"},{"location":"Documentation/languages/fortran/f90/#what-was-in-mind-of-the-language-writers-what-were-they-thinking","text":"Enable portable codes Same precision Include many common extensions More reliable programs Getting away from underlying hardware Move toward parallel programming Run old programs Ease of programming Writing Maintaining Understanding Reading Recover C and C++ users","title":"What was in mind of the language writers? What were they thinking?"},{"location":"Documentation/languages/fortran/f90/#why-fortran","text":"Famous Quote: \"I don't know what the technical characteristics of the standard language for scientific and engineering computation in the year 2000 will be... but I know it will be called Fortran.\" John Backus.* ### Note: He claimed that he never said this. Language of choice for Scientific programming Large installed user base. Fortran 90 has most of the features of C . . . and then some The compilers produce better programs","title":"Why Fortran?"},{"location":"Documentation/languages/fortran/f90/#justification-of-topics","text":"Enhance performance Enhance portability Enhance reliability Enhance maintainability","title":"Justification of topics"},{"location":"Documentation/languages/fortran/f90/#classification-of-topics","text":"New useful features Old tricks Power features Overview of F90","title":"Classification of topics"},{"location":"Documentation/languages/fortran/f90/#listing_of_topics_covered","text":"Listing of topics covered What is a Genetic Algorithm Simple algorithm for a GA Our example problem Start of real Fortran 90 discussion Comparing a FORTRAN 77 routine to a Fortran 90 routine Obsolescent features New source Form and related things New data declaration method Kind facility Modules Module functions and subroutines Allocatable arrays (the basics) Passing arrays to subroutines Interface for passing arrays Optional arguments and intent Derived data types Using defined types User defined operators Recursive functions introduction Fortran 90 recursive functions Pointers Function and subroutine overloading Fortran Minval and Minloc routines Pointer assignment More pointer usage, association and nullify Pointer usage to reference an array Data assignment with structures Using the user defined operator Passing arrays with a given arbitrary lower bounds Using pointers to access sections of arrays Allocating an array inside a subroutine Our fitness function Linked lists Linked list usage Our map representation Date and time functions Non advancing and character IO Internal IO Inquire function Namelist Vector valued functions Complete source for recent discussions Some array specific intrinsic functions The rest of our GA Compiler Information Summary Overview of F90 Fortran 95 References","title":"Listing_of_topics_covered"},{"location":"Documentation/languages/fortran/f90/#what-is-a-genetic-algorithm","text":"A \"suboptimization\" system Find good, but maybe not optimal, solutions to difficult problems Often used on NP-Hard or combinatorial optimization problems Requirements Solution(s) to the problem represented as a string A fitness function Takes as input the solution string Output the desirability of the solution A method of combining solution strings to generate new solutions Find solutions to problems by Darwinian evolution Potential solutions ar though of as living entities in a population The strings are the genetic codes for the individuals Fittest individuals are allowed to survive to reproduce","title":"What is a Genetic Algorithm"},{"location":"Documentation/languages/fortran/f90/#simple-algorithm-for-a-ga","text":"Generate a initial population, a collection of strings do for some time evaluate each individual (string) of the population using the fitness function sort the population with fittest coming to the top allow the fittest individuals to \"sexually\" reproduce replacing the old population allow for mutation end do","title":"Simple algorithm for a GA"},{"location":"Documentation/languages/fortran/f90/#our-example-problem","text":"Instance:Given a map of the N states or countries and a fixed number of colors Find a coloring of the map, if it exists, such that no two states that share a boarder have the same color Notes - In general, for a fixed number of colors and an arbitrary map the only known way to find if there is a valid coloring is a brute force search with the number of combinations = (NUMBER_OF_COLORS)**(NSTATES) The strings of our population are integer vectors represent the coloring Our fitness function returns the number of boarder violations The GA searches for a mapping with few, hopefully 0 violations This problem is related to several important NP_HARD problems in computer science Processor scheduling Communication and grid allocation for parallel computing Routing","title":"Our example problem"},{"location":"Documentation/languages/fortran/f90/#start-of-real-fortran-90-discussion","text":"","title":"Start of real Fortran 90 discussion"},{"location":"Documentation/languages/fortran/f90/#comparing-a-fortran-77-routine-to-a-fortran-90-routine","text":"The routine is one of the random number generators from: Numerical Recipes, The Art of Scientific Computing. Press, Teukolsky, Vetterling and Flannery. Cambridge University Press 1986. Changes correct bugs increase functionality aid portability","title":"Comparing a FORTRAN 77 routine to a Fortran 90 routine"},{"location":"Documentation/languages/fortran/f90/#original","text":"function ran1 ( idum ) real ran1 integer idum real r ( 97 ) parameter ( m1 = 259200 , ia1 = 7141 , ic1 = 54773 ) parameter ( m2 = 134456 , ia2 = 8121 , ic2 = 28411 ) parameter ( m3 = 243000 , ia3 = 4561 , ic3 = 51349 ) integer j integer iff , ix1 , ix2 , ix3 data iff / 0 / if ( idum . lt . 0. or . iff . eq . 0 ) then rm1 = 1.0 / m1 rm2 = 1.0 / m2 iff = 1 ix1 = mod ( ic1 - idum , m1 ) ix1 = mod ( ia1 * ix1 + ic1 , m1 ) ix2 = mod ( ix1 , m2 ) ix1 = mod ( ia1 * ix1 + ic1 , m1 ) ix3 = mod ( ix1 , m3 ) do 11 j = 1 , 97 ix1 = mod ( ia1 * ix1 + ic1 , m1 ) ix2 = mod ( ia2 * ix2 + ic2 , m2 ) r ( j ) = ( real ( ix1 ) + real ( ix2 ) * rm2 ) * rm1 11 continue idum = 1 endif ix1 = mod ( ia1 * ix1 + ic1 , m1 ) ix2 = mod ( ia2 * ix2 + ic2 , m2 ) ix3 = mod ( ia3 * ix3 + ic3 , m3 ) j = 1 + ( 97 * ix3 ) / m3 if ( j . gt . 9 7. or . j . lt . 1 ) then write ( * , * ) ' error in ran1 j=' , j stop endif ran1 = r ( j ) r ( j ) = ( real ( ix1 ) + real ( ix2 ) * rm2 ) * rm1 return end","title":"Original"},{"location":"Documentation/languages/fortran/f90/#fortran-90","text":"module ran_mod contains function ran1 ( idum ) use numz implicit none !note after use statement real ( b8 ) ran1 integer , intent ( inout ), optional :: idum real ( b8 ) r ( 97 ), rm1 , rm2 integer , parameter :: m1 = 259200 , ia1 = 7141 , ic1 = 54773 integer , parameter :: m2 = 134456 , ia2 = 8121 , ic2 = 28411 integer , parameter :: m3 = 243000 , ia3 = 4561 , ic3 = 51349 integer j integer iff , ix1 , ix2 , ix3 data iff / 0 / save ! corrects a bug in the original routine if ( present ( idum )) then if ( idum . lt . 0. or . iff . eq . 0 ) then rm1 = 1.0_b8 m1 rm2 = 1.0_b8 m2 iff = 1 ix1 = mod ( ic1 - idum , m1 ) ix1 = mod ( ia1 * ix1 + ic1 , m1 ) ix2 = mod ( ix1 , m2 ) ix1 = mod ( ia1 * ix1 + ic1 , m1 ) ix3 = mod ( ix1 , m3 ) do j = 1 , 97 ix1 = mod ( ia1 * ix1 + ic1 , m1 ) ix2 = mod ( ia2 * ix2 + ic2 , m2 ) r ( j ) = ( real ( ix1 , b8 ) + real ( ix2 , b8 ) * rm2 ) * rm1 enddo idum = 1 endif endif ix1 = mod ( ia1 * ix1 + ic1 , m1 ) ix2 = mod ( ia2 * ix2 + ic2 , m2 ) ix3 = mod ( ia3 * ix3 + ic3 , m3 ) j = 1 + ( 97 * ix3 ) / m3 if ( j . gt . 9 7. or . j . lt . 1 ) then write ( * , * ) ' error in ran1 j=' , j stop endif ran1 = r ( j ) r ( j ) = ( real ( ix1 , b8 ) + real ( ix2 , b8 ) * rm2 ) * rm1 return end function ran1","title":"Fortran 90"},{"location":"Documentation/languages/fortran/f90/#comments","text":"Modules are a way of encapsulating functions an data. More below. The use numz line is similar to an include file. In this case it defines our real data type. real (b8) is a new way to specify percision for data types in a portable way. integer , intent(inout), optional :: idum we are saying idum is an optional input parameter integer , parameter :: just a different syntax The save statement is needed for program correctness present(idum) is a function to determine if ran1 was called with the optional parameter","title":"Comments"},{"location":"Documentation/languages/fortran/f90/#obsolescent-features","text":"The following are available in Fortran 90. On the other hand, the concept of \"obsolescence\" is introduced. This means that some constructs may be removed in the future. - Arithmetic IF-statement - Control variables in a DO-loop which are floating point or double-precision floating-point - Terminating several DO-loops on the same statement - Terminating the DO-loop in some other way than with CONTINUE or END DO - Alternate return - Jump to END IF from an outer block - PAUSE - ASSIGN and assigned GOTO and assigned FORMAT , that is the whole \"statement number variable\" concept. - Hollerith editing in FORMAT.","title":"Obsolescent features"},{"location":"Documentation/languages/fortran/f90/#new-source-form-and-related-things","text":"","title":"New source form and related things"},{"location":"Documentation/languages/fortran/f90/#summary","text":"! now indicates the start of a comment & indicates the next line is a continuation Lines can be longer than 72 characters Statements can start in any column Use ; to put multiple statements on one line New forms for the do loop Many functions are generic 32 character names Many new array assignment techniques","title":"Summary"},{"location":"Documentation/languages/fortran/f90/#features","text":"Flexibility can aid in program readability Readability decreases errors Got ya! Can no longer use C to start a comment Character in column 5 no longer is continue Tab is not a valid character (may produce a warning) Characters past 72 now count program darwin real a ( 10 ), b ( 10 ), c ( 10 ), d ( 10 ), e ( 10 ), x , y integer odd ( 5 ), even ( 5 ) ! this line is continued by using \"&\" write ( * , * ) \"starting \" ,& \"darwin\" ! this line in a continued from above ! multiple statement per line --rarely a good idea x = 1 ; y = 2 ; write ( * , * ) x , y do i = 1 , 10 ! statement lable is not required for do e ( i ) = i enddo odd = ( / 1 , 3 , 5 , 7 , 9 / ) ! array assignment even = ( / 2 , 4 , 6 , 8 , 10 / ) ! array assignment a = 1 ! array assignment, every element of a = 1 b = 2 c = a + b + e ! element by element assignment c ( odd ) = c ( even ) - 1 ! can use arrays of indices on both sides d = sin ( c ) ! element by element application of intrinsics write ( * , * ) d write ( * , * ) abs ( d ) ! many intrinsic functions are generic a_do_loop : do i = 1 , 10 write ( * , * ) i , c ( i ), d ( i ) enddo a_do_loop do if ( c ( 10 ) . lt . 0.0 ) exit c ( 10 ) = c ( 10 ) - 1 enddo write ( * , * ) c ( 10 ) do while ( c ( 9 ) . gt . 0 ) c ( 9 ) = c ( 9 ) - 1 enddo write ( * , * ) c ( 9 ) end program","title":"Features"},{"location":"Documentation/languages/fortran/f90/#new-data-declaration-method","text":"Motivation Variables can now have attributes such as - Parameter - Save - Dimension Attributes are assigned in the variable declaration statement One variable can have several attributes Requires Fortran 90 to have a new statement form integer , parameter :: in2 = 14 real , parameter :: pi = 3.141592653589793239 real , save , dimension ( 10 ) :: cpu_times , wall_times !**** the old way of doing the same ****! !**** real cpu_times(10),wall_times(10) ****! !**** save cpu_times, wall_times ****! - Other Attributes - allocatable - public - private - target - pointer - intent - optional","title":"New data declaration method"},{"location":"Documentation/languages/fortran/f90/#kind-facility","text":"Motivation Assume we have a program that we want to run on two different machines We want the same representation of reals on both machines (same number of significant digits) Problem: different machines have different representations for reals","title":"Kind facility"},{"location":"Documentation/languages/fortran/f90/#digits-of-precision-for-some-old-machines-and-data-type","text":"Machine Real Double Precision IBM (SP) 6 15 Cray (T90) 15 33 Cray (T3E) 15 15","title":"Digits of precision for some (old) machines and data type"},{"location":"Documentation/languages/fortran/f90/#or","text":"We may want to run with at least 6 digits today and at least 14 digits tomorrow Use the Select_Real_Kind(P) function to create a data type with P digits of precision program darwin ! e has at least 4 significant digits real ( selected_real_kind ( 4 )) e ! b8 will be used to define reals with 14 digits integer , parameter :: b8 = selected_real_kind ( 14 ) real ( b8 ), parameter :: pi = 3.141592653589793239_b8 ! note usage of _b8 ! with a constant ! to force precision e = 2.71828182845904523536 write ( * , * ) \"starting \" ,& ! this line is continued by using \"&\" \"darwin\" ! this line in a continued from above write ( * , * ) \"pi has \" , precision ( pi ), \" digits precision \" , pi write ( * , * ) \"e has \" , precision ( e ), \" digits precision \" , e end program","title":"* or *"},{"location":"Documentation/languages/fortran/f90/#example-output","text":"sp001 % darwin starting darwin pi has 15 digits precision 3.14159265358979312 e has 6 digits precision 2.718281746 sp001 % Can convert to/from given precision for all variables created using \"b8\" by changing definition of \"b8\" Use the Select_Real_Kind(P,R) function to create a data type with P digits of precision and exponent range of R","title":"Example output"},{"location":"Documentation/languages/fortran/f90/#modules","text":"Motivation: Common block usage is prone to error Provide most of capability of common blocks but safer Provide capabilities beyond common blocks Modules can contain: Data definitions Data to be shared much like using a labeled common Functions and subroutines Interfaces (more on this later) You \"include\" a module with a \"use\" statement module numz integer , parameter :: b8 = selected_real_kind ( 14 ) real ( b8 ), parameter :: pi = 3.141592653589793239_b8 integergene_size end module program darwin use numz implicit none ! now part of the standard, put it after the use statements write ( * , * ) \"pi has \" , precision ( pi ), \" digits precision \" , pi call set_size () write ( * , * ) \"gene_size=\" , gene_size end program subroutine set_size use numz gene_size = 10 end subroutine","title":"Modules"},{"location":"Documentation/languages/fortran/f90/#an-example-run","text":"pi has 15 digits precision 3.14159265358979312 gene_size=10","title":"An example run"},{"location":"Documentation/languages/fortran/f90/#module-functions-and-subroutines","text":"Motivation: Encapsulate related functions and subroutines Can \"USE\" these functions in a program or subroutine Can be provided as a library Only routines that contain the use statement can see the routines Example is a random number package: module ran_mod ! module contains three functions ! ran1 returns a uniform random number between 0-1 ! spread returns random number between min - max ! normal returns a normal distribution contains function ran1 () !returns random number between 0 - 1 use numz implicit none real ( b8 ) ran1 , x call random_number ( x ) ! built in fortran 90 random number function ran1 = x end function ran1 function spread ( min , max ) !returns random # between min/max use numz implicit none real ( b8 ) spread real ( b8 ) min , max spread = ( max - min ) * ran1 () + min end function spread function normal ( mean , sigma ) !returns a normal distribution use numz implicit none real ( b8 ) normal , tmp real ( b8 ) mean , sigma integer flag real ( b8 ) fac , gsave , rsq , r1 , r2 save flag , gsave data flag / 0 / if ( flag . eq . 0 ) then rsq = 2.0_b8 do while ( rsq . ge . 1.0_b8 . or . rsq . eq . 0.0_b8 ) ! new from for do r1 = 2.0_b8 * ran1 () - 1.0_b8 r2 = 2.0_b8 * ran1 () - 1.0_b8 rsq = r1 * r1 + r2 * r2 enddo fac = sqrt ( - 2.0_b8 * log ( rsq ) / rsq ) gsave = r1 * fac tmp = r2 * fac flag = 1 else tmp = gsave flag = 0 endif normal = tmp * sigma + mean return end function normal end module ran_mod Exersize 1: Write a program that returns 10 uniform random numbers.","title":"Module functions and subroutines"},{"location":"Documentation/languages/fortran/f90/#allocatable-arrays-the-basics","text":"Motivation: At compile time we may not know the size an array needs to be We may want to change problem size without recompiling Allocatable arrays allow us to set the size at run time We set the size of the array using the allocate statement We may want to change the lower bound for an array A simple example: module numz integer , parameter :: b8 = selected_real_kind ( 14 ) integer gene_size , num_genes integer , allocatable :: a_gene (:), many_genes (:,:) end module program darwin use numz implicit none integer ierr call set_size () allocate ( a_gene ( gene_size ), stat = ierr ) !stat= allows for an error code return if ( ierr /= 0 ) write ( * , * ) \"allocation error\" ! /= is .ne. allocate ( many_genes ( gene_size , num_genes ), stat = ierr ) !2d array if ( ierr /= 0 ) write ( * , * ) \"allocation error\" write ( * , * ) lbound ( a_gene ), ubound ( a_gene ) ! get lower and upper bound ! for the array write ( * , * ) size ( many_genes ), size ( many_genes , 1 ) !get total size and size !along 1st dimension deallocate ( many_genes ) ! free the space for the array and matrix deallocate ( a_gene ) allocate ( a_gene ( 0 : gene_size )) ! now allocate starting at 0 instead of 1 write ( * , * ) allocated ( many_genes ), allocated ( a_gene ) ! shows if allocated write ( * , * ) lbound ( a_gene ), ubound ( a_gene ) end program subroutine set_size use numz write ( * , * ) 'enter gene size:' read ( * , * ) gene_size write ( * , * ) 'enter number of genes:' read ( * , * ) num_genes end subroutine set_size","title":"Allocatable arrays (the basics)"},{"location":"Documentation/languages/fortran/f90/#example-run","text":"enter gene size: 10 enter number of genes: 20 1 10 200 10 F T 0 10","title":"Example run"},{"location":"Documentation/languages/fortran/f90/#passing-arrays-to-subroutines","text":"There are several ways to specify arrays for subroutines Explicit shape integer, dimension(8,8)::an_explicit_shape_array Assumed size integer, dimension(i,*)::an_assumed_size_array Assumed Shape integer, dimension(:,:)::an_assumed_shape_array","title":"Passing arrays to subroutines"},{"location":"Documentation/languages/fortran/f90/#example","text":"subroutine arrays ( an_explicit_shape_array ,& i ,& !note we pass all bounds except the last an_assumed_size_array ,& an_assumed_shape_array ) ! Explicit shape integer , dimension ( 8 , 8 ) :: an_explicit_shape_array ! Assumed size integer , dimension ( i , * ) :: an_assumed_size_array ! Assumed Shape integer , dimension (:,:) :: an_assumed_shape_array write ( * , * ) sum ( an_explicit_shape_array ) write ( * , * ) lbound ( an_assumed_size_array ) ! why does sum not work here? write ( * , * ) sum ( an_assumed_shape_array ) end subroutine","title":"Example"},{"location":"Documentation/languages/fortran/f90/#interface-for-passing-arrays","text":"!!!!Warning!!!! When passing assumed shape arrays as arguments you must provide an interface Similar to C prototypes but much more versatile The interface is a copy of the invocation line and the argument definitions Modules are a good place for interfaces If a procedure is part of a \"contains\" section in a module an interface is not required !!!!Warning!!!! The compiler may not tell you that you need an interface module numz integer , parameter :: b8 = selected_real_kind ( 14 ) integer , allocatable :: a_gene (:), many_genes (:,:) end module module face interface fitness function fitness ( vector ) use numz implicit none real ( b8 ) fitness integer , dimension (:) :: vector end function fitness end interface end module program darwin use numz use face implicit none integer i integer vect ( 10 ) ! just a regular array allocate ( a_gene ( 10 )); allocate ( many_genes ( 3 , 10 )) a_gene = 1 !sets every element of a_gene to 1 write ( * , * ) fitness ( a_gene ) vect = 8 write ( * , * ) fitness ( vect ) ! also works with regular arrays many_genes = 3 !sets every element to 3 many_genes ( 1 ,:) = a_gene !sets column 1 to a_gene many_genes ( 2 ,:) = 2 * many_genes ( 1 ,:) do i = 1 , 3 write ( * , * ) fitness ( many_genes ( i ,:)) enddo write ( * , * ) fitness ( many_genes (:, 1 )) !go along other dimension !!!!write(*,*)fitness(many_genes)!!!!does not work end program function fitness ( vector ) use numz implicit none real ( b8 ) fitness integer , dimension (:) :: vector ! must match interface fitness = sum ( vector ) end function Exersize 2: Run this program using the \"does not work line\". Why? Using intrinsic functions make it work? Exersize 3: Prove that f90 does not \"pass by address\".","title":"Interface for passing arrays"},{"location":"Documentation/languages/fortran/f90/#optional-arguments-and-intent","text":"Motivation: We may have a function or subroutine that we may not want to always pass all arguments Initialization Two examples Seeding the intrinsic random number generator requires keyword arguments To define an optional argument in our own function we use the optional attribute integer :: my_seed","title":"Optional arguments and intent"},{"location":"Documentation/languages/fortran/f90/#becomes","text":"integer, optional :: my_seed Used like this: ! ran1 returns a uniform random number between 0-1 ! the seed is optional and used to reset the generator contains function ran1 ( my_seed ) use numz implicit none real ( b8 ) ran1 , r integer , optional , intent ( in ) :: my_seed ! optional argument not changed in the routine integer , allocatable :: seed (:) integer the_size , j if ( present ( my_seed )) then ! use the seed if present call random_seed ( size = the_size ) ! how big is the intrisic seed? allocate ( seed ( the_size )) ! allocate space for seed do j = 1 , the_size ! create the seed seed ( j ) = abs ( my_seed ) + ( j - 1 ) ! abs is generic enddo call random_seed ( put = seed ) ! assign the seed deallocate ( seed ) ! deallocate space endif call random_number ( r ) ran1 = r end function ran1 end module program darwin use numz use ran_mod ! interface required if we have ! optional or intent arguments real ( b8 ) x , y x = ran1 ( my_seed = 12345 ) ! we can specify the name of the argument y = ran1 () write ( * , * ) x , y x = ran1 ( 12345 ) ! with only one optional argument we don't need to y = ran1 () write ( * , * ) x , y end program Intent is a hint to the compiler to enable optimization intent(in) We will not change this value in our subroutine intent(out) We will define this value in our routine intent(inout) The normal situation","title":"becomes"},{"location":"Documentation/languages/fortran/f90/#derived-data-types","text":"Motivation: Derived data types can be used to group different types of data together (integers, reals, character, complex) Can not be done in F77 although people have \"faked\" it Example In our GA we define a collection of genes as a 2d array We call the fitness function for every member of the collection We want to sort the collection of genes based on result of fitness function Define a data type that holds the fitness value and an index into the 2d array Create an array of this data type, 1 for each member of the collection Call fitness function with the result being placed into the new data type along with a pointer into the array Again modules are a good place for data type definitions module galapagos use numz type thefit !the name of the type sequence ! sequence forces the data elements ! to be next to each other in memory ! where might this be useful? real ( b8 ) val ! our result from the fitness function integer index ! the index into our collection of genes end type thefit end module","title":"Derived data types"},{"location":"Documentation/languages/fortran/f90/#using-defined-types","text":"Use the % to reference various components of the derived data type program darwin use numz use galapagos ! the module that contains the type definition use face ! contains various interfaces implicit none ! define an allocatable array of the data type ! than contains an index and a real value type ( thefit ), allocatable , target :: results (:) ! create a single instance of the data type type ( thefit ) best integer , allocatable :: genes (:,:) ! our genes for the genetic algorithm integer j integer num_genes , gene_size num_genes = 10 gene_size = 10 allocate ( results ( num_genes )) ! allocate the data type ! to hold fitness and index allocate ( genes ( num_genes , gene_size )) ! allocate our collection of genes call init_genes ( genes ) ! starting data write ( * , '(\"input\")' ) ! we can put format in write statement do j = 1 , num_genes results ( j )% index = j results ( j )% val = fitness ( genes ( j ,:)) ! just a dummy routine for now write ( * , \"(f10.8,i4)\" ) results ( j )% val , results ( j )% index enddo end program","title":"Using defined types"},{"location":"Documentation/languages/fortran/f90/#user-defined-operators","text":"Motivation With derived data types we may want (need) to define operations (Assignment is predefined) Example: .lt. .gt. == not defined for our data types - We want to find the minimum of our fitness values so we need < operator - In our sort routine we want to do <, >, == - In C++ terms the operators are overloaded We are free to define new operators Two step process to define operators Define a special interface Define the function that performs the operation module sort_mod !defining the interfaces interface operator (. lt .) ! overloads standard .lt. module procedure theless ! the function that does it end interface interface operator (. gt .) ! overloads standard .gt. module procedure thegreat ! the function that does it end interface interface operator (. ge .) ! overloads standard .ge. module procedure thetest ! the function that does it end interface interface operator (. converged .) ! new operator module procedure index_test ! the function that does it end interface contains ! our module will contain ! the required functions function theless ( a , b ) ! overloads .lt. for the type (thefit) use galapagos implicit none type ( thefit ), intent ( in ) :: a , b logical theless ! what we return if ( a % val . lt . b % val ) then ! this is where we do the test theless = . true . else theless = . false . endif return end function theless function thegreat ( a , b ) ! overloads .gt. for the type (thefit) use galapagos implicit none type ( thefit ), intent ( in ) :: a , b logical thegreat if ( a % val . gt . b % val ) then thegreat = . true . else thegreat = . false . endif return end function thegreat function thetest ( a , b ) ! overloads .gt.= for the type (thefit) use galapagos implicit none type ( thefit ), intent ( in ) :: a , b logical thetest if ( a % val >= b % val ) then thetest = . true . else thetest = . false . endif return end function thetest function index_test ( a , b ) ! defines a new operation for the type (thefit) use galapagos implicit none type ( thefit ), intent ( in ) :: a , b logical index_test if ( a % index . gt . b % index ) then ! check the index value for a difference index_test = . true . else index_test = . false . endif return end function index_test","title":"User defined operators"},{"location":"Documentation/languages/fortran/f90/#recursive-functions-introduction","text":"Notes Recursive function is one that calls itself Anything that can be done with a do loop can be done using a recursive function Motivation Sometimes it is easier to think recursively Divide an conquer algorithms are recursive by nature - Fast FFTs - Searching - Sorting","title":"Recursive functions introduction"},{"location":"Documentation/languages/fortran/f90/#algorithm-of-searching-for-minimum-of-an-array","text":"function findmin ( array ) is size of array 1 ? min in the array is first element else find minimum in left half of array using findmin function find minimum in right half of array using findmin function global minimum is min of left and right half end function","title":"Algorithm of searching for minimum of an array"},{"location":"Documentation/languages/fortran/f90/#fortran-90-recursive-functions","text":"Recursive functions should have an interface The result and recursive keywords are required as part of the function definition Example is a function finds the minimum value for an array recursive function realmin ( ain ) result ( themin ) ! recursive and result are required for recursive functions use numz implicit none real ( b8 ) themin , t1 , t2 integer n , right real ( b8 ) , dimension (:) :: ain n = size ( ain ) if ( n == 1 ) then themin = ain ( 1 ) ! if the size is 1 return value return else right = n / 2 t1 = realmin ( ain ( 1 : right )) ! find min in left half t2 = realmin ( ain ( right + 1 : n )) ! find min in right half themin = min ( t1 , t2 ) ! find min of the two sides endif end function Example 2 is the same except the input data is our derived data type !this routine works with the data structure thefit not reals recursive function typemin ( ain ) result ( themin ) use numz use sort_mod use galapagos implicit none real ( b8 ) themin , t1 , t2 integer n , right type ( thefit ) , dimension (:) :: ain ! this line is different n = size ( ain ) if ( n == 1 ) then themin = ain ( 1 )% val ! this line is different return else right = n / 2 t1 = typemin ( ain ( 1 : right )) t2 = typemin ( ain ( right + 1 : n )) themin = min ( t1 , t2 ) endif end function","title":"Fortran 90 recursive functions"},{"location":"Documentation/languages/fortran/f90/#pointers","text":"Motivation Can increase performance Can improve readability Required for some derived data types (linked lists and trees) Useful for allocating \"arrays\" within subroutines Useful for referencing sections of arrays Notes Pointers can be thought of as an alias to another variable In some cases can be used in place of an array To assign a pointer use => instead of just = Unlike C and C++, pointer arithmetic is not allowed First pointer example Similar to the last findmin routine Return a pointer to the minimum recursive function pntmin ( ain ) result ( themin ) ! return a pointer use numz use galapagos use sort_mod ! contains the .lt. operator for thefit type implicit none type ( thefit ), pointer :: themin , t1 , t2 integer n , right type ( thefit ) , dimension (:), target :: ain n = size ( ain ) if ( n == 1 ) then themin => ain ( 1 ) !this is how we do pointer assignment return else right = n / 2 t1 => pntmin ( ain ( 1 : right )) t2 => pntmin ( ain ( right + 1 : n )) if ( t1 . lt . t2 ) then ; themin => t1 ; else ; themin => t2 ; endif endif end function Exercise 4: Carefully write a recursive N! program.","title":"Pointers"},{"location":"Documentation/languages/fortran/f90/#function-and-subroutine-overloading","text":"Motivation Allows us to call functions or subroutine with the same name with different argument types Increases readability Notes: Similar in concept to operator overloading Requires an interface Syntax for subroutines is same as for functions Many intrinsic functions have this capability - abs (reals,complex,integer) - sin,cos,tan,exp(reals, complex) - array functions(reals, complex,integer) Example - Recall we had two functions that did the same thing but with different argument types recursive function realmin(ain) result (themin) real(b8) ,dimension(:) :: ain recursive function typemin(ain) result (themin) type (thefit) ,dimension(:) :: ain - We can define a generic interface for these two functions and call them using the same name ! note we have two functions within the same interface ! this is how we indicate function overloading ! both functions are called \"findmin\" in the main program interface findmin ! the first is called with an array of reals as input recursive function realmin ( ain ) result ( themin ) use numz real ( b8 ) themin real ( b8 ) , dimension (:) :: ain end function ! the second is called with a array of data structures as input recursive function typemin ( ain ) result ( themin ) use numz use galapagos real ( b8 ) themin type ( thefit ) , dimension (:) :: ain end function end interface","title":"Function and subroutine overloading"},{"location":"Documentation/languages/fortran/f90/#example-usage","text":"program darwin use numz use ran_mod use galapagos ! the module that contains the type definition use face ! contains various interfaces use sort_mod ! more about this later it ! contains our sorting routine ! and a few other tricks implicit none ! create an allocatable array of the data type ! than contains an index and a real value type ( thefit ), allocatable , target :: results (:) ! create a single instance of the data type type ( thefit ) best ! pointers to our type type ( thefit ) , pointer :: worst , tmp integer , allocatable :: genes (:,:) ! our genes for the ga integer j integer num_genes , gene_size real ( b8 ) x real ( b8 ), allocatable :: z (:) real ( b8 ), pointer :: xyz (:) ! we'll talk about this next num_genes = 10 gene_size = 10 allocate ( results ( num_genes )) ! allocate the data type to allocate ( genes ( num_genes , gene_size )) ! hold our collection of genes call init_genes ( genes ) ! starting data write ( * , '(\"input\")' ) do j = 1 , num_genes results ( j )% index = j results ( j )% val = fitness ( genes ( j ,:)) ! just a dummy routine write ( * , \"(f10.8,i4)\" ) results ( j )% val , results ( j )% index enddo allocate ( z ( size ( results ))) z = results (:)% val ! copy our results to a real array ! use a recursive subroutine operating on the real array write ( * , * ) \"the lowest fitness: \" , findmin ( z ) ! use a recursive subroutine operating on the data structure write ( * , * ) \"the lowest fitness: \" , findmin ( results ) end program","title":"Example usage"},{"location":"Documentation/languages/fortran/f90/#fortran-minval-and-minloc-routines","text":"Fortran has routines for finding minimum and maximum values in arrays and the locations minval maxval minloc (returns an array) maxloc (returns an array) ! we show two other methods of getting the minimum fitness ! use the built in f90 routines on a real array write ( * , * ) \"the lowest fitness: \" , minval ( z ), minloc ( z )","title":"Fortran Minval and Minloc routines"},{"location":"Documentation/languages/fortran/f90/#pointer-assignment","text":"This is how we use the pointer function defined above worst is a pointer to our data type note the use of => ! use a recursive subroutine operating on the data ! structure and returning a pointer to the result worst=>pntmin(results) ! note pointer assignment ! what will this line write? write(*,*)\"the lowest fitness: \",worst","title":"Pointer assignment"},{"location":"Documentation/languages/fortran/f90/#more-pointer-usage-association-and-nullify","text":"Motivation Need to find if pointers point to anything Need to find if two pointers point to the same thing Need to deallocate and nullify when they are no longer used Usage We can use associated() to tell if a pointer has been set We can use associated() to compare pointers We use nullify to zero a pointer ! This code will print \"true\" when we find a match, ! that is the pointers point to the same object do j = 1 , num_genes tmp => results ( j ) write ( * , \"(f10.8,i4,l3)\" ) results ( j )% val , & results ( j )% index , & associated ( tmp , worst ) enddo nullify ( tmp ) Notes: If a pointer is nullified the object to which it points is not deallocated. In general, pointers as well as allocatable arrays become undefined on leaving a subroutine This can cause a memory leak","title":"More pointer usage, association and nullify"},{"location":"Documentation/languages/fortran/f90/#pointer-usage-to-reference-an-array-without-copying","text":"Motivation Our sort routine calls a recursive sorting routine It is messy and inefficient to pass the array to the recursive routine Solution We define a \"global\" pointer in a module We point the pointer to our input array module Merge_mod_types use galapagos type ( thefit ), allocatable :: work (:) ! a \"global\" work array type ( thefit ), pointer :: a_pntr (:) ! this will be the pointer to our input array end module Merge_mod_types subroutine Sort ( ain , n ) use Merge_mod_types implicit none integer n type ( thefit ), target :: ain ( n ) allocate ( work ( n )) nullify ( a_pntr ) a_pntr => ain ! we assign the pointer to our array ! in RecMergeSort we reference it just like an array call RecMergeSort ( 1 , n ) ! very similar to the findmin functions deallocate ( work ) return end subroutine Sort In our main program sort is called like this: ! our sort routine is also recursive but ! also shows a new usage for pointers call sort ( results , num_genes ) do j = 1 , num_genes write ( * , \"(f10.8,i4)\" ) results ( j )% val , & results ( j )% index enddo","title":"Pointer usage to reference an array without copying"},{"location":"Documentation/languages/fortran/f90/#data-assignment-with-structures","text":"! we can copy a whole structure ! with a single assignment best = results ( 1 ) write ( * , * ) \"best result \" , best","title":"Data assignment with structures"},{"location":"Documentation/languages/fortran/f90/#using-the-user-defined-operator","text":"! using the user defined operator to see if best is worst ! recall that the operator .converged. checks to see if %index matches worst => pntmin ( results ) write ( * , * ) \"worst result \" , worst write ( * , * ) \"converged=\" ,( best . converged . worst )","title":"Using the user defined operator"},{"location":"Documentation/languages/fortran/f90/#passing-arrays-with-a-given-arbitrary-lower-bounds","text":"Motivation Default lower bound within a subroutine is 1 May want to use a different lower bound if ( allocated ( z )) deallocate ( z ) allocate ( z ( - 10 : 10 )) ! a 21 element array do j =- 10 , 10 z ( j ) = j enddo ! pass z and its lower bound ! in this routine we give the array a specific lower ! bound and show how to use a pointer to reference ! different parts of an array using different indices call boink1 ( z , lbound ( z , 1 )) ! why not just lbound(z) instead of lbound(z,1)? ! lbound(z) returns a rank 1 array subroutine boink1 ( a , n ) use numz implicit none integer , intent ( in ) :: n real ( b8 ), dimension ( n :) :: a ! this is how we set lower bounds in a subroutine write ( * , * ) lbound ( a ), ubound ( a ) end subroutine","title":"Passing arrays with a given arbitrary lower bounds"},{"location":"Documentation/languages/fortran/f90/#warning-because-we-are-using-an-assumed-shape-array-we-need-an-interface","text":"","title":"Warning:  because we are using an assumed shape array we need an interface"},{"location":"Documentation/languages/fortran/f90/#using-pointers-to-access-sections-of-arrays","text":"Motivation Can increase efficiency Can increase readability call boink2 ( z , lbound ( z , 1 )) subroutine boink2 ( a , n ) use numz implicit none integer , intent ( in ) :: n real ( b8 ), dimension ( n :), target :: a real ( b8 ), dimension (:), pointer :: b b => a ( n :) ! b(1) \"points\" to a(-10) write ( * , * ) \"a(-10) =\" , a ( - 10 ), \"b(1) =\" , b ( 1 ) b => a ( 0 :) ! b(1) \"points\" to a(0) write ( * , * ) \"a(-6) =\" , a ( - 6 ), \"b(-5) =\" , b ( - 5 ) end subroutine","title":"Using pointers to access sections of arrays"},{"location":"Documentation/languages/fortran/f90/#allocating-an-array-inside-a-subroutine-and-passing-it-back","text":"Motivation Size of arrays are calculated in the subroutine module numz integer , parameter :: b8 = selected_real_kind ( 14 ) end module program bla use numz real ( b8 ), dimension (:) , pointer :: xyz interface boink subroutine boink ( a ) use numz implicit none real ( b8 ), dimension (:), pointer :: a end subroutine end interface nullify ( xyz ) ! nullify sets a pointer to null write ( * , '(l5)' ) associated ( xyz ) ! is a pointer null, should be call boink ( xyz ) write ( * , '(l5)' , advance = \"no\" ) associated ( xyz ) if ( associated ( xyz )) write ( * , '(i5)' ) size ( xyz ) end program subroutine boink ( a ) use numz implicit none real ( b8 ), dimension (:), pointer :: a if ( associated ( a )) deallocate ( a ) allocate ( a ( 10 )) end subroutine","title":"Allocating an array inside a subroutine and passing it back"},{"location":"Documentation/languages/fortran/f90/#an-example-run_1","text":"F T 10","title":"An example run"},{"location":"Documentation/languages/fortran/f90/#our-fitness-function","text":"Given a fixed number of colors, M, and a description of a map of a collection of N states. Find a coloring of the map such that no two states that share a boarder have the same coloring.","title":"Our fitness function"},{"location":"Documentation/languages/fortran/f90/#example-input-is-a-sorted-list-of-22-western-states","text":"22 ar ok tx la mo xx az ca nm ut nv xx ca az nv or xx co nm ut wy ne ks xx ia mo ne sd mn xx id wa or nv ut wy mt xx ks ne co ok mo xx la tx ar xx mn ia sd nd xx mo ar ok ks ne ia xx mt wy id nd xx nd mt sd wy xx ne sd wy co ks mo ia xx nm az co ok tx mn xx nv ca or id ut az xx ok ks nm tx ar mo xx or ca wa id xx sd nd wy ne ia mn xx tx ok nm la ar xx ut nv az co wy id xx wa id or mt xx wy co mt id ut nd sd ne xx Our fitness function takes a potential coloring, that is, an integer vector of length N and a returns the number of boarders that have states of the same coloring How do we represent the map in memory? One way would be to use an array but it would be very sparse Linked lists are often a better way","title":"Example input is a sorted list of 22 western states"},{"location":"Documentation/languages/fortran/f90/#linked-lists","text":"Motivation We have a collection of states and for each state a list of adjoining states. (Do not count a boarder twice.) Problem is that you do not know the length of the list until runtime. List of adjoining states will be different lengths for different states Solution - Linked list are a good way to handle such situations Linked lists use a derived data type with at least two components Data Pointer to next element module list_stuff type llist integer index ! data type ( llist ), pointer :: next ! pointer to the ! next element end type llist end module","title":"Linked lists"},{"location":"Documentation/languages/fortran/f90/#linked-list-usage","text":"One way to fill a linked list is to use a recursive function `fortran recursive subroutine insert (item, root) use list_stuff implicit none type(llist), pointer :: root integer item if (.not. associated(root)) then allocate(root) nullify(root%next) root%index = item else call insert(item,root%next) endif end subroutine - - - - - - ## Our map representation - An array of the derived data type states - State is name of a state - Linked list containing boarders ```fortran type states character(len=2)name type(llist),pointer:: list end type states - Notes: - We have an array of linked lists - This data structure is often used to represent sparse arrays - We could have a linked list of linked lists - State name is not really required","title":"Linked list usage"},{"location":"Documentation/languages/fortran/f90/#date-and-time-functions","text":"Motivation May want to know the date and time of your program Two functions ! all arguments are optional call date_and_time ( date = c_date , & ! character(len=8) ccyymmdd time = c_time , & ! character(len=10) hhmmss.sss zone = c_zone , & ! character(len=10) +/-hhmm (time zone) values = ivalues ) ! integer ivalues(8) all of the above call system_clock ( count = ic , & ! count of system clock (clicks) count_rate = icr , & ! clicks / second count_max = max_c ) ! max value for count","title":"Date and time functions"},{"location":"Documentation/languages/fortran/f90/#non-advancing-and-character-io","text":"Motivation We read the states using the two character identification One line per state and do not know how many boarder states per line Note: Our list of states is presorted character ( len = 2 ) a ! we have a character variable of length 2 read ( 12 , * ) nstates ! read the number of states allocate ( map ( nstates )) ! and allocate our map do i = 1 , nstates read ( 12 , \"(a2)\" , advance = \"no\" ) map ( i )% name ! read the name !write(*,*)\"state:\",map(i)%name nullify ( map ( i )% list ) ! \"zero out\" our list do read ( 12 , \"(1x,a2)\" , advance = \"no\" ) a ! read list of states ! without going to the ! next line if ( lge ( a , \"xx\" ) . and . lle ( a , \"xx\" )) then ! if state == xx backspace ( 12 ) ! go to the next line read ( 12 , \"(1x,a2)\" , end = 1 ) a ! go to the next line exit endif 1 continue if ( llt ( a , map ( i )% name )) then ! we only add a state to ! our list if its name ! is before ours thus we ! only count boarders 1 time ! what we want put into our linked list is an index ! into our map where we find the bordering state ! thus we do the search here ! any ideas on a better way of doing this search? found =- 1 do j = 1 , i - 1 if ( lge ( a , map ( j )% name ) . and . lle ( a , map ( j )% name )) then !write(*,*)a found = j exit endif enddo if ( found == - 1 ) then write ( * , * ) \"error\" stop endif ! found the index of the boarding state insert it into our list ! note we do the insert into the linked list for a particular state call insert ( found , map ( i )% list ) endif enddo enddo","title":"Non advancing and character IO"},{"location":"Documentation/languages/fortran/f90/#internal-io","text":"Motivation May need to create strings on the fly May need to convert from strings to reals and integers Similar to sprintf and sscanf How it works Create a string Do a normal write except write to the string instead of file number Example 1: creating a date and time stamped file name character ( len = 12 ) tmpstr write ( tmpstr , \"(a12)\" )( c_date ( 5 : 8 ) // c_time ( 1 : 4 ) // \".dat\" ) ! // does string concatination write ( * , * ) \"name of file= \" , tmpstr open ( 14 , file = tmpstr ) name of file = 0327111 4. dat Example 2: Creating a format statement at run time (array of integers and a real) ! test_vect is an array that we do not know its length until run time nstate = 9 ! the size of the array write ( fstr , '(\"(\",i4,\"i1,1x,f10.5)\")' ) nstates write ( * , * ) \"format= \" , fstr write ( * , fstr ) test_vect , fstr format = ( 9 i1 , 1 x , f10 . 5 ) Any other ideas for writing an array when you do not know its length? Example 3: Reading from a string integer ht , minut , sec read ( c_time , \"(3i2)\" ) hr , minut , sec","title":"Internal IO"},{"location":"Documentation/languages/fortran/f90/#inquire-function","text":"Motivation Need to get information about I/O Inquire statement has two forms Information about files (23 different requests can be done) Information about space required for binary output of a value Example: find the size of your real relative to the \"standard\" real Useful for inter language programming Useful for determining data types in MPI (MPI_REAL or MPI_DOUBLE_PRECISION) inquire ( iolength = len_real ) 1.0 inquire ( iolength = len_b8 ) 1.0_b8 write ( * , * ) \"len_b8 \" , len_b8 write ( * , * ) \"len_real\" , len_real iratio = len_b8 / len_real select case ( iratio ) case ( 1 ) my_mpi_type = mpi_real case ( 2 ) my_mpi_type = mpi_double_precision case default write ( * , * ) \"type undefined\" my_mpi_type = 0 end select","title":"Inquire function"},{"location":"Documentation/languages/fortran/f90/#an-example-run_2","text":"len_b8 2 len_real 1","title":"An example run"},{"location":"Documentation/languages/fortran/f90/#namelist","text":"Now part of the standard Motivation A convenient method of doing I/O Good for cases where you have similar runs but change one or two variables Good for formatted output Notes: A little flaky No options for overloading format Example: integer ncolor logical force namelist / the_input / ncolor , force ncolor = 4 force = . true . read ( 13 , the_input ) write ( * , the_input ) On input: & THE_INPUT NCOLOR=4,FORCE = F / Output is &THE_INPUT NCOLOR = 4, FORCE = F /","title":"Namelist"},{"location":"Documentation/languages/fortran/f90/#vector-valued-functions","text":"Motivation May want a function that returns a vector Notes Again requires an interface Use explicit or assumed size array Do not return a pointer to a vector unless you really want a pointer Example: Take an integer input vector which represents an integer in some base and add 1 Could be used in our program to find a \"brute force\" solution function add1 ( vector , max ) result ( rtn ) integer , dimension (:), intent ( in ) :: vector integer , dimension ( size ( vector )) :: rtn integer max integer len logical carry len = size ( vector ) rtn = vector i = 0 carry = . true . do while ( carry ) ! just continue until we do not do a carry i = i + 1 rtn ( i ) = rtn ( i ) + 1 if ( rtn ( i ) . gt . max ) then if ( i == len ) then ! role over set everything back to 0 rtn = 0 else rtn ( i ) = 0 endif else carry = . false . endif enddo end function","title":"Vector valued functions"},{"location":"Documentation/languages/fortran/f90/#usage","text":"test_vect = 0 do test_vect = add1 ( test_vect , 3 ) result = fitness ( test_vect ) if ( result . lt . 1.0_b8 ) then write ( * , * ) test_vect stop endif enddo","title":"Usage"},{"location":"Documentation/languages/fortran/f90/#complete-source-for-recent-discussions","text":"recent.f90 fort.13 Exersize 5 Modify the program to use the random number generator given earlier.","title":"Complete source for recent discussions"},{"location":"Documentation/languages/fortran/f90/#some-array-specific-intrinsic-functions","text":"ALL True if all values are true (LOGICAL) ANY True if any value is true (LOGICAL) COUNT Number of true elements in an array (LOGICAL) DOT_PRODUCT Dot product of two rank one arrays MATMUL Matrix multiplication MAXLOC Location of a maximum value in an array MAXVAL Maximum value in an array MINLOC Location of a minimum value in an array MINVAL Minimum value in an array PACK Pack an array into an array of rank one PRODUCT Product of array elements RESHAPE Reshape an array SPREAD Replicates array by adding a dimension SUM Sum of array elements TRANSPOSE Transpose an array of rank two UNPACK Unpack an array of rank one into an array under a mask Examples program matrix real w ( 10 ), x ( 10 ), mat ( 10 , 10 ) call random_number ( w ) call random_number ( mat ) x = matmul ( w , mat ) ! regular matrix multiply USE IT write ( * , '(\"dot(x,x)=\",f10.5)' ), dot_product ( x , x ) end program program allit character ( len = 10 ) :: f1 = \"(3l1)\" character ( len = 10 ) :: f2 = \"(3i2)\" integer b ( 2 , 3 ), c ( 2 , 3 ), one_d ( 6 ) logical l ( 2 , 3 ) one_d = ( / 1 , 3 , 5 , 2 , 4 , 6 / ) b = transpose ( reshape (( / 1 , 3 , 5 , 2 , 4 , 6 / ), shape = ( / 3 , 2 / ))) C = transpose ( reshape (( / 0 , 3 , 5 , 7 , 4 , 8 / ), shape = ( / 3 , 2 / ))) l = ( b . ne . c ) write ( * , f2 )(( b ( i , j ), j = 1 , 3 ), i = 1 , 2 ) write ( * , * ) write ( * , f2 )(( c ( i , j ), j = 1 , 3 ), i = 1 , 2 ) write ( * , * ) write ( * , f1 )(( l ( i , j ), j = 1 , 3 ), i = 1 , 2 ) write ( * , * ) write ( * , f1 ) all ( b . ne . C ) !is .false. write ( * , f1 ) all ( b . ne . C , DIM = 1 ) !is [.true., .false., .false.] write ( * , f1 ) all ( b . ne . C , DIM = 2 ) !is [.false., .false.] end The output is: 1 3 5 2 4 6 0 3 5 7 4 8 TFF TFT F TFF FF","title":"Some array specific intrinsic functions"},{"location":"Documentation/languages/fortran/f90/#the-rest-of-our-ga","text":"Includes Reproduction Mutation Nothing new in either of these files Source and makefile \"git\" Source and makefile \"*tgz\"","title":"The rest of our GA"},{"location":"Documentation/languages/fortran/f90/#compiler-information","text":"","title":"Compiler Information"},{"location":"Documentation/languages/fortran/f90/#gfortran","text":".f, .for, .ftn .f77 fixed-format Fortran source; compile .f90, .f95 free-format Fortran source; compile -fbacktrace Add debug information for runtime traceback -ffree-form -ffixed-form source form -O0, -O1, -O2, -O3 optimization level .fpp, .FPP, .F, .FOR, .FTN, .F90, .F95, .F03 or .F08 Fortran source file with preprocessor directives -fopenmp turn on OpenMP","title":"gfortran"},{"location":"Documentation/languages/fortran/f90/#intel","text":".f, .for, .ftn fixed-format Fortran source; compile .f90, .f95 free-format Fortran source; compile -O0, -O1, -O2, -O3, -O4 optimization level .fpp, .F, .FOR, .FTN, .FPP, .F90 Fortran source file with preprocessor directives -g compile for debug * -traceback -notraceback (default) Add debug information for runtime traceback -nofree, -free Source is fixed or free format -fopenmp turn on OpenMP","title":"Intel"},{"location":"Documentation/languages/fortran/f90/#portland-group-x86","text":".f, .for, .ftn fixed-format Fortran source; compile .f90, .f95, .f03 free-format Fortran source; compile .cuf free-format CUDA Fortran source; compile .CUF free-format CUDA Fortran source; preprocess, compile -O0, -O1, -O2, -O3, -O4 optimization level -g compile for debug * -traceback (default) -notraceback Add debug information for runtime traceback -Mfixed, -Mfree Source is fixed or free format -qmp turn on OpenMP","title":"Portland Group (x86)"},{"location":"Documentation/languages/fortran/f90/#ibm-xlf","text":"xlf, xlf_r, f77, fort77 Compile FORTRAN 77 source files. _r = thread safe xlf90, xlf90_r, f90 Compile Fortran 90 source files. _r = thread safe xlf95, xlf95_r, f95 Compile Fortran 95 source files. _r = thread safe xlf2003, xlf2003_r,f2003 * Compile Fortran 2003 source files. _r = thread safe xlf2008, xlf2008_r, f2008 * Compile Fortran 2008 source files. .f, .f77, .f90, .f95, .f03, .f08 Fortran source file .F, .F77, .F90, .F95, .F03, .F08 Fortran source file with preprocessor directives -qtbtable=full Add debug information for runtime traceback -qsmp=omp turn on OpenMP -O0, -O1, -O2, -O3, -O4, O5 optimization level -g , g0, g1,...g9 compile for debug","title":"IBM xlf"},{"location":"Documentation/languages/fortran/f90/#summary_1","text":"Fortran 90 has features to: Enhance performance Enhance portability Enhance reliability Enhance maintainability Fortran 90 has new language elements Source form Derived data types Dynamic memory allocation functions Kind facility for portability and easy modification Many new intrinsic function Array assignments Examples Help show how things work Reference for future use","title":"Summary"},{"location":"Documentation/languages/fortran/f90/#overview-of-f90","text":"Introduction to Fortran Language Meta language used in this compact summary Structure of files that can be compiled Executable Statements and Constructs Declarations Key words - other than I/O Key words related to I/O Operators Constants Input/Output Statements Formats Intrinsic Functions","title":"Overview of F90"},{"location":"Documentation/languages/fortran/f90/#introduction-to-fortran-language","text":"Brought to you by ANSI committee X3J3 and ISO-IEC/JTC1/SC22/WG5 (Fortran) This is neither complete nor precisely accurate, but hopefully, after a small investment of time it is easy to read and very useful. This is the free form version of Fortran, no statement numbers, no C in column 1, start in column 1 (not column 7), typically indent 2, 3, or 4 spaces per each structure. The typical extension is .f90 . Continue a statement on the next line by ending the previous line with an ampersand &amp; . Start the continuation with &amp; for strings. The rest of any line is a comment starting with an exclamation mark ! . Put more than one statement per line by separating statements with a semicolon ; . Null statements are OK, so lines can end with semicolons. Separate words with space or any form of \"white space\" or punctuation.","title":"Introduction to Fortran Language"},{"location":"Documentation/languages/fortran/f90/#meta-language-used-in-this-compact-summary","text":"<xxx> means fill in something appropriate for xxx and do not type the \"<\" or \">\" . ... ellipsis means the usual, fill in something, one or more lines [stuff] means supply nothing or at most one copy of \"stuff\" [stuff1 [stuff2]] means if \"stuff1\" is included, supply nothing or at most one copy of stuff2. \"old\" means it is in the language, like almost every feature of past Fortran standards, but should not be used to write new programs.","title":"Meta language used in this compact summary"},{"location":"Documentation/languages/fortran/f90/#structure-of-files-that-can-be-compiled","text":"program <name> usually file name is <name>.f90 use <module_name> bring in any needed modules implicit none good for error detection <declarations> <executable statements> order is important, no more declarations end program <name> block data <name> old <declarations> common, dimension, equivalence now obsolete end block data <name> module <name> bring back in with use <name> implicit none good for error detection <declarations> can have private and public and interface end module <name> subroutine <name> use: call <name> to execute implicit none good for error detection <declarations> <executable statements> end subroutine <name> subroutine <name>(par1, par2, ...) use: call <name>(arg1, arg2,... ) to execute implicit none optional, good for error detection <declarations> par1, par2, ... are defined in declarations and can be specified in, inout, pointer, etc. <executable statements> return optional, end causes automatic return entry <name> (par...) old, optional other entries end subroutine <name> function <name>(par1, par2, ...) result(<rslt>) use: <name>(arg1, arg2, ... argn) as variable implicit none optional, good for error detection <declarations> rslt, par1, ... are defined in declarations <executable statements> <rslt> = <expression> required somewhere in execution [return] optional, end causes automatic return end function <name> old <type> function(...) <name> use: <name>(arg1, arg2, ... argn) as variable <declarations> <executable statements> <name> = <expression> required somewhere in execution [return] optional, end causes automatic return end function <name>","title":"Structure of files that can be compiled"},{"location":"Documentation/languages/fortran/f90/#executable-statements-and-constructs","text":"<statement> will mean exactly one statement in this section a construct is multiple lines <label> : <statement> any statement can have a label (a name) <variable> = <expression> assignment statement <pointer> >= <variable> the pointer is now an alias for the variable <pointer1> >= <pointer2> pointer1 now points same place as pointer2 stop can be in any executable statement group, stop <integer> terminates execution of the program, stop <string> can have optional integer or string return exit from subroutine or function do <variable>=<from>,<to> [,<increment&gt] optional: <label> : do ... <statements> exit \\_optional or exit <label&gt if (<boolean expression>) exit / exit the loop cycle \\_optional or cycle <label> if (<boolean expression>) cycle / continue with next loop iteration end do optional: end do <name> do while (<boolean expression>) ... optional exit and cycle allowed end do do ... exit required to end the loop optional cycle can be used end do if ( <boolean expression> ) <statement> execute the statement if the boolean expression is true if ( <boolean expression1> ) then ... execute if expression1 is true else if ( <boolean expression2> ) then ... execute if expression2 is true else if ( <boolean expression3> ) then ... execute if expression3 is true else ... execute if none above are true end if select case (<expression>) optional <name> : select case ... case (<value>) <statements> execute if expression == value case (<value1>:<value2>) <statements> execute if value1 &le; expression &le; value2 ... case default <statements> execute if no values above match end select optional end select <name> real, dimension(10,12) :: A, R a sample declaration for use with \"where\" ... where (A /= 0.0) conditional assignment, only assignment allowed R = 1.0/A elsewhere R = 1.0 elements of R set to 1.0 where A == 0.0 end where go to <statement number> old go to (<statement number list>), <expression> old for I/O statements, see: section 10.0 Input/Output Statements many old forms of statements are not listed","title":"Executable Statements and Constructs"},{"location":"Documentation/languages/fortran/f90/#declarations","text":"There are five (5) basic types: integer, real, complex, character and logical. There may be any number of user derived types. A modern (not old) declaration starts with a type, has attributes, then ::, then variable(s) names integer i, pivot, query old integer, intent (inout) :: arg1 integer (selected_int_kind (5)) :: i1, i2 integer, parameter :: m = 7 integer, dimension(0:4, -5:5, 10:100) :: A3D double precision x old real (selected_real_kind(15,300) :: x complex :: z logical, parameter :: what_if = .true. character, parameter :: me = \"Jon Squire\" type <name> a new user type, derived type declarations end type <name> type (<name>) :: stuff declaring stuff to be of derived type <name> real, dimension(:,:), allocatable, target :: A real, dimension(:,:), pointer :: P Attributes may be: allocatable no memory used here, allocate later dimension vector or multi dimensional array external will be defined outside this compilation intent argument may be in, inout or out intrinsic declaring function to be an intrinsic optional argument is optional parameter declaring a constant, can not be changed later pointer declaring a pointer private in a module, a private declaration public in a module, a public declaration save keep value from one call to the next, static target can be pointed to by a pointer Note: not all combinations of attributes are legal","title":"Declarations"},{"location":"Documentation/languages/fortran/f90/#key-words-other-than-io","text":"note: \"statement\" means key word that starts a statement, one line unless there is a continuation \"&amp;\" \"construct\" means multiple lines, usually ending with \"end ...\" \"attribute\" means it is used in a statement to further define \"old\" means it should not be used in new code allocatable attribute, no space allocated here, later allocate allocate statement, allocate memory space now for variable assign statement, old, assigned go to assignment attribute, means subroutine is assignment (=) block data construct, old, compilation unit, replaced by module call statement, call a subroutine case statement, used in select case structure character statement, basic type, intrinsic data type common statement, old, allowed overlaying of storage complex statement, basic type, intrinsic data type contains statement, internal subroutines and functions follow continue statement, old, a place to put a statement number cycle statement, continue the next iteration of a do loop data statement, old, initialized variables and arrays deallocate statement, free up storage used by specified variable default statement, in a select case structure, all others do construct, start a do loop double precision statement, old, replaced by selected_real_kind(15,300) else construct, part of if else if else end if else if construct, part of if else if else end if elsewhere construct, part of where elsewhere end where end block data construct, old, ends block data end do construct, ends do end function construct, ends function end if construct, ends if end interface construct, ends interface end module construct, ends module end program construct, ends program end select construct, ends select case end subroutine construct, ends subroutine end type construct, ends type end where construct, ends where entry statement, old, another entry point in a procedure equivalence statement, old, overlaid storage exit statement, continue execution outside of a do loop external attribute, old statement, means defines else where function construct, starts the definition of a function go to statement, old, requires fixed form statement number if statement and construct, if(...) statement implicit statement, \"none\" is preferred to help find errors in a keyword for intent, the argument is read only inout a keyword for intent, the argument is read/write integer statement, basic type, intrinsic data type intent attribute, intent(in) or intent(out) or intent(inout) interface construct, begins an interface definition intrinsic statement, says that following names are intrinsic kind attribute, sets the kind of the following variables len attribute, sets the length of a character string logical statement, basic type, intrinsic data type module construct, beginning of a module definition namelist statement, defines a namelist of input/output nullify statement, nullify(some_pointer) now points nowhere only attribute, restrict what comes from a module operator attribute, indicates function is an operator, like + optional attribute, a parameter or argument is optional out a keyword for intent, the argument will be written parameter attribute, old statement, makes variable real only pause old, replaced by stop pointer attribute, defined the variable as a pointer alias private statement and attribute, in a module, visible inside program construct, start of a main program public statement and attribute, in a module, visible outside real statement, basic type, intrinsic data type recursive attribute, allows functions and derived type recursion result attribute, allows naming of function result result(Y) return statement, returns from, exits, subroutine or function save attribute, old statement, keep value between calls select case construct, start of a case construct stop statement, terminate execution of the main procedure subroutine construct, start of a subroutine definition target attribute, allows a variable to take a pointer alias then part of if construct type construct, start of user defined type type ( ) statement, declaration of a variable for a users type use statement, brings in a module where construct, conditional assignment while construct, a while form of a do loop","title":"Key words (other than I/O)"},{"location":"Documentation/languages/fortran/f90/#key-words-related-to-io","text":"backspace statement, back up one record close statement, close a file endfile statement, mark the end of a file format statement, old, defines a format inquire statement, get the status of a unit open statement, open or create a file print statement, performs output to screen read statement, performs input rewind statement, move read or write position to beginning write statement, performs output","title":"Key words related to I/O"},{"location":"Documentation/languages/fortran/f90/#operators","text":"** exponentiation * multiplication / division + addition - subtraction // concatenation == .eq. equality /= .ne. not equal < .lt. less than > .gt. greater than <= .le. less than or equal >= .ge. greater than or equal .not. complement, negation .and. logical and .or. logical or .eqv. logical equivalence .neqv. logical not equivalence, exclusive or .eq. == equality, old .ne. /= not equal. old .lt. < less than, old .gt. > greater than, old .le. <= less than or equal, old .ge. >= greater than or equal, old Other punctuation: / ... / used in data, common, namelist and other statements (/ ... /) array constructor, data is separated by commas 6*1.0 in some contexts, 6 copies of 1.0 (i:j:k) in some contexts, a list i, i+k, i+2k, i+3k, ... i+nk&le;j (:j) j and all below (i:) i and all above (:) undefined or all in range","title":"Operators"},{"location":"Documentation/languages/fortran/f90/#constants","text":"Logical constants: .true. True .false. False Integer constants: 0 1 -1 123456789 Real constants: 0.0 1.0 -1.0 123.456 7.1E+10 -52.715E-30 Complex constants: (0.0, 0.0) (-123.456E+30, 987.654E-29) Character constants: \"ABC\" \"a\" \"123'abc$%#@!\" \" a quote \"\" \" 'ABC' 'a' '123\"abc$%#@!' ' a apostrophe '' ' Derived type values: type name character (len=30) :: last character (len=30) :: first character (len=30) :: middle end type name type address character (len=40) :: street character (len=40) :: more character (len=20) :: city character (len=2) :: state integer (selected_int_kind(5)) :: zip_code integer (selected_int_kind(4)) :: route_code end type address type person type (name) lfm type (address) snail_mail end type person type (person) :: a_person = person( name(\"Squire\",\"Jon\",\"S.\"), &amp; address(\"106 Regency Circle\", \"\", \"Linthicum\", \"MD\", 21090, 1936)) a_person%snail_mail%route_code == 1936","title":"Constants"},{"location":"Documentation/languages/fortran/f90/#inputoutput-statements","text":"open (<unit number>) open (unit=<unit number>, file=<file name>, iostat=<variable>) open (unit=<unit number>, ... many more, see below ) close (<unit number>) close (unit=<unit number>, iostat=<variable>, err=<statement number>, status=\"KEEP\") read (<unit number>) <input list> read (unit=<unit number>, fmt=<format>, iostat=<variable>, end=<statement number>, err=<statement number>) <input list> read (unit=<unit number>, rec=<record number>) <input list> write (<unit number>) <output list> write (unit=<unit number>, fmt=<format>, iostat=<variable>, err=<statement number>) <output list> write (unit=<unit number>, rec=<record number>) <output list> print *, <output list> print \"(<your format here, use apostrophe, not quote>)\", <output list> rewind <unit number> rewind (<unit number>, err=<statement number>) backspace <unit number> backspace (<unit number>, iostat=<variable>) endfile <unit number> endfile (<unit number>, err=<statement number>, iostat=<variable>) inquire ( <unit number>, exists = <variable>) inquire ( file=<\"name\">, opened = <variable1>, access = <variable2> ) inquire ( iolength = <variable> ) x, y, A ! gives \"recl\" for \"open\" namelist /<name>/ <variable list> defines a name list read(*,nml=<name>) reads some/all variables in namelist write(*,nml=<name>) writes all variables in namelist &amp;<name> <variable>=<value> ... <variable=value> / data for namelist read Input / Output specifiers access one of \"sequential\" \"direct\" \"undefined\" action one of \"read\" \"write\" \"readwrite\" advance one of \"yes\" \"no\" blank one of \"null\" \"zero\" delim one of \"apostrophe\" \"quote\" \"none\" end = <integer statement number> old eor = <integer statement number> old err = <integer statement number> old exist = <logical variable> file = <\"file name\"> fmt = <\"(format)\"> or <character variable> format form one of \"formatted\" \"unformatted\" \"undefined\" iolength = <integer variable, size of unformatted record> iostat = <integer variable> 0==good, negative==eof, positive==bad name = <character variable for file name> named = <logical variable> nml = <namelist name> nextrec = <integer variable> one greater than written number = <integer variable unit number> opened = <logical variable> pad one of \"yes\" \"no\" position one of \"asis\" \"rewind\" \"append\" rec = <integer record number> recl = <integer unformatted record size> size = <integer variable> number of characters read before eor status one of \"old\" \"new\" \"unknown\" \"replace\" \"scratch\" \"keep\" unit = <integer unit number> Individual questions direct = <character variable> \"yes\" \"no\" \"unknown\" formatted = <character variable> \"yes\" \"no\" \"unknown\" read = <character variable> \"yes\" \"no\" \"unknown\" readwrite = <character variable> \"yes\" \"no\" \"unknown\" sequential = <character variable> \"yes\" \"no\" \"unknown\" unformatted = <character variable> \"yes\" \"no\" \"unknown\" write = <character variable> \"yes\" \"no\" \"unknown\"","title":"Input/Output Statements"},{"location":"Documentation/languages/fortran/f90/#formats","text":"format an explicit format can replace * in any I/O statement. Include the format in apostrophes or quotes and keep the parenthesis. examples: print \"(3I5,/(2X,3F7.2/))\", <output list> write(6, '(a,E15.6E3/a,G15.2)' ) <output list> read(unit=11, fmt=\"(i4, 4(f3.0,TR1))\" ) <input list> A format includes the opening and closing parenthesis. A format consists of format items and format control items separated by comma. A format may contain grouping parenthesis with an optional repeat count. Format Items, data edit descriptors: key: w is the total width of the field (filled with *** if overflow) m is the least number of digits in the (sub)field (optional) d is the number of decimal digits in the field e is the number of decimal digits in the exponent subfield c is the repeat count for the format item n is number of columns cAw data of type character (w is optional) cBw.m data of type integer with binary base cDw.d data of type real -- same as E, old double precision cEw.d or Ew.dEe data of type real cENw.d or ENw.dEe data of type real -- exponent a multiple of 3 cESw.d or ESw.dEe data of type real -- first digit non zero cFw.d data of type real -- no exponent printed cGw.d or Gw.dEe data of type real -- auto format to F or E nH n characters follow the H, no list item cIw.m data of type integer cLw data of type logical -- .true. or .false. cOw.m data of type integer with octal base cZw.m data of type integer with hexadecimal base \"<string>\" literal characters to output, no list item '<string>' literal characters to output, no list item Format Control Items, control edit descriptors: BN ignore non leading blanks in numeric fields BZ treat nonleading blanks in numeric fields as zeros nP apply scale factor to real format items old S printing of optional plus signs is processor dependent SP print optional plus signs SS do not print optional plus signs Tn tab to specified column TLn tab left n columns TRn tab right n columns nX tab right n columns / end of record (implied / at end of all format statements) : stop format processing if no more list items <input list> can be: a variable an array name an implied do ((A(i,j),j=1,n) ,i=1,m) parenthesis and commas as shown note: when there are more items in the input list than format items, the repeat rules for formats applies. <output list> can be: a constant a variable an expression an array name an implied do ((A(i,j),j=1,n) ,i=1,m) parenthesis and commas as shown note: when there are more items in the output list than format items, the repeat rules for formats applies. Repeat Rules for Formats: Each format item is used with a list item. They are used in order. When there are more list items than format items, then the following rule applies: There is an implied end of record, /, at the closing parenthesis of the format, this is processed. Scan the format backwards to the first left parenthesis. Use the repeat count, if any, in front of this parenthesis, continue to process format items and list items. Note: an infinite loop is possible print \"(3I5/(1X/))\", I, J, K, L may never stop","title":"Formats"},{"location":"Documentation/languages/fortran/f90/#intrinsic-functions","text":"Intrinsic Functions are presented in alphabetical order and then grouped by topic. The function name appears first. The argument(s) and result give an indication of the type(s) of argument(s) and results. [,dim=] indicates an optional argument \"dim\". \"mask\" must be logical and usually conformable. \"character\" and \"string\" are used interchangeably. A brief description or additional information may appear.","title":"Intrinsic Functions"},{"location":"Documentation/languages/fortran/f90/#intrinsic-functions-alphabetical","text":"abs(integer_real_complex) result(integer_real_complex) achar(integer) result(character) integer to character acos(real) result(real) arccosine |real| &le; 1.0 0&le;result&le;Pi adjustl(character) result(character) left adjust, blanks go to back adjustr(character) result(character) right adjust, blanks to front aimag(complex) result(real) imaginary part aint(real [,kind=]) result(real) truncate to integer toward zero all(mask [,dim]) result(logical) true if all elements of mask are true allocated(array) result(logical) true if array is allocated in memory anint(real [,kind=]) result(real) round to nearest integer any(mask [,dim=}) result(logical) true if any elements of mask are true asin(real) result(real) arcsine |real| &le; 1.0 -Pi/2&le;result&le;Pi/2 associated(pointer [,target=]) result(logical) true if pointing atan(real) result(real) arctangent -Pi/2&le;result&le;Pi/2 atan2(y=real,x=real) result(real) arctangent -Pi&le;result&le;Pi bit_size(integer) result(integer) size in bits in model of argument btest(i=integer,pos=integer) result(logical) true if pos has a 1, pos=0.. ceiling(real) result(real) truncate to integer toward infinity char(integer [,kind=]) result(character) integer to character [of kind] cmplx(x=real [,y=real] [kind=]) result(complex) x+iy conjg(complex) result(complex) reverse the sign of the imaginary part cos(real_complex) result(real_complex) cosine cosh(real) result(real) hyperbolic cosine count(mask [,dim=]) result(integer) count of true entries in mask cshift(array,shift [,dim=]) circular shift elements of array, + is right date_and_time([date=] [,time=] [,zone=] [,values=]) y,m,d,utc,h,m,s,milli dble(integer_real_complex) result(real_kind_double) convert to double digits(integer_real) result(integer) number of bits to represent model dim(x=integer_real,y=integer_real) result(integer_real) proper subtraction dot_product(vector_a,vector_b) result(integer_real_complex) inner product dprod(x=real,y=real) result(x_times_y_double) double precision product eoshift(array,shift [,boundary=] [,dim=]) end-off shift using boundary epsilon(real) result(real) smallest positive number added to 1.0 /= 1.0 exp(real_complex) result(real_complex) e raised to a power exponent(real) result(integer) the model exponent of the argument floor(real) result(real) truncate to integer towards negative infinity fraction(real) result(real) the model fractional part of the argument huge(integer_real) result(integer_real) the largest model number iachar(character) result(integer) position of character in ASCII sequence iand(integer,integer) result(integer) bit by bit logical and ibclr(integer,pos) result(integer) argument with pos bit cleared to zero ibits(integer,pos,len) result(integer) extract len bits starting at pos ibset(integer,pos) result(integer) argument with pos bit set to one ichar(character) result(integer) pos in collating sequence of character ieor(integer,integer) result(integer) bit by bit logical exclusive or index(string,substring [,back=]) result(integer) pos of substring int(integer_real_complex) result(integer) convert to integer ior(integer,integer) result(integer) bit by bit logical or ishft(integer,shift) result(integer) shift bits in argument by shift ishftc(integer, shift) result(integer) shift circular bits in argument kind(any_intrinsic_type) result(integer) value of the kind lbound(array,dim) result(integer) smallest subscript of dim in array len(character) result(integer) number of characters that can be in argument len_trim(character) result(integer) length without trailing blanks lge(string_a,string_b) result(logical) string_a &ge; string_b lgt(string_a,string_b) result(logical) string_a > string_b lle(string_a,string_b) result(logical) string_a &le; string_b llt(string_a,string_b) result(logical) string_a < string_b log(real_complex) result(real_complex) natural logarithm log10(real) result(real) logarithm base 10 logical(logical [,kind=]) convert to logical matmul(matrix,matrix) result(vector_matrix) on integer_real_complex_logical max(a1,a2,a3,...) result(integer_real) maximum of list of values maxexponent(real) result(integer) maximum exponent of model type maxloc(array [,mask=]) result(integer_vector) indices in array of maximum maxval(array [,dim=] [,mask=]) result(array_element) maximum value merge(true_source,false_source,mask) result(source_type) choose by mask min(a1,a2,a3,...) result(integer-real) minimum of list of values minexponent(real) result(integer) minimum(negative) exponent of model type minloc(array [,mask=]) result(integer_vector) indices in array of minimum minval(array [,dim=] [,mask=]) result(array_element) minimum value mod(a=integer_real,p) result(integer_real) a modulo p modulo(a=integer_real,p) result(integer_real) a modulo p mvbits(from,frompos,len,to,topos) result(integer) move bits nearest(real,direction) result(real) nearest value toward direction nint(real [,kind=]) result(real) round to nearest integer value not(integer) result(integer) bit by bit logical complement pack(array,mask [,vector=]) result(vector) vector of elements from array present(argument) result(logical) true if optional argument is supplied product(array [,dim=] [,mask=]) result(integer_real_complex) product radix(integer_real) result(integer) radix of integer or real model, 2 random_number(harvest=real_out) subroutine, uniform random number 0 to 1 random_seed([size=] [,put=] [,get=]) subroutine to set random number seed range(integer_real_complex) result(integer_real) decimal exponent of model real(integer_real_complex [,kind=]) result(real) convert to real repeat(string,ncopies) result(string) concatenate n copies of string reshape(source,shape,pad,order) result(array) reshape source to array rrspacing(real) result(real) reciprocal of relative spacing of model scale(real,integer) result(real) multiply by 2**integer scan(string,set [,back]) result(integer) position of first of set in string selected_int_kind(integer) result(integer) kind number to represent digits selected_real_kind(integer,integer) result(integer) kind of digits, exp set_exponent(real,integer) result(real) put integer as exponent of real shape(array) result(integer_vector) vector of dimension sizes sign(integer_real,integer_real) result(integer_real) sign of second on first sin(real_complex) result(real_complex) sine of angle in radians sinh(real) result(real) hyperbolic sine of argument size(array [,dim=]) result(integer) number of elements in dimension spacing(real) result(real) spacing of model numbers near argument spread(source,dim,ncopies) result(array) expand dimension of source by 1 sqrt(real_complex) result(real_complex) square root of argument sum(array [,dim=] [,mask=]) result(integer_real_complex) sum of elements system_clock([count=] [,count_rate=] [,count_max=]) subroutine, all out tan(real) result(real) tangent of angle in radians tanh(real) result(real) hyperbolic tangent of angle in radians tiny(real) result(real) smallest positive model representation transfer(source,mold [,size]) result(mold_type) same bits, new type transpose(matrix) result(matrix) the transpose of a matrix trim(string) result(string) trailing blanks are removed ubound(array,dim) result(integer) largest subscript of dim in array unpack(vector,mask,field) result(v_type,mask_shape) field when not mask verify(string,set [,back]) result(integer) pos in string not in set","title":"Intrinsic Functions (alphabetical):"},{"location":"Documentation/languages/fortran/f90/#intrinsic-functions-grouped-by-topic","text":"","title":"Intrinsic Functions (grouped by topic):"},{"location":"Documentation/languages/fortran/f90/#intrinsic-functions-numeric","text":"abs(integer_real_complex) result(integer_real_complex) acos(real) result(real) arccosine |real| &le; 1.0 0&le;result&le;Pi aimag(complex) result(real) imaginary part aint(real [,kind=]) result(real) truncate to integer toward zero anint(real [,kind=]) result(real) round to nearest integer asin(real) result(real) arcsine |real| &le; 1.0 -Pi/2&le;result&le;Pi/2 atan(real) result(real) arctangent -Pi/2&le;result&le;Pi/2 atan2(y=real,x=real) result(real) arctangent -Pi&le;result&le;Pi ceiling(real) result(real) truncate to integer toward infinity cmplx(x=real [,y=real] [kind=]) result(complex) x+iy conjg(complex) result(complex) reverse the sign of the imaginary part cos(real_complex) result(real_complex) cosine cosh(real) result(real) hyperbolic cosine dble(integer_real_complex) result(real_kind_double) convert to double digits(integer_real) result(integer) number of bits to represent model dim(x=integer_real,y=integer_real) result(integer_real) proper subtraction dot_product(vector_a,vector_b) result(integer_real_complex) inner product dprod(x=real,y=real) result(x_times_y_double) double precision product epsilon(real) result(real) smallest positive number added to 1.0 /= 1.0 exp(real_complex) result(real_complex) e raised to a power exponent(real) result(integer) the model exponent of the argument floor(real) result(real) truncate to integer towards negative infinity fraction(real) result(real) the model fractional part of the argument huge(integer_real) result(integer_real) the largest model number int(integer_real_complex) result(integer) convert to integer log(real_complex) result(real_complex) natural logarithm log10(real) result(real) logarithm base 10 matmul(matrix,matrix) result(vector_matrix) on integer_real_complex_logical max(a1,a2,a3,...) result(integer_real) maximum of list of values maxexponent(real) result(integer) maximum exponent of model type maxloc(array [,mask=]) result(integer_vector) indices in array of maximum maxval(array [,dim=] [,mask=]) result(array_element) maximum value min(a1,a2,a3,...) result(integer-real) minimum of list of values minexponent(real) result(integer) minimum(negative) exponent of model type minloc(array [,mask=]) result(integer_vector) indices in array of minimum minval(array [,dim=] [,mask=]) result(array_element) minimum value mod(a=integer_real,p) result(integer_real) a modulo p modulo(a=integer_real,p) result(integer_real) a modulo p nearest(real,direction) result(real) nearest value toward direction nint(real [,kind=]) result(real) round to nearest integer value product(array [,dim=] [,mask=]) result(integer_real_complex) product radix(integer_real) result(integer) radix of integer or real model, 2 random_number(harvest=real_out) subroutine, uniform random number 0 to 1 random_seed([size=] [,put=] [,get=]) subroutine to set random number seed range(integer_real_complex) result(integer_real) decimal exponent of model real(integer_real_complex [,kind=]) result(real) convert to real rrspacing(real) result(real) reciprocal of relative spacing of model scale(real,integer) result(real) multiply by 2**integer set_exponent(real,integer) result(real) put integer as exponent of real sign(integer_real,integer_real) result(integer_real) sign of second on first sin(real_complex) result(real_complex) sine of angle in radians sinh(real) result(real) hyperbolic sine of argument spacing(real) result(real) spacing of model numbers near argument sqrt(real_complex) result(real_complex) square root of argument sum(array [,dim=] [,mask=]) result(integer_real_complex) sum of elements tan(real) result(real) tangent of angle in radians tanh(real) result(real) hyperbolic tangent of angle in radians tiny(real) result(real) smallest positive model representation transpose(matrix) result(matrix) the transpose of a matrix","title":"Intrinsic Functions (Numeric)"},{"location":"Documentation/languages/fortran/f90/#intrinsic-functions-logical-and-bit","text":"all(mask [,dim]) result(logical) true if all elements of mask are true any(mask [,dim=}) result(logical) true if any elements of mask are true bit_size(integer) result(integer) size in bits in model of argument btest(i=integer,pos=integer) result(logical) true if pos has a 1, pos=0.. count(mask [,dim=]) result(integer) count of true entries in mask iand(integer,integer) result(integer) bit by bit logical and ibclr(integer,pos) result(integer) argument with pos bit cleared to zero ibits(integer,pos,len) result(integer) extract len bits starting at pos ibset(integer,pos) result(integer) argument with pos bit set to one ieor(integer,integer) result(integer) bit by bit logical exclusive or ior(integer,integer) result(integer) bit by bit logical or ishft(integer,shift) result(integer) shift bits in argument by shift ishftc(integer, shift) result(integer) shift circular bits in argument logical(logical [,kind=]) convert to logical matmul(matrix,matrix) result(vector_matrix) on integer_real_complex_logical merge(true_source,false_source,mask) result(source_type) choose by mask mvbits(from,frompos,len,to,topos) result(integer) move bits not(integer) result(integer) bit by bit logical complement transfer(source,mold [,size]) result(mold_type) same bits, new type","title":"Intrinsic Functions (Logical and bit)"},{"location":"Documentation/languages/fortran/f90/#intrinsic-functions-character-or-string","text":"achar(integer) result(character) integer to character adjustl(character) result(character) left adjust, blanks go to back adjustr(character) result(character) right adjust, blanks to front char(integer [,kind=]) result(character) integer to character [of kind] iachar(character) result(integer) position of character in ASCII sequence ichar(character) result(integer) pos in collating sequence of character index(string,substring [,back=]) result(integer) pos of substring len(character) result(integer) number of characters that can be in argument len_trim(character) result(integer) length without trailing blanks lge(string_a,string_b) result(logical) string_a &ge; string_b lgt(string_a,string_b) result(logical) string_a > string_b lle(string_a,string_b) result(logical) string_a &le; string_b llt(string_a,string_b) result(logical) string_a < string_b repeat(string,ncopies) result(string) concatenate n copies of string scan(string,set [,back]) result(integer) position of first of set in string trim(string) result(string) trailing blanks are removed verify(string,set [,back]) result(integer) pos in string not in set","title":"Intrinsic Functions (Character or string)"},{"location":"Documentation/languages/fortran/f90/#fortran-95","text":"New Features The statement FORALL as an alternative to the DO-statement Partial nesting of FORALL and WHERE statements Masked ELSEWHERE Pure procedures Elemental procedures Pure procedures in specification expressions Revised MINLOC and MAXLOC Extensions to CEILING and FLOOR with the KIND keyword argument Pointer initialization Default initialization of derived type objects Increased compatibility with IEEE arithmetic A CPU_TIME intrinsic subroutine A function NULL to nullify a pointer Automatic deallocation of allocatable arrays at exit of scoping unit Comments in NAMELIST at input Minimal field at input Complete version of END INTERFACE Deleted Features real and double precision DO loop index variables branching to END IF from an outer block PAUSE statements ASSIGN statements and assigned GO TO statements and the use of an assigned integer as a FORMAT specification Hollerith editing in FORMAT See http://www.nsc.liu.se/~boein/f77to90/f95.html#17.5","title":"Fortran 95"},{"location":"Documentation/languages/fortran/f90/#references","text":"http://www.fortran.com/fortran/ Pointer to everything Fortran http://meteora.ucsd.edu/~pierce/fxdr_home_page.html Subroutines to do unformatted I/O across platforms, provided by David Pierce at UCSD http://www.nsc.liu.se/~boein/f77to90/a5.html A good reference for intrinsic functions https://wg5-fortran.org/N1551-N1600/N1579.pdf New Features of Fortran 2003 https://wg5-fortran.org/N1701-N1750/N1729.pdf New Features of Fortran 2008 http://www.nsc.liu.se/~boein/f77to90/ Fortran 90 for the Fortran 77 Programmer Fortran 90 Handbook Complete ANSI/ISO Reference . Jeanne Adams, Walt Brainerd, Jeanne Martin, Brian Smith, Jerrold Wagener Fortran 90 Programming . T. Ellis, Ivor Philips, Thomas Lahey https://github.com/llvm/llvm-project/blob/master/flang/docs/FortranForCProgrammers.md FFT stuff Fortran 95 and beyond","title":"References"},{"location":"Documentation/languages/python/NREL_python/","text":"Python on NREL HPC By design, the HPC is a time-shared multi-machine system which necessarily warrants some nuanced consideration for how environments are managed relative to a single machine with a single user. Sometimes, the default workflow for environment creation and usage is not the most optimal for some use-cases. Below is a list of common pitfalls that users have encountered historically while using Python and Anaconda on NREL HPC. Running a SLURM job that uses a conda environment which is stored in $HOME . Exhausting the $HOME storage quota (50GB on the current HPC system) usually because of conda's package cache combined with their user environments. Trying to share a conda environment from another user's /home directory. Forgetting to install jupyter in a new conda environment, resulting in using the base installation's version which doesn't have your dependencies installed. Let's discuss strategies to mitigate or avoid these kinds of problems Installing Conda Environments in Different Directories By default, conda will install new environments in $HOME/.conda . Generally speaking, this a sensible default\u2014it just happens to be the starting point to frequent issues that users have experienced historically. Something to consider is that conda has a --prefix flag which allows one to arbitrate where a conda environment gets installed to, notably allowing you to place environments on other file-systems and block devices besides the /home network-storage that is mounted on NREL HPC systems. For example, here is how one might create a project in their /scratch directory: ENV_PREFIX = \"/scratch/$USER/demo_scratch_env\" import os ; os . environ [ 'ENV_PREFIX' ] = ENV_PREFIX # Export this variable for cells below ! conda create -- quiet -- use - local -- yes \\ -- prefix $ ENV_PREFIX # `--prefix` in action \\ python = 3.7 Collecting package metadata: ...working... done Solving environment: ...working... done ## Package Plan ## environment location: /scratch/mbartlet/demo_scratch_env Preparing transaction: ...working... done Verifying transaction: ...working... done Executing transaction: ...working... done ! ls - ld $ ENV_PREFIX drwxr-xr-x. 3 mbartlet mbartlet 4096 Dec 3 11:10 /scratch/mbartlet/demo_scratch_env # Delete the demo environment for cleanliness ! conda - env remove -- yes -- quiet -- prefix $ ENV_PREFIX &>/ dev / null Below is a table which discusses the pros and cons of each block-device mount on NREL HPC as a location for storing your software environments. Block-device mounts Situations where you would want to use this block device for your conda environments Caveats to consider when using this mount /home $HOME/.conda is the default location for environments. For one-off environments, or if you don't create environments often, this is a reasonable location for your environments and doesn't require any extra flags or parameters. Files in \\$ HOME will not be purged so long as you have an active NREL HPC account. However, \\$ HOME is limited to a 50GB storage quota so you may have to take care to monitor your storage footprint. /scratch /scratch or /projects is ultimately where you want your environment to end up if your jobs have more than 1 node\u2014if your environment is in /home then every node in your job will be competing for read-access over a non-parallel network fabric to the source files of your environment. /scratch provides simultaneous access to all the nodes. A sensible approach is copying your environments from /home to /scratch as part of your job's initialization. /scratch storage is unlimited. /scratch is a parallel filesystem, meaning simultaneous filesystem operations by several nodes is possible and performant. However, the contents of /scratch are subject to purge after 28 days of inactivity. /projects This is a great place to put a conda environment that you anticipate sharing with your colleagues who are also working on the project. You can structure the permissions such that others in the project have read-only, write-only, or no access (we also encourage restoring these permissions at a later date so others on the project can manage your files without a hassle). /projects is also a parallel filesystem which reaps the same benefits as mentioned above. However, access to projects is contingent on having access to an HPC project allocation. Moreover, the storage quota allotted to each project is relative to the reasonableness of its requested needs, although a conda environment is very unlikely to have a significant storage footprint. As mentioned above, let's demonstrate one might go about copying an environment from /home to /scratch in a SLURM job. The below cell will generate a nice code block based on variables used earlier in this notebook, as well as environment variables within your user account: # Acquire a default project handle to procedurally generate a SLURM job import subprocess command = \"/nopt/nrel/utils/bin/hours_report | tail -1 | awk '{print $1}'\" # Grab a valid project handle command_array = [ '/bin/bash' , '-c' , command ] project_handle = subprocess . run ( command_array , stdout = subprocess . PIPE ) . stdout . decode ( 'utf-8' )[: - 1 ] import os ; os . environ [ 'DEFAULT_HANDLE' ] = project_handle # Export handle for cells below ! echo $ DEFAULT_HANDLE wks conda_home_env = \"py3\" # Acquire info about the default conda environment import subprocess command = f \"module load conda && . activate { conda_home_env } && echo $CONDA_PREFIX\" command_array = [ '/bin/bash' , '-lc' , # Have to run this from a login-shell command ] conda_home_env_prefix = subprocess . run ( command_array , stdout = subprocess . PIPE ) . stdout . decode ( 'utf-8' )[: - 1 ] import os ; os . environ [ 'CONDA_HOME_ENV_PREFIX' ] = conda_home_env_prefix # Export handle for cells below ! echo $ CONDA_HOME_ENV_PREFIX /home/mbartlet/.conda/envs/py3 from IPython.display import Markdown as md from os import environ as env SCRATCH_ENV = f \"/scratch/ { env [ 'USER' ] } /home_conda_clone\" body = f \"\"\" ```bash #!/usr/bin/env bash #SBATCH --account { env [ 'DEFAULT_HANDLE' ] } #SBATCH --time 5 #SBATCH --partition debug #SBATCH --nodes 2 export SCRATCH_ENV=\" { SCRATCH_ENV } \" rsync -avz --ignore-existing \" { env [ 'CONDA_HOME_ENV_PREFIX' ] } \" \"$SCRATCH_ENV\" &>/dev/null srun bash -l <<EOF module purge module load conda . activate \"$SCRATCH_ENV\" which python EOF rm -rf \"$SCRATCH_ENV\" # Optional clean-up \"\"\" #!/usr/bin/env bash #SBATCH --account wks #SBATCH --time 5 #SBATCH --partition debug #SBATCH --nodes 2 export SCRATCH_ENV = \"/scratch/mbartlet/home_conda_clone\" rsync -avz --ignore-existing \"/home/mbartlet/.conda/envs/py3\" \" $SCRATCH_ENV \" & >/dev/null srun bash -l <<EOF module purge module load conda . activate \"$SCRATCH_ENV\" which python EOF rm -rf \" $SCRATCH_ENV \" # Optional clean-up And after running what was generated above: [ mbartlet@el1 ~ ] $ cat slurm-1845968.out /scratch/mbartlet/home_conda_clone/bin/python /scratch/mbartlet/home_conda_clone/bin/python Which shows both nodes sourced the environment from /scratch","title":"NREL HPC Python"},{"location":"Documentation/languages/python/NREL_python/#python-on-nrel-hpc","text":"By design, the HPC is a time-shared multi-machine system which necessarily warrants some nuanced consideration for how environments are managed relative to a single machine with a single user. Sometimes, the default workflow for environment creation and usage is not the most optimal for some use-cases. Below is a list of common pitfalls that users have encountered historically while using Python and Anaconda on NREL HPC. Running a SLURM job that uses a conda environment which is stored in $HOME . Exhausting the $HOME storage quota (50GB on the current HPC system) usually because of conda's package cache combined with their user environments. Trying to share a conda environment from another user's /home directory. Forgetting to install jupyter in a new conda environment, resulting in using the base installation's version which doesn't have your dependencies installed. Let's discuss strategies to mitigate or avoid these kinds of problems","title":"Python on NREL HPC"},{"location":"Documentation/languages/python/NREL_python/#installing-conda-environments-in-different-directories","text":"By default, conda will install new environments in $HOME/.conda . Generally speaking, this a sensible default\u2014it just happens to be the starting point to frequent issues that users have experienced historically. Something to consider is that conda has a --prefix flag which allows one to arbitrate where a conda environment gets installed to, notably allowing you to place environments on other file-systems and block devices besides the /home network-storage that is mounted on NREL HPC systems. For example, here is how one might create a project in their /scratch directory: ENV_PREFIX = \"/scratch/$USER/demo_scratch_env\" import os ; os . environ [ 'ENV_PREFIX' ] = ENV_PREFIX # Export this variable for cells below ! conda create -- quiet -- use - local -- yes \\ -- prefix $ ENV_PREFIX # `--prefix` in action \\ python = 3.7 Collecting package metadata: ...working... done Solving environment: ...working... done ## Package Plan ## environment location: /scratch/mbartlet/demo_scratch_env Preparing transaction: ...working... done Verifying transaction: ...working... done Executing transaction: ...working... done ! ls - ld $ ENV_PREFIX drwxr-xr-x. 3 mbartlet mbartlet 4096 Dec 3 11:10 /scratch/mbartlet/demo_scratch_env # Delete the demo environment for cleanliness ! conda - env remove -- yes -- quiet -- prefix $ ENV_PREFIX &>/ dev / null Below is a table which discusses the pros and cons of each block-device mount on NREL HPC as a location for storing your software environments. Block-device mounts Situations where you would want to use this block device for your conda environments Caveats to consider when using this mount /home $HOME/.conda is the default location for environments. For one-off environments, or if you don't create environments often, this is a reasonable location for your environments and doesn't require any extra flags or parameters. Files in \\$ HOME will not be purged so long as you have an active NREL HPC account. However, \\$ HOME is limited to a 50GB storage quota so you may have to take care to monitor your storage footprint. /scratch /scratch or /projects is ultimately where you want your environment to end up if your jobs have more than 1 node\u2014if your environment is in /home then every node in your job will be competing for read-access over a non-parallel network fabric to the source files of your environment. /scratch provides simultaneous access to all the nodes. A sensible approach is copying your environments from /home to /scratch as part of your job's initialization. /scratch storage is unlimited. /scratch is a parallel filesystem, meaning simultaneous filesystem operations by several nodes is possible and performant. However, the contents of /scratch are subject to purge after 28 days of inactivity. /projects This is a great place to put a conda environment that you anticipate sharing with your colleagues who are also working on the project. You can structure the permissions such that others in the project have read-only, write-only, or no access (we also encourage restoring these permissions at a later date so others on the project can manage your files without a hassle). /projects is also a parallel filesystem which reaps the same benefits as mentioned above. However, access to projects is contingent on having access to an HPC project allocation. Moreover, the storage quota allotted to each project is relative to the reasonableness of its requested needs, although a conda environment is very unlikely to have a significant storage footprint. As mentioned above, let's demonstrate one might go about copying an environment from /home to /scratch in a SLURM job. The below cell will generate a nice code block based on variables used earlier in this notebook, as well as environment variables within your user account: # Acquire a default project handle to procedurally generate a SLURM job import subprocess command = \"/nopt/nrel/utils/bin/hours_report | tail -1 | awk '{print $1}'\" # Grab a valid project handle command_array = [ '/bin/bash' , '-c' , command ] project_handle = subprocess . run ( command_array , stdout = subprocess . PIPE ) . stdout . decode ( 'utf-8' )[: - 1 ] import os ; os . environ [ 'DEFAULT_HANDLE' ] = project_handle # Export handle for cells below ! echo $ DEFAULT_HANDLE wks conda_home_env = \"py3\" # Acquire info about the default conda environment import subprocess command = f \"module load conda && . activate { conda_home_env } && echo $CONDA_PREFIX\" command_array = [ '/bin/bash' , '-lc' , # Have to run this from a login-shell command ] conda_home_env_prefix = subprocess . run ( command_array , stdout = subprocess . PIPE ) . stdout . decode ( 'utf-8' )[: - 1 ] import os ; os . environ [ 'CONDA_HOME_ENV_PREFIX' ] = conda_home_env_prefix # Export handle for cells below ! echo $ CONDA_HOME_ENV_PREFIX /home/mbartlet/.conda/envs/py3 from IPython.display import Markdown as md from os import environ as env SCRATCH_ENV = f \"/scratch/ { env [ 'USER' ] } /home_conda_clone\" body = f \"\"\" ```bash #!/usr/bin/env bash #SBATCH --account { env [ 'DEFAULT_HANDLE' ] } #SBATCH --time 5 #SBATCH --partition debug #SBATCH --nodes 2 export SCRATCH_ENV=\" { SCRATCH_ENV } \" rsync -avz --ignore-existing \" { env [ 'CONDA_HOME_ENV_PREFIX' ] } \" \"$SCRATCH_ENV\" &>/dev/null srun bash -l <<EOF module purge module load conda . activate \"$SCRATCH_ENV\" which python EOF rm -rf \"$SCRATCH_ENV\" # Optional clean-up \"\"\" #!/usr/bin/env bash #SBATCH --account wks #SBATCH --time 5 #SBATCH --partition debug #SBATCH --nodes 2 export SCRATCH_ENV = \"/scratch/mbartlet/home_conda_clone\" rsync -avz --ignore-existing \"/home/mbartlet/.conda/envs/py3\" \" $SCRATCH_ENV \" & >/dev/null srun bash -l <<EOF module purge module load conda . activate \"$SCRATCH_ENV\" which python EOF rm -rf \" $SCRATCH_ENV \" # Optional clean-up And after running what was generated above: [ mbartlet@el1 ~ ] $ cat slurm-1845968.out /scratch/mbartlet/home_conda_clone/bin/python /scratch/mbartlet/home_conda_clone/bin/python Which shows both nodes sourced the environment from /scratch","title":"Installing Conda Environments in Different Directories"},{"location":"Documentation/languages/python/python/","text":"Python Eagle tutorials Python environments : Utilize a specific version of Python on Eagle and install packages Dask : Parallelize your Python code Jupyter notebooks : Run interactive notebooks on Eagle HPC Python Links to External resources * MPI4PY Python bindings to use MPI to distribute computations across cluster nodes * Dask Easily launch Dask workers on one node or across nodes * Numba Optimize your Python code to run faster * PyCUDA Utilize GPUs to accelerate computations","title":"Python"},{"location":"Documentation/languages/python/python/#python","text":"","title":"Python"},{"location":"Documentation/languages/python/python/#eagle-tutorials","text":"Python environments : Utilize a specific version of Python on Eagle and install packages Dask : Parallelize your Python code Jupyter notebooks : Run interactive notebooks on Eagle","title":"Eagle tutorials"},{"location":"Documentation/languages/python/python/#hpc-python","text":"Links to External resources * MPI4PY Python bindings to use MPI to distribute computations across cluster nodes * Dask Easily launch Dask workers on one node or across nodes * Numba Optimize your Python code to run faster * PyCUDA Utilize GPUs to accelerate computations","title":"HPC Python"},{"location":"Documentation/languages/python/dask/dask/","text":"Dask Dask provides a way to parallelize Python code either on a single node or across the cluster. It is similar to the functionality provided by Apache Spark, with easier setup. It provides a similar API to other common Python packages such as NumPY, Pandas, and others. Dask single node Dask can be used locally on your laptop or an individual node. Additionally, it provides wrappers for multiprocessing and threadpools. The advantage of using LocalCluster though is you can easily drop in another cluster configuration to further parallelize. from distributed import Client , LocalCluster import dask import time import random @dask . delayed def inc ( x ): time . sleep ( random . random ()) return x + 1 @dask . delayed def dec ( x ): time . sleep ( random . random ()) return x - 1 @dask . delayed def add ( x , y ): time . sleep ( random . random ()) return x + y def main (): cluster = LocalCluster ( n_workers = 2 ) client = Client ( cluster ) zs = [] for i in range ( 256 ): x = inc ( i ) y = dec ( x ) z = add ( x , y ) zs . append ( z ) result = dask . compute ( * zs ) print ( result ) if __name__ == \"__main__\" : main () Dask MPI Dask-MPI can be used to parallelize calculations across a number of nodes as part of a batch job submitted to slurm. Dask will automatically create a scheduler on rank 0 and workers will be created on all other ranks. Install Note: The version of dask-mpi installed via Conda may be incompatible with the MPI libaries on Eagle. Use the pip install instead. conda create -n daskmpi python=3.7 conda activate daskmpi pip install dask-mpi Python script : This script holds the calculation to be performed in the test function. The script relies on the Dask cluster setup on MPI which is created in the from distributed import Client , LocalCluster import dask import time from dask_mpi import initialize import random @dask . delayed def inc ( x ): time . sleep ( random . random ()) return x + 1 @dask . delayed def dec ( x ): time . sleep ( random . random ()) return x - 1 @dask . delayed def add ( x , y ): time . sleep ( random . random ()) return x + y def main (): initialize ( nanny = False , interface = 'ib0' , protocol = 'tcp' , memory_limit = 0.8 , local_directory = '/tmp/scratch/dask' , nthreads = 1 ) client = Client () zs = [] for i in range ( 256 ): x = inc ( i ) y = dec ( x ) z = add ( x , y ) zs . append ( z ) result = dask . compute ( * zs ) print ( result ) if __name__ == \"__main__\" : main () Running the above script with MPI will automatically set a Dask worker on each MPI rank. mpiexec -np 30 python dask_mpi.py Dask jobqueue Dask can also run using the Slurm scheduler already installed on Eagle. The Jobqueue library can handle submission of a computation to the cluster. This is particularly useful when running an interactive notebook or similar and you need to scale workers. import dask import time from dask_jobqueue import SLURMCluster from distributed import Client import random @dask . delayed def inc ( x ): time . sleep ( random . random ()) return x + 1 @dask . delayed def dec ( x ): time . sleep ( random . random ()) return x - 1 @dask . delayed def add ( x , y ): time . sleep ( random . random ()) return x + y def main (): cluster = SLURMCluster ( cores = 18 , memory = '24GB' , queue = 'short' , project = 'hpcapps' , walltime = '00:30:00' , interface = 'ib0' , processes = 18 , ) cluster . scale ( jobs = 2 ) client = Client ( cluster ) zs = [] for i in range ( 256 ): x = inc ( i ) y = dec ( x ) z = add ( x , y ) zs . append ( z ) result = dask . compute ( * zs ) print ( result ) if __name__ == \"__main__\" : main () References Dask documentation Dask Jobqueue Dask MPI","title":"Dask"},{"location":"Documentation/languages/python/dask/dask/#dask","text":"Dask provides a way to parallelize Python code either on a single node or across the cluster. It is similar to the functionality provided by Apache Spark, with easier setup. It provides a similar API to other common Python packages such as NumPY, Pandas, and others.","title":"Dask"},{"location":"Documentation/languages/python/dask/dask/#dask-single-node","text":"Dask can be used locally on your laptop or an individual node. Additionally, it provides wrappers for multiprocessing and threadpools. The advantage of using LocalCluster though is you can easily drop in another cluster configuration to further parallelize. from distributed import Client , LocalCluster import dask import time import random @dask . delayed def inc ( x ): time . sleep ( random . random ()) return x + 1 @dask . delayed def dec ( x ): time . sleep ( random . random ()) return x - 1 @dask . delayed def add ( x , y ): time . sleep ( random . random ()) return x + y def main (): cluster = LocalCluster ( n_workers = 2 ) client = Client ( cluster ) zs = [] for i in range ( 256 ): x = inc ( i ) y = dec ( x ) z = add ( x , y ) zs . append ( z ) result = dask . compute ( * zs ) print ( result ) if __name__ == \"__main__\" : main ()","title":"Dask single node"},{"location":"Documentation/languages/python/dask/dask/#dask-mpi","text":"Dask-MPI can be used to parallelize calculations across a number of nodes as part of a batch job submitted to slurm. Dask will automatically create a scheduler on rank 0 and workers will be created on all other ranks.","title":"Dask MPI"},{"location":"Documentation/languages/python/dask/dask/#install","text":"Note: The version of dask-mpi installed via Conda may be incompatible with the MPI libaries on Eagle. Use the pip install instead. conda create -n daskmpi python=3.7 conda activate daskmpi pip install dask-mpi Python script : This script holds the calculation to be performed in the test function. The script relies on the Dask cluster setup on MPI which is created in the from distributed import Client , LocalCluster import dask import time from dask_mpi import initialize import random @dask . delayed def inc ( x ): time . sleep ( random . random ()) return x + 1 @dask . delayed def dec ( x ): time . sleep ( random . random ()) return x - 1 @dask . delayed def add ( x , y ): time . sleep ( random . random ()) return x + y def main (): initialize ( nanny = False , interface = 'ib0' , protocol = 'tcp' , memory_limit = 0.8 , local_directory = '/tmp/scratch/dask' , nthreads = 1 ) client = Client () zs = [] for i in range ( 256 ): x = inc ( i ) y = dec ( x ) z = add ( x , y ) zs . append ( z ) result = dask . compute ( * zs ) print ( result ) if __name__ == \"__main__\" : main () Running the above script with MPI will automatically set a Dask worker on each MPI rank. mpiexec -np 30 python dask_mpi.py","title":"Install"},{"location":"Documentation/languages/python/dask/dask/#dask-jobqueue","text":"Dask can also run using the Slurm scheduler already installed on Eagle. The Jobqueue library can handle submission of a computation to the cluster. This is particularly useful when running an interactive notebook or similar and you need to scale workers. import dask import time from dask_jobqueue import SLURMCluster from distributed import Client import random @dask . delayed def inc ( x ): time . sleep ( random . random ()) return x + 1 @dask . delayed def dec ( x ): time . sleep ( random . random ()) return x - 1 @dask . delayed def add ( x , y ): time . sleep ( random . random ()) return x + y def main (): cluster = SLURMCluster ( cores = 18 , memory = '24GB' , queue = 'short' , project = 'hpcapps' , walltime = '00:30:00' , interface = 'ib0' , processes = 18 , ) cluster . scale ( jobs = 2 ) client = Client ( cluster ) zs = [] for i in range ( 256 ): x = inc ( i ) y = dec ( x ) z = add ( x , y ) zs . append ( z ) result = dask . compute ( * zs ) print ( result ) if __name__ == \"__main__\" : main ()","title":"Dask jobqueue"},{"location":"Documentation/languages/python/dask/dask/#references","text":"Dask documentation Dask Jobqueue Dask MPI","title":"References"},{"location":"blog/2020-12-01-numba/","text":"Numba is a just in time (JIT) compiler for Python and NumPy code. From their official website, \"Numba translates Python functions to optimized machine code at runtime using the industry-standard LLVM compiler library. Numba-compiled numerical algorithms in Python can approach the speeds of C or FORTRAN.\" @jit ( nopython = True ) def function_to_be_compiled (): # Standard numerical/NumPy code here ... Importantly, many functions require no changes or refactoring to gain this speedup. In this getting-started guide , we build an example environment on Eagle, test the performance of a Numba-compiled function using the most common implementation of the @jit decorator, and discuss what sorts of functions will see performance improvements when compiled.","title":"Speeding up Python Code with Numba"},{"location":"blog/2021-05-06-tf/","text":"TensorFlow is a widely used and powerful symbolic math library commonly used for a variety of machine learning techniques. TensorFlow has built in API support for regression, clustering, classification, hidden Markov models, neural networks, reinforcement learning, as well as, a variety of activation functions, loss function, and optimizers. TensorFlow has received growing adoption among scientists, researchers, and industry professionals for its broad applicability and flexibility. TensorFlow versions obtained from pip or conda installs may not be optimized for the CPU and GPU architectures found on Eagle. To address this, pre-compiled versions which are optimized both for the CPU and GPU architectures have been created and offer computational benefits compared to other installation approaches. These versions can easily be installed from the wheels provided in /nopt/nrel/apps/wheels/ which contains different TensorFlow versions. Here is an example of how you can install an optimized version of TensorFlow to your environment. pip install --upgrade --no-deps --force-reinstall /nopt/nrel/apps/wheels/tensorflow-2.4.0-cp38-cp38-linux_x86_64.whl These builds provide a significant advantage as illustrated below over the standard conda install of TensorFlow. A recent tutorial was given on this topic, for more information see the recording or checkout the tutorial materials","title":"Faster Machine Learning with Custom Built TensorFlow on Eagle"},{"location":"blog/2021-06-18-srun/","text":"Subjects covered Basics Pointers to Examples Why not just use mpiexec/mpirun? Simple runs Threaded (OpenMP) runs Hybrid MPI/OpenMPI MPMD - a simple distribution MPMD multinode 1. Basics Eagle uses the Slurm scheduler and applications run on a compute node must be run via the scheduler. For batch runs users write a script and submit the script using the sbatch command. The script tells the scheduler what resources are required including a limit on the time to run. The script also normally contains \"charging\" or account information. Here is a very basic script that just runs hostname to list the nodes allocated for a job. #!/bin/bash #SBATCH --nodes=2 #SBATCH --ntasks-per-node=1 #SBATCH --time=00:01:00 #SBATCH --account=hpcapps srun hostname Note we used the srun command to launch multiple (parallel) instances of our application hostname . This article primarily discusses options for the srun command to enable good parallel execution. In the script above we have asked for two nodes --nodes=2 and each node will run a single instance of hostname --ntasks-per-node=1 . If srun is not given options on the command line it will determine the number of tasks to run from the arguments in the header. Thus our output from the script given above will be two lines, a list of nodes allocated for the job. 2. Pointers to examples The page https://www.nrel.gov/hpc/eagle-batch-jobs.html has information about running jobs under Slurm including a link to example batch scripts. The page https://github.com/NREL/HPC/tree/master/slurm has many slurm examples ranging from simple to complex. This article is based on the second page. 3. Why not just use mpiexec/mpirun? The srun command is an integral part of the Slurm scheduling system. It \"knows\" the configuration of the machine and recognizes the environmental variables set by the scheduler, such as cores per nodes. Mpiexec and mpirun come with the MPI compilers. The amount of integration with the scheduler is implementation and install methodology dependent. They may not enable the best performance for your applications. In some cases they flat out just don't work correctly on Eagle. For example, when trying to run MPMD applications (different programs running on different cores) using the mpt version of mpiexec, the same programs gets launched on all cores. 4. Simple runs For our srun examples we will use two glorified \"Hello World\" programs, one in Fortran and the other in C. They are essentially the same program written in the two languages. They can be compiled as MPI, OpenMP, or as hybrid MPI/OpenMP. They are available from the NREL HPC repository https://github.com/NREL/HPC.git in the slurm/source directory or by running the wget commands shown below. wget https://raw.githubusercontent.com/NREL/HPC/master/slurm/source/fhostone.f90 wget https://raw.githubusercontent.com/NREL/HPC/master/slurm/source/mympi.f90 wget https://raw.githubusercontent.com/NREL/HPC/master/slurm/source/phostone.c wget https://raw.githubusercontent.com/NREL/HPC/master/slurm/source/makehello -O makefile After the files are downloaded you can build the programs using the mpt MPI compilers module purge module load mpt gcc/10.1.0 make or using Intel MPI compilers module purge module load intel-mpi gcc/10.1.0 make You will end up with the executables: fomp - Fortran Openmp program fhybrid - Fortran hybrid MPI/Openmp program fmpi - Fortran MPI program comp - C hybrid Openmp program chybrid - C hybrid MPI/Openmp program cmpi - C MPI program These programs have many options. Running with the command line option -h will show them. Not all options are applicable for all versions. Run without options the programs just print the hostname on which they were run. We look at our simple example again. Here we ask for 2 nodes, 4 tasks per node for a total of 8 tasks. #!/bin/bash #SBATCH --job-name=\"hostname\" #SBATCH --nodes=2 #SBATCH --ntasks-per-node=4 #SBATCH --ntasks=8 #SBATCH --time=00:10:00 srun ./cmpi This will produce (sorted) output like: r105u33 r105u33 r105u33 r105u33 r105u37 r105u37 r105u37 r105u37 In the above script we have nodes,ntasks-per-node and ntasks. You do not need to specify all three parameters but values that are specified must be consistent. If nodes is not specified it will default to 1. If ntasks is not specified it will default to 1 tasks per node. You can put --ntasks-per-node and/or --ntasks on the srun line. For example, to run a total of 9 tasks, 5 on one node and 4 on the second: #!/bin/bash #SBATCH --job-name=\"hostname\" #SBATCH --nodes=2 #SBATCH --time=00:10:00 srun --ntasks=9 ./cmpi 5. Threaded (OpenMP) runs The variable used to tell the operating system how many threads to use for an OpenMP program is OMP_NUM_THREADS. In the ideal world you could just set OMP_NUM_THREADS to a value, say 36, the number of cores on each Eagle node, and each thread would be assigned to a core. Unfortunately without setting additional variables you will get the requested number of threads but threads might not be spread across all cores. This can result in a significant slowdown. For a program that is computationally intensive if two threads get mapped to the same core the runtime will increase 100%. If all threads end up on the same core, the slowdown could actually be greater than the number of cores. Our example programs, phostone.c and fhostone.f90, have a nice feature. If you add -F to the command line they will produce a report showing on which core each thread runs. We are going to look at the C version of the code and compile it with both the Intel version of C, icc and with the Gnu compiler gcc. ml comp-intel/2020.1.217 gcc/10.1.0 gcc -fopenmp -DNOMPI phostone.c -o comp.gcc icc -fopenmp -DNOMPI phostone.c -o comp.icc Run the script... #!/bin/bash #SBATCH --job-name=\"hostname\" #SBATCH --cpus-per-task=36 ## ask for 10 minutes #SBATCH --time=00:10:00 #SBATCH --nodes=1 #SBATCH --partition=debug export OMP_NUM_THREADS=36 srun ./comp.gcc -F > gcc.out srun ./comp.gcc -F > icc.out Note we have added the line #SBATCH --cpus-per-task=36 . cpus-per-task should match the value of OMP_NUM_THREADS. We now look at the sorted head of each of the output files el3:nslurm> cat icc.out | sort -k6,6 task thread node name first task # on node core 0000 0030 r5i7n35 0000 0000 0000 0000 0001 r5i7n35 0000 0000 0001 0000 0034 r5i7n35 0000 0000 0001 0000 0002 r5i7n35 0000 0000 0002 0000 0035 r5i7n35 0000 0000 0002 0000 0032 r5i7n35 0000 0000 0003 . . . el3:nslurm> cat gcc.out | sort -k6,6 task thread node name first task # on node core 0000 0031 r5i7n35 0000 0000 0000 0000 0001 r5i7n35 0000 0000 0001 0000 0002 r5i7n35 0000 0000 0002 0000 0034 r5i7n35 0000 0000 0002 0000 0003 r5i7n35 0000 0000 0003 0000 0004 r5i7n35 0000 0000 0004 . . . The last column shows the core on which a thread is run. We see that there is duplication of cores, potentially leading to poor performance. There are two sets of environmental variables that can be used to map threads to cores. One variable is specific to the Intel compilers, KMP_AFFINITY. The others are general for OpenMP compilers and should work for any OpenMP compiler, OMP_PLACES and OMP_PROC_BIND. These are documented at: https://software.intel.com/content/www/us/en/develop/documentation/cpp-compiler-developer-guide-and-reference/top/optimization-and-programming-guide/openmp-support/openmp-library-support/thread-affinity-interface-linux-and-windows.html https://www.openmp.org/spec-html/5.0/openmpse52.html https://www.openmp.org/spec-html/5.0/openmpse53.html We ran each version of our code 100 times with 5 different settings. The settings were: export KMP_AFFINITY=verbose,scatter export KMP_AFFINITY=verbose,compact export OMP_PLACES=cores export OMP_PROC_BIND=spread export OMP_PLACES=cores export OMP_PROC_BIND=close NONE The table below shows the results of our runs. In particular, it shows the minimum number of cores used with the particular settings. 36 is the desired value. We see that for gcc the following settings worked well: export OMP_PLACES=cores export OMP_PROC_BIND=spread or export OMP_PLACES=cores export OMP_PROC_BIND=clone Setting KMP_AFFINITY did not work for gcc but for the Intel compiler KMP_AFFINITY also gave good results. Compiler Setting Worked min cores mean cores max cores gcc cores, close yes 36 36 36 gcc cores, spread yes 36 36 36 gcc KMP_AFFINITY=compact no 25 34.18 36 gcc KMP_AFFINITY=scatter no 26 34.56 36 gcc none no 28 34.14 36 icc cores, close yes 36 36 36 icc cores, spread yes 36 36 36 icc KMP_AFFINITY=compact yes 36 36 36 icc KMP_AFFINITY=scatter yes 36 36 36 icc none no 19 23.56 29 So our final working script for OpenMP programs could be: #!/bin/bash #SBATCH --job-name=\"hostname\" #SBATCH --cpus-per-task=36 ## ask for 10 minutes #SBATCH --time=00:10:00 #SBATCH --nodes=1 #SBATCH --partition=debug export OMP_NUM_THREADS=36 export OMP_PLACES=cores export OMP_PROC_BIND=close #export OMP_PROC_BIND=spread srun ./comp.gcc -F > gcc.out srun ./comp.gcc -F > icc.out When a job is run the SLURM_CPUS_PER_TASK is set to cpus-per-task so you may want to export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK More on this in the next section. 6. Hybrid MPI/OpenMPI The next script is just an extension of the last. We now run hybrid, a combination of MPI and OpenMP. Our base example programs, fhostame.f90 and phostname.c can be compiled in hybrid mode as well as in pure MPI and pure OpenMP. First we look at the (sorted) output from our program run in hybrid mode with 4 tasks on two nodes and 4 threads. MPI VERSION Intel(R) MPI Library 2019 Update 7 for Linux* OS task thread node name first task # on node core 0000 0000 r5i0n4 0000 0000 0000 0000 0001 r5i0n4 0000 0000 0004 0000 0002 r5i0n4 0000 0000 0009 0000 0003 r5i0n4 0000 0000 0014 0001 0000 r5i0n4 0000 0001 0018 0001 0001 r5i0n4 0000 0001 0022 0001 0002 r5i0n4 0000 0001 0027 0001 0003 r5i0n4 0000 0001 0032 0002 0000 r5i0n28 0002 0000 0000 0002 0001 r5i0n28 0002 0000 0004 0002 0002 r5i0n28 0002 0000 0009 0002 0003 r5i0n28 0002 0000 0014 0003 0000 r5i0n28 0002 0001 0018 0003 0001 r5i0n28 0002 0001 0022 0003 0002 r5i0n28 0002 0001 0027 0003 0003 r5i0n28 0002 0001 0032 total time 3.009 The first column is the MPI task number followed by the thread, then the node. The last column is the core on which that give task/thread was run. We can cat a list of unique combinations of nodes and cores by piping the file into grep ^0 | awk '{print $3, $6}' | sort -u | wc -l` We get 16 which is the number of tasks times the number of threads. That is, we have each task/thread assigned to its own core. This will give good performance. The script below runs on a fixed number of tasks (4 = 2 per node * 2 nodes) and using from 1 to cpus-per-task=18 threads. The variable SLURM_CPUS_PER_TASK is set by slurm to be cpus-per-task. After the srun line we post process the output to report core usage. #!/bin/bash #SBATCH --account=hpcapps #SBATCH --time=00:10:00 #SBATCH --nodes=2 #SBATCH --partition=short #SBATCH --cpus-per-task=18 #SBATCH --ntasks=4 module purge module load intel-mpi/2020.1.217 gcc/10.1.0 export OMP_PLACES=cores export OMP_PROC_BIND=spread echo \"CPT TASKS THREADS cores\" for n in `seq 1 $SLURM_CPUS_PER_TASK` ; do request=`python -c \"print($n*$SLURM_NTASKS)\"` have=72 if ((request <= have)); then export OMP_NUM_THREADS=$n srun --ntasks-per-core=1 -n $SLURM_NTASKS ./phostone.icc -F -t 3 > out.$SLURM_NTASKS.$OMP_NUM_THREADS # post process cores=`cat out.$SLURM_NTASKS.$OMP_NUM_THREADS | grep ^0 | awk '{print $3, $6}' | sort -u | wc -l` echo $SLURM_CPUS_PER_TASK \" \" $SLURM_NTASKS \" \" $OMP_NUM_THREADS \" \" $cores fi done Our final output from this script is: el3:stuff> cat slurm-7002718.out CPT TASKS THREADS cores 18 4 1 4 18 4 2 8 18 4 3 12 18 4 4 16 18 4 5 20 18 4 6 24 18 4 7 28 18 4 8 32 18 4 9 36 18 4 10 40 18 4 11 44 18 4 12 48 18 4 13 52 18 4 14 56 18 4 15 60 18 4 16 64 18 4 17 68 18 4 18 72 el3:stuff> The important lines are: #SBATCH --cpus-per-task=18 . . . export OMP_PLACES=cores export OMP_PROC_BIND=spread . . . srun --ntasks-per-core=1 -n $SLURM_NTASKS ./phostone.icc We need to set cpus-per-task to tell slurm we are going to run multithreaded and how many cores we are going to use for our threads. This should be set to the maximum number of threads per task we expect to use. We use the OMP variables to map threads to cores. IMPORTANT: using KMP_AFFINTY will not give the desired results. It will cause all threads for a task to be mapped to a single core. We can run this script for hybrid MPI/OpenMP programs as is or set the number of cpus-per-task and tasks on the sbatch command line. For example: sbatch --cpus-per-task=9 --ntasks=8 simple gives us: el3:stuff> cat slurm-7002858.out CPT TASKS THREADS cores 9 8 1 8 9 8 2 16 9 8 3 24 9 8 4 32 9 8 5 40 9 8 6 48 9 8 7 56 9 8 8 64 9 8 9 72 el3:stuff> 7. MPMD - a simple distribution Here we look at launching Multi Program Multi Data runs. We use a the --multi-prog option with srun. This involves creating a config_file that lists the programs we are going to run along with the task ID. See: https://computing.llnl.gov/tutorials/linux_clusters/multi-prog.html for a quick description of the format for the config_file. Here we create the file on the fly but it could be done beforehand. We have two MPI programs to run together, phostone and fhostone. They are actually the same program written in C and Fortran. In the real world MPMD applications would maybe run a GUI or a manager for one task and rest doing compute. The syntax for running MPMD programs is srun --multi-prog mapfile where mapfile is a config_file that lists the programs to run. It is possible to pass different arguments to each program as discussed in the link above. Here we just add command line arguments for task 0. Our mapfile has 8 programs listed. The even tasks are running phostone and the odd fhostone. Our script uses two for loops to add lines to the mapfile and then uses sed to append command line arguments to the first line. #!/bin/bash #SBATCH --account=hpcapps #SBATCH --time=00:10:00 #SBATCH --nodes=1 #SBATCH --partition=debug #SBATCH --cpus-per-task=1 # create our mapfile app1=./phostone for n in 0 2 4 6 ; do echo $n $app1 >> mapfile done app2=./fhostone for n in 1 3 5 7 ; do echo $n $app2 >> mapfile done # add a command line option to the first line # sed does an in-place change to the first line # of our mapfile adding *-F* sed -i \"1 s/$/ -F /\" mapfile cat mapfile export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK srun -n8 --multi-prog mapfile Here is the complete output including the mapfile and output from our two programs. Lines with three digits for core number were created by the Fortran version of the program. el3:stuff> cat *7003104* 0 ./phostone -F 2 ./phostone 4 ./phostone 6 ./phostone 1 ./fhostone 3 ./fhostone 5 ./fhostone 7 ./fhostone MPI VERSION Intel(R) MPI Library 2019 Update 7 for Linux* OS task thread node name first task # on node core 0000 0000 r1i7n35 0000 0000 0022 0001 0000 r1i7n35 0001 0000 021 0002 0000 r1i7n35 0000 0001 0027 0003 0000 r1i7n35 0003 0000 023 0004 0000 r1i7n35 0000 0002 0020 0005 0000 r1i7n35 0005 0000 025 0006 0000 r1i7n35 0000 0003 0026 0007 0000 r1i7n35 0007 0000 019 el3:stuff> 8. MPMD multinode Our final example again just extends the previous one. We want to add the capability to launch different numbers of tasks on a set of nodes and at the same time have different programs on each of the nodes. We create a mapfile to list the programs to run as was done above. In this case for illustration purposes we are running one copy of phostone and seven instances of fhostone. We add to that a hostfile that lists the nodes on which to run. The hostfile has one host per MPI task. #!/bin/bash #SBATCH --account=hpcapps #SBATCH --time=00:10:00 #SBATCH --nodes=2 #SBATCH --partition=debug export OMP_NUM_THREADS=1 # Create our mapfile rm -rf mapfile app1=./phostone for n in 0 ; do echo $n $app1 >> mapfile done app2=./fhostone for n in 1 2 3 4 5 6 7 ; do echo $n $app2 >> mapfile done # Add a command line option to the first line # sed does an in-place change to the first line # of our mapfile adding *-F* sed -i \"1 s/$/ -F /\" mapfile # Count of each app to run on a node counts=\"1 7\" # Get a list of nodes on a single line nodes=`scontrol show hostnames | tr '\\n' ' '` # Create our hostfile and tell slrum its name export SLURM_HOSTFILE=hostlist # It is possible to do this in bash but # I think this is easier to understand # in python. It uses the values for # counts and nodes set above. python - > $SLURM_HOSTFILE << EOF c=\"$counts\".split() nodes=\"$nodes\".split() k=0 for i in c: i=int(i) node=nodes[k] for j in range(0,i): print(node) k=k+1 EOF srun -n 8 --multi-prog mapfile Here is the output from our run including the mapfile and hostlist. Notice that the first instance of the set of running programs is the C version. It is the only thing running on the first nodes. The rest of the MPI tasks are the Fortran version of the program running on the second node. el3:stuff> cat slurm-7003587.out | sort -k3,3 -k1,1 MPI VERSION Intel(R) MPI Library 2019 Update 7 for Linux* OS task thread node name first task # on node core 0000 0000 r102u34 0000 0000 0004 0001 0000 r102u35 0001 0000 003 0002 0000 r102u35 0002 0000 000 0003 0000 r102u35 0001 0001 006 0004 0000 r102u35 0004 0000 007 0005 0000 r102u35 0001 0002 004 0006 0000 r102u35 0002 0001 005 0007 0000 r102u35 0001 0003 002 el3:stuff> cat mapfile 0 ./phostone -F 1 ./fhostone 2 ./fhostone 3 ./fhostone 4 ./fhostone 5 ./fhostone 6 ./fhostone 7 ./fhostone el3:stuff> c el3:stuff> cat hostlist r102u34 r102u35 r102u35 r102u35 r102u35 r102u35 r102u35 r102u35 el3:stuff>","title":"Using srun to launch applications under slurm"},{"location":"blog/2021-06-18-srun/#subjects-covered","text":"Basics Pointers to Examples Why not just use mpiexec/mpirun? Simple runs Threaded (OpenMP) runs Hybrid MPI/OpenMPI MPMD - a simple distribution MPMD multinode","title":"Subjects covered"},{"location":"blog/2021-06-18-srun/#1-basics","text":"Eagle uses the Slurm scheduler and applications run on a compute node must be run via the scheduler. For batch runs users write a script and submit the script using the sbatch command. The script tells the scheduler what resources are required including a limit on the time to run. The script also normally contains \"charging\" or account information. Here is a very basic script that just runs hostname to list the nodes allocated for a job. #!/bin/bash #SBATCH --nodes=2 #SBATCH --ntasks-per-node=1 #SBATCH --time=00:01:00 #SBATCH --account=hpcapps srun hostname Note we used the srun command to launch multiple (parallel) instances of our application hostname . This article primarily discusses options for the srun command to enable good parallel execution. In the script above we have asked for two nodes --nodes=2 and each node will run a single instance of hostname --ntasks-per-node=1 . If srun is not given options on the command line it will determine the number of tasks to run from the arguments in the header. Thus our output from the script given above will be two lines, a list of nodes allocated for the job.","title":"1. Basics"},{"location":"blog/2021-06-18-srun/#2-pointers-to-examples","text":"The page https://www.nrel.gov/hpc/eagle-batch-jobs.html has information about running jobs under Slurm including a link to example batch scripts. The page https://github.com/NREL/HPC/tree/master/slurm has many slurm examples ranging from simple to complex. This article is based on the second page.","title":"2. Pointers to examples"},{"location":"blog/2021-06-18-srun/#3-why-not-just-use-mpiexecmpirun","text":"The srun command is an integral part of the Slurm scheduling system. It \"knows\" the configuration of the machine and recognizes the environmental variables set by the scheduler, such as cores per nodes. Mpiexec and mpirun come with the MPI compilers. The amount of integration with the scheduler is implementation and install methodology dependent. They may not enable the best performance for your applications. In some cases they flat out just don't work correctly on Eagle. For example, when trying to run MPMD applications (different programs running on different cores) using the mpt version of mpiexec, the same programs gets launched on all cores.","title":"3. Why not just use mpiexec/mpirun?"},{"location":"blog/2021-06-18-srun/#4-simple-runs","text":"For our srun examples we will use two glorified \"Hello World\" programs, one in Fortran and the other in C. They are essentially the same program written in the two languages. They can be compiled as MPI, OpenMP, or as hybrid MPI/OpenMP. They are available from the NREL HPC repository https://github.com/NREL/HPC.git in the slurm/source directory or by running the wget commands shown below. wget https://raw.githubusercontent.com/NREL/HPC/master/slurm/source/fhostone.f90 wget https://raw.githubusercontent.com/NREL/HPC/master/slurm/source/mympi.f90 wget https://raw.githubusercontent.com/NREL/HPC/master/slurm/source/phostone.c wget https://raw.githubusercontent.com/NREL/HPC/master/slurm/source/makehello -O makefile After the files are downloaded you can build the programs","title":"4. Simple runs"},{"location":"blog/2021-06-18-srun/#using-the-mpt-mpi-compilers","text":"module purge module load mpt gcc/10.1.0 make","title":"using the mpt MPI compilers"},{"location":"blog/2021-06-18-srun/#or-using-intel-mpi-compilers","text":"module purge module load intel-mpi gcc/10.1.0 make You will end up with the executables: fomp - Fortran Openmp program fhybrid - Fortran hybrid MPI/Openmp program fmpi - Fortran MPI program comp - C hybrid Openmp program chybrid - C hybrid MPI/Openmp program cmpi - C MPI program These programs have many options. Running with the command line option -h will show them. Not all options are applicable for all versions. Run without options the programs just print the hostname on which they were run. We look at our simple example again. Here we ask for 2 nodes, 4 tasks per node for a total of 8 tasks. #!/bin/bash #SBATCH --job-name=\"hostname\" #SBATCH --nodes=2 #SBATCH --ntasks-per-node=4 #SBATCH --ntasks=8 #SBATCH --time=00:10:00 srun ./cmpi This will produce (sorted) output like: r105u33 r105u33 r105u33 r105u33 r105u37 r105u37 r105u37 r105u37 In the above script we have nodes,ntasks-per-node and ntasks. You do not need to specify all three parameters but values that are specified must be consistent. If nodes is not specified it will default to 1. If ntasks is not specified it will default to 1 tasks per node. You can put --ntasks-per-node and/or --ntasks on the srun line. For example, to run a total of 9 tasks, 5 on one node and 4 on the second: #!/bin/bash #SBATCH --job-name=\"hostname\" #SBATCH --nodes=2 #SBATCH --time=00:10:00 srun --ntasks=9 ./cmpi","title":"or using Intel MPI compilers"},{"location":"blog/2021-06-18-srun/#5-threaded-openmp-runs","text":"The variable used to tell the operating system how many threads to use for an OpenMP program is OMP_NUM_THREADS. In the ideal world you could just set OMP_NUM_THREADS to a value, say 36, the number of cores on each Eagle node, and each thread would be assigned to a core. Unfortunately without setting additional variables you will get the requested number of threads but threads might not be spread across all cores. This can result in a significant slowdown. For a program that is computationally intensive if two threads get mapped to the same core the runtime will increase 100%. If all threads end up on the same core, the slowdown could actually be greater than the number of cores. Our example programs, phostone.c and fhostone.f90, have a nice feature. If you add -F to the command line they will produce a report showing on which core each thread runs. We are going to look at the C version of the code and compile it with both the Intel version of C, icc and with the Gnu compiler gcc. ml comp-intel/2020.1.217 gcc/10.1.0 gcc -fopenmp -DNOMPI phostone.c -o comp.gcc icc -fopenmp -DNOMPI phostone.c -o comp.icc Run the script... #!/bin/bash #SBATCH --job-name=\"hostname\" #SBATCH --cpus-per-task=36 ## ask for 10 minutes #SBATCH --time=00:10:00 #SBATCH --nodes=1 #SBATCH --partition=debug export OMP_NUM_THREADS=36 srun ./comp.gcc -F > gcc.out srun ./comp.gcc -F > icc.out Note we have added the line #SBATCH --cpus-per-task=36 . cpus-per-task should match the value of OMP_NUM_THREADS. We now look at the sorted head of each of the output files el3:nslurm> cat icc.out | sort -k6,6 task thread node name first task # on node core 0000 0030 r5i7n35 0000 0000 0000 0000 0001 r5i7n35 0000 0000 0001 0000 0034 r5i7n35 0000 0000 0001 0000 0002 r5i7n35 0000 0000 0002 0000 0035 r5i7n35 0000 0000 0002 0000 0032 r5i7n35 0000 0000 0003 . . . el3:nslurm> cat gcc.out | sort -k6,6 task thread node name first task # on node core 0000 0031 r5i7n35 0000 0000 0000 0000 0001 r5i7n35 0000 0000 0001 0000 0002 r5i7n35 0000 0000 0002 0000 0034 r5i7n35 0000 0000 0002 0000 0003 r5i7n35 0000 0000 0003 0000 0004 r5i7n35 0000 0000 0004 . . . The last column shows the core on which a thread is run. We see that there is duplication of cores, potentially leading to poor performance. There are two sets of environmental variables that can be used to map threads to cores. One variable is specific to the Intel compilers, KMP_AFFINITY. The others are general for OpenMP compilers and should work for any OpenMP compiler, OMP_PLACES and OMP_PROC_BIND. These are documented at: https://software.intel.com/content/www/us/en/develop/documentation/cpp-compiler-developer-guide-and-reference/top/optimization-and-programming-guide/openmp-support/openmp-library-support/thread-affinity-interface-linux-and-windows.html https://www.openmp.org/spec-html/5.0/openmpse52.html https://www.openmp.org/spec-html/5.0/openmpse53.html We ran each version of our code 100 times with 5 different settings. The settings were: export KMP_AFFINITY=verbose,scatter export KMP_AFFINITY=verbose,compact export OMP_PLACES=cores export OMP_PROC_BIND=spread export OMP_PLACES=cores export OMP_PROC_BIND=close NONE The table below shows the results of our runs. In particular, it shows the minimum number of cores used with the particular settings. 36 is the desired value. We see that for gcc the following settings worked well: export OMP_PLACES=cores export OMP_PROC_BIND=spread or export OMP_PLACES=cores export OMP_PROC_BIND=clone Setting KMP_AFFINITY did not work for gcc but for the Intel compiler KMP_AFFINITY also gave good results. Compiler Setting Worked min cores mean cores max cores gcc cores, close yes 36 36 36 gcc cores, spread yes 36 36 36 gcc KMP_AFFINITY=compact no 25 34.18 36 gcc KMP_AFFINITY=scatter no 26 34.56 36 gcc none no 28 34.14 36 icc cores, close yes 36 36 36 icc cores, spread yes 36 36 36 icc KMP_AFFINITY=compact yes 36 36 36 icc KMP_AFFINITY=scatter yes 36 36 36 icc none no 19 23.56 29 So our final working script for OpenMP programs could be: #!/bin/bash #SBATCH --job-name=\"hostname\" #SBATCH --cpus-per-task=36 ## ask for 10 minutes #SBATCH --time=00:10:00 #SBATCH --nodes=1 #SBATCH --partition=debug export OMP_NUM_THREADS=36 export OMP_PLACES=cores export OMP_PROC_BIND=close #export OMP_PROC_BIND=spread srun ./comp.gcc -F > gcc.out srun ./comp.gcc -F > icc.out When a job is run the SLURM_CPUS_PER_TASK is set to cpus-per-task so you may want to export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK More on this in the next section.","title":"5. Threaded (OpenMP) runs"},{"location":"blog/2021-06-18-srun/#6-hybrid-mpiopenmpi","text":"The next script is just an extension of the last. We now run hybrid, a combination of MPI and OpenMP. Our base example programs, fhostame.f90 and phostname.c can be compiled in hybrid mode as well as in pure MPI and pure OpenMP. First we look at the (sorted) output from our program run in hybrid mode with 4 tasks on two nodes and 4 threads. MPI VERSION Intel(R) MPI Library 2019 Update 7 for Linux* OS task thread node name first task # on node core 0000 0000 r5i0n4 0000 0000 0000 0000 0001 r5i0n4 0000 0000 0004 0000 0002 r5i0n4 0000 0000 0009 0000 0003 r5i0n4 0000 0000 0014 0001 0000 r5i0n4 0000 0001 0018 0001 0001 r5i0n4 0000 0001 0022 0001 0002 r5i0n4 0000 0001 0027 0001 0003 r5i0n4 0000 0001 0032 0002 0000 r5i0n28 0002 0000 0000 0002 0001 r5i0n28 0002 0000 0004 0002 0002 r5i0n28 0002 0000 0009 0002 0003 r5i0n28 0002 0000 0014 0003 0000 r5i0n28 0002 0001 0018 0003 0001 r5i0n28 0002 0001 0022 0003 0002 r5i0n28 0002 0001 0027 0003 0003 r5i0n28 0002 0001 0032 total time 3.009 The first column is the MPI task number followed by the thread, then the node. The last column is the core on which that give task/thread was run. We can cat a list of unique combinations of nodes and cores by piping the file into grep ^0 | awk '{print $3, $6}' | sort -u | wc -l` We get 16 which is the number of tasks times the number of threads. That is, we have each task/thread assigned to its own core. This will give good performance. The script below runs on a fixed number of tasks (4 = 2 per node * 2 nodes) and using from 1 to cpus-per-task=18 threads. The variable SLURM_CPUS_PER_TASK is set by slurm to be cpus-per-task. After the srun line we post process the output to report core usage. #!/bin/bash #SBATCH --account=hpcapps #SBATCH --time=00:10:00 #SBATCH --nodes=2 #SBATCH --partition=short #SBATCH --cpus-per-task=18 #SBATCH --ntasks=4 module purge module load intel-mpi/2020.1.217 gcc/10.1.0 export OMP_PLACES=cores export OMP_PROC_BIND=spread echo \"CPT TASKS THREADS cores\" for n in `seq 1 $SLURM_CPUS_PER_TASK` ; do request=`python -c \"print($n*$SLURM_NTASKS)\"` have=72 if ((request <= have)); then export OMP_NUM_THREADS=$n srun --ntasks-per-core=1 -n $SLURM_NTASKS ./phostone.icc -F -t 3 > out.$SLURM_NTASKS.$OMP_NUM_THREADS # post process cores=`cat out.$SLURM_NTASKS.$OMP_NUM_THREADS | grep ^0 | awk '{print $3, $6}' | sort -u | wc -l` echo $SLURM_CPUS_PER_TASK \" \" $SLURM_NTASKS \" \" $OMP_NUM_THREADS \" \" $cores fi done Our final output from this script is: el3:stuff> cat slurm-7002718.out CPT TASKS THREADS cores 18 4 1 4 18 4 2 8 18 4 3 12 18 4 4 16 18 4 5 20 18 4 6 24 18 4 7 28 18 4 8 32 18 4 9 36 18 4 10 40 18 4 11 44 18 4 12 48 18 4 13 52 18 4 14 56 18 4 15 60 18 4 16 64 18 4 17 68 18 4 18 72 el3:stuff> The important lines are: #SBATCH --cpus-per-task=18 . . . export OMP_PLACES=cores export OMP_PROC_BIND=spread . . . srun --ntasks-per-core=1 -n $SLURM_NTASKS ./phostone.icc We need to set cpus-per-task to tell slurm we are going to run multithreaded and how many cores we are going to use for our threads. This should be set to the maximum number of threads per task we expect to use. We use the OMP variables to map threads to cores. IMPORTANT: using KMP_AFFINTY will not give the desired results. It will cause all threads for a task to be mapped to a single core. We can run this script for hybrid MPI/OpenMP programs as is or set the number of cpus-per-task and tasks on the sbatch command line. For example: sbatch --cpus-per-task=9 --ntasks=8 simple gives us: el3:stuff> cat slurm-7002858.out CPT TASKS THREADS cores 9 8 1 8 9 8 2 16 9 8 3 24 9 8 4 32 9 8 5 40 9 8 6 48 9 8 7 56 9 8 8 64 9 8 9 72 el3:stuff>","title":"6. Hybrid MPI/OpenMPI"},{"location":"blog/2021-06-18-srun/#7-mpmd-a-simple-distribution","text":"Here we look at launching Multi Program Multi Data runs. We use a the --multi-prog option with srun. This involves creating a config_file that lists the programs we are going to run along with the task ID. See: https://computing.llnl.gov/tutorials/linux_clusters/multi-prog.html for a quick description of the format for the config_file. Here we create the file on the fly but it could be done beforehand. We have two MPI programs to run together, phostone and fhostone. They are actually the same program written in C and Fortran. In the real world MPMD applications would maybe run a GUI or a manager for one task and rest doing compute. The syntax for running MPMD programs is srun --multi-prog mapfile where mapfile is a config_file that lists the programs to run. It is possible to pass different arguments to each program as discussed in the link above. Here we just add command line arguments for task 0. Our mapfile has 8 programs listed. The even tasks are running phostone and the odd fhostone. Our script uses two for loops to add lines to the mapfile and then uses sed to append command line arguments to the first line. #!/bin/bash #SBATCH --account=hpcapps #SBATCH --time=00:10:00 #SBATCH --nodes=1 #SBATCH --partition=debug #SBATCH --cpus-per-task=1 # create our mapfile app1=./phostone for n in 0 2 4 6 ; do echo $n $app1 >> mapfile done app2=./fhostone for n in 1 3 5 7 ; do echo $n $app2 >> mapfile done # add a command line option to the first line # sed does an in-place change to the first line # of our mapfile adding *-F* sed -i \"1 s/$/ -F /\" mapfile cat mapfile export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK srun -n8 --multi-prog mapfile Here is the complete output including the mapfile and output from our two programs. Lines with three digits for core number were created by the Fortran version of the program. el3:stuff> cat *7003104* 0 ./phostone -F 2 ./phostone 4 ./phostone 6 ./phostone 1 ./fhostone 3 ./fhostone 5 ./fhostone 7 ./fhostone MPI VERSION Intel(R) MPI Library 2019 Update 7 for Linux* OS task thread node name first task # on node core 0000 0000 r1i7n35 0000 0000 0022 0001 0000 r1i7n35 0001 0000 021 0002 0000 r1i7n35 0000 0001 0027 0003 0000 r1i7n35 0003 0000 023 0004 0000 r1i7n35 0000 0002 0020 0005 0000 r1i7n35 0005 0000 025 0006 0000 r1i7n35 0000 0003 0026 0007 0000 r1i7n35 0007 0000 019 el3:stuff>","title":"7. MPMD - a simple distribution"},{"location":"blog/2021-06-18-srun/#8-mpmd-multinode","text":"Our final example again just extends the previous one. We want to add the capability to launch different numbers of tasks on a set of nodes and at the same time have different programs on each of the nodes. We create a mapfile to list the programs to run as was done above. In this case for illustration purposes we are running one copy of phostone and seven instances of fhostone. We add to that a hostfile that lists the nodes on which to run. The hostfile has one host per MPI task. #!/bin/bash #SBATCH --account=hpcapps #SBATCH --time=00:10:00 #SBATCH --nodes=2 #SBATCH --partition=debug export OMP_NUM_THREADS=1 # Create our mapfile rm -rf mapfile app1=./phostone for n in 0 ; do echo $n $app1 >> mapfile done app2=./fhostone for n in 1 2 3 4 5 6 7 ; do echo $n $app2 >> mapfile done # Add a command line option to the first line # sed does an in-place change to the first line # of our mapfile adding *-F* sed -i \"1 s/$/ -F /\" mapfile # Count of each app to run on a node counts=\"1 7\" # Get a list of nodes on a single line nodes=`scontrol show hostnames | tr '\\n' ' '` # Create our hostfile and tell slrum its name export SLURM_HOSTFILE=hostlist # It is possible to do this in bash but # I think this is easier to understand # in python. It uses the values for # counts and nodes set above. python - > $SLURM_HOSTFILE << EOF c=\"$counts\".split() nodes=\"$nodes\".split() k=0 for i in c: i=int(i) node=nodes[k] for j in range(0,i): print(node) k=k+1 EOF srun -n 8 --multi-prog mapfile Here is the output from our run including the mapfile and hostlist. Notice that the first instance of the set of running programs is the C version. It is the only thing running on the first nodes. The rest of the MPI tasks are the Fortran version of the program running on the second node. el3:stuff> cat slurm-7003587.out | sort -k3,3 -k1,1 MPI VERSION Intel(R) MPI Library 2019 Update 7 for Linux* OS task thread node name first task # on node core 0000 0000 r102u34 0000 0000 0004 0001 0000 r102u35 0001 0000 003 0002 0000 r102u35 0002 0000 000 0003 0000 r102u35 0001 0001 006 0004 0000 r102u35 0004 0000 007 0005 0000 r102u35 0001 0002 004 0006 0000 r102u35 0002 0001 005 0007 0000 r102u35 0001 0003 002 el3:stuff> cat mapfile 0 ./phostone -F 1 ./fhostone 2 ./fhostone 3 ./fhostone 4 ./fhostone 5 ./fhostone 6 ./fhostone 7 ./fhostone el3:stuff> c el3:stuff> cat hostlist r102u34 r102u35 r102u35 r102u35 r102u35 r102u35 r102u35 r102u35 el3:stuff>","title":"8. MPMD multinode"}]}