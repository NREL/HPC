{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"NREL HPC Resources","text":""},{"location":"#intro","title":"Intro","text":"<p>A collection of various resources, examples, and executables for the general NREL HPC user community's benefit.</p> <p>Documentation</p>"},{"location":"#contributing","title":"Contributing","text":"<p>These docs are driven by the NREL HPC community. They are currently under active development. If you would like to contribute or recommend a topic to be covered please open an issue or pull request in the repository. </p> <p>Docs repository </p>"},{"location":"#workshops","title":"Workshops","text":"<p>The HPC community also hosts workshops covering various topics. Check the training calendar below as well as the Computational Sciences Tutorials team to view all tutorials and workshops put on by the Computational Science Center. Join the team to receive notifications and seek out new or upcoming tutorials. For more information on joining this channel as a user external to NREL, see our Help and Support page. For a list of slide desks and recordings of past trainings, visit the HPC Tutorials Sharepoint page.</p>"},{"location":"#additional-nrel-resources","title":"Additional NREL resources","text":"<ul> <li>About NREL HPC</li> <li>User Basics</li> </ul>"},{"location":"#calendar","title":"Calendar","text":"<p>Calendar of training events and office hours for NREL's HPC. </p> <p>"},{"location":"Documentation/","title":"Documentation Home","text":"<p>Welcome to the central source of user-contributed documentation for NREL's HPC systems. This repository is open to both NREL and non-NREL HPC users. You can browse the documentation here, or start contributing by visiting the repository in Git for more information.</p>"},{"location":"Documentation/#where-to-begin","title":"Where to Begin","text":"<p>Please use the navigation bar on the left to explore the available documentation by category.</p>"},{"location":"Documentation/#highlights","title":"Highlights","text":"<ul> <li>Systems Guide to learn about our HPC systems</li> <li>Jupyterhub to get started with Jupyter Notebooks </li> <li>Conda environment howto and HPC-specific information</li> </ul>"},{"location":"Documentation/#other-nrel-documentation-resources","title":"Other NREL Documentation Resources","text":"<ul> <li>The NREL HPC Website is the home of Advanced Computing at NREL</li> <li>Our Github Repository for specific application examples, scripts, workshop content, the contributor guide, and more. </li> <li>The gh-pages branch (this site) is also open for contribution.</li> </ul>"},{"location":"Documentation/getting_started/","title":"Getting Started","text":"<p>In order to use the NREL HPC systems, you will need to request a user account. For a guide to accessing our systems, please see our User Basics guide</p> <p>Below we've collected answers for many of the most frequently asked questions. </p>"},{"location":"Documentation/getting_started/#frequently-asked-questions","title":"Frequently Asked Questions","text":"What is high-performance computing? <p>Generally speaking, HPC infrastructure is coordinating many discrete units capable of independent computation to cooperate on portions of a task to complete far more computation in a given amount of time than any of the units could do individually. In other words, an HPC system is lots of individual computers working together.</p> Is NREL HPC related to the Information Technology Services Desk? <p>HPC Operations and Information Technology Services (ITS) are separate groups with different responsibilities. ITS will handle issues with your workstation or any other digital device you are issued by NREL. HPC Operations will assist with issues regarding HPC systems.  Note that your NREL HPC account is a different account from your ITS credentials that you use to login to your workstation, e-mail, and the many other IT services provided by the Service Desk.</p> What are project allocations? <p>Over the fiscal year, there is a given amount of time each computer in the HPC system(s) can be expected to be operational and capable of performing computation. HPC project allocations allocate a portion of the total assumed available computing time. The sum of all awarded project allocations' compute-time approximates the projected availability of the entire system. Project allocations are identified by a unique \"handle\" which doubles as a Linux account under which you submit HPC jobs related to the project to the job scheduler. Learn more about requesting an allocation.</p> How can I access NREL HPC systems? <p>Begin by requesting an NREL HPC account.  Then, consult our guide on how to connect to  NREL HPC systems.</p> What is a one-time password (OTP) token? <p>OTP tokens are a means of two-factor authentication by combining a temporary (usually lasting 60 seconds) token to use along with your account password. Tokens are generated using the current time stamp and a secure hashing algorithm.  Note that you only need an  OTP to access systems outside the NREL firewall, namely if you are an external collaborator.  NREL employees can be on-site or use a VPN to access HPC systems via the *.hpc.nrel.gov domain.</p> What is a virtual private network (VPN)? <p>VPNs simulate being within a firewall (which is an aggressive filter on inbound network traffic) by encapsulating your traffic in a secure channel that funnels through the NREL network. While connected to a VPN, internal network domains such as *.hpc.nrel.gov can be accessed without secondary authentication (as the VPN itself counts as a secondary authentication). NREL employees may use the NREL VPN while external collaborators may use the NREL HPC VPN using their OTP token. This provides the convenience of not having to continually type in your current OTP token when accessing multiple systems in a single session.</p> What is a \"job?\" <p>This is the general term used for any task submitted to the HPC systems to be queued and wait for available resources to be executed. Jobs vary in how computationally intensive they are.</p> What is a \"node?\" <p>A node is a complete, independent system with its own operating system and resources, much like your laptop or desktop. HPC nodes are typically designed to fit snugly in tight volumes, but in principle you could convert several laptops into a cluster, and they would then be \"nodes.\"</p> What are \"login\" and \"compute\" nodes? <p>Login nodes are the immediate systems your session is opened on once you successfully authenticate. They serve as preparation systems to stage your user\u00a0environment and launch jobs. These login nodes are shared resources, and because of that the HPC team employs a program called Arbiter2 to ensure that these resources aren't being used inappropriately (see 'What is proper NREL HPC login node etiquette' for more detail). Compute nodes are where your jobs get computed when submitted to the scheduler.  You gain exclusive access to compute nodes that are executing your jobs, whereas there  are often many users logged into the login nodes at any given time.</p> What is proper NREL HPC login node etiquette? <p>As mentioned above, login nodes are a shared resource, and are subject to process limiting based on usage. If you do computationally intensive work on these systems, it will unfairly occupy resources and make the system less responsive for other users. Please reserve your computationally intensive tasks (especially those that will fully utilize CPU cores) for jobs submitted to compute nodes. Offenders of login node abuse will be admonished accordingly. For more information please see our policy on what  constitutes inappropriate use.</p> What is \"system time?\" <p>System time is a regularly occurring interval of time during which NREL HPC systems are taken offline for necessary patches, updates, software installations, and anything else to keep the systems useful, updated, and secure. You will not be able to access  the system or submit jobs during system times. A reminder announcement is sent out prior to every system time detailing  what changes will take place, and includes an estimate of how long the system time will be.  You can check the system status page if you are ever  unsure if an NREL HPC system is currently down for system time.</p> How can I more closely emulate a Linux/macOS workflow on my Windows workstation? <p>As you become familiar with navigating the HPC Linux systems you may come to prefer to use the same command-line interfaces locally on your workstation to keep your workflow consistent. There are many terminal emulators that can be used on Windows which provide the common Linux and macOS command-line interface. The official Linux command-line emulator for Windows is known as the Windows Subsystem for Linux.  Other recommended terminal applications include: Git Bash, Git for WIndows,  Cmder, and MYSYS2. Note that PuTTY is not a terminal emulator,  it is only an SSH client. The applications listed above implement an <code>ssh</code> command,  which mirrors the functionality of PuTTY.</p> What is the secure shell (SSH) protocol? <p>Stated briefly, the SSH protocol establishes an encrypted channel to share various kinds of network traffic. Not to be confused with the <code>ssh</code> terminal command or  SSH clients which are applications that implement the SSH protocol in software to  create secure connections to remote systems.</p> Why aren't my jobs running? <p>Good question! There may be hundreds of reasons why. Please contact HPC support with a message containing as many relevant details as you can provide so we are more likely to be able to offer useful guidance (such as what software you're using, how you are submitting your job, what sort of data you are using, how you are setting up your software environment, etc.).</p>"},{"location":"Documentation/help/","title":"Help and Support","text":"<p>Get quick access to help resources for NREL's high-performance computing (HPC) systems.</p>"},{"location":"Documentation/help/#support-contact-information","title":"Support Contact Information","text":"<p>HPC-Help@nrel.gov - Email for general HPC questions, technical troubleshooting, account requests, and software installation assistance. In the email, please include your username, the system name, project handle, and any information that will help us identify and troubleshoot the issue. </p> <p>HPC-Requests@nrel.gov - Email for HPC questions related to allocation requests and to request increases/decreases to allocation units or storage. </p>"},{"location":"Documentation/help/#microsoft-teams","title":"Microsoft Teams","text":"<p>Each system has a Microsoft Teams channel where users can collaborate and post questions. The Swift and Vermilion Team chats are one of the primary ways we communicate announcements and status updates for these systems. </p> <p>We update the team channel members annually based on HPC project members. However, if we missed you and you would like to join, please use the following instructions:</p> Internal Users (NREL) <ol> <li>In Teams, click on the \"Teams\" icon in the far left navigation bar.</li> <li>Click \"Join or create a team\" in the lower left corner.</li> <li>In the \"Search teams\" bar in the upper far right corner, type the name of the channel you need to join (e.g., \"Vermilion\" or \"Swift\") and hit return.</li> <li>Click Join.</li> </ol> External Users (Non-NREL) <ol> <li>You will receive a welcome email from the team owner with information about the team.  Click on accept. </li> <li>If you have never created a MS Office 365 account, you will be prompted to create one. If you already have a MS Office 365 account, login. </li> <li>The first time you log in, you will be prompted to set up Microsoft Authenticator or other authenticator app.</li> <li>From your mobile device, download and install the app from the Apple Store (for iOS) or the Google Play Store (for Android) and open the app.<ul> <li>On your mobile device, you will be prompted to allow notifications. Select Allow.</li> <li>On your mobile device, click OK on the screen for what information Microsoft gathers.</li> <li>Click Skip on the \"Add personal account\" page.</li> <li>Click Skip on the \"Add non-Microsoft account\" page.</li> <li>Click Add Work Account on the \"Add work account\" page.</li> <li>Click OK to allow access to the camera.</li> </ul> </li> <li>Going forward, anytime you login, you will get a prompt on your phone to authenticate.</li> </ol>"},{"location":"Documentation/help/#additional-resources","title":"Additional Resources","text":"<p>HPC Website - Resources to get access to systems, basics on getting started with HPC, accounts and allocation information, and refer to our policies.</p> <p>Computational Sciences Tutorials Team: Staff in the Computational Science Center host multiple tutorials and workshops on various computational science topics throughout the year, such as Visualization, Cloud, HPC, and others. The team has a calendar of the upcoming training schedule and past slide decks and recordings. Please use the above instructions if you would like to join the team.  </p> <p>Code Repository: The repository contains a collection of code examples, executables, and utilities. It is open for contributions from the user community.</p> <p>HPC Office Hours: The HPC technical staff holds live office hours on alternating Tuesdays and Thursdays. Bring your HPC related questions for real-time discussion. </p>"},{"location":"Documentation/Applications/","title":"Applications","text":"<p>NREL maintains a variety of applications for use on the HPC systems. Please see the navigation bar on the left under \"Applications\" for more information on a specific application. </p> <p>These applications can be accessed through environment modules on the systems. Some may not be available on all systems, and there may be some additional packages installed that don't have a dedicated page here. Please run the <code>module avail</code> command on a system to see what is available. </p> <p>The following are packages that the NREL Computational Science Center supports:</p> Name Description AlphaFold Open-source inference pipeline to predict three-dimensional protein structures from input biological sequence data Ansys Enables modeling, simulation, and visualization of flow, turbulence, heat transfer and reactions for industrial applications ExaWind A suite of applications that simulate wind turbines and wind farms on accelerated systems Bonmin Open-source solver that leverages CBC and IPOPT to solve general mixed integer nonlinear programs (MINLP) CBC Open-source optimizer for solving mixed integer programs (MIP) Chemical Kinetics An overview of packages for chemical kinetics, thermodynamics, transport processes etc. CLP Open-source linear program solver COMSOL Multiphysics simulation environment Converge HPC CFD+, focused on engine modeling and simulation Couenne Open-source mixed integer nonlinear programming (MINLP) global optimization solver FEniCS Solving partial differential equations by the finite element method GAMS High-level modeling system for mathematical programming and optimization Gaussian Program for calculating molecular electronic structure and reactivity Gurobi Solver for mathematical programming IPOPT Open-source interior point nonlinear optimizer LAMMPS Open-source classical molecular dynamics program designed for massively parallel systems MACE MACE is a machine learning framework for developing interatomic potentials MATLAB General technical computing framework OpenFAST/FAST.Farm Software for wind turbine aero-servo-elastic calculations and wind farm load and performance analysis OpenFOAM Software for computational fluid dynamics PLEXOS Simulation software for modeling electric, gas, and water systems for optimizing energy markets Q-Chem ab initio quantum chemistry package for predicting molecular structures, reactivities, and vibrational, electronic and NMR spectra STAR-CCM+ Engineering simulation package from CD-adapco for solving problems involving flow of fluids or solids, heat transfer, and stress VASP Atomic scale materials modeling, e.g., electronic structure calculations and quantum-mechanical molecular dynamics, from first principles WRF Mesoscale numerical weather prediction system designed for both atmospheric research and operational forecasting applications Xpress Optimization algorithms and technologies to solve linear, mixed integer and non-linear problems"},{"location":"Documentation/Applications/alphafold/","title":"AlphaFold","text":"<p>AlphaFold is an open-source inference pipeline developed by Google DeepMind to predict three-dimensional protein structures from input biological sequence data. The default model from the third major release of AlphaFold (AlphaFold3) is currently only supported on Kestrel.</p>"},{"location":"Documentation/Applications/alphafold/#using-the-alphafold-module","title":"Using the AlphaFold module","text":"<p>AlphaFold runs as a containerized module on Kestrel and can leverage GPU resources. As such, you must be on a GPU node to load the module: <code>ml alphafold</code>. Once the module is loaded, run <code>run_alphafold --help</code> for the pipeline's help page.</p> <p>Note</p> <p>You need to be part of the <code>esp-alphafold3</code> group on Kestrel to access the model and its reference databases in <code>/kfs2/shared-projects/alphafold3</code>. To be added to the group, contact HPC-Help@nrel.gov.</p>"},{"location":"Documentation/Applications/alphafold/#input-data","title":"Input data","text":"<p>AlphaFold requires input data to be formatted in a special JSON style. Please see the link for the full breadth of available JSON elements, noting that anything specific to \"AlphaFold Server\" does not apply to the module on Kestrel. A simple input JSON file (<code>fold_input.json</code>) is provided below as an example. Refer to the next section for instructions on how to run AlphaFold on JSON files like this one.</p> Input JSON format example: <code>fold_input.json</code> <p>This is a simple example input JSON file for an AlphaFold run: <pre><code>{\n\"name\": \"2PV7\",\n\"sequences\": [\n    {\n    \"protein\": {\n        \"id\": [\"A\", \"B\"],\n        \"sequence\": \"GMRESYANENQFGFKTINSDIHKIVIVGGYGKLGGLFARYLRASGYPISILDREDWAVAESILANADVVIVSVPINLTLETIERLKPYLTENMLLADLTSVKREPLAKMLEVHTGAVLGLHPMFGADIASMAKQVVVRCDGRFPERYEWLLEQIQIWGAKIYQTNATEHDHNMTYIQALRHFSTFANGLHLSKQPINLANLLALSSPIYRLELAMIGRLFAQDAELYADIIMDKSENLAVIETLKQTYDEALTFFENNDRQGFIDAFHKVRDWFGDYSEQFLKESRQLLQQANDLKQG\"\n    }\n    }\n],\n\"modelSeeds\": [1],\n\"dialect\": \"alphafold3\",\n\"version\": 1\n}\n</code></pre></p>"},{"location":"Documentation/Applications/alphafold/#running-alphafold-on-kestrel","title":"Running AlphaFold on Kestrel","text":"<p>As of February 2025, AlphaFold3 is generally limited to running its inference pipeline on a single JSON on a single GPU device at a time. In other words, multi-GPU and/or multi-node jobs are not inherently supported by the software. As such, running the model sequentially on multiple JSONs requires the least amount of effort on the user's part. However, depending on the number of input JSONs, this limited throughput can lead to prohibitively long jobs in terms of research productivity. For cases in which you need to run AlphaFold on many JSON files representing as many individual sequences, we can instead leverage the \"embarrassing parallelism\" that Slurm provides with job arrays. </p> <p>Example submission scripts representing both strategies are given in the next sections.</p> <p>Note</p> <p>As for every GPU job, please ensure that you submit your AlphaFold jobs from one of the GPU login nodes.</p>"},{"location":"Documentation/Applications/alphafold/#low-throughput-sequentially-loop-through-json-files","title":"Low throughput: Sequentially loop through JSON files","text":"<p>If throughput is not a concern, running AlphaFold on several JSON files is straightforward. Simply load the AlphaFold module, save your JSON files into a folder that is found by the <code>--input_dir</code> option, and define an output directory with <code>--output_dir</code> in the <code>run_alphafold</code> command: </p> Sequentially loop through input JSON files in a folder: <code>af3_sequential.sh</code> <pre><code>#!/bin/bash\n#SBATCH -A hpcapps # Replace with your HPC account\n#SBATCH -t 00:30:00\n#SBATCH -n 32 # note that genetic searches run on the CPUs\n#SBATCH -N 1\n#SBATCH --job-name=af3\n#SBATCH --gres=gpu:1 # Alphafold inference cannot use more than one GPU\n#SBATCH --mem=80G # Note this is for system (i.e., CPU) RAM\n#SBATCH -o %x-%j.log\n\n# Load Alphafold3 module\nml alphafold\n\n# Run Alphafold3 inference pipeline to predict 3d structure from sequence\n# Note: the \"af_input\" folder contains \"fold_input.json\"\nrun_alphafold --input_dir af_input --output_dir af_output\n</code></pre>"},{"location":"Documentation/Applications/alphafold/#high-throughput-embarrassing-parallelism-using-job-arrays","title":"High throughput: \"Embarrassing parallelism\" using job arrays","text":"<p>If you need to run AlphaFold on many (e.g., dozens to hundreds) of JSON files, it is worth setting up a job array to run the model on each input in separate jobs from the same submission script. This is a form of \"embarrassing parallelism,\" i.e., a form of parallel computing in which each task is independent of any other and therefore does not require communication between nodes. Thinking of each input JSON as its own \"array task\" allows us to submit separate jobs that can run simultaneously, which can significantly increase throughput when compared to the <code>af3_sequential.sh</code> example above:</p> Submit separate jobs for each input JSON file: <code>af3_job_array.sh</code> <pre><code>#!/bin/bash\n#SBATCH -A hpcapps # Replace with your HPC account\n#SBATCH -t 00:30:00\n#SBATCH -n 32 # note that genetic searches run on the CPUs\n#SBATCH -N 1\n#SBATCH --job-name=af3\n#SBATCH --array=0-9 # length of the array corresponds to how many inputs you have (10 in this example)\n#SBATCH --gres=gpu:1 # Alphafold inference cannot use more than one GPU\n#SBATCH --mem=80G # Note this is for system (i.e., CPU) RAM\n#SBATCH -o %A_%a-%x.log\n\n# Load Alphafold3 module\nml alphafold\n\n# input_json_list.txt is a list of 10 individual JSON files for the run\nIFS=$'\\n' read -d '' -r -a input_jsons &lt; input_json_list.txt\n# JSON_INPUT indexes the $input_jsons bash array with the special SLURM_ARRAY_TASK_ID variable\nJSON_INPUT=${input_jsons[$SLURM_ARRAY_TASK_ID]}\n\n# Run Alphafold3 inference pipeline to predict 3d structure from sequence\n# Note the use of --json_path instead of --input_dir to run on a specific JSON\nrun_alphafold --json_path $JSON_INPUT --output_dir af_output\n</code></pre> <p>Let's break down some parts of the script to better understand its function:</p> <ul> <li><code>#SBATCH --array=0-9</code>: This submits an array of 10 individual jobs. The length of this array should match the number of inputs listed in <code>input_json_list.txt</code>.</li> <li><code>#SBATCH -o %A_%a-%x.log</code>: Send stdout and stderr to a file named after the parent Slurm array job ID (<code>%A</code>), the given array task ID (<code>%a</code>), and the name of the job (<code>%x</code>). For example, a 2-job array with a parent ID of <code>1000</code> and a job name of <code>af3</code> would create two log files named <code>1000_0-af3.log</code> and <code>1000_1-af3.log</code>.</li> <li><code>input_json_list.txt</code>: A file containing the paths to 10 input JSON files, each on a new line.</li> <li><code>IFS=$'\\n' read -d '' -r -a input_jsons &lt; input_json_list.txt</code>: Read in <code>input_json_list.txt</code> as a bash array named <code>input_jsons</code>, using a new line character (<code>\\n</code>) as the \"internal field separator\" (<code>IFS</code>).</li> <li><code>JSON_INPUT=${input_jsons[$SLURM_ARRAY_TASK_ID]}</code>: Index the <code>input_jsons</code> bash array with the special <code>$SLURM_ARRAY_TASK_ID</code> variable, which will point to a single input file for the given job based on what is specified in <code>input_json_list.txt</code>.</li> <li><code>run_alphafold --json_path $JSON_INPUT --output_dir af_output</code>: Note that unlike <code>af3_sequential.sh</code>, we use the <code>--json_path</code> option to point to the specific input JSON that was indexed via <code>${input_jsons[$SLURM_ARRAY_TASK_ID]}</code>.</li> </ul> <p>Submitting this script results in 10 individual jobs submitted to Slurm that run independently of each other, rather than sequentially. Since AlphaFold can only run on 1 GPU at a time, you can theoretically fit four simultaneous AlphaFold jobs on a single GPU node.</p> <p>As with the <code>af3_sequential.sh</code> example, each output will be found as a subfolder in the defined <code>--output_dir</code> passed to <code>run_alphafold</code>.</p>"},{"location":"Documentation/Applications/alphafold/#other-considerations-when-running-alphafold","title":"Other considerations when running AlphaFold","text":""},{"location":"Documentation/Applications/alphafold/#genetic-search-stage","title":"Genetic search stage","text":"<p>AlphaFold3 queries genetic/peptide databases found in <code>/kfs2/shared-projects/alphafold3</code> before it runs the structure prediction stage. Sequeunce querying with <code>jackhmmer</code> in the AlphaFold3 pipeline can currently only run on CPUs. As such, it benefits you to request up to 32 tasks on the requested node to speed up <code>jackhmmer</code> and other CPU-only packages. Since requesting 32 CPUs is 1/4th of the CPUs available on a GPU node, this does not affect how AUs are charged.</p> <p>Sequence querying can also consume a significant amount of system RAM. The AlphaFold documentation states that longer queries can use up to 64G of system (i.e., CPU) memory. As such, you should start with requesting 80G of system RAM via <code>#SBATCH --mem=80G</code>. This is below 1/4th of a GPU node's available system RAM and should be plenty for most queries. If you receive an \"out of memory\" error from Slurm, try increasing <code>--mem</code>, but keep in mind that if you exceed 1/4th of the system RAM on the node, you will be charged more AUs for that fraction of the node. Alternatively, you could reduce the number of CPU tasks with <code>#SBATCH -n</code> while keeping <code>#SBATCH --mem=80G</code> constant to increase the amount of effective RAM per CPU. However, keep in mind that with this approach, sequence querying will likely take longer, which will also increase the number of AUs charged for the job.</p>"},{"location":"Documentation/Applications/alphafold/#visualizing-the-outputs","title":"Visualizing the outputs","text":"<p>Each output subfolder will contain <code>&lt;job_name&gt;_model.cif</code>, which is the best predicted structure saved in mmCIF format. Note that you will need to use an external tool such as PyMol to visualize the predicted protein structure.</p> <p>Refer to the AlphaFold page for details regarding all files in the output directory.</p>"},{"location":"Documentation/Applications/ansys/","title":"ANSYS CFD","text":""},{"location":"Documentation/Applications/ansys/#ansys","title":"Ansys","text":"<p>The current Ansys license is an unlimited license that covers all Ansys products, with no restrictions on quantities. However, since Ansys is unable to provide a license file that includes all products in unlimited quantities, we have requested licenses based on our anticipated needs. You can check the available licenses on Kestrel using the command <code>lmstat.ansys</code>. If the module you need is not listed, please submit a ticket by emailing HPC-Help@nrel.gov so that we can request an updated license to include the specific module you require.</p> <p>The main workflow that we support has two stages. The first is interactive graphical usage, e.g., for interactively building meshes or visualizing boundary geometry. For this, Ansys should be run on a FastX desktop. The second stage is batch (i.e., non-interactive) parallel processing, which should be run on compute nodes via a Slurm job script. Of course, if you have Ansys input from another location ready to run in batch mode, the first stage is not needed. We unfortunately cannot support running parallel jobs on the DAV nodes, nor launching parallel jobs from interactive sessions on compute nodes.</p>"},{"location":"Documentation/Applications/ansys/#shared-license-etiquette","title":"Shared License Etiquette","text":"<p>Network floating licenses are a shared resource. Whenever you open an Ansys Fluent window, a license is pulled from the pool and becomes unavailable to other users. Please do not keep idle windows open if you are not actively using the application, close it and return the associated licenses to the pool. Excessive retention of software licenses falls under the inappropriate use policy.</p>"},{"location":"Documentation/Applications/ansys/#a-note-on-licenses-and-job-scaling","title":"A Note on Licenses and Job Scaling","text":"<p>HPC Pack licenses are used to distribute Ansys batch jobs to run in parallel across many compute cores. The HPC Pack model is designed to enable exponentially more computational resources per each additional license, roughly 2x4^(num_hpc_packs).  A table summarizing this relationship is shown below.</p> <pre><code>| HPC Pack Licenses Used | Total Cores Enabled           |\n|------------------------|-------------------------------|\n| 0                      | 4 (0 `hpc_pack` + 4 solver)     |\n| 1                      | 12 (8 `hpc_pack` + 4 solver)    |\n| 2                      | 36 (32 `hpc_pack` + 4 solver)   |\n| 3                      | 132 (128 `hpc_pack` + 4 solver) |\n| 4                      | 516 (512 `hpc_pack` + 4 solver) |\n</code></pre> <p>Additionally, Ansys allows you to use up to four cores without consuming any of the HPC Pack licenses.  When scaling these jobs to more than four cores, the four cores are added to the total amount made available by the HPC Pack licenses. For example, a batch job designed to completely fill a node with 36 cores requires one <code>cfd_base</code> license and two HPC Pack licenses (32 + 4 cores enabled).</p>"},{"location":"Documentation/Applications/ansys/#building-models-in-the-ansys-gui","title":"Building Models in the Ansys GUI","text":"<p>GUI access is provided through FastX desktops. Open a terminal, load, and launch the Ansys Workbench with:</p> <pre><code>module load ansys/&lt;version&gt;\nvglrun runwb2\n</code></pre> <p>where <code>&lt;version&gt;</code> will be replaced with an Ansys version/release e.g., <code>2025R1</code>. Press <code>tab</code> to auto-suggest all available versions. If no version is specified, it will load the default version which is 2025R1 at this moment. Because FastX desktop sessions are supported from DAV nodes shared between multiple HPC users, limits are placed on how much memory and compute resources can be consumed by a single user/job. For this reason, it is recommended that the GUI be primarily used to define the problem and run small-scale tests to validate its operation before moving the model to a compute node for larger-scale runs.</p>"},{"location":"Documentation/Applications/ansys/#running-ansys-model-in-parallel-batch-mode","title":"Running Ansys Model in Parallel Batch Mode","text":""},{"location":"Documentation/Applications/ansys/#ansys-fluent","title":"Ansys Fluent","text":"<p>Ansys Fluent is a general-purpose computational fluid dynamics (CFD) software used to model fluid flow, heat and mass transfer, chemical reactions, and more. It comes with the features of advanced physics modeling, turbulence modeling, single and multiphase flows, combustion, battery modeling, fluid-structure interaction.</p>"},{"location":"Documentation/Applications/ansys/#cpu-solver","title":"CPU Solver","text":"<p>To launch Ansys Fluent jobs in parallel batch mode on CPU nodes, you can build on the batch script presented below.</p> <pre><code>bash\n#!/bin/bash\n#SBATCH --job-name=jobname\n#SBATCH --account=&lt;your_account&gt;\n#SBATCH -o fluent_%j.out\n#SBATCH -e fluent_%j.err\n#SBATCH --nodes=2\n#SBATCH --time=1:00:00\n#SBATCH --ntasks-per-node=104\n#SBATCH --exclusive\n\ncd $SLURM_SUBMIT_DIR\nmodule load ansys/&lt;version&gt;\n\nexport FLUENT_AFFINITY=0\nexport SLURM_ENABLED=1\nexport SCHEDULER_TIGHT_COUPLING=13\nexport I_MPI_HYDRA_BOOTSTRAP=slurm\n\nscontrol show hostnames &gt; nodelist\n\nFLUENT=`which fluent`\nVERSION=3ddp\nJOURNAL=journal_name.jou\nLOGFILE=fluent.log\nMPI=intel\n\nOPTIONS=\"-i$JOURNAL -t$SLURM_NPROCS -mpi=$MPI -cnf=$PWD/nodelist\"\n\nnodelist &gt; fluent.log\n\n$FLUENT $VERSION -g $OPTIONS &gt; $LOGFILE 2&gt;&amp;1\n</code></pre> <p>Once this script file (assumed to be named <code>ansys-job.slurm</code>) is saved, it can be submitted to the job scheduler with</p> <pre><code>[user@kl3 ~]$ sbatch ansys-job.slurm\n</code></pre> <p>In this example batch script, <code>3ddp</code> can be replaced with the version of FLUENT your job requires (<code>2d</code>, <code>3d</code>, <code>2ddp</code>, or <code>3ddp</code>), <code>-g</code> specifies that the job should run without the GUI, <code>-t</code> specifies the number of processors to use (in this example, 2 x 104 processors), <code>-cnf</code> specifies the hosts file (the list of nodes allocated to this job), <code>-mpi</code> specifies the MPI implementation (intel or openmpi, Ansys uses its own mpi comes with the package instead of the mpi installed on our cluster, the current Ansys version only supports intel or openmpi), and <code>-i</code> is used to specify the job input file. For more Fluent options, you can run <code>fluent -help</code> to show after load the Ansys module.</p> <p>In addition, the following commands in the slurm script are included to make sure the right bootstrap is used:</p> <pre><code>export FLUENT_AFFINITY=0\nexport SLURM_ENABLED=1\nexport SCHEDULER_TIGHT_COUPLING=13\nexport I_MPI_HYDRA_BOOTSTRAP=slurm\n</code></pre>"},{"location":"Documentation/Applications/ansys/#gpu-solver","title":"GPU Solver","text":"<p>Ansys Fluent supports native GPU solver which is a solver architecture specifically designed to run on GPUs. With native GPU solver, the pre-processing (meshing, defining BCs, etc.) and post-processing are usually still done on the CPU while the solver takes over and runs almost entirely on the GPU. </p> <p>Note that, not all Fluent features support GPU solver. Specifically, the GPU solver is not available in the Ansys Workbench environment and profiles in cylindrical coordinate systems, which includes those used for swirl inlets, are not supported. For more information about GPU solver limitation, please refer to Ansys documentation.</p> <p>To launch Ansys Fluent jobs in parallel batch mode with GPU solver, you can build on the batch script presented below.</p> <p><pre><code>#!/bin/bash\n#SBATCH --job-name=fluent_GPU\n#SBATCH --account=&lt;your_account&gt;\n#SBATCH -o fluent_GPU_%j.out\n#SBATCH -e fluent_GPU_%j.err\n#SBATCH --nodes=2\n#SBATCH --time=2:00:00\n#SBATCH --ntasks-per-node=1\n#SBATCH --gres=gpu:1\n#SBATCH --mem-per-cpu=4G\n\nmodule load ansys/&lt;version&gt;\nexport FLUENT_AFFINITY=0\nexport SLURM_ENABLED=1\nexport SCHEDULER_TIGHT_COUPLING=13\nexport I_MPI_HYDRA_BOOTSTRAP=slurm\n\nscontrol show hostnames &gt; nodelist\n\nFLUENT=`which fluent`\nVERSION=3ddp\nJOURNAL=inputjournal.jou\nLOGFILE=fluent_GPU.log\nMPI=openmpi\n\nOPTIONS=\"-i$JOURNAL -t$SLURM_NTASKS -gpu -mpi=$MPI -cnf=$PWD/nodelist\"\n$FLUENT $VERSION -g $OPTIONS &gt; $LOGFILE 2&gt;&amp;1\n</code></pre> In this case, we are running this GPU job on 2 GPU nodes, 1 GPU per node, and 1 CPU per node. The <code>-gpu</code> flag in the lauch command enables the GPU solver. <code>-t</code> specifies the CPU and GPU configurations for the GPU solver as follows (for <code>-tn</code>):</p> <p>If only 1 GPU is available, 1 GPU + n\u00a0CPUs;</p> <p>If multiple GPUs are available and n\u00a0is less than the number of available GPUs, n\u00a0CPU processes and\u00a0n\u00a0GPUs.</p> <p>If multiple GPUs are avilable and the value of\u00a0n\u00a0is greater than or equal to the number of available GPUs, n\u00a0CPUs + all of the GPUs.</p>"},{"location":"Documentation/Applications/ansys/#ansys-mechanical","title":"Ansys Mechanical","text":"<p>Ansys Mechanical is a finite element analysis (FEA) software used to perform structural analysis using advanced solver options, including linear dynamics, nonlinearities, thermal analysis, materials, composites, hydrodynamic, explicit, and more. </p>"},{"location":"Documentation/Applications/ansys/#cpu-job","title":"CPU Job","text":"<p>The slurm script for Ansys Mechanical CPU jobs is presented as follows.</p> <p><pre><code>#!/bin/bash\n#\n#SBATCH --job-name=jobname\n#SBATCH --nodes=2\n#SBATCH --ntasks-per-node=2\n#SBATCH --time=1:00:00\n#SBATCH --exclusive\n#SBATCH --account=&lt;your_account&gt;\n#SBATCH --output=\"ansys-%j.out\"\n#SBATCH --error=\"ansys-%j.err\"\n\ncd $SLURM_SUBMIT_DIR\n\nmodule load ansys\n\nexport ANSYS_LOCK=OFF\nexport I_MPI_HYDRA_BOOTSTRAP=slurm\n\nunset HYDRA_LAUNCHER_EXTRA_ARGS\nunset I_MPI_HYDRA_BOOTSTRAP_EXEC_EXTRA_ARGS\nunset OMPI_MCA_plm_slurm_args\nunset PRTE_MCA_plm_slurm_args\n\nmachines=$(srun hostname | sort | uniq -c | awk '{print $2 \":\" $1}' | paste -s -\nd \":\" -)\n\nansys251 -dis -mpi intelmpi2018 -machines $machines -i inputfilename.\ndat -o joboutput.out\n</code></pre> In the slurm script, <code>ansys251</code> starts the Ansys mechanical module, <code>-dis</code> enables distributed-memory parallel processing, <code>-mpi</code> specifies the mpi to be used (intelmpi2018 or openmpi), <code>-machine</code> specifies the host names, <code>-i</code> is used to specify the job input file, and <code>-o</code> is used to specify the job output file.</p>"},{"location":"Documentation/Applications/ansys/#gpu-acceleration","title":"GPU Acceleration","text":"<p>Ansys Mechanical supports GPU acceleration in which GPUs are used to assist the CPUs in solving parts of the simulation \u2014 it\u2019s still mostly a CPU-based solver, but the GPU helps offload some of the heavy linear algebra (like matrix operations). In situations where the analysis type is not supported by the GPU accelerator capability, the solution will continue but GPU acceleration will not be used.</p> <p>The slurm script for Ansys Mechanical with GPU acceleration is presented as follows.</p> <pre><code>#!/bin/bash\n#SBATCH --job-name=jobname\n#SBATCH --nodes=2\n#SBATCH --ntasks-per-node=2\n#SBATCH --time=4:00:00\n#SBATCH --gres=gpu:2\n#SBATCH --mem-per-cpu=2G\n#SBATCH --account=&lt;your account&gt;\n#SBATCH --output=mechanical_GPGPU_%j.out\n#SBATCH --error=mechanical_GPGPU_%j.err\n\nmodule load ansys\n\nexport ANSYS_LOCK=OFF\nexport I_MPI_HYDRA_BOOTSTRAP=slurm\n\nunset HYDRA_LAUNCHER_EXTRA_ARGS\nunset I_MPI_HYDRA_BOOTSTRAP_EXEC_EXTRA_ARGS\nunset OMPI_MCA_plm_slurm_args\nunset PRTE_MCA_plm_slurm_args\n\nmachines=$(srun hostname | sort | uniq -c | awk '{print $2 \":\" $1}' | paste -s -d \":\" -)\n\ninput_file=test_mech.inp\noutput_file=test_mech_GPGPU.out\n\nansys251 -dis -mpi intelmpi2021 -acc nvidia -na 2 -machines $machines -i $input_file -o $output_file\n</code></pre> <p>In this example, the job will run on 2 GPU nodes with 2 GPUs per node and 2 CPUs per node. In the launching command, the flag <code>-acc nvidia</code> is an openacc flag to indicate Ansys Mechanical to compile and run on Nvidia GPUs. The flag <code>-na</code> specifies the number of GPU accelerator devices to use per node. </p>"},{"location":"Documentation/Applications/ansys/#a-few-notes","title":"A Few Notes","text":"<p>When running an Ansys job, the out of memory error (OOM) is commonly encountered. To overcome the out of memory issue, you can try the following:</p> <p>If you are running on shared nodes, by default, your job will be allocated about 1G of RAM per core requested. To change this amount, you can use the <code>--mem</code> or <code>--mem-per-cpu</code> flag in your job submission.</p> <p>Try to run the job on nodes with local disk by using the <code>--tmp</code> option in your job submission script (e.g. <code>--tmp=1600000</code> ).</p>"},{"location":"Documentation/Applications/ansys/#connect-to-your-own-license","title":"Connect to Your Own License","text":"<p>At NREL, a few groups own their own Ansys license. In order to connect to the private license, the user can set the environment variable <code>ANSYSLMD_LICENSE_FILE</code> (e.g. <code>export ANSYSLMD_LICENSE_FILE=1055@10.60.1.85</code>, replacing your corresponding port and license server hostname or IP address).  </p>"},{"location":"Documentation/Applications/ansys/#contact","title":"Contact","text":"<p>For information about accessing licenses beyond CSC's base capability, please contact Emily Cousineau.</p>"},{"location":"Documentation/Applications/chemicalKinetics/","title":"Chemical kinetics: Cantera, zero-RK, PelePhysics","text":"<p>Chemical kinetics packages are tools for problems involving chemical kinetics, thermodynamics, and transport processes. They can be useful in applications including but not limited to combustion, detonations, electrochemical energy conversion and storage, fuel cells, batteries, aqueous electrolyte solutions, plasmas, and thin film deposition.</p>"},{"location":"Documentation/Applications/chemicalKinetics/#overview","title":"Overview","text":"<p>A wide variety of packages are available for the purpose, each with their strengths, pros and cons. The matrix below provides a birds eye view of some of the packages. When applicable, please refer to the footnotes marked at the bottom of the page. (All company, product and service names used on this page are for identification purposes only. Use of these names, trademarks and brands does not imply endorsement.)</p> C++ Fortran Python Matlab GPU Speed*<sup>5</sup> Features Cost Compatibility Speciality Cantera y y y y x ++ ++++ Free Research codes*<sup>8</sup> Simplicity, large user base zero-RK y x x*<sup>6</sup> x y*<sup>1</sup> ++++*<sup>7</sup> ++*<sup>4</sup> Free Converge CFD ($) Model reduction tools PelePhysics y x x x y +++++ +++ Free Amrex/Pele HPC, NREL popular framework Chemkin Pro y y*<sup>2</sup> x x x*<sup>3</sup> ++++ ++++ $ Ansys ($) Legacy, professional support"},{"location":"Documentation/Applications/chemicalKinetics/#strategy","title":"Strategy","text":"<p>A typical workflow could look as follows:</p> <ol> <li> <p>Create mechanisms and validate with Cantera:</p> <p>\ud83d\udd3c Feature rich, multi language, very well documented, large support forum.</p> <p>\ud83d\udd3d Slower than competition, no GPU support.</p> </li> <li> <p>Perform model reduction with zero-RK if necessary:</p> <p>\ud83d\udd3c  Faster than Cantera &amp; Chemkin for more than 100 species, some GPU support.</p> <p>\ud83d\udd3d Fewer features, sparse documentation, C++ only.</p> </li> <li> <p>Convert to a high performance C++ code with PelePhysics and link to a CFD solver: </p> <p>\ud83d\udd3c  GPU, well documented, amrex/Pele CFD &amp; Cantera mechanisms compatible.</p> <p>\ud83d\udd3d Fewer features than Cantera &amp; Chemkin, C++ only. </p> </li> </ol>"},{"location":"Documentation/Applications/chemicalKinetics/#installation-and-testing-on-kestrel","title":"Installation and testing on Kestrel","text":"<p>Note</p> <p>Cantera can also be installed from source apart from the conda method explained below. The performance of packages mentioned on this page can vary depending on the choice of dependency library variants and optimization flags while compiling from source. We are more than happy to learn from power users about choices which lead to the best performance. Please report your experiences by email.</p> <p>Warning</p> <p>Conda environments should always be installed outside of your home directory for storage and performance reasons. This is especially important if linking a chemical kinetics package with a C++ code whose parallel processes can strain the <code>/home</code> filesystem. Please refer to our dedicated conda documentation for more information on how to setup your conda environments to redirect the installation outside of <code>/home</code> by default.</p>"},{"location":"Documentation/Applications/chemicalKinetics/#cantera","title":"Cantera","text":"<p>Installation: Python version <pre><code>$ module load anaconda3/2024.06.1\n$ cd /projects/&lt;projectname&gt;/&lt;username&gt;\n$ conda create --prefix ./ct-env --channel cantera cantera ipython matplotlib jupyter\n$ conda activate ./ct-env\n\n$ python3\n&gt;&gt;&gt; import cantera as ct\n&gt;&gt;&gt; ct.one_atm\n101325.0\n&gt;&gt;&gt; exit() \n$ conda deactivate\n</code></pre></p> Example interactive jupyter usage <p><pre><code>$ ssh username@kestrel.hpc.nrel.gov\n</code></pre> To access your scratch from the jupyter notebook, execute the following from your Kestrel home directory (optional) <pre><code>$ ln -s /scratch/username scratch\n</code></pre> <pre><code>$ module load anaconda3/2024.06.1\n$ conda activate /projects/&lt;projectname&gt;/&lt;username&gt;/ct-env\n</code></pre> Create a jupyter kernel from ct-env <pre><code>$ python -m ipykernel install --user --name=ct-env\n</code></pre> In a browser, go to Kestrel JupyterHub, select \u201cct-env\u201d in the Notebook section to open a new jupyter notebook with the \u2018ct-env\u2019 loaded</p> <p>Try Cantera python API within the notebook, for example, <pre><code>import cantera as ct\nct.one_atm\n</code></pre></p> <p>Installation: C++ version <pre><code>$ module load anaconda3/2024.06.1\n$ cd /projects/&lt;projectname&gt;/&lt;username&gt;\n$ conda create --prefix ./ct-dev --channel cantera libcantera-devel\n$ conda activate ./ct-dev\n$ conda install cmake scons pkg-config\n\n$ cd /projects/&lt;projectname&gt;/&lt;username&gt;/ct-dev/share/cantera/samples/cxx/demo\n$ scons &amp;&amp; ./demo\n$ cmake . &amp;&amp; cmake --build . &amp;&amp; ./demo\n$ g++ demo.cpp -o demo $(pkg-config --cflags --libs cantera) &amp;&amp; ./demo\n</code></pre></p> Example interactive C++ usage <p><pre><code>$ ssh username@kestrel.hpc.nrel.gov\n</code></pre> Allocate resources <pre><code>salloc --account=allocationName --time=00:30:00 --nodes=1 --ntasks-per-core=1 --ntasks-per-node=104 --cpus-per-task=1 --partition=debug\n</code></pre> Load Cantera <pre><code>module load anaconda3/2024.06.1\nconda activate /projects/&lt;projectname&gt;/&lt;username&gt;/ct-dev\n</code></pre> Compile your code <pre><code>CC -DEIGEN_USE_LAPACKE -DEIGEN_USE_BLAS -fopenmp -O3 -march=native -std=c++17 -I /projects/&lt;projectname&gt;/&lt;username&gt;/ct-dev/include/cantera/ext -I . mainYourCode.C $(pkg-config --cflags --libs cantera) -o flame.out\n</code></pre> Execute <pre><code>srun -n 5 ./flame.out\n</code></pre> Please refer to the job submission documentation for larger jobs in Batch mode.   </p>"},{"location":"Documentation/Applications/chemicalKinetics/#zero-rk","title":"zero-RK","text":"<p>Please follow the official installation instructions. The procedure has been tested successfully on the Kestrel and repeated below from the official instructions for convenience. <pre><code>$ cd /projects/&lt;projectname&gt;/&lt;username&gt;\n$ git clone https://github.com/llnl/zero-rk   #git\n$ cd zero-rk\n$ mkdir build\n$ cd build\n$ cmake ../                                   #configure\n$ make                                        #build\n$ ctest                                       #test\n$ make install                                #install\n</code></pre></p>"},{"location":"Documentation/Applications/chemicalKinetics/#pelephysics","title":"PelePhysics","text":"<p>Note</p> <p>Please mind the amrex dependency and remember to set the <code>AMREX_HOME</code> environment variable to your amrex location before beginning to compile PelePhysics.</p> <p>Please follow the official instructions for obtaining the PelePhysics library and compiling examples. The procedure has been tested successfully on the Kestrel. The process for obtaining PelePhysics and compiling the ReactEval example is repeated below from the official instructions for convenience. <pre><code>$ cd /projects/&lt;projectname&gt;/&lt;username&gt;\n$ git clone --recursive https://github.com/AMReX-Combustion/PelePhysics.git\n$ cd PelePhysics\n$ git pull &amp;&amp; git submodule update\n$ cd Testing/Exec/ReactEval\n$ make TPL\n$ make\n</code></pre></p>"},{"location":"Documentation/Applications/chemicalKinetics/#footnotes","title":"Footnotes","text":"<ol> <li> <p>Not clear from the documentation but \u2018gpu\u2019 exists in the code in several places. No actual GPU users amongst those surveyed at the NREL.\u00a0\u21a9</p> </li> <li> <p>Also possible through Chemkin II, which was a free Fortran library, not available online anymore.\u00a0\u21a9</p> </li> <li> <p>The Ansys Fluent CFD solver uses GPU, the Chemkin Pro module does not.\u00a0\u21a9</p> </li> <li> <p>Features unclear due to very poor documentation. Estimate based on reading parts of the code and NREL user comments.\u00a0\u21a9</p> </li> <li> <p>Very vague estimate from documentation and NREL user comments. Benchmarking not performed.\u00a0\u21a9</p> </li> <li> <p>Python scripts exist which gather parameters to execute C++ executables, no actual Python / Cython API like Cantera.\u00a0\u21a9</p> </li> <li> <p>Faster than Chemkin and Cantera for mechanisms involving more than 100 species, information from documentation.\u00a0\u21a9</p> </li> <li> <p>Coupling with various codes such as OpenFoam, Nek5000, JAX-Fluids etc. has been possible.\u00a0\u21a9</p> </li> </ol>"},{"location":"Documentation/Applications/comsol/","title":"COMSOL Multiphysics","text":"<p>COMSOL Multiphysics is a versatile finite element analysis and simulation package. The COMSOL graphical user interface (GUI) environment is supported primarily for building and solving small models while operation in batch mode allows users to scale their models to larger, higher-fidelity studies. Currently, we host three floating network licenses and a number of additional modules. Two COMSOL versions are available on Kestrel, they are 6.2 and 6.3.</p>"},{"location":"Documentation/Applications/comsol/#building-a-comsol-model","title":"Building a COMSOL Model","text":"<p>Extensive documentation is available in the menu: Help &gt; Documentation. For beginners, it is highly recommended to follow the steps in Introduction to COMSOL Multiphysics found in Help &gt; Documentation.</p> <p>For instructional videos, see the COMSOL website Video Gallery.</p>"},{"location":"Documentation/Applications/comsol/#building-models-in-the-comsol-gui","title":"Building Models in the COMSOL GUI","text":"<p>Before beginning, it is a good practice to check the license status. To do so, you need to run the following script command:</p> <pre><code>[user@kl3 ~]$ ./lmstat.comsol\n</code></pre> <p>When licenses are available, COMSOL can be used by starting the COMSOL GUI which allows you to build models, run the COMSOL computational engine, and analyze results. The COMSOL GUI can be accessed through a FastX desktop by opening a terminal in a FastX window and running the following commands:</p> <pre><code>[user@kl3 ~]$ module load comsol\n[user@kl3 ~]$ vglrun comsol\n</code></pre> <p>Because FastX desktop sessions are supported from DAV nodes shared between multiple HPC users, limits are placed on how much memory and compute resources can be consumed by a single user/job. For this reason, it is recommended that the GUI be primarily used to define the problem and run small-scale tests to validate its operation before moving the model to a compute node for larger-scale runs. </p> <p>For jobs that require both large-scale compute resources and GUI interactivity simultaneously, there is partial support for running the GUI from an X-enabled shell on a compute node.</p> <p>To do so, submit your interactive job and wait to obtain a node, and take note of the node name once the job has started. Open a secondary terminal on the FastX desktop, and <code>ssh -Y &lt;nodename&gt;</code> to connect to the job node with X-forwarding enabled. Then <code>module load comsol</code> on that node, and launch comsol. To run comsol with software rendering on a CPU-only node:</p> <pre><code>[user@kd1 ~]$ ssh -Y x1000c0s0b1n0\n\n[user@x1000c0s0b1n0 ~]$ module load comsol\n[user@x1000c0s0b1n0 ~]$ comsol -3drend sw\n</code></pre> <p>Note that performance with software rendering may be slow and certain display features may behave unexpectedly.</p>"},{"location":"Documentation/Applications/comsol/#running-a-single-node-comsol-model-in-batch-mode","title":"Running a Single-Node COMSOL Model in Batch Mode","text":"<p>You can save your model built in FastX+GUI mode into a file such as <code>myinputfile.mph</code>. Once that's available, the following job script shows how to run a single process multithreaded job in batch mode:</p> Example Submission Script <pre><code>#!/bin/bash                                                                                                                                                                                     \n#SBATCH --job-name=\"comsol-batch-single-node\"                                                                                                                                                   \n#SBATCH --nodes=1                                                                                                                                                                               \n#SBATCH --ntasks-per-node=104                                                                                                                                                                   \n#SBATCH --cpus-per-task=1                                                                                                                                                                       \n#SBATCH --time=00:10:0        \n#SBATCH --partition=debug\n#SBATCH --account=&lt;allocation handle&gt;\n#SBATCH --output=\"comsol-%j.out\"\n#SBATCH --error=\"comsol-%j.err\"\n\n# This helps ensure your job runs from the directory\n# from which you ran the sbatch command\nSLURM_SUBMIT_DIR=&lt;your working directory&gt;\ncd $SLURM_SUBMIT_DIR\n\n# Set up environment, and list to stdout for verification\nmodule load comsol\necho \" \"\nmodule list\necho \" \"\n\ninputfile=$SLURM_SUBMIT_DIR/myinputfile.mph\noutputfile=$SLURM_SUBMIT_DIR/myoutputfilename\nlogfile=$SLURM_SUBMIT_DIR/mylogfilename\n\n# Run a COMSOL job with 104 threads.\n\ncomsol batch -np 104 -inputfile $inputfile -outputfile $outputfile \u2013batchlog $logfile\n</code></pre> <p>Once this script file (e.g., <code>submit_single_node_job.sh</code>) is saved, it can be submitted to the job scheduler with</p> <pre><code>[user@kl3 ~]$ sbatch ./submit_single_node_job.sh\n</code></pre>"},{"location":"Documentation/Applications/comsol/#running-a-multi-node-comsol-model-in-batch-mode","title":"Running a Multi-Node COMSOL Model in Batch Mode","text":"<p>To configure a COMSOL job with multiple MPI ranks, required for any job where the number of nodes &gt;1, you can build on the following template:</p> Example Multiprocess Submission Script <pre><code>#!/bin/bash                                                                                                                                                                                     \n#SBATCH --job-name=\"comsol-batch-multinode-hybrid\"                                                                                                                                                  \n#SBATCH --nodes=4                                                                                                                                                                               \n#SBATCH --ntasks-per-node=8                                                                                                                                                                     \n#SBATCH --cpus-per-task=13                                                                                                                                                                      \n#SBATCH --time=00:10:0                                                                                                                                                                          \n#SBATCH --partition=debug                                                                                                                                                                       \n#SBATCH --exclusive                                                                                                                                                                             \n#SBATCH --account=&lt;allocation handle&gt;                                                                                                                                                                  \n#SBATCH --output=\"comsol-%j.out\"                                                                                                                                                                \n#SBATCH --error=\"comsol-%j.err\"                                                                                                                                                                 \n\n# This helps ensure your job runs from the directory                                                                                                                                            \n# from which you ran the sbatch command                                                                                                                                                         \nSLURM_SUBMIT_DIR= &lt;your working directory&gt;\ncd $SLURM_SUBMIT_DIR\n\n# Set up environment, and list to stdout for verification                                                                                                                                       \nmodule load comsol\necho \" \"\nmodule list\necho \" \"\n\nexport SLURM_MPI_TYPE=pmi2\nexport OMP_NUM_THREADS=13\n\ninputfile=$SLURM_SUBMIT_DIR/myinputfile.mph\noutputfile=$SLURM_SUBMIT_DIR/myoutputfilename\nlogfile=$SLURM_SUBMIT_DIR/mylogfilename\n\n# Run a 4-node job with 32 MPI ranks and 13 OpenMP threads per each rank.                                                                                                                        \ncomsol batch -mpibootstrap slurm -inputfile $inputfile -outputfile $outputfile \u2013batchlog $logfile\n</code></pre> <p>The job script can be submitted to SLURM just the same as above for the single-node example. The option <code>-mpibootstrap slurm</code> helps COMSOL to deduce runtime parameters such as <code>-nn</code>, <code>-nnhost</code> and <code>-np</code>. For large jobs that require more than one node, this approach, which uses MPI and/or OpenMP, can be used to efficiently utilize the available resources. Note that in this case, we choose 32 MPI ranks, 8 per node, and each rank using 13 threads for demonstration purpose, but not as an optimal performance recommendation. The optimal configuration depends on your particular problem, workload, and choice of solver, so some experimentation may be required.</p>"},{"location":"Documentation/Applications/comsol/#running-a-comsol-model-with-gpu","title":"Running a COMSOL Model with GPU","text":"<p>In COMSOL Multiphysics\u00ae, GPU acceleration can significantly increase performance for time-dependent simulations that use the discontinuous Galerkin (dG) method, such as those using the Pressure Acoustics, Time Explicit interface, and for training deep neural network (DNN) surrogate models. The following is a job script example used to run COMSOL jobs on GPU nodes.</p> Example GPU Submission Script <pre><code>#!/bin/bash\n#SBATCH --job-name=comsol-batch-GPUs\n#SBATCH --time=00:20:00\n#SBATCH --gres=gpu:1  # request 1 gpu per node, each gpu has 80 Gb of memory\n#SBATCH --mem-per-cpu=2G # requested memory per CPU core\n#SBATCH --ntasks-per-node=30\n#SBATCH --nodes=2\n#SBATCH --account=&lt;allocation handle&gt; \n#SBATCH --output=comsol-%j.out\n#SBATCH --error=comsol-%j.err\n\n# This helps ensure your job runs from the directory\n# from which you ran the sbatch command\ncd $SLURM_SUBMIT_DIR\n\n# Set up environment, and list to stdout for verification\nmodule load comsol\necho \" \"\nmodule list\necho \" \"\n\ninputfile=$SLURM_SUBMIT_DIR/myinputfile.mph\noutputfile=$SLURM_SUBMIT_DIR/myoutputfilename\nlogfile=$SLURM_SUBMIT_DIR/mylogfilename\n\n# Run a 2-node, 64-rank parallel COMSOL job with 1 threads for each rank and 1 gpu per node\n# -nn = total number of MPI ranks\n# -nnhost = number of MPI ranks per host\n# -np = number of threads per rank\n\ncomsol \u2013nn 128 -nnhost 64 batch -np 1 -inputfile $inputfile -outputfile $outputfile \u2013batchlog $logfile\n</code></pre> <p>Note, when launching a GPU job on Kestrel, be sure to do so from one of its dedicated GPU login nodes.</p> <p>The Complex Systems Simulation and Optimization group has hosted introductory and advanced COMSOL trainings. The introductory training covered how to use the COMSOL GUI and run COMSOL in batch mode on Kestrel. The advanced training showed how to do a parametric study using different sweeps (running an interactive session is also included) and introduced equation-based simulation and parameter estimation. To learn more about using COMSOL on Kestrel, please refer to the training. The recording can be accessed at Computational Sciences Tutorials and the slides and models used in the training can be downloaded from Github.</p>"},{"location":"Documentation/Applications/exawind/","title":"ExaWind","text":"<p><code>ExaWind</code> is a suite of applications that simulate wind turbines and wind farms on accelerated systems. The applications include AMR-Wind, Nalu-Wind, TIOGA, and OpenFAST. <code>AMR-Wind</code> is a massively parallel, block-structured adaptive-mesh, incompressible flow solver for wind turbine and wind farm simulations. <code>Nalu-Wind</code> is a generalized, unstructured, massively parallel, incompressible flow solver for wind turbine and wind farm simulations. <code>TIOGA</code> is a library for overset grid assembly on parallel distributed systems. <code>OpenFAST</code> is a multi-physics, multi-fidelity tool for simulating the coupled dynamic response of wind turbines.</p>"},{"location":"Documentation/Applications/exawind/#building-exawind","title":"Building ExaWind","text":"<p>We recommend installing ExaWind packages, either coupled or standalone, via <code>ExaWind-manager</code>. While ExaWind-manager is recommended, CMake could be used as a substitute for installing the necessary packages. Instructions for building ExaWind packages with ExaWind-manager and AMR-Wind/OpenFAST with CMake are described below.</p>"},{"location":"Documentation/Applications/exawind/#building-exawind-using-exawind-manager-on-kestrel-cpu","title":"Building ExaWind using ExaWind-manager on Kestrel-CPU","text":"<p>The following examples demonstrate how to use ExaWind-manager for building common ExaWind applications on Kestrel. The build requires a compute node having at least 36 cores and using Intel or GNU compiler. To avoid space and speed issues, clone ExaWind-manager to scratch, not your home directory; then activate it, create and activate a Spack environment, and finally concretize and build. When making a Spack environment, you can add (+) or remove (-) specs, and adjust versions (@) for the main and dependent (^) applications. The first example outlines the process of building ExaWind using the master branch, omitting GPU functionalities and the AMR-Wind and Nalu-Wind as its dependencies. The second example outlines the process of building a coupled release of AMR-Wind and OpenFAST from the develop branch. The final two examples illustrate how to build the released AMR-Wind and Nalu-Wind versions.</p> Building <code>ExaWind</code> <pre><code>$ salloc --time=01:00:00 --account=&lt;project account&gt; --partition=shared --nodes=1 --ntasks-per-node=36\n\n# Intel\n$ module load PrgEnv-intel\n$ module load cray-mpich/8.1.28\n$ module load cray-libsci/23.12.5\n$ module load cray-python\n\n# clone ExaWind-manager\n$ cd /scratch/${USER}\n$ git clone --recursive https://github.com/Exawind/exawind-manager.git\n$ cd exawind-manager\n\n# Activate exawind-manager\n$ export EXAWIND_MANAGER=`pwd`\n$ source ${EXAWIND_MANAGER}/start.sh &amp;&amp; spack-start\n\n# Create Spack environment and change the software versions if needed\n$ mkdir environments\n$ cd environments\n$ spack manager create-env --name exawind-cpu --spec 'exawind@master~amr_wind_gpu~cuda~gpu-aware-mpi~nalu_wind_gpu ^amr-wind@main~cuda~gpu-aware-mpi+hypre+mpi+netcdf+shared ^nalu-wind@master~cuda~fftw~gpu-aware-mpi+hypre+shared ^tioga@develop %oneapi'\n\n# Activate the environment\n$ spack env activate -d ${EXAWIND_MANAGER}/environments/exawind-cpu\n\n# concretize specs and dependencies\n$ spack concretize -f\n\n# Build software\n$ spack -d install\n</code></pre> Building Coupled <code>AMR-Wind</code> and <code>OpenFAST</code> <pre><code>$ salloc --time=01:00:00 --account=&lt;project account&gt; --partition=shared --nodes=1 --ntasks-per-node=52\n\n# Intel\n$ module load PrgEnv-intel\n$ module load cray-mpich/8.1.28\n$ module load cray-libsci/23.12.5\n$ module load cray-python\n\n# clone ExaWind-manager\n$ cd /scratch/${USER}\n$ git clone --recursive https://github.com/Exawind/exawind-manager.git\n$ cd exawind-manager\n\n# Activate exawind-manager\n$ export EXAWIND_MANAGER=`pwd`\n$ source ${EXAWIND_MANAGER}/start.sh &amp;&amp; spack-start\n\n# Create Spack environment and change the software versions if needed\n$ mkdir environments\n$ cd environments\n$ spack manager create-env --name amrwind-openfast-cpu --spec 'amr-wind+hypre+netcdf+openfast ^openfast@develop+openmp+rosco %oneapi'\n\n# Activate the environment\n$ spack env activate -d ${EXAWIND_MANAGER}/environments/amrwind-openfast-cpu\n\n# concretize specs and dependencies\n$ spack concretize -f\n\n# Build software\n$ spack -d install\n</code></pre> Building <code>AMR-Wind</code> <pre><code>$ salloc --time=01:00:00 --account=&lt;project account&gt; --partition=shared --nodes=1 --ntasks-per-node=52\n\n# Intel\n$ module load PrgEnv-intel\n$ module load cray-mpich/8.1.28\n$ module load cray-libsci/23.12.5\n$ module load cray-python\n\n# clone ExaWind-manager\n$ cd /scratch/${USER}\n$ git clone --recursive https://github.com/Exawind/exawind-manager.git\n$ cd exawind-manager\n\n# Activate exawind-manager  \n$ export EXAWIND_MANAGER=`pwd`\n$ source ${EXAWIND_MANAGER}/start.sh &amp;&amp; spack-start\n\n# Create Spack environment and change the software versions if needed\n$ mkdir environments\n$ cd environments\n$ spack manager create-env --name amrwind-cpu --spec 'amr-wind+hypre+netcdf %oneapi'\n\n# Activate the environment\n$ spack env activate -d ${EXAWIND_MANAGER}/environments/amrwind-cpu\n\n# concretize specs and dependencies\n$ spack concretize -f\n\n# Build software\n$ spack -d install\n</code></pre> Building <code>Nalu-Wind</code> <pre><code>$ salloc --time=01:00:00 --account=&lt;project account&gt; --partition=shared --nodes=1 --ntasks-per-node=52\n\n# Intel\n$ module load PrgEnv-intel\n$ module load cray-mpich/8.1.28\n$ module load cray-libsci/23.12.5\n$ module load cray-python\n\n# clone ExaWind-manager\n$ cd /scratch/${USER}\n$ git clone --recursive https://github.com/Exawind/exawind-manager.git\n$ cd exawind-manager\n\n# Activate exawind-manager\n$ export EXAWIND_MANAGER=`pwd`\n$ source ${EXAWIND_MANAGER}/start.sh &amp;&amp; spack-start\n\n# Create Spack environment and change the software versions if needed\n$ mkdir environments\n$ cd environments\n$ spack manager create-env --name naluwind-cpu --spec 'nalu-wind+hypre+netcdf %oneapi'\n\n# Activate the environment\n$ spack env activate -d ${EXAWIND_MANAGER}/environments/naluwind-cpu\n\n# concretize specs and dependencies\n$ spack concretize -f\n\n# Build software\n$ spack -d install\n</code></pre>"},{"location":"Documentation/Applications/exawind/#building-amr-wind-and-openfast-using-cmake-on-kestrel-cpu","title":"Building AMR-Wind and OpenFAST using CMake on Kestrel-CPU","text":"<p>This section describes how to install AMR-Wind and the coupled AMR-Wind/OpenFAST using provided CMake scripts. AMR-wind can be installed by following the instructions here. You can clone your desired version of AMR-wind from here. Once cloned, <code>cd</code> into the AMR-Wind directory and create a <code>build</code> directory. Use the scripts given below from within the <code>build</code> directory to build AMR-Wind. On a Kestrel CPU node, build AMR-Wind for CPUs by executing the following script from within the <code>build</code> directory:</p> Sample job script: Building AMR-Wind using <code>cmake</code> on Kestrel-CPU <pre><code>#!/bin/bash\n\nmodule purge\nmodule load PrgEnv-intel\nmodule load netcdf/4.9.2-intel-oneapi-mpi-intel\nmodule load netlib-scalapack/2.2.0-gcc\nexport LD_LIBRARY_PATH=/nopt/nrel/apps/cray-mpich-stall/libs_mpich_nrel_intel:$LD_LIBRARY_PATH\nexport LD_PRELOAD=/nopt/nrel/apps/cray-mpich-stall/libs_mpich_nrel_intel/libmpi_intel.so.12:/nopt/nrel/apps/cray-mpich-stall/libs_mpich_nrel_intel/libmpifort_intel.so.12\nexport MPICH_VERSION_DISPLAY=1\nexport MPICH_ENV_DISPLAY=1\nexport MPICH_OFI_CXI_COUNTER_REPORT=2\nexport FI_MR_CACHE_MONITOR=memhooks\nexport FI_CXI_RX_MATCH_MODE=software\nexport MPICH_SMP_SINGLE_COPY_MODE=NONE\n\necho $LD_LIBRARY_PATH |tr ':' '\\n'\n\nmodule list\n\ncmake .. \\\n    -DCMAKE_C_COMPILER=mpicc \\\n    -DCMAKE_CXX_COMPILER=mpicxx \\\n    -DMPI_Fortran_COMPILER=mpifort \\\n    -DCMAKE_Fortran_COMPILER=ifx \\\n    -DAMR_WIND_ENABLE_CUDA:BOOL=OFF \\\n    -DAMR_WIND_ENABLE_MPI:BOOL=ON \\\n    -DAMR_WIND_ENABLE_OPENMP:BOOL=OFF \\\n    -DAMR_WIND_TEST_WITH_FCOMPARE:BOOL=OFF \\\n    -DCMAKE_BUILD_TYPE=Release \\\n    -DAMR_WIND_ENABLE_NETCDF:BOOL=ON \\\n    -DAMR_WIND_ENABLE_HYPRE:BOOL=OFF \\\n    -DAMR_WIND_ENABLE_MASA:BOOL=OFF \\\n    -DAMR_WIND_ENABLE_TESTS:BOOL=ON \\\n    -DAMR_WIND_ENABLE_ALL_WARNINGS:BOOL=ON \\\n    -DBUILD_SHARED_LIBS:BOOL=ON \\\n    -DCMAKE_INSTALL_PREFIX:PATH=${PWD}/install\n\nnice make -j32\nmake install\n</code></pre> <p>If coupling to OpenFAST is needed, an additional flag must be passed to <code>cmake</code>. A complete example is given below.</p> Sample job script: Building AMR-Wind coupled to OpenFAST using <code>cmake</code> on Kestrel-CPU <pre><code>#!/bin/bash\n\nopenfastpath=/full/path/to/your/openfast/build/install\n\nmodule purge\nmodule load PrgEnv-intel\nmodule load netcdf/4.9.2-intel-oneapi-mpi-intel\nmodule load netlib-scalapack/2.2.0-gcc\nexport LD_LIBRARY_PATH=/nopt/nrel/apps/cray-mpich-stall/libs_mpich_nrel_intel:$LD_LIBRARY_PATH\nexport LD_PRELOAD=/nopt/nrel/apps/cray-mpich-stall/libs_mpich_nrel_intel/libmpi_intel.so.12:/nopt/nrel/apps/cray-mpich-stall/libs_mpich_nrel_intel/libmpifort_intel.so.12\nexport MPICH_VERSION_DISPLAY=1\nexport MPICH_ENV_DISPLAY=1\nexport MPICH_OFI_CXI_COUNTER_REPORT=2\nexport FI_MR_CACHE_MONITOR=memhooks\nexport FI_CXI_RX_MATCH_MODE=software\nexport MPICH_SMP_SINGLE_COPY_MODE=NONE\n\necho $LD_LIBRARY_PATH |tr ':' '\\n'\n\nmodule list\n\ncmake .. \\\n    -DCMAKE_C_COMPILER=mpicc \\\n    -DCMAKE_CXX_COMPILER=mpicxx \\\n    -DMPI_Fortran_COMPILER=mpifort \\\n    -DCMAKE_Fortran_COMPILER=ifx \\\n    -DAMR_WIND_ENABLE_CUDA:BOOL=OFF \\\n    -DAMR_WIND_ENABLE_MPI:BOOL=ON \\\n    -DAMR_WIND_ENABLE_OPENMP:BOOL=OFF \\\n    -DAMR_WIND_TEST_WITH_FCOMPARE:BOOL=OFF \\\n    -DCMAKE_BUILD_TYPE=Release \\\n    -DAMR_WIND_ENABLE_NETCDF:BOOL=ON \\\n    -DAMR_WIND_ENABLE_HYPRE:BOOL=OFF \\\n    -DAMR_WIND_ENABLE_MASA:BOOL=OFF \\\n    -DAMR_WIND_ENABLE_TESTS:BOOL=ON \\\n    -DAMR_WIND_ENABLE_ALL_WARNINGS:BOOL=ON \\\n    -DBUILD_SHARED_LIBS:BOOL=ON \\\n    -DOpenFAST_ROOT:PATH=${openfastpath} \\\n    -DCMAKE_INSTALL_PREFIX:PATH=${PWD}/install\n\nnice make -j32\nmake install\n</code></pre>"},{"location":"Documentation/Applications/exawind/#building-exawind-with-exawind-manager-on-kestrel-gpu","title":"Building ExaWind with ExaWind-manager on Kestrel-GPU","text":"<p>Building ExaWind applications on GPUs uses processes similar to those on CPUs described earlier. Example one details building the released ExaWind GPU version with its dependencies (AMR-Wind and Nalu-Wind), while examples two and three build the released AMR-Wind and Nalu-Wind independently.</p> Building <code>ExaWind</code> on GPUs <pre><code>$ salloc --time=02:00:00 --account=hpcapps --partition=gpu-h100 --gres=gpu:h100:1 --nodes=1 --ntasks-per-node=52 --gres=gpu:h100:1\n\n# GNU\n$ module load PrgEnv-gnu\n$ module load cray-mpich/8.1.28\n$ module load  cray-libsci/23.12.5\n$ module load cuda\n$ module load cray-python\n\n# clone ExaWind-manager\n$ cd /scratch/${USER}\n$ git clone --recursive https://github.com/Exawind/exawind-manager.git\n$ cd exawind-manager\n\n# Activate exawind-manager\n$ export EXAWIND_MANAGER=`pwd`\n$ source ${EXAWIND_MANAGER}/start.sh &amp;&amp; spack-start\n\n# Create Spack environment and change the software versions if needed\n$ mkdir environments\n$ cd environments\n$ spack manager create-env --name exawind-gpu --spec 'exawind+cuda+gpu-aware-mpi+amr_wind_gpu+nalu_wind_gpu cuda_arch=90 %gcc'\n\n# Activate the environment\n$ spack env activate -d ${EXAWIND_MANAGER}/environments/exawind-gpu\n\n# concretize specs and dependencies\n$ spack concretize -f\n\n# Build software\n$ spack -d install\n</code></pre> Building <code>AMR-Wind</code> on GPUs <pre><code>$ salloc --time=02:00:00 --account=hpcapps --partition=gpu-h100 --gres=gpu:h100:1 --nodes=1 --ntasks-per-node=52 --gres=gpu:h100:1\n\n# GNU\n$ module load PrgEnv-gnu\n$ module load cray-mpich/8.1.28\n$ module load  cray-libsci/23.12.5\n$ module load cuda\n$ module load cray-python\n\n# clone ExaWind-manager\n$ cd /scratch/${USER}\n$ git clone --recursive https://github.com/Exawind/exawind-manager.git\n$ cd exawind-manager\n\n# Activate exawind-manager\n$ export EXAWIND_MANAGER=`pwd`\n$ source ${EXAWIND_MANAGER}/start.sh &amp;&amp; spack-start\n\n# Create Spack environment and change the software versions if needed\n$ mkdir environments\n$ cd environments\n$ spack manager create-env --name amrwind-gpu --spec 'amr-wind+cuda+gpu-aware-mpi+hypre+netcdf+hdf5 cuda_arch=90  %gcc'\n\n# Activate the environment\n$ spack env activate -d ${EXAWIND_MANAGER}/environments/amrwind-gpu\n\n# concretize specs and dependencies\n$ spack concretize -f\n\n# Build software\n$ spack -d install\n</code></pre> Building <code>Nalu-Wind</code> on GPUs <pre><code>$ salloc --time=02:00:00 --account=hpcapps --partition=gpu-h100 --gres=gpu:h100:1 --nodes=1 --ntasks-per-node=52 --gres=gpu:h100:1\n\n# GNU\n$ module load PrgEnv-gnu\n$ module load cray-mpich/8.1.28\n$ module load  cray-libsci/23.12.5\n$ module load cuda\n$ module load cray-python\n\n# clone ExaWind-manager\n$ cd /scratch/${USER}\n$ git clone --recursive https://github.com/Exawind/exawind-manager.git\n$ cd exawind-manager\n\n# Activate exawind-manager\n$ export EXAWIND_MANAGER=`pwd`\n$ source ${EXAWIND_MANAGER}/start.sh &amp;&amp; spack-start\n\n# Create Spack environment and change the software versions if needed\n$ mkdir environments\n$ cd environments\n$ spack manager create-env --name naluwind-gpu --spec 'nalu-wind+cuda+gpu-aware-mpi+hypre cuda_arch=90  %gcc'\n\n# Activate the environment\n$ spack env activate -d ${EXAWIND_MANAGER}/environments/naluwind-gpu\n\n# concretize specs and dependencies\n$ spack concretize -f\n\n# Build software\n$ spack -d install\n</code></pre>"},{"location":"Documentation/Applications/exawind/#building-amr-wind-using-cmake-on-kestrel-gpu","title":"Building AMR-Wind Using CMake on Kestrel-GPU","text":"<p>On a Kestrel GPU node, build AMR-Wind for GPUs by executing the follow script from the <code>build</code> directory:</p> Sample job script: Building AMR-Wind using <code>cmake</code> on GPUs <pre><code>#!/bin/bash\n\nmodule purge\nmodule load binutils\nmodule load PrgEnv-nvhpc\nmodule load cray-libsci/22.12.1.1\nmodule load cmake\nmodule load cmake/3.27.9\nmodule load cray-python\nmodule load netcdf-fortran/4.6.1-oneapi\nmodule load craype-x86-genoa\nmodule load craype-accel-nvidia90 \n\nexport MPICH_GPU_SUPPORT_ENABLED=1\nexport CUDAFLAGS=\"-L/nopt/nrel/apps/gpu_stack/libraries-gcc/06-24/linux-rhel8-zen4/gcc-12.3.0/hdf5-1.14.3-zoremvtiklvvkbtr43olrq3x546pflxe/lib -I/nopt/nrel/apps/gpu_stack/libraries-gcc/06-24/linux-rhel8-zen4/gcc-12.3.0/hdf5-1.14.3-zoremvtiklvvkbtr43olrq3x546pflxe/include -lhdf5 -lhdf5_hl -I${MPICH_DIR}/include -L${MPICH_DIR}/lib -lmpi ${PE_MPICH_GTL_DIR_nvidia90} ${PE_MPICH_GTL_LIBS_nvidia90}\"\nexport CXXFLAGS=\"-L/nopt/nrel/apps/gpu_stack/libraries-gcc/06-24/linux-rhel8-zen4/gcc-12.3.0/hdf5-1.14.3-zoremvtiklvvkbtr43olrq3x546pflxe/lib -I/nopt/nrel/apps/gpu_stack/libraries-gcc/06-24/linux-rhel8-zen4/gcc-12.3.0/hdf5-1.14.3-zoremvtiklvvkbtr43olrq3x546pflxe/include -lhdf5 -lhdf5_hl -I${MPICH_DIR}/include -L${MPICH_DIR}/lib -lmpi ${PE_MPICH_GTL_DIR_nvidia90} ${PE_MPICH_GTL_LIBS_nvidia90}\"\n\nmodule list\n\ncmake .. \\\n    -DAMR_WIND_ENABLE_CUDA=ON \\\n    -DAMR_WIND_ENABLE_TINY_PROFILE:BOOL=ON \\\n    -DAMReX_CUDA_ERROR_CAPTURE_THIS:BOOL=ON \\\n    -DCMAKE_CUDA_COMPILE_SEPARABLE_COMPILATION:BOOL=ON \\\n    -DCMAKE_CXX_COMPILER:STRING=CC \\\n    -DCMAKE_C_COMPILER:STRING=cc \\\n    -DMPI_CXX_COMPILER=/opt/cray/pe/mpich/8.1.28/ofi/nvidia/23.3/bin/mpicxx \\\n    -DMPI_C_COMPILER=/opt/cray/pe/mpich/8.1.28/ofi/nvidia/23.3/bin/mpicc \\\n    -DMPI_Fortran_COMPILER=/opt/cray/pe/mpich/8.1.28/ofi/nvidia/23.3/bin/mpifort \\\n    -DAMReX_DIFFERENT_COMPILER=ON \\\n    -DCMAKE_CUDA_ARCHITECTURES=90 \\\n    -DAMR_WIND_ENABLE_CUDA=ON \\\n    -DAMR_WIND_ENABLE_CUDA:BOOL=ON \\\n    -DAMR_WIND_ENABLE_OPENFAST:BOOL=OFF \\\n    -DAMR_WIND_ENABLE_NETCDF:BOOL=ON \\\n    -DAMR_WIND_ENABLE_HDF5:BOOL=ON \\\n    -DAMR_WIND_ENABLE_MPI:BOOL=ON \\\n    -DCMAKE_BUILD_TYPE=Release \\\n    -DAMR_WIND_ENABLE_HYPRE:BOOL=OFF \\\n    -DAMR_WIND_ENABLE_MASA:BOOL=OFF \\\n    -DAMR_WIND_ENABLE_TESTS:BOOL=ON \\\n    -DCMAKE_INSTALL_PREFIX:PATH=./install\n\nmake -j32 amr_wind\n</code></pre> <p>You should now have a successful installation of AMR-Wind. At runtime, make sure to load the same modules used to build, as discussed below.</p>"},{"location":"Documentation/Applications/exawind/#running-exawind","title":"Running ExaWind","text":""},{"location":"Documentation/Applications/exawind/#running-exawind-on-kestrel-cpu","title":"Running ExaWind on Kestrel-CPU","text":"<p>ExaWind applications on more than 8 nodes run better on the <code>hbw</code> (\"high bandwidth\") partition than on <code>short</code>, <code>standard</code>, or <code>long</code> partitions. We strongly recommend submitting multi-node jobs to the <code>hbw</code> partition for the best performance and to save AUs. Our benchmark studies show ExaWind applications perform best on the <code>hbw</code> partition with 72 MPI ranks per node. Following are example scripts illustrating the above recommendations.</p> <p>Note</p> <p>Single-node jobs are not allowed to be submitted to <code>hbw</code>; they should instead be submitted to <code>short</code>, <code>standard</code>, or <code>long</code>.</p> Sample job script: Running ExaWind with the <code>hbw</code> partition <pre><code>#!/bin/bash\n\n#SBATCH --job-name=&lt;job-name&gt;\n#SBATCH --nodes=16\n#SBATCH --ntasks-per-node=72\n#SBATCH --time=1:00:00\n#SBATCH --partition=hbw\n#SBATCH --account=&lt;account-name&gt;\n#SBATCH --exclusive\n#SBATCH --mem=0\n\nmodule load PrgEnv-intel\nmodule load cray-python \nmodule list\n\nexport EXAWIND_MANAGER=/scratch/{user}/exawind-manager\nsource ${EXAWIND_MANAGER}/start.sh &amp;&amp; spack-start\nspack env activate -d ${EXAWIND_MANAGER}/environments/exawind-cpu\nspack load exawind\n\nexport MPICH_OFI_NIC_POLICY=NUMA\n\n# Adjust the ratio of total MPI ranks for AMR-Wind and Nalu-Wind as needed by a job \nsrun -N $SLURM_JOB_NUM_NODES -n $(($SLURM_NTASKS_PER_NODE * $SLURM_JOB_NUM_NODES)) \\\n--distribution=block:block --cpu_bind=rank_ldom exawind --awind $(($SLURM_NTASKS_PER_NODE * $SLURM_JOB_NUM_NODES) * 0.25) \\\n--nwind $(($SLURM_NTASKS_PER_NODE * $SLURM_JOB_NUM_NODES) * 0.75) &lt;input-name&gt;.yaml\n</code></pre> Sample job script: Running AMR-Wind using the <code>hbw</code> partition <pre><code>#!/bin/bash\n\n#SBATCH --job-name=&lt;job-name&gt;\n#SBATCH --nodes=16\n#SBATCH --ntasks-per-node=72\n#SBATCH --time=1:00:00\n#SBATCH --partition=hbw\n#SBATCH --account=&lt;account-name&gt;\n#SBATCH --exclusive\n#SBATCH --mem=0\n\nmodule load PrgEnv-intel\nmodule load cray-mpich/8.1.28\nmodule load cray-libsci/23.12.5\nmodule load cray-python\nmodule list\n\nexport EXAWIND_MANAGER=/scratch/{user}/exawind-manager\nsource ${EXAWIND_MANAGER}/start.sh &amp;&amp; spack-start\nspack env activate -d ${EXAWIND_MANAGER}/environments/amrwind-cpu\nspack load amr-wind\n\nexport MPICH_OFI_NIC_POLICY=NUMA\n\nsrun -N $SLURM_JOB_NUM_NODES -n $(($SLURM_NTASKS_PER_NODE * $SLURM_JOB_NUM_NODES)) --distribution=block:block --cpu_bind=rank_ldom amr_wind &lt;input-name&gt;.inp\n</code></pre> Sample job script: Running Nalu-Wind using the <code>hbw</code> partition <pre><code>#!/bin/bash\n\n#SBATCH --job-name=&lt;job-name&gt;\n#SBATCH --nodes=16\n#SBATCH --ntasks-per-node=72\n#SBATCH --time=1:00:00\n#SBATCH --partition=hbw\n#SBATCH --account=&lt;account-name&gt;\n#SBATCH --exclusive\n#SBATCH --mem=0\n\nmodule load PrgEnv-intel\nmodule load cray-mpich/8.1.28\nmodule load cray-libsci/23.12.5\nmodule load cray-python\nmodule list\n\nexport EXAWIND_MANAGER=/scratch/{user}/exawind-manager\nsource ${EXAWIND_MANAGER}/start.sh &amp;&amp; spack-start\nspack env activate -d ${EXAWIND_MANAGER}/environments/naluwind-cpu\nspack load nalu-wind\n\nexport MPICH_OFI_NIC_POLICY=NUMA\n\nsrun -N $SLURM_JOB_NUM_NODES -n $(($SLURM_NTASKS_PER_NODE * $SLURM_JOB_NUM_NODES)) --distribution=block:block --cpu_bind=rank_ldom naluX &lt;input-name&gt;.yaml\n</code></pre> <p>Our benchmark studies suggest that using the <code>stall</code> library with the <code>hbw</code> partition could further improve ExaWind application performance. Moreover, the stall library is highly recommended for ExaWind applications running on 8 nodes or less, each with 96 cores, across short, standard, or long partitions. Optimizing the <code>MPICH_OFI_CQ_STALL_USECS</code> parameter is key to acheive the best performance. Following are sample scripts demonstrating the aforementioned recommendations.</p> Sample job script: Running ExaWind using the stall library <pre><code>#!/bin/bash\n\n#SBATCH --job-name=&lt;job-name&gt;\n#SBATCH --partition=&lt;partition-name&gt; # hbw, short, standard or long\n#SBATCH --nodes=&lt;nodes&gt; # &gt;=16 nodes for hbw or &lt;=8 nodes for short, standard or long \n#SBATCH --ntasks-per-node=&lt;cores&gt; # 72 cores for hbw or 96 cores for short, standard or long\n#SBATCH --time=1:00:00\n#SBATCH --account=&lt;account-name&gt;\n#SBATCH --exclusive\n#SBATCH --mem=0\n\nmodule load PrgEnv-intel\nmodule load cray-mpich/8.1.28\nmodule load cray-libsci/23.12.5\nmodule load cray-python\nmodule list\n\nexport EXAWIND_MANAGER=/scratch/{user}/exawind-manager\nsource ${EXAWIND_MANAGER}/start.sh &amp;&amp; spack-start\nspack env activate -d ${EXAWIND_MANAGER}/environments/exawind-cpu\nspack load exawind\n\nexport LD_PRELOAD=/nopt/nrel/apps/cray-mpich-stall/libs_mpich_nrel_intel/libmpi_intel.so.12:/nopt/nrel/apps/cray-mpich-stall/libs_mpich_nrel_intel/libmpifort_intel.so.12\nexport MPICH_OFI_CQ_STALL=1\n# Find an optimal value from this list [1,3,6,9,12,16,20,24]\nexport MPICH_OFI_CQ_STALL_USECS=12\nexport MPICH_OFI_CQ_MIN_PPN_PER_NIC=26\nexport MPICH_OFI_NIC_POLICY=NUMA\n\n# Adjust the ratio of total MPI ranks for AMR-Wind and Nalu-Wind as needed by a job\nsrun -N $SLURM_JOB_NUM_NODES -n $(($SLURM_NTASKS_PER_NODE * $SLURM_JOB_NUM_NODES)) \\\n--distribution=block:block --cpu_bind=rank_ldom exawind --awind $(($SLURM_NTASKS_PER_NODE * $SLURM_JOB_NUM_NODES) * 0.25) \\\n--nwind $(($SLURM_NTASKS_PER_NODE * $SLURM_JOB_NUM_NODES) * 0.75) &lt;input-name&gt;.yaml\n</code></pre> Sample job script: Running AMR-Wind using the stall library nodes <pre><code>#!/bin/bash\n\n#SBATCH --job-name=&lt;job-name&gt;\n#SBATCH --partition=&lt;partition-name&gt; # hbw, short, standard or long\n#SBATCH --nodes=&lt;nodes&gt; # &gt;=16 nodes for hbw or &lt;=8 nodes for short, standard or long\n#SBATCH --ntasks-per-node=&lt;cores&gt; # 72 cores for hbw or 96 cores for short, standard or long\n#SBATCH --time=1:00:00\n#SBATCH --account=&lt;account-name&gt;\n#SBATCH --exclusive\n#SBATCH --mem=0\n\nmodule load PrgEnv-intel\nmodule load cray-mpich/8.1.28\nmodule load cray-libsci/23.12.5\nmodule load cray-python\nmodule list\n\nexport EXAWIND_MANAGER=/scratch/{user}/exawind-manager\nsource ${EXAWIND_MANAGER}/start.sh &amp;&amp; spack-start\nspack env activate -d ${EXAWIND_MANAGER}/environments/amrwind-cpu\nspack load amr-wind\n\nexport LD_PRELOAD=/nopt/nrel/apps/cray-mpich-stall/libs_mpich_nrel_intel/libmpi_intel.so.12:/nopt/nrel/apps/cray-mpich-stall/libs_mpich_nrel_intel/libmpifort_intel.so.12\nexport MPICH_OFI_CQ_STALL=1\n# Find an optimal value from this list [1,3,6,9,12,16,20,24]\nexport MPICH_OFI_CQ_STALL_USECS=12\nexport MPICH_OFI_CQ_MIN_PPN_PER_NIC=26\nexport MPICH_OFI_NIC_POLICY=NUMA\n\nsrun -N $SLURM_JOB_NUM_NODES -n $(($SLURM_NTASKS_PER_NODE * $SLURM_JOB_NUM_NODES)) --distribution=block:block --cpu_bind=rank_ldom amr_wind &lt;input-name&gt;.inp\n</code></pre> Sample job script: Running Nalu-Wind using the stall library nodes <pre><code>#!/bin/bash\n\n#SBATCH --job-name=&lt;job-name&gt;\n#SBATCH --partition=&lt;partition-name&gt; # hbw, short, standard or long\n#SBATCH --nodes=&lt;nodes&gt; # &gt;=16 nodes for hbw or &lt;=8 nodes for short, standard or long\n#SBATCH --ntasks-per-node=&lt;cores&gt; # 72 cores for hbw or 96 cores for short, standard or long\n#SBATCH --time=1:00:00\n#SBATCH --account=&lt;account-name&gt;\n#SBATCH --exclusive\n#SBATCH --mem=0\n\nmodule load PrgEnv-intel\nmodule load cray-mpich/8.1.28\nmodule load cray-libsci/23.12.5\nmodule load cray-python\nmodule list\n\nexport EXAWIND_MANAGER=/scratch/{user}/exawind-manager\nsource ${EXAWIND_MANAGER}/start.sh &amp;&amp; spack-start\nspack env activate -d ${EXAWIND_MANAGER}/environments/naluwind-cpu\nspack load nalu-wind\n\nexport LD_PRELOAD=/nopt/nrel/apps/cray-mpich-stall/libs_mpich_nrel_intel/libmpi_intel.so.12:/nopt/nrel/apps/cray-mpich-stall/libs_mpich_nrel_intel/libmpifort_intel.so.12\nexport MPICH_OFI_CQ_STALL=1\n# Find an optimal value from this list [1,3,6,9,12,16,20,24]\nexport MPICH_OFI_CQ_STALL_USECS=12\nexport MPICH_OFI_CQ_MIN_PPN_PER_NIC=26\nexport MPICH_OFI_NIC_POLICY=NUMA\n\nsrun -N $SLURM_JOB_NUM_NODES -n $(($SLURM_NTASKS_PER_NODE * $SLURM_JOB_NUM_NODES)) --distribution=block:block --cpu_bind=rank_ldom naluX &lt;input-name&gt;.yaml\n</code></pre>"},{"location":"Documentation/Applications/exawind/#running-exawind-on-kestrel-gpu","title":"Running ExaWind on Kestrel-GPU","text":"<p>Running ExaWind on GPUs yields optimal performance. The following scripts illustrate how to submit jobs on the <code>gpu-h100</code> partition.</p> Sample job script: Running ExaWind on GPU nodes <pre><code>#!/bin/bash\n\n#SBATCH --time=1:00:00 \n#SBATCH --account=&lt;user-account&gt;\n#SBATCH --nodes=2\n#SBATCH --ntasks-per-node=4\n#SBATCH --gpus=h100:4\n#SBATCH --exclusive\n#SBATCH --mem=0\n\nmodule load PrgEnv-gnu\nmodule load cray-mpich/8.1.28\nmodule load  cray-libsci/23.12.5\nmodule load cuda\nmodule load cray-python\n\nexport EXAWIND_MANAGER=/scratch/${USER}/exawind-manager\nsource ${EXAWIND_MANAGER}/start.sh &amp;&amp; spack-start\nspack env activate -d ${EXAWIND_MANAGER}/environments/exawind-gpu\nspack load exawind@master\n\nexport MPICH_OFI_NIC_POLICY=NUMA\n\n# Adjust the ratio of total MPI ranks for AMR-Wind and Nalu-Wind as needed by a job \nsrun -N $SLURM_JOB_NUM_NODES -n $(($SLURM_NTASKS_PER_NODE * $SLURM_JOB_NUM_NODES)) --distribution=block:block --cpu_bind=rank_ldom \\\nexawind --nwind $(($SLURM_NTASKS_PER_NODE * $SLURM_JOB_NUM_NODES) * 0.75) --awind $(($SLURM_NTASKS_PER_NODE * $SLURM_JOB_NUM_NODES) * 0.25) &lt;input-name&gt;.yaml \nwait\n</code></pre> Sample job script: Running AMR-Wind on GPU nodes <pre><code>#!/bin/bash\n\n#SBATCH --time=1:00:00\n#SBATCH --account=&lt;user-account&gt; # Replace with your HPC account\n#SBATCH --nodes=2\n#SBATCH --ntasks-per-node=4\n#SBATCH --gpus=h100:4\n#SBATCH --exclusive\n#SBATCH --mem=0\n\nmodule load PrgEnv-gnu\nmodule load cray-mpich/8.1.28\nmodule load  cray-libsci/23.12.5\nmodule load cuda\nmodule load cray-python\n\nexport EXAWIND_MANAGER=/scratch/${USER}/exawind-manager\nsource ${EXAWIND_MANAGER}/start.sh &amp;&amp; spack-start\nspack env activate -d ${EXAWIND_MANAGER}/environments/amrwind-gpu\nspack load amr-wind\n\nexport MPICH_OFI_NIC_POLICY=NUMA\n\nsrun -N $SLURM_JOB_NUM_NODES -n $(($SLURM_NTASKS_PER_NODE * $SLURM_JOB_NUM_NODES)) --distribution=block:block --cpu-bind=rankldom amr_wind &lt;input-name&gt;.inp\n</code></pre> Sample job script: Running Nalu-Wind on GPU nodes <pre><code>#!/bin/bash\n\n#SBATCH --time=1:00:00\n#SBATCH --account=&lt;user-account&gt; \n#SBATCH --nodes=2\n#SBATCH --ntasks-per-node=4\n#SBATCH --gpus=h100:4\n#SBATCH --exclusive\n#SBATCH --mem=0\n\nmodule load PrgEnv-gnu\nmodule load cray-mpich/8.1.28\nmodule load  cray-libsci/23.12.5\nmodule load cuda\nmodule load cray-python\n\nexport EXAWIND_MANAGER=/scratch/${USER}/exawind-manager\nsource ${EXAWIND_MANAGER}/start.sh &amp;&amp; spack-start\nspack env activate -d ${EXAWIND_MANAGER}/environments/naluwind-gpu\nspack load nalu-wind\n\nexport MPICH_OFI_NIC_POLICY=NUMA\n\nsrun -N $SLURM_JOB_NUM_NODES -n $(($SLURM_NTASKS_PER_NODE * $SLURM_JOB_NUM_NODES)) --distribution=block:block --cpu-bind=rankldom naluX &lt;input-name&gt;.yaml\n</code></pre>"},{"location":"Documentation/Applications/fenics/","title":"FEniCS/FEniCSx","text":"<p>Documentation: FEniCS 2019.1.0, FEniCSx</p> <p>FEniCS is a collection of open-source software components designed to enable the automated solution of differential equations by the finite element method.</p> <p>Note</p> <p>There are two version of FEniCS. The original FEniCS ended in 2019 with version 2019.1.0 and development began on a complete refactor known as FEniCSx. FEniCS 2019.1.0 is still actively used and the main focus of this documentation. Since FEniCSx is in pre-release, HPC support is a work in progress.</p>"},{"location":"Documentation/Applications/fenics/#getting-started","title":"Getting Started","text":"<p>FEniCS is organized as a collection of interoperable components that together form the FEniCS Project. These components include the problem-solving environment DOLFIN, the form compiler FFC, the finite element tabulator FIAT, the just-in-time compiler Instant, the form language UFL, and a range of additional components.</p> <p>FEniCS can be programmed both in C++ and Python, but Python programming is the simplest approach to exploring FEniCS and can give high performance.</p> <p>Currently, FEniCS is supported through Anaconda. Users are required to build their own FEniCS environment with the following commands after loading the conda/anaconda module (see Example Job Scripts):</p> <pre><code>module load conda\nconda create -n myfenics -c conda-forge fenics  matplotlib scipy jupyter \n</code></pre> <p>The packages <code>matplotlib</code>, <code>scipy</code>, and <code>jupyter</code> are not required, but they are very handy to have. </p> <p>These commands will create a new environment named <code>myfenics</code> which contains all necessary packages as well as some commonly-used packages for programming FEniCS simulations. By default, this Conda environment will be installed in the directory <code>/home/&lt;username&gt;/.conda-envs/myfenics</code>. It will take roughly 3 GB of storage. Please make sure you have enough storage quota in the home directory before installation by running the <code>du -hs ~</code> command (which will take a minute or two to complete). </p> <p>FEniCSx can also be installed via conda using:  <pre><code>conda create -n myfenics -c conda-forge fenics-dolfinx\n</code></pre></p>"},{"location":"Documentation/Applications/fenics/#example-job-scripts","title":"Example Job Scripts","text":"Kestrel CPU <pre><code>#!/bin/bash\n\n# This test file is designed to run the Poisson demo on one node with a 4 cores\n\n#SBATCH --time=01:00:00\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=4\n#SBATCH --partition=standard\n#SBATCH --account=\n\nmodule purge\nmodule load conda\n\n# This is to prevent FEniCS from unnecessarily attempting to multi-thread\nexport OMP_NUM_THREADS=1\n\ncd /scratch/USERNAME/poisson_demo/\nsrun -n 4 python poisson_demo.py\n</code></pre> Vermilion <pre><code>#!/bin/bash\n\n# This test file is designed to run the Poisson demo on one node with a 4 cores\n\n#SBATCH --time=01:00:00\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=4\n#SBATCH --partition=standard\n#SBATCH --account=\n\nmodule purge\nmodule load anaconda3\n\n# This is to prevent FEniCS from unnecessarily attempting to multi-thread\nexport OMP_NUM_THREADS=1\n\ncd /scratch/USERNAME/poisson_demo/\nsrun -n 4 python poisson_demo.py\n</code></pre> Swift <pre><code>#!/bin/bash\n\n# This test file is designed to run the Poisson demo on one node with a 4 cores\n\n#SBATCH --time=01:00:00\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=4\n#SBATCH --partition=standard\n#SBATCH --account=\n\nmodule purge\nmodule load conda\n\n# This is to prevent FEniCS from unnecessarily attempting to multi-thread\nexport OMP_NUM_THREADS=1\n\ncd /home/USERNAME/poisson_demo/\nsrun -n 4 python poisson_demo.py\n</code></pre> <p>To run this script, first download the Poisson demo  here  and place it in a folder titled \"poisson_demo\" in your scratch directory (home for Swift). Next, replace \"USERNAME\" in the script with your username. Then save the script as \"demo_script.sh\" and submit it with <code>sbatch demo_script.sh</code>. This demo is only supported by FEniCS 2019.1.0 and not FEniCSx. </p>"},{"location":"Documentation/Applications/fenics/#supported-versions","title":"Supported Versions","text":"Kestrel Vermilion Swift 2019.1.0 2019.1.0 2019.1.0"},{"location":"Documentation/Applications/gams/","title":"Using the General Algebraic Modeling System","text":"<p>The General Algebraic Modeling System (GAMS) is a commercial high-level modeling system for mathematical programming and optimization. It is licensed software.</p> <p>GAMS includes a DSL compiler and also a stable of integrated high-performance solvers. GAMS is able to solve complex, large-scale modeling problems. For documentation, forums, and FAQs, see the GAMS website.</p> <p>A 80-user license of GAMS is made available to NREL users. This GAMS license requires users to be a member of the \"gams workgroup.\" If you need the GAMS software package or a specific solver from GAMS, or if you have trouble running GAMS, please contact us.</p>"},{"location":"Documentation/Applications/gams/#initializing-your-environment","title":"Initializing Your Environment","text":"<p>To initialize your environment to use GAMS, simply type <code>module load gams/&lt;version&gt;</code> \u2014 see <code>module avail gams</code> output to see available versions. GAMS is run with the command format <code>gams &lt;input filename&gt;</code>. A file <code>&lt;input filename&gt;.lst</code> will be created as the output file.   </p> <p>For a test run, in your home directory, type the following:</p> <pre><code>module load gams/&lt;version&gt;\ncp /nopt/nrel/apps/gams/example/trnsport.gms .\ngams trnsport\n</code></pre> <p>A result of 153.675 should be found from screen output. More detailed output is in the file <code>trnsport.lst</code>. </p>"},{"location":"Documentation/Applications/gams/#selecting-an-alternative-solver","title":"Selecting an Alternative Solver","text":"<p>The available solvers for different procedures are shown in the following with the default solver being the first one:</p> <ul> <li>LP: GUROBI BDMLP CBC IPOPT SOPLEX</li> <li>MIP: GUROBI BDMP CBC SCIP</li> <li>RMIP: GUROBI BDMLP CBC IPOPT SOPLEX</li> <li>NLP: SCIP COUENNE IPOPT</li> <li>MCP: NLPEC MILES</li> <li>MPEC: NLPEC</li> <li>RMPEC: NLPEC</li> <li>CNS: SCIP COUENNE IPOPT </li> <li>DNLP: SCIP COUENNE IPOPT </li> <li>RMINLP: SCIP COUENNE IPOPT </li> <li>MINLP: SCIP BONMIN COUENNE </li> <li>QCP: GUROBI COUENNE IPOPT SCIP</li> <li>MIQCP: GUROBI BONMIN COUENNE SCIP</li> <li>RMIQCP: GUROBI COUENNE IPOPT SCIP</li> <li>EMP: JAMS LOGMIP SELKIE </li> </ul> <p>By typing <code>gams &lt;input_filename&gt;</code> on the command line, the default procedure LP and the default solver Gurobi will be used. In order to override the default option to use, e.g., Soplex, you can try the following two methods: </p> <ol> <li> <p>Use the option statement in your GAMS input file. For example, if your model input uses LP procedure and you want to use Gurobi solver to solve it, just add <code>option lp=soplex</code> to your input file.</p> </li> <li> <p>Specify the solver in the command line, e.g., <code>gams &lt;input_filename&gt; lp=soplex</code>. </p> </li> </ol> <p>A sample script for batch submission is provided here:</p> <p>Sample Submission Script</p> <pre><code>#!/bin/bash --login\n#SBATCH --name gams_run\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=104\n#SBATCH --time=00:05:00\n#SBATCH --account=&lt;allocation-id&gt;\n#SBATCH --error=gams-%j.err\n#SBATCH --output=gams-%j.out\n\n# Ensure script location\ncd $SLURM_SUBMIT_DIR\n\n# Create runtime environment\nmodule load gams/&lt;version&gt;\n\n# Run GAMS\ngams trnsport lp=gurobi\n</code></pre> <p>For a certain solver, necessary control parameters for the algorithm\u2014such as convergence criteria\u2014can be loaded from the option file named as <code>&lt;solver_name&gt;.opt</code> in the directory that you run GAMS. For example, for the Gurobi solver, its option file would be \"gurobi.opt\". For the details of how to set those parameters, please see the GAMS Solver Manuals. </p>"},{"location":"Documentation/Applications/gams/#important-tip","title":"Important Tip","text":"<p>When using the Gurobi solver in GAMS, the user should NOT try to load the Gurobi module. Simply using \"module load gams\" will automatically load the Gurobi solver.</p>"},{"location":"Documentation/Applications/gams/#using-gams-python-api","title":"Using GAMS Python API","text":""},{"location":"Documentation/Applications/gams/#for-gams-version-400","title":"For GAMS version &lt; 40.0","text":"<p>In order to use GAMS python API, the environment parameter <code>$PYTHONPATH</code> should include these two directories: </p> <p><code>$GAMS_PYTHON_API_FILES/gams</code> <code>$GAMS_PYTHON_API_FILES/api_[version-of-python]</code></p> <p>where <code>version-of-python</code> =  27, 36, 37, or 38 for python version 2.7, 3.6, 3.7, or 3.8, respectively. The python version can be obtained by using command <code>python --version</code>. </p> <p>For example, for python 3.7 and the bash shell, <code>$PYTHONPATH</code> can be set using the following script:</p> <pre><code>module purge\nmodule load gams/31.1.0\nif [ -z ${PYTHONPATH+x} ]\nthen\n        export PYTHONPATH=$GAMS_PYTHON_API_FILES/api_37:$GAMS_PYTHON_API_FILES/gams\nelse\n        export PYTHONPATH=$GAMS_PYTHON_API_FILES/api_37:$GAMS_PYTHON_API_FILES/gams:$PYTHONPATH\nfi\n</code></pre>"},{"location":"Documentation/Applications/gams/#for-gams-version-400_1","title":"For GAMS version &gt; 40.0","text":"<p>The GAMS API can be installed using Anaconda and Pip. Please follow the instruction on the GAMS website. Currently GAMS supports python version 3.7~3.11. In general, it can be installed using the following command:</p> <pre><code>pip install gams[your choice of sub-module] --find-links $GAMS_PYTHON_API_FILES\n</code></pre>"},{"location":"Documentation/Applications/gaussian/","title":"Running Gaussian16 Software Jobs","text":"<p>Learn about the Gaussian16 electronic structure program and how to run Gaussian16 jobs at NREL.</p> <p>Important</p> <p>To run Gaussian16, users must be a member of the Gaussian user group. To be added to the group, contact HPC-Help. In your email message, include your username and copy the following text agreeing not to compete with Gaussian, Inc.:</p> <pre><code>I am not actively developing applications for a competing software program, or for a project in \ncollaboration with someone who is actively developing for a competing software program. I agree \nthat Gaussian output cannot be provided to anyone actively developing for a competing software program.\n\nI agree to this statement.\n</code></pre>"},{"location":"Documentation/Applications/gaussian/#configuration-and-default-settings","title":"Configuration and Default Settings","text":"<p>NREL currently has Gaussian16 Revision C.01 installed, and the user manual can be found at the Gaussian website.  Gaussian currently doesn't have support for H100 GPUs.</p> <p>Previous Gaussian 09 users sometimes may feel Gaussian 16 runs slower than Gaussian 09. That's because Gaussian G16 has changed the default accuracy into <code>Int=Acc2E=12 Grid=Ultrafine</code>, which means that individual SCF iterations will take longer with G16 than with G09. </p>"},{"location":"Documentation/Applications/gaussian/#sample-job-scripts","title":"Sample Job Scripts","text":"<p>Gaussian may be configured to run on one or more physical nodes, with or without shared memory parallelism. Distributed memory, parallel setup is taken care of automatically based on settings in the SLURM script example below.</p> Sample Submission Script <pre><code>#!/bin/bash\n#SBATCH --job-name G16_test\n#SBATCH --nodes=2\n#SBATCH --time=1:00:00\n#SBATCH --account=[your account]\n#SBATCH --error=std.err\n#SBATCH --output=std.out\n#SBATCH --exclusive\n#SBATCH -p debug\n\n# Load Gaussian module to set environment\nmodule load gaussian python\nmodule list\n\ncd $SLURM_SUBMIT_DIR\n\nINPUT_BASENAME=G16_test\nGAUSSIAN_EXEC=g16\n\nif [ -e /dev/nvme0n1 ]; then\nSCRATCH=$TMPDIR\necho \"This node has a local storage and will use $SCRATCH as the scratch path\"\nelse\nSCRATCH=/scratch/$USER/$SLURM_JOB_ID\necho \"This node does not have a local storage drive and will use $SCRATCH as the scratch path\"\nfi\n\nmkdir -p $SCRATCH\n\nexport GAUSS_SCRDIR=$SCRATCH\n\n# Run gaussian NREL script (performs much of the Gaussian setup)\ng16_nrel\n\n#Setup Linda parameters\nif [ $SLURM_JOB_NUM_NODES -gt 1 ]; then \nexport GAUSS_LFLAGS='-vv -opt \"Tsnet.Node.lindarsharg: ssh\"' \nexport GAUSS_EXEDIR=$g16root/g16/linda-exe:$GAUSS_EXEDIR \nfi \n\n# Run Gaussian job \n$GAUSSIAN_EXEC &lt; $INPUT_BASENAME.com &gt;&amp; $INPUT_BASENAME.log \n\nrm $SCRATCH/*\nrmdir $SCRATCH\n</code></pre> <p>This script and sample Gaussian input are located at /nopt/nrel/apps/gaussian/examples. The gaussian module is loaded by the script automatically, so the user does not need to have loaded the module before submitting the job. The g16_nrel python script edits the Default.Route file based on the SLURM environment set when the script is submitted to the queue. The user also must supply the name of the input file (<code>INPUT_BASENAME</code>). </p> <p>The user scratch space is set to a directory in the default scratch space, with a name containing the job ID so different jobs will not overwrite the disk space. The default scratch space is /tmp/scratch when a local disk is available or /scratch/$USER. The script sets the directories for scratch files and environment variables needed by Gaussian (eg <code>GAUSS_SCRDIR</code>).</p> <p>To submit a job with the example script, named g16.slurm, one would type:</p> <p><code>sbatch g16.slurm</code></p> <p>For large size jobs, which may have extensive I/O for reading and writing scratch files, using nodes with a local disk would improve job performance. To request standard CPU nodes with local disk, use the <code>nvme</code> partition by adding <code>--partition=nvme</code> to your job script.  Note that all of the Bigmem nodes have a local disk.</p>"},{"location":"Documentation/Applications/gurobi/","title":"Gurobi","text":"<p>Gurobi Optimizer is a suite of solvers for mathematical programming.</p> <p>License Request Required</p> <p>Starting soon, Gurobi jobs will require explicit license requests in your job submission.  If you see the following warning message:</p> <pre><code>WARNING: SLURM_JOB_LICENSES is not set.\nPlease request a license with your job submission using '-L gurobi:numberoflicenses'.\nGurobi will fail in the future if you do not add the above to your job submission.\n</code></pre> <p>You must add the license request to your job submission using the <code>-L</code> flag. See the sbatch example below.</p> <p>Gurobi includes a linear programming solver (LP), quadratic programming solver (QP), quadratically constrained programming solver (QCP), mixed-integer linear programming solver (MILP), mixed-integer quadratic programming solver (MIQP), and a mixed-integer quadratically constrained programming solver (MIQCP).</p> <p>Gurobi is available on multiple systems. There are 24 license tokens available for  concurrent use - 6 are for general use (including commercial) and 18 standalone license tokens are for academic/government use. After logging onto the appropriate cluster, load the default Gurobi module using  <code>module load gurobi</code>.  The Gurobi interactive shell is run by typing  \"<code>gurobi.sh</code>\". Gurobi can also be interfaced with C/C++/Java/MATLAB/R codes by  linking with the Gurobi libraries.</p> <p>For details on Gurobi programming, see the Gurobi Resource Center and Gurobi documentation.</p>"},{"location":"Documentation/Applications/gurobi/#available-modules","title":"Available Modules","text":"Kestrel Swift gurobi/12.0.0 gurobi/11.0.2 gurobi/10.0.2 gurobi/10.0.1 gurobi/9.5.1 gurobi/9.5.1 <p>Tip</p> <p>You can check how many Gurobi licenses are available for use by running the following command after loading the Gurobi module <pre><code>gurobi_cl -t\n</code></pre></p>"},{"location":"Documentation/Applications/gurobi/#requesting-gurobi-licenses-in-job-submissions","title":"Requesting Gurobi Licenses in Job Submissions","text":"<p>When submitting jobs that use Gurobi, you need to request the appropriate number of licenses. Here's an example sbatch script:</p> <pre><code>#!/bin/bash\n#SBATCH --job-name=gurobi_job\n#SBATCH --time=01:00:00\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=4\n#SBATCH --mem=8GB\n#SBATCH -L gurobi:1\n\nmodule load gurobi\n\n# Your Gurobi commands here\ngurobi.sh your_model.lp\n</code></pre> <p>The <code>-L gurobi:1</code> flag requests 1 Gurobi license token. Adjust the number based on your needs, keeping in mind that there are 24 license tokens available for concurrent use.</p>"},{"location":"Documentation/Applications/gurobi/#gurobi-and-matlab","title":"Gurobi and MATLAB","text":"<p>To use the Gurobi solver with MATLAB, make sure you have the Gurobi and MATLAB environment modules loaded, then issue the following two commands from the MATLAB prompt or your script:</p> <pre><code>&gt;&gt; grb = getenv('GRB_MATLAB_PATH')\n&gt;&gt; path(path,grb)\n</code></pre>"},{"location":"Documentation/Applications/gurobi/#gurobi-and-general-algebraic-modeling-system-gams","title":"Gurobi and General Algebraic Modeling System (GAMS)","text":"<p>The General Algebraic Modeling System (GAMS) is a high-level modeling system for mathematical programming and optimization. The GAMS package installed at NREL includes Gurobi solvers. For more information, see using GAMS.</p> <p>Note that the Gurobi license for this interface is separate from the standalone Gurobi license, and supports far more instances.</p> <p>Important</p> <p>When using the Gurobi solver in GAMS, the user should NOT load the Gurobi module. Simply using \"module load gams\" will be enough to load the required Gurobi components and access rights.</p>"},{"location":"Documentation/Applications/idaes_solvers/","title":"IDAES Solvers","text":"<p>Institute for Design of Advanced Energy Systems (IDAES) Solvers are a collection of pre-compiled optimizer binaries with efficient linear algebra solvers that enable solving a variety of MINLP problems.</p> <p>Available optimizers include:</p> <ol> <li>Bonmin</li> <li>CBC</li> <li>CLP</li> <li>Couenne</li> <li>IPOPT + HSL</li> </ol>"},{"location":"Documentation/Applications/idaes_solvers/#available-modules","title":"Available Modules","text":"<p>Info</p> <p>IDAES solvers are currently not available on GPU compute nodes.</p> Kestrel (CPU nodes) idaes_solvers/3.4.0-cray-libsci idaes_solvers/3.4.0-intel-oneapi-mkl idaes_solvers/3.4.0-netlib-lapack"},{"location":"Documentation/Applications/idaes_solvers/#v340","title":"v3.4.0","text":"<p>IDAES Solvers v3.4.0 contains the following optimizer versions</p> Optimizer Version Bonmin 1.8.8 CBC 2.10.10 CLP 1.17.8 Couenne 0.5.8 IPOPT + HSL 3.13.2 <p>Note</p> <p>IPOPT is available with performant HSL MA27, MA57, and MA97 linear solvers. These have been shown to perform better than the default MUMPS solver for a variety of renewable energy optimization problems. Please see documentation here.</p>"},{"location":"Documentation/Applications/idaes_solvers/#usage","title":"Usage","text":"<p>Users can run any of the IDAES solvers simply by loading the appropriate module, e.g.,</p> <pre><code>module load idaes_solvers/3.4.0-cray-libsci # OR \nmodule load idaes_solvers/3.4.0-netlib-lapack # OR\nmodule load idaes_solvers/3.4.0-intel-oneapi-mkl\n</code></pre>"},{"location":"Documentation/Applications/idaes_solvers/#bonmin","title":"Bonmin","text":"<p>Bonmin (Basic Open-source Nonlinear Mixed Integer) is an open source solver that leverages CBC and IPOPT to solve general mixed integer nonlinear programs (MINLP).  Please refer to the Bonmin documentation here</p>"},{"location":"Documentation/Applications/idaes_solvers/#cbc","title":"CBC","text":"<p>COIN-OR Branch and Cut (CBC) solver is an opensource optimizer for solving mixed integer programs (MIP). Please refer to the documentation here for more details.</p>"},{"location":"Documentation/Applications/idaes_solvers/#clp","title":"CLP","text":"<p>COIN-OR Linear Program (CLP) is an open-source solver for solving linear programs. Please refer to the documentaion here for further details. </p>"},{"location":"Documentation/Applications/idaes_solvers/#couenne","title":"Couenne","text":"<p>Convex Over and Under Envelopes for Nonlinear Estimation (Couenne) is an open-source mixed integer nonlinear programming (MINLP) global optimization solver. Please visit the following website for more details regarding the solver.</p>"},{"location":"Documentation/Applications/idaes_solvers/#ipopt-hsl","title":"IPOPT + HSL","text":"<p>Interior Point Optimizer (IPOPT) is an open-source nonlinear optimizer. Harwell Subroutine Library (HSL) is a collection of efficient linear solvers used by IPOPT. HSL solvers have been demonstrated to be more performant than the default MUMPS (Multifrontal Massively Parallel sparse direct Solver) solver that comes with IPOPT, and are highly recommended.</p> <p>IPOPT that is distributed as part of IDAES solvers comes pre-compiled with 3 HSL solvers:</p> <ol> <li>MA27 is a serial linear solver suitable for small problems</li> <li>MA57 has threaded BLAS operations and is suitable for small to medium-sized problems.</li> <li>MA97 is a parallel direct linear solver for sparse symmetric systems. It is more suitable for medium and large problem sizes. Users will may see worse performance on small problems when compared to MA27 and MA57.</li> </ol> <p>All three solvers produce repeatable answers unlike their sibling MA86.</p> <p>Info</p> <p>For additional details regarding IPOPT on Kestrel, e.g., building a custom version, please visit here. Please click here for additional details regarding HSL solvers on Kestrel.</p>"},{"location":"Documentation/Applications/ipopt/","title":"IPOPT","text":"<p>IPOPT (Interior Point OPTimizer, pronounced \"Eye-Pea-Opt\") is an open-source non-linear optimizer using the interior point method.</p> <p>IPOPT is commonly used in solving power flow, e.g., AC Optimal Power Flow, and controls problems. Please refer to their project website for the source code. The documentation can be found here.</p> <p>Note</p> <p>IPOPT with HSL linear solvers is available as a module on Kestrel. Please see IDAES Solvers for additional details. We recommend using the system module for ease-of-use and only build if the module does not meet your needs.</p>"},{"location":"Documentation/Applications/ipopt/#installation-from-source","title":"Installation from source","text":"<p>Info</p> <p>We advise building all applications on a compute node using an interactive session. Please see Running Interactive Jobs for additional details.</p>"},{"location":"Documentation/Applications/ipopt/#optional-pre-requisites","title":"Optional Pre-requisites","text":"<p>We will build IPOPT using all prerequisites mentioned below. Users may pick and choose depending on their needs.</p>"},{"location":"Documentation/Applications/ipopt/#metis","title":"Metis","text":"<p>It is highly recommended to install Metis - Serial Graph Partitioning and Fill-reducing Matrix Ordering software to  improve the performance of linear solvers such as MUMPS and HSL.</p> <p>Warning</p> <p>Using HSL linear solvers requires installing Metis. Metis is optional for MUMPS.</p> <p>We will install Metis using Anaconda. However, it can also be installed from source. To install using Anaconda, we will create a clean environment with only Metis. For this example, the conda environment is being constructed within a directory in the <code>hpcapps</code> project on  Kestrel. Users can create a conda environment in any place of their choice.</p> <pre><code>module load conda\nconda create -p /projects/hpcapps/kpanda/conda-envs/metis python\nconda activate /projects/hpcapps/kpanda/conda-envs/metis\nconda install conda-forge::metis\n</code></pre>"},{"location":"Documentation/Applications/ipopt/#coinbrew","title":"Coinbrew","text":"<p>Coinbrew is a package manager to install COIN-OR tools. It makes installing IPOPT and its dependencies easier. However, it  is not necessary to the installation if one clones the repositories individually. A user can download <code>coinbrew</code> by running the following command</p> <pre><code>wget https://raw.githubusercontent.com/coin-or/coinbrew/master/coinbrew\n</code></pre>"},{"location":"Documentation/Applications/ipopt/#intel-oneapi-mkl","title":"Intel oneAPI MKL","text":"<p>Intel oneAPI MKL provides BLAS and LAPACK libraries for efficient linear algebra. Additionally, it also provides access to oneMKL PARDISO linear solver that is  compatible with IPOPT.</p> <p>Note</p> <p>oneMKL PARDISO is not available on Kestrel GPU nodes since they consist of AMD processors.</p>"},{"location":"Documentation/Applications/ipopt/#hsl","title":"HSL","text":"<p>HSL (Harwell Subroutine Library) is a set of linear solvers  that can greatly accelerate the speed of optimization over other linear solvers, e.g., MUMPS. HSL can be installed separately as well using ThirdParty-HSL. Please see here for installation on Kestrel.</p>"},{"location":"Documentation/Applications/ipopt/#installation","title":"Installation","text":"<p>In this demonstration, we will install Ipopt within <code>/projects/msoc/kpanda/apps/Ipopt/install</code>. However, one is free to set their install directory as they wish. Starting with the base working directory <code>/projects/msoc/kpanda/apps/</code> we will do the following</p> <pre><code>cd /projects/msoc/kpanda/apps/ # go into the base working directory\nwget https://raw.githubusercontent.com/coin-or/coinbrew/master/coinbrew # install coinbrew\ncoinbrew fetch Ipopt # Fetch Ipopt and its dependencies\n</code></pre> <p>This will download 2 additional directories <code>Ipopt</code> and <code>ThirdParty</code>. <code>ThirdParty</code>, furthermore, contains 3 subdirectories <code>ASL</code>, <code>HSL</code>, and <code>Mumps</code>. The source code of all but <code>HSL</code> will be downloaded. </p> <p>Next, we will create our install directories and subdirectories</p> <pre><code>mkdir -p /projects/msoc/kpanda/apps/Ipopt/install # create the install directory\ncd /projects/msoc/kpanda/apps/Ipopt/install # enter the directory\nmkdir bin lib include # create some subdirectories\n</code></pre> <p>We then add symbolic links to Metis in the install directory. </p> <p>Note</p> <p>If <code>libmetis.so</code> is in your <code>LD_LIBRARY_PATH</code> you do not need to do this step.</p> <pre><code>cd /projects/msoc/kpanda/apps/Ipopt/install/lib\nln -s /projects/hpcapps/kpanda/conda-envs/metis/lib/libmetis.so libmetis.so\ncd ../include\nln -s /projects/hpcapps/kpanda/conda-envs/metis/include/metis.h metis.h\ncd /projects/msoc/kpanda/apps/ # go back base directory\n</code></pre> <p>This has two advantages. First, we don't need to add <code>/projects/hpcapps/kpanda/conda-envs/metis/lib/</code> to the <code>LD_LIBRARY_PATH</code>.  The second advantage is that anaconda puts all the  environments libraries and include files in the same directories with <code>libmetis.so</code> and <code>metis.h</code>.  Many of these libraries overlap with those used by HSL, Mumps and IPOPT but are not necessarily the same versions.  Loading a different version of a library than those compiled against can cause unexpected behavior.</p> <p>Next, we will load additional modules. If users require oneMKL PARDISO or would like to leverage intel performance optimization, run the following commands</p> <pre><code>module load intel-oneapi-mkl\n</code></pre> <p>Alternatively, users can load the open source Netlib LAPACK using the command</p> <pre><code>module load netlib-lapack # Please ensure you do not have intel-oneapi-mkl loaded\n</code></pre> <p>We will now copy the HSL source code tarball into  <code>/projects/msoc/kpanda/apps/ThirdParty/HSL/</code>, unpack it, and rename or (create a  symbolic link to the unpacked directory) as <code>coinhsl</code>. </p> <p>We are now ready to install IPOPT and its dependencies. We will use the default compilers available in the Kestrel programming environment. Going back to the base  directory, we will run the following commands</p> <pre><code>cd /projects/msoc/kpanda/apps/ # go back base directory\n./coinbrew build Ipopt --disable-java \\\n--prefix=/kfs2/projects/msoc/kpanda/apps/Ipopt/install \\\n--with-metis \\\n--with-metis-cflags=-I/projects/hpcapps/kpanda/conda-envs/metis/include \\\n--with-metis-lflags=\"-L/projects/hpcapps/kpanda/conda-envs/metis/lib -lmetis\" \\\n--parallel-jobs 4 \\\n--verbosity 4 \\\n--reconfigure\n</code></pre>"},{"location":"Documentation/Applications/ipopt/#usage","title":"Usage","text":"<p>The installed Ipopt is now ready to be used. We need to update our <code>PATH</code> AND  <code>LD_LIBRARY_PATH</code> environment variables. In our demonstrations this will be</p> <pre><code>export PATH=/projects/msoc/kpanda/apps/Ipopt/install/bin:${PATH}\nexport LD_LIBRARY_PATH=/projects/msoc/kpanda/apps/Ipopt/install/lib:${LD_LIBRARY_PATH}\n</code></pre> <p>Note</p> <p>Do not forget to load <code>intel-oneapi-mkl</code> or <code>netlib-lapack</code> before running IPOPT else your runs will fail.</p>"},{"location":"Documentation/Applications/ipopt/#using-custom-ipopt-with-jump","title":"Using Custom IPOPT with JuMP","text":"<p>To use our custom installation of IPOPT with <code>Ipopt.jl</code>, we do the following:</p> <ol> <li>Open the Julia REPL and activate an environment that has IPOPT installed</li> <li>Tell Julia and <code>Ipopt.jl</code> the location of our IPOPT library and executable     <pre><code>ENV[\"JULIA_IPOPT_LIBRARY_PATH\"] = ENV[\"/projects/msoc/kpanda/apps/Ipopt/install/lib\"]\nENV[\"JULIA_IPOPT_EXECUTABLE_PATH\"] = ENV[\"/projects/msoc/kpanda/apps/Ipopt/install/bin\"]\n</code></pre></li> <li>Rebuild <code>Ipopt.jl</code> with the above environment variables set to pick up the new library and executable     <pre><code>using Pkg; Pkg.build(\"Ipopt\");\n</code></pre></li> <li>Print the path <code>Ipopt.jl</code> has stored for <code>libipopt.so</code>. This should be the location of your compiled version.     <pre><code>using Ipopt; println(Ipopt.libipopt_path)\n</code></pre></li> </ol> <p>Info</p> <p>The IPOPT build that comes with <code>Ipopt.jl</code> seems to expect the HSL library to have the name <code>libhsl.so</code>. The repo ThirdParty-HSL builds the library <code>libcoinhsl.so</code>.  The simplest fix is to do the following:</p> <pre><code>cd /projects/msoc/kpanda/apps/Ipopt/install/lib # install directory\n# Create a symbolic link called libhsl.so\nln -s libcoinhsl.so libhsl.so\n</code></pre> <p>The following Julia code is useful for testing the HSL linear solvers are working</p> <pre><code>using JuMP, IPOPT\n\nm = JuMP.Model(()-&gt;IPOPT.Optimizer(linear_solver=\"ma97\"))\n@variable(m, x)\n@objective(m, Min, x^2)\nJuMP.optimize!(m)\n</code></pre>"},{"location":"Documentation/Applications/lammps/","title":"Using LAMMPS Software","text":"<p>Learn how to use LAMMPS software \u2014 an open-source, classical molecular dynamics program designed for massively parallel systems. It is distributed by Sandia National Laboratories.</p> <p>LAMMPS has numerous built-in potentials for simulations of solid-state, soft matter, and coarse-grained systems. It can be run on a single processor or in parallel using MPI. To learn more, see the LAMMPS website. </p> <p>The versions of LAMMPS on Kestrel, Swift, and Vermilion can be checked by running <code>module avail lammps</code>. Usually there are two recent stable versions available that were compiled using different compiler and MPI toolchains. The following packages have been installed: asphere, body, bocs, class2, colloid, dielectric, diffraction, dipole, dpd-basic, drude, eff, electrode, extra-fix, extra-pair, fep, granular, h5md, intel, interlayer, kspace, manifold, manybody, mc, meam, misc, molecule, mpiio, openmp, opt, python, phonon, qep, qmmm, reaction, reaxff, replica, rigid, shock, spin, voronoi.</p> <p>If you need other packages or a certain LAMMPS version, please contact us. </p>"},{"location":"Documentation/Applications/lammps/#sample-cpu-slurm-script","title":"Sample CPU Slurm Script","text":"<p>A sample Slurm script for running LAMMPS on Kestrel CPU nodes is given below:</p> <pre><code>#!/bin/bash\n#SBATCH --job-name cpu-test\n#SBATCH --nodes=2 #Request two CPU nodes\n#SBATCH --time=1:00:00\n#SBATCH --account=[your allocation name]\n#SBATCH --error=std.err\n#SBATCH --output=std.out\n#SBATCH --tasks-per-node=104\n#SBATCH --exclusive\n#SBATCH -p debug\n\nmodule load lammps/080223-intel-mpich\nmodule list\n\nrun_cmd=\"srun --mpi=pmi2 \"\nlmp_path=lmp\nname=my_job\n$run_cmd $lmp_path -in $name.in &gt;&amp; $name.log\n</code></pre> <p>where <code>my_job.in</code> is the input and <code>my_job.log</code> is the output. This runs LAMMPS using two nodes with 208 MPI ranks. </p>"},{"location":"Documentation/Applications/lammps/#sample-gpu-slurm-script","title":"Sample GPU Slurm Script","text":"<p>A sample Slurm script for running LAMMPS on Kestrel GPU nodes is given below:</p> <pre><code>#!/bin/bash\n#SBATCH --job-name gpu-test\n#SBATCH --nodes=1 #Request one GPU node\n#SBATCH --time=1:00:00\n#SBATCH --account=[your_allocation_name]\n#SBATCH --error=std.err\n#SBATCH --output=std.out\n#SBATCH --tasks-per-node=8 #Running 8 MPI tasks per node\n#SBATCH --mem=16G #Request memory\n#SBATCH --gres=gpu:2 #Request 2 GPU per node\n#SBATCH -p debug\n\nmodule load lammps/080223-gpu\nmodule list\n\nexport MPICH_GPU_SUPPORT_ENABLED=1\n#Request 2 GPU per node\nexport CUDA_VISIBLE_DEVICES=0,1 \n\nrun_cmd=\"srun --mpi=pmi2 \"\nlmp_path=lmp\nname=medium\n#Request 2 GPU per node\ngpu_opt=\"-sf gpu -pk gpu 2\"\n$run_cmd $lmp_path $gpu_opt -in $name.in &gt;&amp; $name.gpu.log\n</code></pre> <p>This runs LAMMPS using one nodes with 8 MPI ranks and 2 GPUs. The following information will be printed out in my_job.log file: <pre><code>--------------------------------------------------------------------------\n- Using acceleration for pppm:\n-  with 4 proc(s) per device.\n-  Horizontal vector operations: ENABLED\n-  Shared memory system: No\n--------------------------------------------------------------------------\nDevice 0: NVIDIA H100 80GB HBM3, 132 CUs, 77/79 GB, 2 GHZ (Mixed Precision)\nDevice 1: NVIDIA H100 80GB HBM3, 132 CUs, 2 GHZ (Mixed Precision)\n--------------------------------------------------------------------------\n</code></pre></p>"},{"location":"Documentation/Applications/lammps/#sample-high-bandwidth-partition-slurm-script","title":"Sample High-Bandwidth Partition Slurm Script","text":"<p>When running LAMMPs on more than 10 nodes, it is recommended to run LAMMPs on the High-Bandwidth Partition (hbw) \u2013 this partition consists of nodes that have dual-NICs as part of its hardware architecture, which can significantly improve LAMMPs performance. </p> <pre><code>#!/bin/bash\n#SBATCH --job-name lammps-16nodes-96ranks\n#SBATCH --nodes=16\n#SBATCH --time=01:30:00\n#SBATCH --account=&lt;your allocation name here&gt;\n#SBATCH --error=std.err\n#SBATCH --output=std.out\n#SBATCH --tasks-per-node=96\n#SBATCH --exclusive\n#SBATCH -p hbw\n#SBATCH --array=1-5\n#SBATCH --output=lammps-96nodes/lammps-16nodes-96ranks_%a.out\n\n\nmodule load lammps/062322-cray-mpich\n\nexport OMP_NUM_THREADS=1\n\nCPUBIND='--cpu-bind=map_cpu:0,52,13,65,26,78,39,91,1,53,14,66,27,79,40,92,2,54,15,67,28,80,41,93,3,55,16,68,29,81,42,94,4,56,17,69,30,82,43,95,5,57,18,70,31,83,44,96,6,58,19,71,32,84,45,97,7,59,20,72,33,85,46,98,8,60,21,73,34,86,47,99,9,61,22,74,35,87,48,100,10,62,23,75,36,88,49,101,11,63,24,76,37,89,50,102,12,64,25,77,38,90,51,103'\n\nexport MPICH_OFI_NIC_POLICY=\"NUMA\"\n\n#MPI only, no OpenMP\nrun_cmd=\"srun --mpi=pmi2 $CPUBIND\"\nlmp_path=lmp\nrun_name=(medium)\n\n$run_cmd $lmp_path -in $name.in &gt;&amp; $name.log\n</code></pre> <p>Please note \u2013 the CPU binding and MPICH_OFI_NIC_POLICY being set explicitly allow for extra performance gains on the high-bandwidth partition. If not set, there are still performance gains on the high-bandwidth nodes, just not as much as there would be otherwise. </p>"},{"location":"Documentation/Applications/lammps/#hints-and-additional-resources","title":"Hints and Additional Resources","text":"<ol> <li>For calculations requesting more than ~10 nodes, running on the high-bandwidth partition is recommended. Further information on the High-Bandwidth partition can be found here: High-Bandwidth Partition.</li> <li>For CPU runs, especially for multi-nodes runs, the optimal performance for a particular job may be at a tasks-per-node value less than 104. For GPU runs, number of GPUs should also be varied to achieve the optimal performance. Users should investigate those parameters for large jobs by performing some short test runs. Some tasks-per-node values that could be useful to test are: 72, 52, and 48.</li> <li>For instructions on running LAMMPS with OpenMP, see the HPC Github code repository.</li> </ol>"},{"location":"Documentation/Applications/mace/","title":"MACE","text":"<p>The Multi Atomic Cluster Expansion (MACE) is a neural network-based approach for atomic scale materials modeling. Using higher order equivariant message passing, MACE represents the state-of-the-art in machine-learned interatomic potentials (MLIPs), needing orders of magnitude less training data than non-equivariant approaches and enabling approximately a million times more energy and force evaluations than is possible with DFT with the same computational resources. Researchers can use this enhanced throughput to do material screenings, calculate time-averaged quantities over time scales relevant to experiment, or treat heterogeneous, multicomponent systems. The approach is implemented in two steps: </p> <ol> <li> <p>The training step is done in a standalone software package from the MACE developers.</p> </li> <li> <p>The evaluation step can be done with the standalone MACE package, but is more typically done within LAMMPS or ASE.</p> </li> </ol>"},{"location":"Documentation/Applications/mace/#installing-mace-standalone-package","title":"Installing MACE standalone package","text":"<p>The Kestrel-optimized MACE conda environment can be found at: <code>/nopt/nrel/apps/gpu_stack/software/mace/environments</code></p> <p>Copy the <code>mace_kestrel.yml</code> to one of your own directories, and build the environment as from that directory as follows:</p> <p><code>$ module load conda</code></p> <p><code>$ conda env create -n &lt;your_mace_env_name&gt; -f mace_kestrel.yml</code></p> <p>Then check that MACE is installed within the conda environment: </p> <p><code>$ conda list mace</code></p> <p>Should show: Name                    Version                   Build  Channel mace-torch                0.3.5                    pypi_0    pypi</p> <p>And check that mace-torch working by issuing the following commands: </p> <p><code>$ python</code> <code>&gt;&gt;&gt; import torch</code> <code>&gt;&gt;&gt; x = torch.rand(5, 3)</code> <code>&gt;&gt;&gt; print(x)</code> <code>&gt;&gt;&gt; exit</code></p> <p>This should show the following after the print statement: tensor([[0.1828, 0.4496, 0.8743],         [0.9411, 0.3940, 0.1104],         [0.9148, 0.7720, 0.7389],         [0.6979, 0.0421, 0.1763],         [0.6563, 0.7603, 0.8917]])</p> <p>This environment is required for all commands in the MACE standalone package, including  those for training (<code>mace_run_train</code>), evaluation (<code>mace_eval_configs</code>) and creating lammps models (<code>mace_create_lammps_model</code>). </p>"},{"location":"Documentation/Applications/mace/#generating-training-data","title":"Generating training data","text":"<p>Training data for your MACE MLIP can be generated with any quantum mechanical software package (e.g. VASP, JDFTx, Quantum ESPRESSO, Q-Chem, etc.) This should be in an extended XYZ format that contains the forces and energies in addition to the species and position. Training xyz files may be concatenated together, e.g.: </p> <pre><code>137 \nLattice=\"8.298 0.000 0.000 4.149 7.186 0.000 0.000 0.000 30.000\" Properties=species:S:1:pos:R:3:forces:R:3 energy=-112196.4817837208 \nPt   1.382959   0.798452   7.258370  -0.009086   0.070059  -0.293957 \nPt   2.765916   3.193808   7.258370   0.003288  -0.015350  -0.455340 \nPt   4.148873   5.589164   7.258370  -0.061542  -0.013544  -0.227623\n[...] \n137 \nLattice=\"8.298 0.000 0.000 4.149 7.186 0.000 0.000 0.000 30.000\" Properties=species:S:1:pos:R:3:forces:R:3 energy=-112225.6448941744 \nPt   1.382959   0.798452   7.258370  -0.008809   0.094532  -0.233627 \nPt   2.765916   3.193808   7.258370   0.011519  -0.015212  -0.446426 \nPt   4.148873   5.589164   7.258370  -0.130966  -0.038721  -0.129331 \n[...]\n[etc.]\n</code></pre>"},{"location":"Documentation/Applications/mace/#training-with-mace","title":"Training with MACE","text":"<p>After installing the MACE package and generating your training data, you are ready to train a MLIP using the script below. A complete description of the training parameters can be found on the MACE github package website or the MACE documentation website.</p> <pre><code>#!/bin/bash\n#SBATCH --account=&lt;your-account-name&gt; \n#SBATCH --nodes=1\n#SBATCH --gpus=1 #Ngpus used per node\n#SBATCH --ntasks-per-node=1 # =Ngpus used per node\n#SBATCH --cpus-per-task=1 # =Ngpus\n#SBATCH --time=04:00:00\n#SBATCH --job-name=&lt;your-job-name&gt;\n#SBATCH --mem=85G # =Ngpus*85G\n\nconda activate your_mace_env_name\n\nmace_run_train \\\n    --name=\"MACE_model\" \\\n    --train_file=\"train.xyz\" \\\n    --valid_fraction=0.05 \\\n    --test_file=\"test.xyz\" \\\n    --config_type_weights='{\"Default\":1.0}' \\\n    --E0s='{1:-13.663181292231226, 6:-1029.2809654211628, 7:-1484.1187695035828, 8:-2042.0330099956639}' \\\n    --model=\"MACE\" \\\n    --hidden_irreps='128x0e + 128x1o' \\\n    --r_max=5.0 \\\n    --batch_size=10 \\\n    --max_num_epochs=1500 \\\n    --swa \\\n    --start_swa=1200 \\\n    --ema \\\n    --ema_decay=0.99 \\\n    --amsgrad \\\n    --restart_latest \\\n    --device=cuda \\\n</code></pre> <p>Upon completion of this training, a MACE MLIP model file will be generated with a <code>.model</code> extension. This can be used for evaluation of the model within the standalone package or within ASE. If you plan on using your MACE MLIP to run molecular dynamics calculations using LAMMPS, you need to run <code>mace_create_lammps_model</code> to create a LAMMPS MACE MLIP model file with an added <code>-lammps.pt</code> extension: </p> <p><code>$ mace_create_lammps_model my_mace.model</code></p> <p>The output will be: <code>my_mace.model-lammps.pt</code></p>"},{"location":"Documentation/Applications/mace/#evaluation-with-mace","title":"Evaluation with MACE","text":"<p>Once your MACE MLIP is generated, you can evaluate atomic configurations specified in an xyz format within our provided MACE environment as follows:</p> <pre><code>mace_eval_configs \\\n    --configs=\"your_configs.xyz\" \\\n    --model=\"your_model.model\" \\\n    --output=\"./your_output.xyz\"\n</code></pre> <p>Alternatively, you can use the <code>.model</code> file within ASE detailed here, or the <code>.pt</code> file within LAMMPS detailed here.</p>"},{"location":"Documentation/Applications/openfast/","title":"OpenFAST and FAST.Farm","text":"<p>OpenFAST is a multi-physics, multi-fidelity tool for simulating the coupled dynamic response of wind turbines. Practically speaking, OpenFAST is the framework (or \u201cglue code\u201d) that couples computational modules for aerodynamics, hydrodynamics for offshore structures, control and electrical system (servo) dynamics, and structural dynamics to enable coupled nonlinear aero-hydro-servo-elastic simulation in the time domain. OpenFAST enables the analysis of a range of wind turbine configurations, including two- or three-blade horizontal-axis rotor, pitch or stall regulation, rigid or teetering hub, upwind or downwind rotor, and lattice or tubular tower. The wind turbine can be modeled on land or offshore on fixed-bottom or floating substructures.</p> <p>FAST.Farm is a midfidelity multiphysics engineering tool for predicting the power performance and structural loads of wind turbines within a wind farm. FAST.Farm uses OpenFAST to solve the aero-hydro-servo-elastic dynamics of each individual turbine, but considers additional physics for wind farm-wide ambient wind in the atmospheric boundary layer; a wind-farm super controller; and wake deficits, advection, deflection, meandering, and merging. FAST.Farm is based on some of the principles of the dynamic wake meandering (DWM) model\u2014including passive tracer modeling of wake meandering\u2014but addresses many of the limitations of previous DWM implementations. FAST.Farm maintains low computational cost to support the often highly iterative and probabilistic design process.</p>"},{"location":"Documentation/Applications/openfast/#building-openfast-and-fastfarm","title":"Building OpenFAST and FAST.Farm","text":"<p>In this section we provide cmake scripts for installation of OpenFAST and FAST.Farm. Both tools can be installed by following the instructions here.</p> <p>FAST.Farm is installed as a <code>cmake</code> option to the build of OpenFAST.</p> <p>You can clone your desired verstion of OpenFAST from here. Once cloned, <code>cd</code> into the OpenFAST directory and create a <code>build</code> directory. Use the scripts given below from within the <code>build</code> directory to build OpenFAST and FAST.Farm. On a Kestrel CPU node, build OpenFAST by executing the following script from within the <code>build</code> directory:</p> Sample job script: Building OpenFAST and FAST.Farm using <code>cmake</code> on CPUs <pre><code>#!/bin/bash\n\nmodule purge\nmodule load PrgEnv-intel/8.5.0\nmodule load intel-oneapi-mkl/2024.0.0-intel\nmodule load intel-oneapi\nmodule load binutils\nmodule load hdf5/1.14.3-intel-oneapi-mpi-intel\n\nmodule list\n\ncmake .. \\\n    -DCMAKE_Fortran_COMPILER=ifx \\\n    -DCMAKE_CXX_COMPILER=icpx \\\n    -DCMAKE_C_COMPILER=icx \\\n    -DCMAKE_CXX_FLAGS=-fPIC \\\n    -DCMAKE_C_FLAGS=-fPIC \\\n    -DBUILD_OPENFAST_CPP_API=ON \\\n    -DBUILD_FASTFARM=ON \\\n    -DDOUBLE_PRECISION:BOOL=ON \\\n    -DCMAKE_BUILD_TYPE=Release \\\n    -DOPENMP=ON \\\n    -DCMAKE_INSTALL_PREFIX:PATH=${PWD}/install\n\nnice make -j48\nmake install\n</code></pre> <p>Note</p> <p>The option <code>OPENFAST_CPP_API</code> needs to be enabled if further coupling to AMR-Wind is desired. Otherwise, the user can set the option to <code>OFF</code>.</p> <p>Note</p> <p>OpenFAST is not GPU-ready.</p>"},{"location":"Documentation/Applications/openfast/#running-openfast","title":"Running OpenFAST","text":"<p>OpenFAST is a serial tool and can be executed by simply calling it directly</p> Sample job script: Running OpenFAST on a dedicated node <pre><code>    #!/bin/bash\n\n    #SBATCH --account=&lt;user-account&gt; # Replace with your HPC account\n    #SBATCH --time=01:00:00\n    #SBATCH \u2013-nodes=1\n    #SBATCH --partition=shared\n\n    module purge\n    module load PrgEnv-intel/8.5.0\n    module load intel-oneapi-mkl/2024.0.0-intel\n    module load intel-oneapi\n    module load binutils\n    module load hdf5/1.14.3-intel-oneapi-mpi-intel\n\n    openfast &lt;your_turbine_input_file&gt;.fst\n</code></pre> <p>Examples of turbine models are available in the regression tests repository, here.</p> <p>Note</p> <p>OpenFAST input files change from one version to another. If you build a certain version, ensure your input files (or exemples from the regression tests repository linked above) are compatible with your version.</p>"},{"location":"Documentation/Applications/openfast/#running-fastfarm","title":"Running FAST.Farm","text":"<p>FAST.Farm is OpenMP-capable, but still runs within a node. Its execution is similar to OpenFAST.</p> Sample job script: Running FAST.Farm on a dedicated node <pre><code>    #!/bin/bash\n\n    #SBATCH --account=&lt;user-account&gt; # Replace with your HPC account\n    #SBATCH --time=01:00:00\n    #SBATCH \u2013-nodes=1\n\n    module purge\n    module load PrgEnv-intel/8.5.0\n    module load intel-oneapi-mkl/2024.0.0-intel\n    module load intel-oneapi\n    module load binutils\n    module load hdf5/1.14.3-intel-oneapi-mpi-intel\n\n    FAST.Farm &lt;your_fastfarm_input_file&gt;.fstf\n</code></pre> <p>FAST.Farm input deck can become complex depending on your simulation model. FAST.Farm users are encouraged to use accompanying toolbox for case setup available here.</p>"},{"location":"Documentation/Applications/openfoam/","title":"OpenFOAM","text":""},{"location":"Documentation/Applications/openfoam/#openfoam-installation","title":"OpenFOAM Installation","text":""},{"location":"Documentation/Applications/openfoam/#building-openfoam-with-cray-mpich-and-gcc","title":"Building OpenFOAM with cray-mpich and gcc","text":"<p>Instructions for installing OpenFOAM are available here.</p> <p>In the instructions, you will be cloning the OpenFOAM folder which we will refer to as <code>$OPENFOAM</code>.</p> <p>In order to build OpenFOAM with cray-mpich, two files need to be edited.</p> <ol> <li> <p><code>$OPENFOAM/etc/bashrc</code></p> <p>In this file, the variable <code>WM_MPLIB</code> will be defined as <code>MPICH</code>.  Search for the line where the variable is exported and replace it with </p> <pre><code>export WM_MPLIB=MPICH\n</code></pre> </li> <li> <p><code>$OPENFOAM/etc/config.sh/mpi</code></p> <p>This file defines where mpich is defined on the system.  You will search for the mpich definition block and replace it with </p> <pre><code>export MPI_ARCH_PATH=/opt/cray/pe/mpich/8.1.28/ofi/gnu/10.3\nexport LD_LIBRARY_PATH=\"${MPI_ARCH_PATH}/lib:${LD_LIBRARY_PATH}\"\nexport PATH=\"${MPI_ARCH_PATH}/bin:${PATH}\"\nexport FOAM_MPI=mpich-8.1.28\nexport MPI_HOME=/opt/cray/pe/mpich/8.1.28/ofi/gnu/10.3\n#export FOAM_MPI=mpich2-1.1.1p1\n#export MPI_HOME=$WM_THIRD_PARTY_DIR/$FOAM_MPI\n#export MPI_ARCH_PATH=$WM_THIRD_PARTY_DIR/platforms/$WM_ARCH$WM_COMPILER/$FOAM_MPI\n\n\n_foamAddPath    $MPI_ARCH_PATH/bin\n\n\n# 64-bit on OpenSuSE 12.1 uses lib64 others use lib\n_foamAddLib     $MPI_ARCH_PATH/lib$WM_COMPILER_LIB_ARCH\n_foamAddLib     $MPI_ARCH_PATH/lib\n\n\n_foamAddMan     $MPI_ARCH_PATH/share/man\n;;\n</code></pre> </li> </ol> <p>Before you install OpenFOAM, make sure to load <code>Prgenv-gnu</code>. This will load gcc and cray-mpich.  Make sure the same module is loaded at runtime.</p>"},{"location":"Documentation/Applications/openfoam/#running-openfoam-cases-using-modules","title":"Running OpenFOAM cases using Modules","text":"<p>There are several modules for builds of OpenFOAM. After logging in to a CPU node on Kestrel, please use the <code>module avail</code> command to view available versions. </p> <pre><code>CPU $ module avail openfoam\n----------------------------- /nopt/nrel/apps/cpu_stack/modules/default/application -----------------------------\n   openfoam/v2306-openmpi-gcc      openfoam/9-craympich (D)    openfoam/11-craympich\n   openfoam/v2406-craympich-gcc    openfoam/9-ompi             openfoam/12-intelmpi\n</code></pre> Sample job script: Kestrel <pre><code>#!/bin/bash\n#SBATCH --job-name=myOpenFOAMjob\n#SBATCH --account=&lt;your-account-name&gt;\n#SBATCH --output=foamOutputLog.out\n#SBATCH --error=foamErrorLog.out\n#SBATCH --mail-user=&lt;yourEmailAddress&gt;@nrel.gov \n#SBATCH --nodes=2\n#SBATCH --partition=hbw\n#SBATCH --ntasks-per-node=104 # set number of MPI ranks per node\n#SBATCH --time=04:00:00\n\n\nmodule load openfoam/&lt;version&gt;\n\ndecomposePar\n\nsrun -n 200 --cpu-bind=v,rank_ldom rhoReactingBuoyantFoam -parallel &gt;&gt; log.h2\n\nreconstructPar -time 0:5000  -fields '(H2 X_H2)'\n</code></pre>"},{"location":"Documentation/Applications/openfoam/#installing-additional-openfoam-packages","title":"Installing additional OpenFOAM packages","text":"<p>Additional packages built on top of the OpenFOAM API can be installed after loading a compatible module. As an example, we show the process to install the OpenFuelCell2 package. <pre><code># Download or clone the required package\n$ git clone https://github.com/openFuelCell2/openFuelCell2.git\n\n$ cd openFuelCell2\n\n# Request an interactive node for compiling in parallel\n$ salloc --account=&lt;your-account-name&gt; --time=00:30:00 --nodes=1 --ntasks-per-core=1 --ntasks-per-node=104 --cpus-per-task=1 --partition=debug\n\n# Load the module compatible with your package\n$ module load openfoam/v2306-openmpi-gcc\n\n# Compile the application with the official instructions from the developers, e.g.\n$ cd src\n$ ./Allwmake  -j -prefix=${PWD}\n\n# Test\n$ cd ../run/SOFC/\n$ make mesh\n$ export NPROCS=4\n$ make decompose\n$ make parallel\n$ make run\n</code></pre></p>"},{"location":"Documentation/Applications/openfoam/#benchmarks","title":"Benchmarks","text":"<p>OpenFOAM v2412 compiled with cray-mpich has been used to perform strong scaling tests of the DrivAer automobile model on Kestrel. The results are shown below for three levels of mesh resolution. For this particular setup, the application has shown to scale poorly beyond 2 nodes. However, for jobs requiring more than 1 node, using high bandwith nodes with <code>#SBATCH --partition=hbw</code> in your job script might yield better performance. Since the behaviour is consistent with some user reports about their own setups, we encourage users to switch to newer versions and perform strong &amp; weak scaling tests on their own before submitting a new large job.</p> <p> </p>"},{"location":"Documentation/Applications/qchem/","title":"Using Q-Chem","text":"<p>Q-Chem is a comprehensive ab initio quantum chemistry package with special strengths in excited state methods, non-adiabatic coupling, solvation models, explicitly correlated wave-function methods, and cutting-edge density functional theory (DFT). </p>"},{"location":"Documentation/Applications/qchem/#running-q-chem","title":"Running Q-Chem","text":"<p>The <code>q-chem</code> module should be loaded to set up the necessary environment. The <code>module help</code> output can provide more detail. In particular, the modulefile does not set the needed environment variable <code>QCSCRATCH</code>, as this is likely unique for each run. Users should do this in their Slurm scripts or at the command line via <code>export</code> (bash) or <code>setenv</code> (csh). </p> <p>The simplest means of starting a Q-Chem job is via the supplied <code>qchem</code> wrapper. The general syntax is: </p> <p><code>qchem -slurm &lt;-nt number_of_OpenMP_threads&gt; &lt;input file&gt; &lt;output file&gt; &lt;savename&gt;</code></p> <p>For example, to run a job with 104 threads:</p> <p><code>qchem -slurm -nt 104 example.in</code></p> <p>Note</p> <p>The Q-Chem input file must be in the same directory in which you issue the qchem command. In other words, <code>qchem ... SOMEPATH/&lt;input file&gt;</code> won't work. </p> <p>For a full list of which types of calculation are parallelized and the types of parallelism, see the Q-Chem User's Manual.</p> <p>To save certain intermediate files for, e.g., restart, a directory name needs to be provided. If not provided, all scratch files will be automatically deleted at job's end by default. If provided, a directory <code>$QCSCRATCH/savename</code> will be created and will hold saved files. In order to save all intermediate files, you can add the <code>-save</code> option. </p> <p>A template Slurm script to run Q-Chem with 104 threads is:</p>"},{"location":"Documentation/Applications/qchem/#sample-submission-script-for-kestrel","title":"Sample Submission Script for Kestrel","text":"<pre><code>    #!/bin/bash\n    #SBATCH --job-name=my_qchem_job\n    #SBATCH --account=[my_allocation_ID]\n    #SBATCH --nodes=1\n    #SBATCH --tasks-per-node=104\n    #SBATCH --time=01:00:00\n    #SBATCH --exclusive\n    #SBATCH --mail-type=BEGIN,END,FAIL\n    #SBATCH --mail-user=your_email@domain.name\n    #SBATCH --output=std-%j.out\n    #SBATCH --error=std-%j.err\n\n    # Load the Q-Chem environment\n    module load q-chem/6.2\n\n    if [ -e /dev/nvme0n1 ]; then\n     SCRATCH=$TMPDIR\n     echo \"This node has a local storage and will use $SCRATCH as the scratch path\"\n    else\n     SCRATCH=/scratch/$USER/$SLURM_JOB_ID\n     echo \"This node does not have a local storage drive and will use $SCRATCH as the scratch path\"\n    fi\n\n    mkdir -p $SCRATCH\n\n    export QCSCRATCH=$SCRATCH\n    export QCLOCALSCR=$SCRATCH\n\n    jobnm=qchem_test\n\n    if [ $SLURM_JOB_NUM_NODES -gt 1 ]; then\n     QCHEMOPT=\"-mpi -np $SLURM_NTASKS\"\n    else\n     QCHEMOPT=\"-nt $SLURM_NTASKS\"\n    fi\n\n    echo Running Q-Chem with this command: qchem $QCHEMOPT $jobnm.com $jobnm.out\n    qchem $QCHEMOPT $jobnm.com $jobnm.out\n\n    rm $SCRATCH/*\n    rmdir $SCRATCH\n</code></pre> <p>To run this script on systems other than Kestrel, the number of threads should be changed accordingly. In addition, the scratch directory will set to the local disk if a local disk is available.</p> <p>For large size jobs, which may have extensive I/O for reading and writing scratch files, using nodes with a local disk would improve job performance. To request standard CPU nodes with local disk, use the <code>nvme</code> partition (e.g. <code>--partition=nvme</code>). Note that all of the Bigmem nodes and H100 GPU nodes have local disks.</p> <p>A large number of example Q-Chem input examples are available in <code>/nopt/nrel/apps/q-chem/&lt;version&gt;/samples</code>.</p>"},{"location":"Documentation/Applications/qchem/#running-brianqc","title":"Running BrianQC","text":"<p>BrianQC is the GPU version of Q-Chem and can perform Q-Chem calculations on GPUs, which is significantly faster for some larger ab initio jobs. BrianQC uses the same input file as Q-Chem. Below is a sample slurm script for BrianQC, which should be submitted on the GPU login nodes of Kestrel. If running on Swift, please also add \"#SBATCH -p gpu\" to the header of this script. <pre><code>    #!/bin/bash\n    #SBATCH --job-name=my_qchem_job\n    #SBATCH --account=[my_allocation_ID]\n    #SBATCH --nodes=1\n    #SBATCH --tasks-per-node=&lt;ntasks per node&gt; \n    #SBATCH --time=01:00:00\n    #SBATCH --gres=gpu:[number of gpu]\n    #SBATCH --mem=[requested memory]\n    #SBATCH --mail-type=BEGIN,END,FAIL\n    #SBATCH --mail-user=your_email@domain.name\n    #SBATCH --output=std-%j.out\n    #SBATCH --error=std-%j.err\n\n    # Load the Q-Chem environment\n    module load brianqc\n\n    if [ -e /dev/nvme0n1 ]; then\n     SCRATCH=$TMPDIR\n     echo \"This node has a local storage and will use $SCRATCH as the scratch path\"\n    else\n     SCRATCH=/scratch/$USER/$SLURM_JOB_ID\n     echo \"This node does not have a local storage drive and will use $SCRATCH as the scratch path\"\n    fi\n\n    mkdir -p $SCRATCH\n\n    export QCSCRATCH=$SCRATCH\n    export QCLOCALSCR=$SCRATCH\n\n    jobnm=qchem_test\n\n    if [ $SLURM_JOB_NUM_NODES -gt 1 ]; then\n     QCHEMOPT=\"-gpu -mpi -np $SLURM_NTASKS\"\n    else\n     QCHEMOPT=\"-gpu -nt $SLURM_NTASKS\"\n    fi\n\n    echo Running Q-Chem with this command: qchem $QCHEMOPT $jobnm.com $jobnm.out\n    qchem $QCHEMOPT $jobnm.com $jobnm.out\n\n    rm $SCRATCH/*\n    rmdir $SCRATCH\n</code></pre></p>"},{"location":"Documentation/Applications/starccm/","title":"Running STAR-CCM+ Software","text":"<p>Simcenter STAR-CCM+ is a multiphysics CFD software that enables CFD engineers to model the complexity and explore the possibilities of products operating under real-world conditions. For information about the software's features, see the STAR-CCM+ website.</p> <p>STAR-CCM+ is installed on Kestrel but it is not supported on Vermilion or Swift. The only available version is starccm/20.02.007.</p> <p>Important</p> <p>NREL does not have general use STAR-CCM+ licenses available. Users must have their own STAR-CCM+ license. For help with using your          license on NREL HPC, please contact HPC-Help.</p>"},{"location":"Documentation/Applications/starccm/#running-star-ccm-in-gui","title":"Running STAR-CCM+ in GUI","text":"<p>STAR-CCM+ can be run interactively on Kestrel using X windows by running the following commands in the terminal of an X window.</p> <pre><code>module load starccm\nstarccm+\n</code></pre>"},{"location":"Documentation/Applications/starccm/#running-star-ccm-in-batch-mode","title":"Running STAR-CCM+ in Batch Mode","text":"<p>To run STAR-CCM+ in batch mode, first, you need to build your simulation <code>&lt;your_simulation.sim&gt;</code> and put it in your project directory:</p> <pre><code>ls /projects/&lt;your_project&gt;/sim_dir\nyour_simulation.sim\n</code></pre> <p>Then you need to create a Slurm script <code>&lt;your_scriptfile&gt;</code> as shown below to submit the job:</p> Example Submission Script <pre><code>#!/bin/bash -l\n#SBATCH --time=2:00:00             # walltime limit of 2 hours\n#SBATCH --nodes=2                  # number of nodes\n#SBATCH --ntasks-per-node=96       # number of tasks per node (&lt;=104 on Kestrel)\n#SBATCH --ntasks=192                # total number of tasks\n#SBATCH --job-name=your_simulation # name of job\n#SBATCH --account=&lt;allocation-id&gt;  # name of project allocation\n\nmodule load starccm                # load starccm module\n\nrm -rf /projects/&lt;your_project&gt;/sim_dir/simulation.log   # remove the log file from last run\n# Run Job\n\necho \"------ Running Starccm+ ------\"\n\nstarccm+ -np $SLURM_NTASKS -batch /projects/&lt;your_project&gt;/sim_dir/your_simulation.sim &gt;&gt; simulation.log\n\necho \"------ End of the job ------\"\n</code></pre> <p>Note that you must give the full path of your input file in the script.</p> <p>By default, STAR-CCM+ uses OpenMPI. However, the performance of OpenMPI on Kestrel is poor when running on multiple nodes. Intel MPI and Cray MPI are recommended for STAR-CCM+ on Kestrel. Cray MPI is expected to have a better performance than Intel MPI. </p>"},{"location":"Documentation/Applications/starccm/#running-star-ccm-with-intel-mpi","title":"Running STAR-CCM+ with Intel MPI","text":"<p>STAR-CCM+ comes with its own Open MPI and Intel MPI. To use the Intel MPI, the Slurm script should be modified to be:</p> Example Intel MPI Submission Script <pre><code>#!/bin/bash -l\n#SBATCH --time=2:00:00             # walltime limit of 2 hours\n#SBATCH --nodes=2                  # number of nodes\n#SBATCH --ntasks-per-node=96       # number of tasks per node (&lt;=104 on Kestrel)\n#SBATCH --ntasks=192                # total number of tasks\n#SBATCH --job-name=your_simulation # name of job\n#SBATCH --account=&lt;allocation-id&gt;  # name of project allocation\n\nmodule load starccm                # load starccm module\n\nexport UCX_TLS=tcp                 # telling IntelMPI to treat the network as ethernet (Kestrel Slingshot can be thought of as ethernet) \n                                   # by using the tcp protocol\n\nrm -rf /projects/&lt;your_project&gt;/sim_dir/simulation.log   # remove the log file from last run\n# Run Job\n\necho \"------ Running Starccm+ ------\"\n\nstarccm+ -mpi intel -np $SLURM_NTASKS -batch /projects/&lt;your_project&gt;/sim_dir/your_simulation.sim &gt;&gt; simulation.log\n\necho \"------ End of the job ------\"\n</code></pre> <p>We are specifying the MPI to be Intel MPI in the launch command. By default, Intel MPI thinks the network on which it is running is Infiniband. Kestrel\u2019s is Slingshot, which you can think of as ethernet on steroids. The command <code>export UCX_TLS=tcp</code> is telling Intel MPI to treat the network as ethernet by using the tcp protocol.</p> <p>To modify the settings for built-in Intel MPI, users can refer to the documentation of STAR-CCM by running <code>starccm+ --help</code>.</p>"},{"location":"Documentation/Applications/starccm/#running-star-ccm-with-cray-mpi","title":"Running STAR-CCM+ with Cray MPI","text":"<p>STAR-CCM+ does not come with its own Cray MPI, but it can run using the one installed on Kestrel. In the current STAR-CCM+ version, there is a bug that it clears all loaded modules if Crayex is specified. To overcome this, we devised a solution by reloading the required modules in the wrapper. However, this will break the Open MPI and Intel MPI. In this case, we installed two starccm versions: one for Open MPI and Intel MPI (default starccm module), and the other one for Cray MPI (starccm/20.02.007_crayex). The following Slurm script submits a STAR-CCM+ job to run with Cray MPI. The craympich module is not loaded in the slurm script as it has been loaded from the wrapper.</p> Example Cray MPI Script <pre><code>#!/bin/bash -l\n#SBATCH --time=2:00:00             # walltime limit of 2 hours\n#SBATCH --nodes=2                  # number of nodes\n#SBATCH --ntasks-per-node=96       # number of tasks per node (&lt;=104 on Kestrel)\n#SBATCH --ntasks=192                # total number of tasks\n#SBATCH --job-name=your_simulation # name of job\n#SBATCH --account=&lt;allocation-id&gt;  # name of project allocation\n\nmodule load starccm/20.02.007_crayex                # load starccm module\n\nrm -rf /projects/&lt;your_project&gt;/sim_dir/simulation.log   # remove the log file from last run\n# Run Job\n\necho \"------ Running Starccm+ ------\"\n\nstarccm+ -mpi crayex -np $SLURM_NTASKS -batch /projects/&lt;your_project&gt;/sim_dir/your_simulation.sim &gt;&gt; simulation.log\n\necho \"------ End of the job ------\"\n</code></pre>"},{"location":"Documentation/Applications/vasp/","title":"VASP","text":"<p>The Vienna Ab initio Simulation Package (VASP) is an application for atomic scale materials modelling from first principles. VASP computes an approximate solution to the many-body Schr\u00f6dinger equation, either within density functional theory or within the Hartree-Fock approximation using pseudopotentials and plane wave basis sets. VASP can carry out a range of electronic structure and quantum-mechanical molecular dynamics calculations and has many features including hybrid functionals, Green's functions methods (GW quasiparticles, and ACFDT-RPA) and many-body perturbation theory (2nd-order M\u00f8ller-Plesset). For a full list of capabilities, please see the About VASP page and for further details, documentation, forums, and FAQs, visit the VASP website.</p>"},{"location":"Documentation/Applications/vasp/#accessing-vasp-on-nrels-hpc-clusters","title":"Accessing VASP on NREL's HPC Clusters","text":"<p>Important</p> <p>The VASP license requires users to be a member of a \"workgroup\" defined by the University of Vienna or Materials Design. If you are receiving \"Permission denied\" errors when trying to use VASP, you must be made part of the \"vasp\" Linux group first. To join, please contact HPC Help with the following information:</p> <p><pre><code>- Your name\n- The workgroup PI\n- Whether you are licensed through Vienna (academic) or Materials Design, Inc. (commercial)\n- If licensed through Vienna:\n    - The e-mail address under which you are registered with Vienna as a workgroup member (this may not be the e-mail address you used to get an HPC account)\n    - Your VASP license ID\n- If licensed through Materials Design:\n    - Proof of current licensed status\n</code></pre> Once status can be confirmed, we can provide access to our VASP builds. </p>"},{"location":"Documentation/Applications/vasp/#getting-started","title":"Getting Started","text":"<p>VASP is available through modules on all HPC systems. To view the available versions of VASP modules on each cluster, use the command <code>module avail vasp</code>. To see details for a specific version, use <code>module show vasp/&lt;version&gt;</code>. To load a specific version, use <code>module load vasp/&lt;version&gt;</code>. If no version is specified, the default module (marked with \"(D)\") will be loaded.  In the following sections, we will give sample submission scripts and performance recommendations. To run VASP, the following 4 input files are needed: POSCAR, POTCAR, INCAR, KPOINTS. For more information about VASP input files, see the VASP wiki.</p> <p>Each VASP module provides three executables where the correct one should be chosen for the type of job:</p> <ol> <li> <p><code>vasp_std</code> is for general k-point meshes with collinear spins</p> </li> <li> <p><code>vasp_ncl</code> is for general k-point meshes with non-collinear spins</p> </li> <li> <p><code>vasp_gam</code> is for Gamma-point-only calculations</p> </li> </ol> <p>NREL also offers build and module support for additional functionalities such as transition state theory tools from University of Texas-Austin, implicit solvation models from the University of Florida, and BEEF-vdw functionals. Please contact HPC-Help if a functionality you need is not present in one of our builds.</p> <p>Attention</p> <p>If you would like to build your own VASP on Kestrel, please read our section Building VASP on Kestrel carefully before compiling on Kestrel's cray architecture. </p>"},{"location":"Documentation/Applications/vasp/#supported-versions","title":"Supported Versions","text":"<p>NREL offers modules for VASP 5 and VASP 6 on CPUs as well as GPUs on certain systems. See table below for current availability, as well as system specific documentation for more details on running different builds.</p> Kestrel Swift Vermilion VASP 5 X X VASP 6 X X X VASP 6 GPU X X X"},{"location":"Documentation/Applications/vasp/#vasp-on-kestrel","title":"VASP on Kestrel","text":""},{"location":"Documentation/Applications/vasp/#running-using-modules","title":"Running Using Modules","text":""},{"location":"Documentation/Applications/vasp/#cpu","title":"CPU","text":"<p>There are several modules for CPU builds of VASP 5 and VASP 6.</p> <pre><code>CPU $ module avail vasp\n------------- /nopt/nrel/apps/cpu_stack/modules/default/application -------------\n   vasp/5.4.4+tpc    vasp/6.3.2_openMP+tpc    vasp/6.4.2_openMP+tpc\n   vasp/5.4.4        vasp/6.3.2_openMP        vasp/6.4.2_openMP     (D)\n</code></pre> <p>Notes:</p> <ul> <li>These modules have been built with the latest Cray Programming Environment (cpe23), updated compilers, and math libraries.</li> <li>OpenMP capability has been added to VASP 6 builds.</li> <li>Modules that include third-party codes (e.g., libXC, libBEEF, VTST tools, and VASPsol) are now denoted with +tpc. Use <code>module show vasp/&lt;version&gt;</code> to see details of a specific version.</li> </ul> <p>We encourage users to switch to the new builds and strongly recommend using OpenMP parallelism.</p> <p>Important: Conserving your AUs on Kestrel</p> <p>Kestrel nodes have nearly 3x as many cores as Eagle's did. Our testing has indicated VASP DFT jobs up to 200 atoms run more efficiently on a fraction of a node (see performance notes below). We therefore highly recommend that VASP DFT users check the efficiency of their calculations and consider using the shared partition to get the most out of their allocations. Please see the sample shared job script provided below and the Shared partition documentation.</p> Sample job script: Kestrel - Full node w/ OpenMP <p>Note: (--ntasks-per-node) x (--cpus-per-task) = total number of physical cores you want to use per node. Here 4x26=104, all cores/node.</p> <pre><code>#!/bin/bash\n#SBATCH --nodes=2\n#SBATCH --tasks-per-node=26 # set number of MPI ranks per node\n#SBATCH --cpus-per-task=4 # set number of OpenMP threads per MPI rank\n#SBATCH --time=2:00:00\n#SBATCH --account=&lt;your-account-name&gt;\n#SBATCH --job-name=&lt;your-job-name&gt;\n\nmodule load vasp/&lt;version with openMP&gt;\n\nsrun vasp_std &amp;&gt; out\n</code></pre> Performance Note <p>The use of OpenMP threads is highly recommended on a system with as many cores per node as Kestrel. Testing of benchmark 2 has shown that OpenMP threads can both increase performance (faster time to solution) as well as scaling:</p> <p></p> Sample job script: Kestrel - Full node <pre><code>#!/bin/bash\n#SBATCH --nodes=2\n#SBATCH --tasks-per-node=104\n#SBATCH --cpus-per-task=1\n#SBATCH --time=2:00:00\n#SBATCH --account=&lt;your-account-name&gt;\n#SBATCH --job-name=&lt;your-job-name&gt;\n\nmodule load vasp/&lt;version&gt;\n\nsrun vasp_std &amp;&gt; out\n</code></pre> Sample job script: Kestrel - Shared (partial) node <p>As described in detail in the Shared partition documentation, when you run on part of a node, you will be charged for the greater of either the fraction of cores (104 total) or of memory (about 240G total or 2.3G/core) requested. The script below shows how to request 1/4 of a node, but you can freely set <code>--tasks</code> and <code>--mem-per-cpu</code> as you see fit.</p> <pre><code>#!/bin/bash\n#SBATCH --nodes=1\n#SBATCH --partition=shared\n#SBATCH --tasks=26 #How many cpus you want\n#SBATCH --mem-per-cpu=2G #Default is 1 GB/core but this is likely too little for electronic structure calculations\n#SBATCH --time=2:00:00\n#SBATCH --account=&lt;your-account-name&gt;\n#SBATCH --job-name=&lt;your-job-name&gt;\n\nmodule load vasp/&lt;version&gt;\n\nsrun vasp_std &amp;&gt; out\n</code></pre> Performance Note <p>Internal testing at NREL has indicated that standard VASP DFT calculations from sizes 50-200 atoms run most efficiently on a quarter to a half node. The graph below shows the performance of a 192-atom VASP DFT job using partial nodes on the shared partition. Up to 1/2 a node, near perfect scaling is observed, but using the full node gives a speedup of only 1.5 relative to using 1/2 a node. So, the calculation will cost 50% more AUs if run on a single node compared to a half node. For a 48-atom surface Pt calculation, using the full node gives no speedup relative to using 1/2 a node, so the calculation will cost 100% more AUs if run on a single node compared to half a node. </p> <p></p>"},{"location":"Documentation/Applications/vasp/#gpu","title":"GPU","text":"<p>Important</p> <p>Submit GPU jobs from a GPU login node. <code>$ ssh &lt;username&gt;@kestrel-gpu.hpc.nrel.gov</code></p> <p>There are several modules for GPU builds of VASP 5 and VASP 6: </p> <pre><code>GPU $ module avail vasp\n\n------------ /nopt/nrel/apps/gpu_stack/modules/default/application -------------\n   vasp/6.3.2_openMP    vasp/6.3.2    vasp/6.4.2_openMP    vasp/6.4.2 (D)\n</code></pre> Sample job script: Kestrel - Full GPU node <pre><code>#!/bin/bash\n#SBATCH --account=&lt;your-account-name&gt; \n#SBATCH --nodes=1\n#SBATCH --gpus=4 \n#SBATCH --ntasks-per-node=4\n#SBATCH --cpus-per-task=1 #The GPU partition is shared: You must specify CPUs needed even when requesting all the GPU resources\n#SBATCH --time=02:00:00\n#SBATCH --job-name=&lt;your-job-name&gt;\n#SBATCH --mem=350G # The GPU partition is shared: you must specify memory needed even when requesting all the GPU resources\n\nexport MPICH_GPU_SUPPORT_ENABLED=1\n\nmodule load vasp/&lt;version&gt;\n\nsrun vasp_std &amp;&gt; out\n</code></pre> <p>GPU nodes can be shared so you may request fewer than all 4 GPUs on a node. When doing so, you must also request appropriate CPU cores and memory. To run VASP on N GPUs, we recommend requesting <code>--gpus=N</code>, <code>--ntasks-per-node=N</code>, and <code>--mem=N*85G</code>. See the below sample script for running on 2 GPUs. </p> Sample job script: Kestrel - Partial GPU node <pre><code>#!/bin/bash\n#SBATCH --account=&lt;your-account-name&gt; \n#SBATCH --nodes=1\n#SBATCH --gpus=2 \n#SBATCH --ntasks-per-node=2\n#SBATCH --mem=170G # request cpu memory \n#SBATCH --cpus-per-task=1\n#SBATCH --time=02:00:00\n#SBATCH --job-name=&lt;your-job-name&gt;\n\nexport MPICH_GPU_SUPPORT_ENABLED=1\n\nmodule load vasp/&lt;version&gt;\n\nsrun vasp_std &amp;&gt; out\n</code></pre>"},{"location":"Documentation/Applications/vasp/#building-vasp-on-kestrel","title":"Building VASP on Kestrel","text":"<p>Sample makefiles for vasp5 (cpu version) and vasp6 (cpu and gpu versions) on Kestrel can be found in our Kestrel Repo under the vasp folder.</p> <p>Important</p> <p>On Kestrel, any modules you have loaded on the login node will be copied to a compute node, and there are many loaded by default for the cray programming environment. Make sure you are using what you intend to. Please see the Kestrel Environments page for more details on programming environments.</p>"},{"location":"Documentation/Applications/vasp/#cpu_1","title":"CPU","text":""},{"location":"Documentation/Applications/vasp/#compiling-your-build","title":"Compiling your build","text":"Build recommendations for VASP - CPU  <p>We recommend building vasp with a full intel toolchain and launching with the cray-mpich-abi at runtime. Additionally, you should build on a compute node so that you have the same architecture as at runtime:</p> <p><pre><code>salloc -N 1 -t &lt;time&gt; -A &lt;account&gt;\n</code></pre> Then, load appropriate modules for your mpi, compilers, and math packages: <pre><code>module purge\nmodule load craype-x86-spr  #specifies sapphire rapids architecture\nmodule load intel-oneapi-compilers\nmodule load intel-oneapi-mpi\nmodule load intel-oneapi-mkl\n</code></pre></p> <p>Sample makefiles for vasp5 and vasp6 on Kestrel can be found in our Kestrel Repo under the vasp folder.</p>"},{"location":"Documentation/Applications/vasp/#running-your-build","title":"Running your build","text":"<p>Important</p> <p>We have found that it is optimal to run an Intel toolchain build of VASP using cray-mpich-abi at runtime. Cray-mpich-abi has several dependencies on cray network modules, so the easiest way to load it is to first load <code>PrgEnv-intel</code> and then swap the default cray-mpich module for the cray-mpich-abi <code>module swap cray-mpich cray-mpich-abi</code>. You must then load your intel compilers and math libraries, and unload cray's libsci. A sample script showing all of this is in the dropdown below.</p> Sample job script: How to run your own build - CPU  <pre><code>#!/bin/bash\n#SBATCH --nodes=2\n#SBATCH --tasks-per-node=104\n#SBATCH --time=2:00:00\n#SBATCH --account=&lt;your-account-name&gt;\n#SBATCH --job-name=&lt;your-job-name&gt;\n\n# Load cray-mpich-abi and its dependencies within PrgEnv-intel, intel compilers, mkl, and unload cray's libsci\nmodule purge\nmodule load PrgEnv-intel\nmodule load craype-x86-spr\nmodule swap cray-mpich cray-mpich-abi\nmodule unload cray-libsci\nmodule load intel-oneapi-compilers\nmodule load intel-oneapi-mkl\n\nexport VASP_PATH=/PATH/TO/YOUR/vasp_exe\n\nsrun ${VASP_PATH}/vasp_std &amp;&gt; out\n</code></pre>"},{"location":"Documentation/Applications/vasp/#gpu_1","title":"GPU","text":"<p>Important</p> <p>Make sure to build GPU software on a GPU login node or GPU compute node.</p>"},{"location":"Documentation/Applications/vasp/#compiling-your-build_1","title":"Compiling your build","text":"Build recommendations for VASP - GPU <pre><code># Load appropriate modules for your build. For our example these are:\nml gcc-stdalone/13.1.0\nml PrgEnv-nvhpc/8.5.0\nml nvhpc/23.9   #do not use the default nvhpc/24.1\nml cray-libsci/23.05.1.4\n\nmake DEPS=1 -j8 all\n</code></pre>"},{"location":"Documentation/Applications/vasp/#running-your-build_1","title":"Running your build","text":"Sample job script: How to run your own build - GPU <p>See sample jobs scripts above for SBATCH and export directives to request full or shared gpu nodes.</p> <pre><code># Load modules appropriate for your build. For ours these are:\nml gcc-stdalone/13.1.0\nml PrgEnv-nvhpc/8.5.0\nml nvhpc/23.9   #do not use the default nvhpc/24.1\nml cray-libsci/23.05.1.4\n\n# Export path to your buid\nexport VASP_PATH=/PATH/TO/YOUR/BUILD/bin\n\nsrun ${VASP_PATH}/vasp_std &amp;&gt; out\n</code></pre>"},{"location":"Documentation/Applications/vasp/#vasp-on-swift","title":"VASP on Swift","text":""},{"location":"Documentation/Applications/vasp/#cpu_2","title":"CPU","text":"Sample job script: Swift - VASP 6 CPU (Intel MPI) <pre><code>#!/bin/bash\n#SBATCH --job-name=\"benchmark\"\n#SBATCH --account=myaccount\n#SBATCH --time=4:00:00\n#SBATCH --ntasks-per-node=64\n#SBATCH --nodes=1\n\n#Set --exclusive if you would like to prevent any other jobs from running on the same nodes (including your own)\n#You will be charged for the full node regardless of the fraction of CPUs/node used\n#SBATCH --exclusive\n\nmodule purge\n\n#Load Intel MPI VASP build and necessary modules\nml vaspintel \nml slurm/21-08-1-1-o2xw5ti \nml gcc/9.4.0-v7mri5d \nml intel-oneapi-compilers/2021.3.0-piz2usr \nml intel-oneapi-mpi/2021.3.0-hcp2lkf \nml intel-oneapi-mkl/2021.3.0-giz47h4\n\nsrun -n 64 vasp_std &amp;&gt; out\n</code></pre> Sample job script: Swift - VASP 6 CPU (Open MPI) <pre><code>#!/bin/bash\n#SBATCH --job-name=\"benchmark\"\n#SBATCH --account=myaccount\n#SBATCH --time=4:00:00\n#SBATCH --ntasks-per-node=64\n#SBATCH --nodes=1\n\n#Set --exclusive if you would like to prevent any other jobs from running on the same nodes (including your own)\n#You will be charged for the full node regardless of the fraction of CPUs/node used\n#SBATCH --exclusive\n\nmodule purge\n\n#Load OpenMPI VASP build and necessary modules\nml vasp \nml slurm/21-08-1-1-o2xw5ti \nml openmpi/4.1.1-6vr2flz\n\nsrun -n 64 vasp_std &amp;&gt; out\n</code></pre> Sample job script: Swift - run multiple jobs on the same node(s) <p>The following script launches two instances of <code>srun vasp_std</code> on the same node using an array job. Each job will be constricted to 32 cores on the node.  <pre><code>#!/bin/bash\n#SBATCH --job-name=\"benchmark\"\n#SBATCH --account=myaccount\n#SBATCH --time=4:00:00\n#SBATCH --ntasks-per-node=32\n#SBATCH --nodes=1\n\n#Set --exclusive=user if you would like to prevent anyone else from running on the same nodes as you\n#You will be charged for the full node regardless of the fraction of CPUs/node used\n#SBATCH --exclusive=user\n\n#Set how many jobs you would like to run at the same time as an array job\n#In this example, an array of 2 jobs will be run at the same time. This script will be run once for each job.\n#SBATCH --array=1-2\n\n#The SLURM_ARRAY_TASK_ID variable can be used to modify the parameters of the distinct jobs in the array.\n#In the case of array=1-2, the first job will have SLURM_ARRAY_TASK_ID=1, and the second will have SLURM_ARRAY_TASK_ID=2.\n#For example, you could assign different input files to runs 1 and 2 by storing them in directories input_1 and input_2 and using the following code:\n\nmkdir run_${SLURM_ARRAY_TASK_ID}\ncd run_${SLURM_ARRAY_TASK_ID}\ncp ../input_${SLURM_ARRAY_TASK_ID}/POSCAR .\ncp ../input_${SLURM_ARRAY_TASK_ID}/POTCAR .\ncp ../input_${SLURM_ARRAY_TASK_ID}/INCAR .\ncp ../input_${SLURM_ARRAY_TASK_ID}/KPOINTS .\n\n#Now load vasp and run the job...\n\nmodule purge\n\n#Load Intel MPI VASP build and necessary modules\nml vaspintel \nml slurm/21-08-1-1-o2xw5ti \nml gcc/9.4.0-v7mri5d \nml intel-oneapi-compilers/2021.3.0-piz2usr \nml intel-oneapi-mpi/2021.3.0-hcp2lkf \nml intel-oneapi-mkl/2021.3.0-giz47h4\n\nsrun -n 32 vasp_std &amp;&gt; out\n</code></pre></p> Sample job script: Swift - run a single job on a node shared with other users <p>The following script launches <code>srun vasp_std</code> on only 32 cores on a single node. The other 32 cores remain open for other users to use. You will only be charged for half of the node hours. </p> <pre><code>#!/bin/bash\n#SBATCH --job-name=\"benchmark\"\n#SBATCH --account=myaccount\n#SBATCH --time=4:00:00\n#SBATCH --ntasks-per-node=32\n#SBATCH --nodes=1\n\n#To make sure that you are only being charged for the CPUs your job is using, set mem=2GB*CPUs/node\n#--mem sets the memory used per node\n#SBATCH --mem=64G\n\nmodule purge\n\n#Load Intel MPI VASP build and necessary modules\nml vaspintel \nml slurm/21-08-1-1-o2xw5ti \nml gcc/9.4.0-v7mri5d \nml intel-oneapi-compilers/2021.3.0-piz2usr \nml intel-oneapi-mpi/2021.3.0-hcp2lkf \nml intel-oneapi-mkl/2021.3.0-giz47h4\n\nsrun -n 32 vasp_std &amp;&gt; out\n</code></pre> Performance Notes <p>The Intel MPI builds are recommended over the Open MPI builds as they exhibit fastest performance.</p> <p>Use at most 64 cores/node. On Swift, each node has 64 physical cores, and each core is subdivided into two virtual cores in a process that is identical to hyperthreading. Because of this, up to 128 cores can be requested from a single Swift node, but each core will only represent half of a physical core. </p> <p>On Swift, VASP is most efficiently run on partially full nodes. </p> <p>Multiple jobs can run on the same nodes on Swift. If you are only using a fraction of a node, other users' jobs could be assigned to the rest of the node, which might deteriorate the performance. Setting \"#SBATCH --exclusive\" in your run script prevents other users from using the same node as you, but you will be charged the full 5AUs/node, regardless of the number of CPUs/node you are using. </p>"},{"location":"Documentation/Applications/vasp/#gpu_2","title":"GPU","text":"Sample job script: Swift - VASP 6 GPU (OpenACC) <pre><code>#!/bin/bash\n#SBATCH --nodes=1\n#SBATCH --partition=gpu\n#SBATCH --gres=gpu:4\n#SBATCH --gpu-bind=map_gpu:0,1,2,3\n#SBATCH --exclusive\n#SBATCH --time=1:00:00\n#SBATCH --account=&lt;your-account-name&gt;\n#SBATCH --job-name=&lt;your-job-name&gt;\n\n#Load environment and openACC VASP module:\nmodule purge\n. /nopt/nrel/apps/env.sh\nmodule use /nopt/nrel/apps/modules\nmodule load vasp/openacc\n\n# Note: environment will soon become default and the module will be able to be loaded with\n# module purge\n# module load vasp/openacc\n\n#Launch vasp using mpirun\nmpirun -npernode 4 vasp_std &amp;&gt; out\n</code></pre>"},{"location":"Documentation/Applications/vasp/#vasp-on-vermilion","title":"VASP on Vermilion","text":""},{"location":"Documentation/Applications/vasp/#cpu_3","title":"CPU","text":"Sample job script: Vermilion - VASP 6 CPU (Intel MPI) <pre><code>#!/bin/bash\n#SBATCH --job-name=vasp\n#SBATCH --nodes=1\n#SBATCH --time=8:00:00\n#SBATCH --error=std.err\n#SBATCH --output=std.out\n#SBATCH --partition=lg\n#SBATCH --exclusive\n#SBATCH --account=myaccount\n\nmodule purge\nml vasp/6.3.1\n\nsource /nopt/nrel/apps/220525b/myenv.2110041605\nml intel-oneapi-compilers/2022.1.0-k4dysra\nml intel-oneapi-mkl/2022.1.0-akthm3n\nml intel-oneapi-mpi/2021.6.0-ghyk7n2\n\n# some extra lines that have been shown to improve VASP reliability on Vermilion\nulimit -s unlimited\nexport UCX_TLS=tcp,self\nexport OMP_NUM_THREADS=1\nml ucx\n\nsrun --mpi=pmi2 -n 60 vasp_std\n\n# If the multi-node calculations are breaking, replace the srun line with this line\n# I_MPI_OFI_PROVIDER=tcp mpirun -iface ens7 -np 60 vasp_std\n</code></pre> Sample job script: Vermilion - VASP 6 CPU (Open MPI) <pre><code>#!/bin/bash\n#SBATCH --job-name=vasp\n#SBATCH --nodes=1\n#SBATCH --time=8:00:00\n#SBATCH --error=std.err\n#SBATCH --output=std.out\n#SBATCH --partition=lg\n#SBATCH --exclusive\n#SBATCH --account=myaccount\n\nmodule purge\nml gcc\nml vasp/6.1.1-openmpi\n\n# some extra lines that have been shown to improve VASP reliability on Vermilion\nulimit -s unlimited\nexport UCX_TLS=tcp,self\nexport OMP_NUM_THREADS=1\nml ucx\n\n# lines to set \"ens7\" as the interconnect network\nmodule use /nopt/nrel/apps/220525b/level01/modules/lmod/linux-rocky8-x86_64/gcc/12.1.0\nmodule load openmpi\nOMPI_MCA_param=\"btl_tcp_if_include ens7\"\n\nsrun --mpi=pmi2 -n 60 vasp_std\n</code></pre> Sample job script: Vermilion - VASP 5 CPU (Intel MPI) <pre><code>#!/bin/bash\n#SBATCH --job-name=vasp\n#SBATCH --nodes=1\n#SBATCH --time=8:00:00\n##SBATCH --error=std.err\n##SBATCH --output=std.out\n#SBATCH --partition=lg\n#SBATCH --exclusive\n#SBATCH --account=myaccount\n\nmodule purge\n\nml vasp/5.4.4\n\nsource /nopt/nrel/apps/220525b/myenv.2110041605\nml intel-oneapi-compilers/2022.1.0-k4dysra\nml intel-oneapi-mkl/2022.1.0-akthm3n\nml intel-oneapi-mpi/2021.6.0-ghyk7n2\n\n# some extra lines that have been shown to improve VASP reliability on Vermilion\nulimit -s unlimited\nexport UCX_TLS=tcp,self\nexport OMP_NUM_THREADS=1\nml ucx\n\nsrun --mpi=pmi2 -n 60 vasp_std\n\n# If the multi-node calculations are breaking, replace the srun line with this line\n# I_MPI_OFI_PROVIDER=tcp mpirun -iface ens7 -np 60 vasp_std\n</code></pre> Performance Notes <p>On Vermilion, VASP runs more performantly on a single node. Many issues have been reported for running VASP on multiple nodes, especially when requesting all available cores on each node. In order for MPI to work reliably on Vermilion, it is necessary to specify the interconnect network that Vermilion should use to communicate between nodes. If many cores are needed for your VASP calculation, it is recommended to run VASP on a singe node in the lg partition (60 cores/node), which provides the largest numbers of cores per node and use the following settings that have been shown to work well for multi-node jobs on 2 nodes. The Open MPI multi-node jobs are more reliable on Vermilion, but Intel MPI VASP jobs show better runtime performance as usual.</p> <p>If your multi-node Intel MPI VASP job is crashing on Vermilion, try replacing your srun line with the following mpirun run line. <code>-iface ens7</code> sets ens7 as the interconnect.  <pre><code>I_MPI_OFI_PROVIDER=tcp mpirun -iface ens7 -np 16 vasp_std\n</code></pre></p> <p>If your multi-node Open MPI VASP job is crashing on Vermilion, replace a call to load an openmpi module with the following lines. The OMPI_MCA_param variable sets ens7 as the interconnect. </p> <pre><code>module use /nopt/nrel/apps/220525b/level01/modules/lmod/linux-rocky8-x86_64/gcc/12.1.0\nmodule load openmpi\nOMPI_MCA_param=\"btl_tcp_if_include ens7\"\n</code></pre>"},{"location":"Documentation/Applications/vasp/#gpu_3","title":"GPU","text":"Sample job script: Vermilion - VASP 6 CPU (OpenACC) <pre><code>#!/bin/bash\n#SBATCH --job-name=vasp\n#SBATCH --nodes=2\n#SBATCH --time=1:00:00\n##SBATCH --error=std.err\n##SBATCH --output=std.out\n#SBATCH --partition=gpu\n#SBATCH --gpu-bind=map_gpu:0,1,0,1\n#SBATCH --exclusive\n#SBATCH --account=myaccount\n\n# Load the OpenACC build of VASP\nml vasp/6.3.1-nvhpc_acc\n\n# Load some additional modules\nmodule use  /nopt/nrel/apps/220421a/modules/lmod/linux-rocky8-x86_64/gcc/11.3.0/\nml nvhpc\nml fftw\n\nmpirun -npernode 1 vasp_std &gt; vasp.$SLURM_JOB_ID\n</code></pre> Performance Notes <p>The OpenACC build shows significant performance improvement compared to the Cuda build, but is more susceptible to running out of memory. The OpenACC GPU-port of VASP was released with VASP 6.2.0, and the Cuda GPU-port of VASP was dropped in VASP 6.3.0.</p>"},{"location":"Documentation/Applications/wrf/","title":"How to Use the WRF Application Software","text":"<p>Documentation: Weather Research Framework (WRF) Model</p> <p>The WRF model is a state of the art mesoscale numerical weather prediction system designed for both atmospheric research and operational forecasting applications.</p>"},{"location":"Documentation/Applications/wrf/#getting-started","title":"Getting Started","text":"<p>This section provides the minimum amount of information necessary to successfully run a WRF job on the NREL Kestrel cluster. First, we show how to use WRF given that we may have different versions of WRF in different toolchains already built and available as modules.</p> <pre><code>% module avail wrf\n     wrf/4.2.2-cray (D)    \n     wrf/4.2.2-intel\n     wrf/4.6.1-cray-mpich-gcc\n</code></pre> <p>The command <code>module avail wrf</code> displays the available WRF modules for various WRF versions and toolchains. The WRF version 4.2.2 uses the Cray and Intel toolchains, while version 4.6.1 uses the GNU toolchain. We suggest using the newest module because of its bug fixes, large domain handling capacity, enhanced parallel IO, and faster solution times. Since WRF doesn't currently support GPUs, no modules are available for running it on GPUs.</p> <p>The following job script demonstrates the use of the latest WRF module. This job needs 8 nodes, each running 96 tasks, for a total of 3072 tasks. When running your job, only modify the node count, total core count, job name, runtime, partition, and account in the example. For optimal performance, configure the NIC policy to NUMA, use a single OMP thread, 96 tasks per node, employ block-block distribution, and bind tasks by rank to CPUs within NUMA nodes.</p>"},{"location":"Documentation/Applications/wrf/#sample-job-script","title":"Sample Job Script","text":"Kestrel-CPU <pre><code>#!/bin/bash\n\n#SBATCH --job-name=&lt;\"job-name\"&gt;\n#SBATCH --nodes=8\n#SBATCH --ntasks-per-node=96\n#SBATCH --time=&lt;hour:minute:second&gt;\n#SBATCH --partition=&lt;partition-name&gt;\n#SBATCH --account=&lt;account-name&gt;\n#SBATCH --exclusive\n#SBATCH --mem=0\n\nmodule load PrgEnv-gnu/8.5.0\nmodule load cray-mpich/8.1.28\nmodule load cray-libsci/23.12.5\nmodule load wrf/4.6.1-cray-mpich-gcc\nmodule list\n\nexport MPICH_OFI_NIC_POLICY=NUMA\nexport OMP_NUM_THREADS=1\n\nsrun -N 8 -n 3072 --ntasks-per-node=96 --distribution=block:block --cpu_bind=rank_ldom wrf.exe\n</code></pre> <p>To submit this job script, named <code>submit_wrf.sh</code>, do <code>sbatch ./submit_wrf.sh</code></p>"},{"location":"Documentation/Applications/wrf/#supported-version","title":"Supported Version","text":"Kestrel 4.6.1"},{"location":"Documentation/Applications/wrf/#advanced","title":"Advanced","text":""},{"location":"Documentation/Applications/wrf/#build-instructions-from-source","title":"Build Instructions from Source","text":"<p>All WRF versions are available for download at this link. To build WRF, load the <code>netcdf</code> module; this automatically loads <code>hdf5</code>, <code>pnetcdf</code>, and other necessary dependencies. After completing the WRF build, download and build the WRF Pre-processing System (WPS) version from here. Building WPS requires loading the <code>jasper</code> module, which will automatically load <code>libpng</code>. The instructions below will guide you through installing your chosen WRF and WPS versions.</p> Building on Kestrel with the GNU Toolchain <pre><code># Get a compute node\n$ salloc --time=02:00:00 --account= &lt;project account&gt; --partition=shared --nodes=1 --ntasks-per-node=52\n\n# Load the netcdf and jasper modules\n$ module load PrgEnv-gnu/8.5.0\n$ module load cray-mpich/8.1.28\n$ module load cray-libsci/23.12.5\n$ module load netcdf/4.9.3-cray-mpich-gcc\n$ module load jasper/1.900.1-cray-mpich-gcc\n\n# Set the runtime environment\n$ export PATH=\"/usr/bin:${PATH}\"\n$ export LD_LIBRARY_PATH=\"/usr/lib64:${LD_LIBRARY_PATH}\"\n\n# Set paths to the WRF and WPS directories\n$ export WRF_DIR=&lt;Path to WRF directory&gt;\n$ export WPS_DIR=&lt;Path to WPS directory&gt;\n\n# Configure WRF\n$ cd ${WRF_DIR}\n$ ./configure\n$ Enter selection [1-83] : 35\n$ Compile for nesting? (1=basic, 2=preset moves, 3=vortex following) [default 1]:1\n\n# Compile WRF\n$ ./compile -j 48 em_real\n\n# Configure WPS\n$ cd ${WRF_DIR}\n$ ./configure\n$ Enter selection [1-44] : 3\n\n# Append \u201c-fopenmp\u201d to the WRF_LIB line in the configuration.wps file\nWRF_LIB         = -L$(WRF_DIR)/external/io_grib1 -lio_grib1 \\\n                    -L$(WRF_DIR)/external/io_grib_share -lio_grib_share \\\n                    -L$(WRF_DIR)/external/io_int -lwrfio_int \\\n                    -L$(WRF_DIR)/external/io_netcdf -lwrfio_nf \\\n                    -L$(NETCDF)/lib -lnetcdff -lnetcdf -fopenmp\n    # Compile WPS\n$ ./compile\n</code></pre>"},{"location":"Documentation/Applications/wrf/#wrf-resources","title":"WRF Resources","text":"<p>The WRF community offers helpful resources, including tutorials and user support.</p>"},{"location":"Documentation/Applications/xpressmp/","title":"Xpress Solver","text":"<p>FICO Xpress Optimizer provides optimization algorithms and technologies to solve linear, mixed integer and non-linear problems</p> <p>For documentation, forums, and FAQs, see the FICO website.</p> <p>The Xpress solver includes algorithms that can solve</p> <ul> <li>Linear Programs</li> <li>Mixed Integer Programs</li> <li>Quadratic Programs</li> <li>Quadratically Constrained Quadratic Programs</li> <li>Second Order Cone Problems</li> </ul> <p>Xpress solver cannot be used to solve nonlinear programs. </p>"},{"location":"Documentation/Applications/xpressmp/#available-modules","title":"Available Modules","text":"Kestrel (CPU) xpressmp/9.0.2 xpressmp/9.2.2 xpressmp/9.2.5 <p>Info</p> <p>Xpress is available as a module on Kestrel. Additionally, NREL has a site-wide license for Xpress to run locally on an NREL-issued computer. Please see instructions here.</p>"},{"location":"Documentation/Applications/xpressmp/#running-xpress-solver-on-kestrel","title":"Running Xpress Solver on Kestrel","text":"<p>Important</p> <p>While Xpress Solver is available as a module on Kestrel for use by all NREL-users, you MUST be a part of the <code>xpressmp</code> group on Kestrel. If you are new or have not used Xpress in a while, you can:</p> <ol> <li>Check whether you are a part of this group by running the <code>groups</code> command from your terminal, or</li> <li>Load the <code>xpressmp</code> module and run an example</li> </ol> <p>If you are not a part of the <code>xpressmp</code> linux group and/or are unable to run an Xpress instance, please submit a ticket to HPC-Help@nrel.gov requesting access to Xpress on HPC systems and provide a business justification that describes how you intend to use Xpress in your workflow.</p> <p>Xpress solvers can be used by simply loading the module</p> <pre><code>module load xpressmp/9.2.5\n</code></pre> <p>Once the module is loaded, Xpress Solver can be used directly using the command line  by running the <code>optimizer</code> command.</p> <pre><code>$ optimizer\nFICO Xpress Solver 64bit v9.2.5 Nov  9 2023\n(c) Copyright Fair Isaac Corporation 1983-2023. All rights reserved\n Optimizer v42.01.04    [/nopt/nrel/apps/software/xpressmp/9.2.5/lib/libxprs.so.42.01.04]\n[xpress kpanda] \n</code></pre> <p>Alternatively, Xpress can now be used directly in Python or Julia by loading the necessary modules and programming environments.</p>"},{"location":"Documentation/Applications/Matlab/","title":"Using MATLAB Software","text":"<p>Learn how to use MATLAB software on the NREL HPC systems.</p>"},{"location":"Documentation/Applications/Matlab/#running-matlab-in-batch-mode","title":"Running MATLAB in Batch Mode","text":"<p>Details on how to run MATLAB scripts in batch mode. Steps are illustrated by a simple example.</p>"},{"location":"Documentation/Applications/Matlab/#running-matlab-interactively","title":"Running MATLAB Interactively","text":"<p>How to run interactively using either a terminal or FastX.</p>"},{"location":"Documentation/Applications/Matlab/#using-the-parallel-computing-toolbox","title":"Using the Parallel Computing Toolbox","text":"<p>Toolbox used to run parallel MATLAB code on a single, multi-core compute node. Use of the toolbox is demonstrated via a parallel \"hello world\" example and a Monte Carlo example that leverages MATLAB's parfor command.</p>"},{"location":"Documentation/Applications/Matlab/#understanding-versions-and-licenses","title":"Understanding Versions and Licenses","text":"<p>Learn about the MATLAB software versions and licenses available for use.</p>"},{"location":"Documentation/Applications/Matlab/#additional-resources","title":"Additional Resources","text":"<p>If you're an NREL user, on GitHub view MATLAB presentations and code examples.</p> <p>For all users, see a summary PowerPoint deck on the MATLAB Compiler, MATLAB Coder, and MATLAB Engine for Python.</p>"},{"location":"Documentation/Applications/Matlab/batch/","title":"Running MATLAB in Batch Mode","text":"<p>Learn how to run MATLAB software in batch mode</p> <p>Below is an example MATLAB script, matlabTest.m, that creates and populates a vector using a simple for-loop and writes the result to a binary file, x.dat. The shell script matlabTest.sb can be passed to the scheduler to run the job in batch (non-interactive) mode.</p> <p>To try the example out, create both matlabTest.sb and matlabTest.m files in an appropriate directory, <code>cd</code> to that directory, and call sbatch:</p> <pre><code>$ sbatch matlabTest.sb\n</code></pre> <p>Note</p> <p>Note: MATLAB comprises many independently licensed components, and in your work it might be necessary to wait for multiple components to become available. Currently, the scheduler does not handle this automatically. Because of this, we strongly recommend using compiled MATLAB code for batch processing.</p> <p>Calling <code>squeue</code> should show that your job is queued:</p> <pre><code>JOBID       PARTITION       NAME       USER       ST       TIME       NODES       NODELIST(REASON)\n&lt;JobID&gt;     &lt;partition&gt;     matlabTe   username   PD       0:00       1           (&lt;reason&gt;)\n</code></pre> <p>Once the job has finished, the standard output is saved in a file called <code>slurm-&lt;JobID&gt;.out</code>, standard error to <code>slurm-&lt;JobID&gt;.out</code>, and the binary file <code>x.dat</code> contains the result of the MATLAB script.</p>"},{"location":"Documentation/Applications/Matlab/batch/#notes-on-matlabtestsb-file","title":"Notes on matlabTest.sb File","text":"<ul> <li>Setting a low walltime increases the chances that the job will be scheduled   sooner due to backfill.</li> <li>The <code>--account=&lt;account_string&gt;</code> flag must include a valid account string or   the job will encounter a permanent hold (it will appear in the queue but will   never run).  For more information, see user   accounts.</li> <li>The environment variable <code>$SLURM_SUBMIT_DIR</code> is set by the scheduler to the   directory from which the sbatch command was executed, e.g., <code>/scratch/$USER.</code>   In this example, it is also the directory into which MATLAB will write the   output file x.dat.</li> </ul> <p>matlabTest.sb</p> <pre><code>#!/bin/bash\n#SBATCH --time=05:00                   # Maximum time requested for job (5 min.)\n#SBATCH --nodes=1                      # Number of nodes\n#SBATCH --job-name=matlabTest          # Name of job\n#SBATCH --account=&lt;your_account&gt;       # account associated with job\n\nmodule load matlab\n\n# execute code\ncd $SLURM_SUBMIT_DIR                   # Change directories (output will save here)\nmatlab -nodisplay -r matlabTest        # Run the MATLAB script\n</code></pre> <p>matlabTest.m</p> <pre><code>format long\nxmin = 2;\nxmax = 10;\nx = zeros(xmax-xmin+1,1);\nfor i = xmin:xmax\n    display(i);\n    x(i-xmin+1) = i\nend\nsavefile = 'x.dat';\nsave(savefile,'x','-ASCII')\nexit\n</code></pre>"},{"location":"Documentation/Applications/Matlab/interactive/","title":"Running MATLAB Software Interactively","text":"<p>Learn how to run MATLAB software interactively on NREL HPC systems.</p> <p>To run MATLAB interactively there are two ways to proceed: you can choose to start an interactive job and use a basic MATLAB terminal (no GUI), or you can use the GUI with a FastX session on a DAV node.  For information on how to connect to NREL HPC systems, see System Connection.</p>"},{"location":"Documentation/Applications/Matlab/interactive/#running-matlab-via-an-interactive-job","title":"Running MATLAB via an Interactive Job","text":"<p>After connecting to the login node, the next step is to start an interactive job. For example, the following command gets a user-selected number of nodes for interactive use, taking as input tasks per node, job duration, and account.</p> <pre><code>$ salloc --nodes=&lt;number of nodes&gt; --ntasks-per-node=&lt;tasks per node&gt; --account=&lt;your account here&gt; --time=&lt;desired time&gt;\n</code></pre> <p>When your job starts, you will have a shell on a compute node.</p> <p>Note</p> <ol> <li>To submit an interactive job you must include the <code>--account=&lt;handle&gt;</code> flag    and include a valid project allocation handle. For more information, see    User Accounts.</li> <li>For more information on interactive jobs, see Running Interactive    Jobs.</li> </ol> <p>From the shell on the compute node, the next steps are to load the MATLAB module to set up your user environment, which includes setting the location of the license server,</p> <pre><code>$ module load matlab\n</code></pre> <p>and starting a simple MATLAB terminal (no GUI),</p> <pre><code>$ matlab -nodisplay\n</code></pre>"},{"location":"Documentation/Applications/Matlab/interactive/#running-matlab-via-a-fastx-session-on-a-dav-node","title":"Running MATLAB via a FastX Session on a DAV Node","text":"<p>For instructions on starting a FastX session on a DAV node, see the FastX page. Once you have started a FastX session and have access to a terminal, load the MATLAB module to set up your user environment, which includes setting the location of the license server,</p> <pre><code>$ module load matlab\n</code></pre> <p>and start the MATLAB GUI,</p> <pre><code>$ matlab &amp;\n</code></pre> <p>With FastX, this will enable you to use the GUI as if MATLAB was running directly on your laptop. The ampersand \"&amp;\" lets MATLAB run as a background job so the terminal is freed up for other uses.</p>"},{"location":"Documentation/Applications/Matlab/parallel/","title":"Using the Parallel Computing Toolbox with MATLAB","text":"<p>Learn how to use the Parallel Computing Toolbox (PCT) with MATLAB software on the NREL HPC systems.</p> <p>Note</p> <p>Due to an issue with the scheduler and software licenses, we strongly recommend the use of compiled MATLAB code for batch processing. Using the PCT with MATLAB in batch mode may lead to failed jobs due to unavailability of licenses.</p> <p>PCT provides the simplest way for users to run parallel MATLAB code on a single, multi-core compute node. Here, we describe how to configure your local MATLAB settings to utilize the PCT and provide some basic examples of running parallel code on NREL HPC systems.</p> <p>For more extensive examples of PCT usage and code examples, see the MathWorks documentation.</p>"},{"location":"Documentation/Applications/Matlab/parallel/#configuration-in-matlab-r2023a","title":"Configuration in MATLAB R2023a","text":"<p>Configuration of the PCT is done most easily through the interactive GUI. However, the opening of parallel pools can be significantly slower in interactive mode than in non-interactive (batch) mode. For this reason, the interactive GUI will only be used to set up your local configuration. Runtime examples will include batch scripts that submit jobs directly to the scheduler.</p> <p>To configure your local parallel settings, start an interactive MATLAB session with X11 forwarding (see Running Interactive Jobs on Kestrel and Environment Modules on the Kestrel System). Open MATLAB R2023a and do the following:</p> <ol> <li>Under the Home tab, go to Parallel &gt; Parallel Preferences.</li> <li>In the Parallel Pool box, set the \"Preferred number of workers in a parallel    pool\" to at least 104 (the max number of cores currently available on a standard Kestrel    compute node).</li> <li>Click OK.</li> <li>Exit MATLAB.</li> </ol> <p>For various reasons, you might not have 104 workers available at runtime. In this case, MATLAB will just use the largest number available.</p> <p>Note</p> <p>Specifying the number of tasks for an interactive job (i.e., using <code>salloc --ntasks-per-node=&lt;n&gt;</code> to start your interactive job) will interfere with parallel computing toolbox. We recommend not specifying the number of tasks.</p>"},{"location":"Documentation/Applications/Matlab/parallel/#examples","title":"Examples","text":"<p>Here we demonstrate how to use the PCT on a single compute node on NREL HPC systems. Learn how to open a local parallel pool with some examples of how to use it for parallel computations. Because the opening of parallel pools can be extremely slow in interactive sessions, the examples here will be restricted to non-interactive (batch) job submission.</p> <p>Note</p> <p>Each example below will check out one \"MATLAB\" and one \"Distrib_Computing_Toolbox\" license at runtime.</p>"},{"location":"Documentation/Applications/Matlab/parallel/#hello-world-example","title":"Hello World Example","text":"<p>In this example, a parallel pool is opened and each worker identifies itself via <code>spmd</code> (\"single program multiple data\"). Create the MATLAB script helloWorld.m:</p> MATLAB Hello World script <pre><code>% open the local cluster profile\np = parcluster('Processes');\n\n% open the parallel pool, recording the time it takes\ntic;\nparpool(p); % open the pool\nfprintf('Opening the parallel pool took %g seconds.\\n', toc)\n\n% \"single program multiple data\"\nspmd\n  fprintf('Worker %d says Hello World!\\n', labindex)\nend\n\ndelete(gcp); % close the parallel pool\nexit\n</code></pre> <p>To run the script on a compute node, create the file helloWorld.sb:</p> Slurm batch script for Hello World <pre><code>#!/bin/bash\n#SBATCH --time=05:00\n#SBATCH --nodes=1\n#SBATCH --job-name=helloWorld\n#SBATCH --account=&lt;account_string&gt;\n\n# load modules\nmodule purge\nmodule load matlab/R2023a\n\n# define an environment variable for the MATLAB script and output\nBASE_MFILE_NAME=helloWorld\nMATLAB_OUTPUT=${BASE_MFILE_NAME}.out\n\n# execute code\ncd $SLURM_SUBMIT_DIR\nmatlab -nodisplay -r $BASE_MFILE_NAME &gt; $MATLAB_OUTPUT\n</code></pre> <p>where, again, the fields in <code>&lt; &gt;</code> must be properly specified.  Finally, at the terminal prompt, submit the job to the scheduler:</p> <pre><code>$ sbatch helloWorld.sb\n</code></pre> <p>The output file helloWorld.out should contain messages about the parallel pool and a \"Hello World\" message from each of the available workers.</p>"},{"location":"Documentation/Applications/Matlab/parallel/#example-of-speed-up-using-parfor","title":"Example of Speed-Up Using Parfor","text":"<p>MATLAB's <code>parfor</code> (\"parallel for-loop\") can be used to parallelize tasks that require no communication between workers. In this example, the aim is to solve a stiff, one-parameter system of ordinary differential equations (ODE) for different (randomly sampled) values of the parameter and to compare the compute time when using serial and parfor loops. This is a quintessential example of Monte Carlo simulation that is suitable for parfor: the solution for each value of the parameter is time-consuming to compute but can be computed independently of the other values.</p> <p>First, create a MATLAB function stiffODEfun.m that defines the right-hand side of the ODE system:</p> MATLAB code stiffODEfun.m <pre><code>function dy = stiffODEfun(t,y,c)\n  % This is a modified example from MATLAB's documentation at:\n  % http://www.mathworks.com/help/matlab/ref/ode15s.html\n  % The difference here is that the coefficient c is passed as an argument.\n    dy = zeros(2,1);\n    dy(1) = y(2);\n    dy(2) = c*(1 - y(1)^2)*y(2) - y(1);\nend\n</code></pre> <p>Second, create a driver file stiffODE.m that samples the input parameter and solves the ODE using the ode15s function.</p> MATLAB script stiffODE.m <pre><code>%{\n   This script samples a parameter of a stiff ODE and solves it both in\n   serial and parallel (via parfor), comparing both the run times and the\n   max absolute values of the computed solutions. The code -- especially the\n   serial part -- will take several minutes to run on Eagle.\n%}\n\n% open the local cluster profile\np = parcluster('Processes');\n\n% open the parallel pool, recording the time it takes\ntime_pool = tic;\nparpool(p);\ntime_pool = toc(time_pool);\nfprintf('Opening the parallel pool took %g seconds.\\n', time_pool)\n\n% create vector of random coefficients on the interval [975,1050]\nnsamples = 10000; % number of samples\ncoef = 975 + 50*rand(nsamples,1); % randomly generated coefficients\n\n% compute solutions within serial loop\ntime_ser = tic;\ny_ser = cell(nsamples,1); % cell to save the serial solutions\nfor i = 1:nsamples\n  if mod(i,10)==0\n    fprintf('Serial for loop, i = %d\\n', i);\n  end\n  [~,y_ser{i}] = ode15s(@(t,y) stiffODEfun(t,y,coef(i)) ,[0 10000],[2 0]);\nend\ntime_ser = toc(time_ser);\n\n% compute solutions within parfor\ntime_parfor = tic;\ny_par = cell(nsamples,1); % cell to save the parallel solutions\nerr = zeros(nsamples,1); % vector of errors between serial and parallel solutions\nparfor i = 1:nsamples\n  if mod(i,10)==0\n    fprintf('Parfor loop, i = %d\\n', i);\n  end\n  [~,y_par{i}] = ode15s(@(t,y) stiffODEfun(t,y,coef(i)) ,[0 10000],[2 0]);\n  err(i) = norm(y_par{i}-y_ser{i}); % error between serial and parallel solutions\nend\ntime_parfor = toc(time_parfor);\ntime_par = time_parfor + time_pool;\n\n% print results\nfprintf('RESULTS\\n\\n')\nfprintf('Serial time : %g\\n', time_ser)\nfprintf('Parfor time : %g\\n', time_par)\nfprintf('Speedup : %g\\n\\n', time_ser/time_par)\nfprintf('Max error between serial and parallel solutions = %e\\n', max(abs(err)))\n\n% close the parallel pool\ndelete(gcp)\nexit\n</code></pre> <p>Finally, create the batch script stiffODE.sb:</p> Slurm batch script stiffODE.sb <pre><code>#!/bin/bash\n#SBATCH --time=20:00\n#SBATCH --nodes=1\n#SBATCH --job-name=stiffODE\n#SBATCH --account=&lt;account_string&gt;\n\n# load modules\nmodule purge\nmodule load matlab/R2023a\n\n# define environment variables for MATLAB script and output\nBASE_MFILE_NAME=stiffODE\nMATLAB_OUTPUT=${BASE_MFILE_NAME}.out\n\n# execute code\ncd $SLURM_SUBMIT_DIR\nmatlab -nodisplay -r $BASE_MFILE_NAME &gt; MATLAB_OUTPUT\n</code></pre> <p>Next, submit the job (which will take several minutes to complete):</p> <pre><code>$ sbatch stiffODE.sb\n</code></pre> <p>If the code executed correctly, the end of the text file stiffODE.out should contain the times needed to compute the solutions in serial and parallel as well as the error between the serial and parallel solutions (which should be 0!). There should be a significant speed-up \u2014 how much depends on the runtime environment \u2014 for the parallelized computation.</p>"},{"location":"Documentation/Applications/Matlab/versions/","title":"MATLAB Software Versions and Licenses","text":"<p>Learn about the MATLAB software versions and licenses available for the NREL HPC systems.</p>"},{"location":"Documentation/Applications/Matlab/versions/#versions","title":"Versions","text":"<p>The latest version available on NREL HPC systems is R2023a.</p>"},{"location":"Documentation/Applications/Matlab/versions/#licenses","title":"Licenses","text":"<p>MATLAB is proprietary software. As such, users have access to a limited number of licenses both for the base MATLAB software as well as some specialized toolboxes.</p> <p>To see which toolboxes are available, regardless of how they are licensed, start an interactive MATLAB session and run:</p> <pre><code>&gt;&gt; ver\n</code></pre> <p>For a comprehensive list of available MATLAB-related licenses (including those not under active maintenance, such as the Database Toolbox), as well as their current availability, run the following terminal command:</p> <pre><code>$ lmstat.matlab\n</code></pre> <p>Among other things, you should see the following:</p> <pre><code>Feature usage info:\n\nUsers of MATLAB: (Total of 6 licenses issued; Total of ... licenses in use)\n\nUsers of Compiler: (Total of 1 license issued; Total of ... licenses in use)\n\nUsers of Distrib_Computing_Toolbox: (Total of 4 licenses issued; Total of ... licenses in use)\n\nUsers of MATLAB_Distrib_Comp_Engine: (Total of 16 licenses issued; Total of ... licenses in use)\n</code></pre> <p>This documentation only covers the base MATLAB package and the Parallel Computing Toolbox, which check out the \"MATLAB\" and \"Distrib_Computing_Toolbox\" licenses, respectively.</p>"},{"location":"Documentation/Applications/Plexos/","title":"PLEXOS","text":"<p>PLEXOS is a simulation software for modeling electric, gas, and water systems for optimizing energy markets. </p> <p>Users can run PLEXOS models on NREL's computing clusters. However, users need to build the PLEXOS models on a Windows system as there is no GUI available on the clusters and on Linux in general</p>"},{"location":"Documentation/Applications/Plexos/#available-modules","title":"Available Modules","text":"Kestrel Swift Vermilion plexos/9.000R09 plexos/9.000R09 plexos/9.200R06 plexos/11.000R01 <p>Info</p> <p>A user can only run PLEXOS with Gurobi solvers at this time. Please set up your model accordingly.</p>"},{"location":"Documentation/Applications/Plexos/#contents","title":"Contents","text":"<ol> <li>Setting up PLEXOS</li> <li>Running PLEXOS</li> </ol>"},{"location":"Documentation/Applications/Plexos/run_plexos/","title":"Running Plexos","text":"<p>Please follow the setup instructions before running the examples. Example scripts for new users are available within the master branch.</p> <p>Note</p> <p>Sometimes newer modules may be available in a <code>test</code> directory which is hidden by default from the general user base. This obscured release is done to iron out any bugs that may arise during the installation and use of the module while avoiding breaking users existing jobs and workflows. You can use these test modules by running</p> <pre><code>module use /nopt/nrel/apps/software/plexos/modules/test\nmodule avail\n</code></pre> <p>This should display all of the test modules available in addition to the defaults. We encourage you to reach out to us at HPC-Help@nrel.gov for access if you would like access to these modules.</p>"},{"location":"Documentation/Applications/Plexos/run_plexos/#example-run","title":"Example Run","text":"<p>We will load the requisite modules for running PLEXOS 9.2R06 for this example. Please see the module compatibility chart for loading the correct modules</p> <pre><code>module load plexos/9.200R06\n</code></pre> <p>Recall that we can only use the Gurobi solver while running the PLEXOS on the NREL cluster. Now that we have the modules loaded, PLEXOS can be called as follows</p> <pre><code>$PLEXOS/PLEXOS64 -n 5_bus_system_v2.xml -m 2024_yr_15percPV_MT_Gurobi\n</code></pre> <p>The command above assumes that we are running the model <code>2024_yr_15percPV_MT_Gurobi</code> from file <code>5_bus_system_v2.xml</code>. PLEXOS 9.0RX requires validating user-credentials for a local  PLEXOS account for each run. Therefore, if we ran the above command in an interactive session, we would need to enter the following username and password</p> <pre><code>username : nrelplexos\npassword : Nr3lplex0s\n</code></pre> <p>Fortunately, we can bypass the prompt for a local PLEXOS account username and password (useful for slurm batch jobs) by passing them as command line arguments as follows.</p> <pre><code>$PLEXOS/PLEXOS64 -n 5_bus_system_v2.xml -m 2024_yr_15percPV_MT_Gurobi -cu nrelplexos -cp Nr3lplex0s\n</code></pre> <p>Warning</p> <p>Not providing the username and password in batch jobs WILL cause your jobs to fail.</p> <p>Note</p> <p>PLEXOS will automatically upgrade or downgrade the XML database to the version you are running at run time. You do not need to do so manually. However, your solution may be different or erroneous.</p>"},{"location":"Documentation/Applications/Plexos/run_plexos/#example-scripts","title":"Example Scripts","text":"<p>The example scripts are available here. Please clone the repository to run those examples.</p> <p>Note</p> <p>The slurm output files generated by PLEXOS may not load correctly because of special characters that PLEXOS output introduces. To remove thoses special characters, open the slurm output file and run the following command</p> <pre><code># On PC\n:%s/&lt;CTRL-2&gt;//g\n# On Mac\n%s/&lt;CTRL-SHIFT-2&gt;//g\n</code></pre> <p><code>&lt;CTRL-2&gt;</code> or <code>&lt;CTRL-SHIFT-2&gt;</code> should generate the symbol <code>^@</code> that is messing up the output. Please refer to this stack exchange post for further information</p>"},{"location":"Documentation/Applications/Plexos/run_plexos/#1-basic-functionality-test","title":"1: Basic Functionality Test","text":"<p>The basic functionality test is the same as the example run in the section above. We will</p> <ol> <li>Request an interactive node</li> <li>Go to the correct example directory</li> <li>Run the PLEXOS example interactively</li> </ol> Simple 5 bus problem <pre><code># Request an interactive session on the cluster\nsalloc -N 1 --account=&lt;your_hpc_allocation_name&gt; --time=1:00:00 --partition=debug\n\n# Go to the working directory that contains the 5_bus_system_v2.xml example\ncd /to/you/XML/file/\n\n# Load the requisite modules\nmodule load plexos/9.200R06\n\n# Finally run the PLEXOS executable\n$PLEXOS/PLEXOS64 -n 5_bus_system_v2.xml -m 2024_yr_15percPV_MT_Gurobi -cu nrelplexos -cp Nr3lplex0s\n</code></pre>"},{"location":"Documentation/Applications/Plexos/run_plexos/#2-simple-batch-script-submission","title":"2: Simple batch script submission","text":"<p>We will run the same example by submitting the job to the SLURM queue. This example uses the batch file <code>submit_simple.sh</code>. In order to run this example as is, run the following commands</p> Submit job in a batch file. <pre><code># SSH into Kestrel or your cluster of choice\nssh $USER@kestrel.hpc.nrel.gov\n\n# Clone the HPC master branch in your scratch folder\ncd /scratch/${USER}/\ngit clone git@github.com:NREL/HPC.git\n\n# Go to the appropriate folder and submit the job on the HPC\ncd HPC/applications/plexos/RunFiles\nsbatch -A account_name --mail-user=your.email@nrel.gov submit_simple.sh\n</code></pre>"},{"location":"Documentation/Applications/Plexos/run_plexos/#3-enhanced-batch-script-submission","title":"3: Enhanced batch script submission","text":"<p>This builds upon the previous example where it tries to run the same model as before, but adds redundancy where the job doesn't fail if a license is not found. The submission script <code>submit_enhanced.sh</code> attempts to re-run the job after waiting 120 seconds for each attempt.</p> Slightly enhanced batch submission script <pre><code># Skip this if you already have the repo cloned in your scratch directory\nssh $USER@kestrel.hpc.nrel.gov\ncd /scratch/${USER}/\ngit clone git@github.com:NREL/HPC.git\n\n# Go into the appropriate directory\ncd /scratch/${USER}/HPC/applications/plexos/RunFiles\nsbatch -A account_name --mail-user=your.email@nrel.gov submit_enhanced.sh\n</code></pre>"},{"location":"Documentation/Applications/Plexos/run_plexos/#4-submitting-multiple-plexos-jobs","title":"4: Submitting multiple PLEXOS jobs","text":"<p>This example demonstrates how to submit multiple PLEXOS jobs. The model names are present in a file called <code>models.txt</code>. <code>submit_multiple.sh</code> is simply a wrapper that calls the batch file <code>submit_plexos.sh</code>.  </p> Submit multiple PLEXOS jobs <pre><code># Skip this if you already have the repo cloned in your scratch directory\nssh $USER@kestrel.hpc.nrel.gov\ncd /scratch/${USER}/\ngit clone git@github.com:NREL/HPC.git\n\n# Go into the appropriate directory\ncd /scratch/${USER}/HPC/applications/plexos/RunFiles\n./submit_multiple.sh 5_bus_system_v2.xml models.txt\n</code></pre>"},{"location":"Documentation/Applications/Plexos/run_plexos/#5-running-plexos-with-slurm-array-jobs","title":"5: Running PLEXOS with SLURM array jobs","text":"<p>This example demonstrates the use of SLURM job arrays to run multiple PLEXOS jobs using the script <code>submit_job_array.sh</code></p> Submit Slurm job-array for PLEXOS <pre><code># Skip this if you already have the repo cloned in your scratch directory\nssh $USER@kestrel.hpc.nrel.gov\ncd /scratch/${USER}/\ngit clone git@github.com:NREL/HPC.git\n\n# Go into the appropriate directory\ncd /scratch/${USER}/HPC/applications/plexos/RunFiles\nexport filename=5_bus_system_v2 # Export the XML dataset name\nexport models.txt # Export the file that contains the models names within the XML dataset\nsbatch -A account_name -t 5 --mail-user=your.email@nrel.gov --array=1-4 submit_job_array.sh\n</code></pre>"},{"location":"Documentation/Applications/Plexos/setup_plexos/","title":"Setting Up Plexos","text":""},{"location":"Documentation/Applications/Plexos/setup_plexos/#loading-the-appropriate-modules","title":"Loading the Appropriate Modules","text":"<p>PLEXOS XML model files can only run with Gurobi that is pre-packaged with a given PLEXOS version. The most common version combinations you may encounter at NREL are</p> PLEXOS Module Gurobi Version plexos/9.000R09 9.5.1 plexos/9.200R06 10.0.2 plexos/11.000R01 11.0.2 <p>Please contact us if you encounter any issues or require a newer version.</p>"},{"location":"Documentation/Applications/Plexos/setup_plexos/#setting-up-the-license","title":"Setting up the License","text":"<p>Before we can run PLEXOS, we need to create a license file on the cluster. For this, run the following commands with some minor modifications</p> EE_reg.xml <pre><code>mkdir -p ~/.config/PLEXOS\necho '&lt;?xml version=\"1.0\"?&gt;\n&lt;XmlRegistryRoot&gt;\n  &lt;comms&gt;\n    &lt;licServer_IP val=\"plexos.hpc.nrel.gov\" /&gt;\n    &lt;licServer_CommsPort val=\"8888\" /&gt;\n    &lt;licServer_IP_Secondary /&gt;\n    &lt;connect&gt;\n      &lt;PrimaryServer_Port /&gt;\n      &lt;SecondaryServer_Port /&gt;\n    &lt;/connect&gt;\n    &lt;licServer_CommsPort_Secondary /&gt;\n    &lt;LastLicTypeUsed val=\"server\" /&gt;\n  &lt;/comms&gt;\n  &lt;server&gt;\n    &lt;licServer_LogFolder val=\"/tmp/\" /&gt;\n    &lt;licServer_LogEvents val=\"true\" /&gt;\n  &lt;/server&gt;\n  &lt;proxy_cred&gt;\n    &lt;proxy_ip val=\"\" /&gt;\n    &lt;proxy_port val=\"\" /&gt;\n    &lt;proxy_uname val=\"\" /&gt;\n    &lt;proxy_pass val=\"\" /&gt;\n  &lt;/proxy_cred&gt;\n  &lt;BannedList&gt;\n    &lt;BanListedMachines val=\"true\" /&gt;\n  &lt;/BannedList&gt;\n  &lt;ProductUpdates&gt;\n    &lt;LastUpdateDate val=\"10/10/2021 13:11:10\" /&gt;\n  &lt;/ProductUpdates&gt;\n  &lt;UserName /&gt;\n  &lt;Company /&gt;\n  &lt;UserEmail /&gt;\n  &lt;CompanyCode /&gt;\n  &lt;LicenseServerRequestCount /&gt;\n&lt;/XmlRegistryRoot&gt;'   &gt; ~/.config/PLEXOS/EE_reg.xml\n</code></pre>"},{"location":"Documentation/Applications/Plexos/setup_plexos/#optional-conda-environment-for-plexos-with-python-and-r","title":"Optional: Conda environment for PLEXOS with Python and R","text":"<p>Note</p> <p>The following instructions are NOT required for only running PLEXOS. One only needs to load the relevant Gurobi and PLEXOS modules to run a PLEXOS XML database. Users may combine these runs with conda, Julia, or other software simply by loading the relevant modules and activating the appropriate conda and Julia environments.</p> <ol> <li>We need to load a few modules and create the requisite conda environment. First, we need to create a conda environment for PLEXOS.     <pre><code>module purge\nmodule load conda\nconda create -n plex1 r-essentials\n</code></pre></li> <li>Log out and log back in. Load the following modules and activate the conda environment     <pre><code>module purge\nmodule load comp-intel intel-mpi mkl conda\nconda activate plex1\n</code></pre></li> <li> <p>Install additional R libraries using conda     <pre><code>conda install r-doParallel\nconda install r-RSQLite\nconda install r-testthat\nconda install r-covr\n</code></pre></p> <p>Note</p> <p>Most of the R libraries should be added as part of the initial install, but keep an eye out for the following packages.</p> <p>Info</p> <p>See below if you wish to use your own version of R and Python for PLEXOS.</p> </li> <li> <p>We need to install one, <code>rplexos</code> library from source. To do this, execute the following commands     <pre><code>mkdir /home/$USER/temporary    \ncd /home/$USER/temporary\ngit clone https://github.com/NREL/rplexos.git\ncd rplexos\nCXX=`which icpc` R CMD INSTALL .\n</code></pre></p> <p>Note</p> <p><code>rplexos</code> needs to be built using an Intel compiler and R always wishes to build libraries using the same compilers that was used in its creation. If setting <code>CXX=which icpc</code> shown above does not work, we need to fool R by renaming the intel C++ compiler using a symbolic link. This is a hack and should only be used if the above way of installation fails. In order for the hack run the following after replacing username in the 3rd line with your own username. <pre><code>ln -s `which icpc` x86_64-conda_cos6-linux-gnu-c++\nexport PATH=`pwd`:$PATH\nRscript -e  \"install.packages('/home/username/temporary/rplexos/',repos=NULL,type='source')\"\nrm x86_64-conda_cos6-linux-gnu-c++\n</code></pre></p> </li> <li> <p>For some PLEXOS examples, we need to install an additional package called <code>plexos-coad</code>. For this run the following     <pre><code>cd /scratch/$USER\ngit clone https://github.com/Computational-Energy/plexos-coad.git\ncd plexos-coad\n\n#patch for python 3.9\ntofix=`grep -lr getchild`\nfor f in $tofix ; do sed -i3x \"s/for el_data in elem.getchildren()/for el_data in list\\(elem\\)/\" $f ; done\npip install Cython\npython setup.py install\n</code></pre></p> </li> <li> <p>Finally make sure we have numpy and pandas in the <code>plex1</code> conda environment.     <pre><code>pip install numpy pandas\n</code></pre></p> </li> </ol>"},{"location":"Documentation/Applications/Plexos/setup_plexos/#loading-an-existing-plexos-environment","title":"Loading an existing PLEXOS environment","text":"<p>If you have successfully followed all the instructions in the previous subsection and installed PLEXOS, you can simply load the following modules and activate the conda environment</p> <pre><code>module purge\nmodule load comp-intel intel-mpi mkl conda\nconda activate plex1\n</code></pre>"},{"location":"Documentation/Applications/Plexos/setup_plexos/#using-your-own-version-of-r-and-python","title":"Using your own version of R and Python","text":"<p>This section is in regards to Point 3 in setting up the PLEXOS environment. The following R libraries will need to be installed manually in this case.</p> <pre><code>install.packages(\"data.table\")\ninstall.packages(\"DBI\")\ninstall.packages(\"dbplyr\")\ninstall.packages(\"doParallel\")\ninstall.packages(\"dplyr\")\ninstall.packages(\"foreach\")\ninstall.packages(\"lubridate\")\ninstall.packages(\"magrittr\")\ninstall.packages(\"parallel\")\ninstall.packages(\"Rcpp\")\ninstall.packages(\"RSQLite\")\ninstall.packages(\"stringi\")\ninstall.packages(\"tidyr\")\ninstall.packages(\"knitr\")\ninstall.packages(\"testthat\")\ninstall.packages(\"ggplot2\")\ninstall.packages(\"covr\")\ninstall.packages(\"tidyverse\")\n</code></pre> <p>After installing the above, follow the remainder of the installation starting with point 4.</p>"},{"location":"Documentation/Development/Build_Tools/cmake/","title":"CMake","text":"<p>Documentation: https://cmake.org/documentation/</p> <p>CMake is a cross-platform build tool that is used to manage software compilation and testing.  From the CMake web site:</p> <p>CMake is an open-source, cross-platform family of tools designed to build, test and package software. CMake is used to control the software compilation process using simple platform and compiler independent configuration files, and generate native makefiles and workspaces that can be used in the compiler environment of your choice.</p>"},{"location":"Documentation/Development/Build_Tools/cmake/#getting-started","title":"Getting Started","text":"<p>On the NREL HPC systems, CMake is available through:</p> <pre><code>module load cmake\n</code></pre> <p>New users are encouraged to refer to the documentation linked above, in particular the CMake tutorial.  To build software that includes a <code>CMakeLists.txt</code> file, the steps often follow a pattern similar to:</p> <pre><code>mkdir build\ncd build\n# Reference the path to the CMakeLists.txt file:\nCC=&lt;c_compiler&gt; CXX=&lt;c++_compiler&gt; cmake ..\nmake\n</code></pre> <p>Here the <code>CC</code> and <code>CXX</code> environment variables are used to explicitly specify the C and C++ compiler that CMake should use.  If not specified, CMake will determine a default compiler to use.</p>"},{"location":"Documentation/Development/Build_Tools/git/","title":"Using Git Revision Control","text":"<p>Learn how to set up and use the Git software tool for development on the HPC systems</p> <p>Git is used locally to track incremental development and modifications to a collection of files. GitHub is a git-repository hosting web-service, which serves as a synchronized, common access point for the file collections. GitHub also has social aspects, like tracking who changed what and why. There are other git hosting services like GitLab which are similar to GitHub but offer slightly different features.</p> <p>NREL has a Github Enterprise server (github.nrel.gov) for internally-managed repos. Please note that github.nrel.gov is only available internally using the NREL network or VPN. NREL's git server uses SAML/SSO for logging into GitHub Enterprise. To get help accessing the server or creating a repository, please contact NREL ITS.</p>"},{"location":"Documentation/Development/Build_Tools/git/#git-configuration-set-up","title":"Git Configuration Set Up","text":"<p>The git software tool is already installed on the HPC systems. </p> <p>Git needs to know your user name and an email address at a minimum:</p> <pre><code>$ git config --global user.name \"Your name\"\n$ git config --global user.email \"your.name@nrel.gov\"\n</code></pre> <p>Github does not accept account passwords for authenticated Git operations. Instead, token-based authentication (PAT or SSH key) is required.</p>"},{"location":"Documentation/Development/Build_Tools/git/#set-up-ssh-authorization","title":"Set Up SSH Authorization","text":"<p>Users already have SSH keys created on the HPC systems. To set up Github SSH authorization, you can add the existing SSH (secure shell) key(s) to your Github profile. You will also need to change any remote repo URL to use SSH instead of HTTPS. </p> Set up SSH Key <ol> <li>On the HPC system, copy the content of ~/.ssh/id_rsa.pub. </li> <li>On Github, click on: your git profile &gt;  Settings &gt; SSH and GPG keys &gt; New SSH key</li> <li>Paste the content of ~/.ssh/id_rsa.pub into the \"Key\" window</li> <li>In your local git repo directory, type: <pre><code>git remote set-url origin &lt;git@github.nrel.gov:username/my-projectname.git&gt;.\n</code></pre> Your URL can be retrieved in the Github UI by going to the remote repo, then \"Code\" &gt; \"SSH\".</li> </ol> <p>Warning</p> <p>Please do not alter or delete the key pair that exists on the HPC systems in ~/.ssh/. You can copy the public key to Github. </p>"},{"location":"Documentation/Development/Build_Tools/git/#git-vocabulary","title":"Git Vocabulary","text":"Repository/repo <p>A git repository is an independent grouping of files to be tracked. A git repo has a \"root\" which is the directory that it sits in, and tracks further directory nesting from that. A single repo is often thought of as a complete project or application, though it's not uncommon to nest modules of an application as child repositories to isolate the development history of those submodules.</p> Commit <p>A commit, or \"revision\", is an individual change to a file (or set of files). It's like when you save a file, except with Git, every time you save it creates a unique ID (a.k.a. the \"SHA\" or \"hash\") that allows you to keep record of what changes were made when and by who. Commits usually contain a commit message which is a brief description of what changes were made.</p> Fork <p>A fork is a personal copy of another user's repository that lives on your account. Forks allow you to freely make changes to a project without affecting the original. Forks remain attached to the original, allowing you to submit a pull request to the original's author to update with your changes. You can also keep your fork up to date by pulling in updates from the original.</p> Pull <p>Pull refers to when you are fetching in changes and merging them. For instance, if someone has edited the remote file you're both working on, you'll want to pull in those changes to your local copy so that it's up to date.</p> Pull Request <p>Pull requests are proposed changes to a repository submitted by a user and accepted or rejected by a repository's collaborators. Like issues, pull requests each have their own discussion forum. </p> Push <p>Pushing refers to sending your committed changes to a remote repository, such as a repository hosted on GitHub. For instance, if you change something locally, you'd want to then push those changes so that others may access them.</p> Branch <p>A branch is a new/separate version of the repository. Use branches when you want to work on a new feature, but don't want to mess-up the main branch while testing your ideas. </p>"},{"location":"Documentation/Development/Build_Tools/git/#tool-use","title":"Tool Use","text":"Clone an existing repo <p>For example, you could create a local working copy of the \"test_repo\" repo (puts it in a folder in your current directory): <pre><code>cd /some/project/dir\ngit clone &lt;git@github.nrel.gov:username/test_repo.git&gt;\n</code></pre> Now, make changes to whatever you need to work on. Recommendation: commit your changes often, e.g., whenever you have a workable chunk of work completed.</p> See what files you've changed <p><code>git status</code></p> Push your changes to the repo <pre><code>git add &lt;filename(s)-you-changed&gt;\ngit commit -m \"A comment about the changes you just made.\"\ngit push\n</code></pre> Get remote changes from the repo <p>If you collaborate with others in this repo, you'll want to pull their changes into your copy of the repo. You may want to do this first-thing when you sit down to work on something to minimize the number of merges you'll need to handle: <code>git pull</code></p> Create a new local git code repo <pre><code>mkdir my.projectname\ncd my.projectname\ngit init\ntouch README.txt\ngit add README.txt\ngit commit -m 'first commit'\n# Push the repo to Github\ngit remote add origin git@hpc/my.projectname.git\ngit push origin main\n</code></pre> Revert a commit <p>You can use <code>git revert</code> to remove unwanted changes. Find the hash of the commit that you need to undo: <code>git log</code> Once you have the hash: <code>git revert &lt;hash of commit to undo&gt;</code> The <code>git revert</code> command will undo only the changes associated with the chosen commit, even if it is not the most recent commit. The reverted commit will still be stored in the history of changes, so it can still be accessed or reviewed in the future. </p> Make a branch <p>Create a local branch called \"experimental\" based on the current master branch: <pre><code>git checkout master #Switch to the master branch\ngit branch experimental\n</code></pre></p> <p>Use Your Branch (start working on that experimental branch....): <pre><code>git checkout experimental\n# If this branch exists on the remote repo, pull in new changes:\ngit pull origin experimental\n# work, work, work, commit....:\n</code></pre></p> <p>Send local branch to the repo: <code>git push origin experimental</code></p> <p>Get the remote repo and its branches: <code>git fetch origin</code></p> <p>Merge the branch into the master branch: <pre><code>git checkout master\ngit merge experimental\n</code></pre> If there are conflicts, git adds &gt;&gt;&gt;&gt; and &lt;&lt;&lt;&lt;&lt; markers in files to mark where you need to fix/merge your code. Examine your code with git diff: <code>git diff</code>  Make any updates needed, then <code>git add</code> and <code>git commit</code> your changes. </p> Delete a branch <p>Once you've merged a branch and you are done with it, you can delete it: <pre><code>git branch --delete &lt;branchName&gt; # deletes branchName from your local repo\ngit push origin --delete &lt;branchName&gt; # deletes the remote branch if you pushed it to the remote server\n</code></pre></p> Git diff tricks <p>You can use <code>git log</code> to see when the commits happened, and then <code>git diff</code> has some options that can help identify changes. What changed between two commits (hopefully back to back commits): <code>git diff 57357fd9..4f890708 &gt; my.patch</code> Just the files that changed:   <code>git diff --name-only 57357fd9 4f890708</code></p> Tags <p>You can tag a set of code in git, and use a specific tagged version. List tags: <code>git tags -l</code> Set a tag: <code>git tag -a \"2.2\" -m \"Tagging current rev at 2.2\"</code> Push your tag: <code>git push --tags</code> Use tag tagname: <code>git checkout tagname</code></p> Unmodify a modified file <p>To revert your file back to your last commit and discard current changes, use the output from <code>git status</code> to easily un-modify it.  <pre><code>$ git status\n# Changes not staged for commit:  \n# (use \"git add &lt;file&gt;...\" to update what will be committed)\n# (use \"git restore &lt;file&gt;...\" to discard changes in working directory)\n    # modified: modified_code.py  \n\n# Run the command in the above output to discard changes:  \n$ git restore modified_code.py\n</code></pre> If you run <code>git status</code> again you will see that the changes have been reverted. Just be sure that you want to revert the file before doing so, because all current changes will not be recoverable. </p> Point your repo to a different remote server <p>For example, you may need to do this if you were working on code from a repo that was checked-out from Github.com, and you want to check that code into a repository on NREL's github server. Once you've requested a new NREL git repo from ITS and it's configured, you can:   <pre><code>git remote set-url origin git@github.nrel.gov:hpc/my.&lt;newprojectname&gt;.git\n</code></pre> See <code>git help remote</code> for more details or you can just edit <code>.git/config</code> and change the URLs there.  This shouldn't cause any lost repo history, but if you want to be sure, you can make a copy of your repo until the url change is confirmed. </p> Send someone a copy of your current code (not the whole repo) <p>You can export a copy of your code to your $HOME directory using the following command:   <code>git archive master --prefix=my.projectname/ --output=~/my.projectname.tgz</code></p>"},{"location":"Documentation/Development/Build_Tools/spack/","title":"Spack","text":""},{"location":"Documentation/Development/Build_Tools/spack/#introduction","title":"Introduction","text":"<p>Spack is an HPC-centric package manager for acquiring, building, and managing HPC applications as well as all their dependencies, down to the compilers themselves. Like frameworks such as Anaconda, it is associated with a repository of both source-code and binary packages. Builds are fully configurable through a DSL at the command line as well as in YAML files. Maintaining many build-time permutations of packages is simple through an automatic and user-transparent hashing mechanism. The Spack system also automatically creates (customizable) environment modulefiles for each built package.</p>"},{"location":"Documentation/Development/Build_Tools/spack/#installation","title":"Installation","text":"<p>Multiple installations of Spack can easily be kept, and each is separate from the others by virtue of the environment variable <code>SPACK_ROOT</code>.  All package, build, and modulefile content is kept inside the <code>SPACK_ROOT</code> path, so working with different package collections is as simple as setting <code>SPACK_ROOT</code> to the appropriate location.  The only exception to this orthogonality are <code>YAML</code> files in <code>$HOME/.spack/&lt;platform&gt;</code>. Installing a Spack instance is as easy as</p> <p><code>git clone https://github.com/spack/spack.git</code></p> <p>Once the initial Spack instance is set up, it is easy to create new ones from it through</p> <p><code>spack clone &lt;new_path&gt;</code></p> <p><code>SPACK_ROOT</code> will need to point to <code>&lt;new_path&gt;</code> in order to be consistent.</p> <p>Spack environment setup can be done by sourcing <code>$SPACK_ROOT/share/spack/setup-env.sh</code>, or by simply adding <code>$SPACK_ROOT/bin</code> to your PATH. </p> <p><code>source $SPACK_ROOT/share/spack/setup-env.sh</code> or  <code>export PATH=$SPACK_ROOT/bin:$PATH</code></p>"},{"location":"Documentation/Development/Build_Tools/spack/#setting-up-compilers","title":"Setting Up Compilers","text":"<p>Spack is able to find certain compilers on its own, and will add them to your environment as it does.  In order to obtain the list of available compilers on Eagle the user can run <code>module avail</code>, the user can then load the compiler of interest using <code>module use &lt;compiler&gt;</code>. To see which compilers your Spack collections know about, type</p> <p><code>spack compilers</code></p> <p>To add an existing compiler installation to your collection, point Spack to its location through</p> <p><code>spack add compiler &lt;path to Spack-installed compiler directory with hash in name&gt;</code></p> <p>The command will add to <code>$HOME/.spack/linux/compilers.yaml</code>.  To configure more generally, move changes to one of the lower-precedence <code>compilers.yaml</code> files (paths described below in Configuration section). Spack has enough facility with standard compilers (e.g., GCC, Intel, PGI, Clang) that this should be all that\u2019s required to use the added compiler successfully.</p>"},{"location":"Documentation/Development/Build_Tools/spack/#available-packages-in-repo","title":"Available Packages in Repo","text":"Command Description <code>spack list</code> all available packages by name. Dumps repo content, so if use local repo, this should dump local package load. <code>spack list &lt;pattern&gt;</code> all available packages that have <code>&lt;pattern&gt;</code> somewhere in their name. <code>&lt;pattern&gt;</code> is simple, not regex. <code>spack info &lt;package_name&gt;</code> available versions classified as safe, preferred, or variants, as well as dependencies. Variants are important for selecting certain build features, e.g., with/without Infiniband support. <code>spack versions &lt;package_name&gt;</code> see which versions are available"},{"location":"Documentation/Development/Build_Tools/spack/#installed-packages","title":"Installed packages","text":"Command Description <code>spack find</code> list all locally installed packages <code>spack find --deps &lt;package&gt;</code> list dependencies of <code>&lt;package&gt;</code> <code>spack find --explicit</code> list packages that were explicitly requested via spack install <code>spack find --implicit</code> list packages that were installed as a dependency to an explicitly installed package <code>spack find --long</code> include partial hash in package listing. Useful to see distinct builds <code>spack find --paths</code> show installation paths <p>Finding how an installed package was built does not seem as straightforward as it should be.  Probably the best way is to examine <code>&lt;install_path&gt;/.spack/build.env</code>, where <code>&lt;install_path&gt;</code> is the Spack-created directory with the hash for the package being queried.  The environment variable <code>SPACK_SHORT_SPEC</code> in <code>build.env</code> contains the Spack command that can be used to recreate the package (including any implicitly defined variables, e.g., arch).  The 7-character short hash is also included, and should be excluded from any spack install command.</p> Symbols Description <code>@</code> package versions. Can use range operator \u201c:\u201d, e.g., X@1.2:1.4 . Range is inclusive and open-ended, e.g., \u201cX@1.4:\u201d matches any version of package X 1.4 or higher. <code>%</code> compiler spec. Can include versioning, e.g., X%gcc@4.8.5 <code>+,-,~</code> build options. +opt, -opt, \u201c~\u201d is equivalent to \u201c-\u201c <code>name=value</code> build options for non-Boolean flags. Special names are cflags, cxxflags, fflags, cppflags, ldflags, and ldlibs <code>target=value</code> for defined CPU architectures, e.g., target=haswell <code>os=value</code> for defined operating systems <code>^</code> dependency specification, using above specs as appropriate <code>^/&lt;hash&gt;</code> specify dependency where <code>&lt;hash&gt;</code> is of sufficient length to resolve uniquely"},{"location":"Documentation/Development/Build_Tools/spack/#external-packages","title":"External Packages","text":"<p>Sometimes dependencies are expected to be resolved through a package that is installed as part of the host system, or otherwise outside of the Spack database.  One example is Slurm integration into MPI builds.  If you were to try to add a dependency on one of the listed Slurms in the Spack database, you might see, e.g.,</p> <pre><code>[$user@el2 ~]$ spack spec openmpi@3.1.3%gcc@7.3.0 ^slurm@19-05-3-2\nInput spec\n--------------------------------\nopenmpi@3.1.3%gcc@7.3.0\n    ^slurm@19-05-3-2\n\nConcretized\n--------------------------------\n==&gt; Error: The spec 'slurm' is configured as not buildable, and no matching external installs were found\n</code></pre> <p>Given that something like Slurm is integrated deeply into the runtime infrastructure of our local environment, we really want to point to the local installation.  The way to do that is with a <code>packages.yaml</code> file, which can reside in the standard Spack locations (see Configuration below).  See the Spack docs on external packages for more detail.  In the above example at time of writing, we would like to build OpenMPI against our installed <code>Slurm 19.05.2</code>.  So, you can create file <code>~/.spack/linux/packages.yaml</code> with the contents</p> <pre><code>packages:\n  slurm:\n    paths:\n      slurm@18-08-0-3: /nopt/slurm/18.08.3\n      slurm@19-05-0-2: /nopt/slurm/19.05.2\n</code></pre> <p>that will enable builds against both installed Slurm versions.  Then you should see</p> <pre><code>[$user@el2 ~]$ spack spec openmpi@3.1.3%gcc@7.3.0 ^slurm@19-05-0-2\nInput spec\n--------------------------------\nopenmpi@3.1.3%gcc@7.3.0\n    ^slurm@19-05-0-2\n\nConcretized\n--------------------------------\nopenmpi@3.1.3%gcc@7.3.0 cflags=\"-O2 -march=skylake-avx512 -mtune=skylake-avx512\" cxxflags=\"-O2 -march=skylake-avx512 -mtune=skylake-avx512\" fflags=\"-O2 -march=skylake-avx512 -mtune=skylake-avx512\" +cuda+cxx_exceptions fabrics=verbs ~java~legacylaunchers~memchecker+pmi schedulers=slurm ~sqlite3~thread_multiple+vt arch=linux-centos7-x86_64\n-\n    ^slurm@19-05-0-2%gcc@7.3.0 cflags=\"-O2 -march=skylake-avx512 -mtune=skylake-avx512\" cxxflags=\"-O2 -march=skylake-avx512 -mtune=skylake-avx512\" fflags=\"-O2 -march=skylake-avx512 -mtune=skylake-avx512\" ~gtk~hdf5~hwloc~mariadb+readline arch=linux-centos7-x86_64\n</code></pre> <p>where the Slurm dependency will be satisfied with the installed Slurm (cflags, cxxflags, and arch are coming from site-wide configuration in <code>/nopt/nrel/apps/base/2018-12-02/spack/etc/spack/compilers.yaml</code>; the variants string is likely coming from the configuration in the Spack database, and should be ignored).</p>"},{"location":"Documentation/Development/Build_Tools/spack/#virtual-packages","title":"Virtual Packages","text":"<p>It is possible to specify some packages for which multiple options are available at a higher level.  For example, <code>mpi</code> is a virtual package specifier that can resolve to mpich, openmpi, Intel MPI, etc.  If a package's dependencies are spec'd in terms of a virtual package, Spack will choose a specific package at build time according to site preferences. Choices can be constrained by spec, e.g.,</p> <p><code>spack install X ^mpich@3</code></p> <p>would satisfy package X\u2019s mpi dependency with some version 3 of MPICH. You can see available providers of a virtual package with</p> <p><code>spack providers &lt;vpackage&gt;</code></p>"},{"location":"Documentation/Development/Build_Tools/spack/#extensions","title":"Extensions","text":"<p>In many cases, frameworks have sub-package installations in standard locations within their own installations.  A familiar example of this is Python and its usual module location in <code>lib(64)/python&lt;version&gt;/site-packages</code>, and pointed to via the environment variable <code>PYTHONPATH</code>.</p> <p>To find available extensions</p> <p><code>spack extensions &lt;package&gt;</code></p> <p>Extensions are just packages, but they are not enabled for use out of the box. To do so (e.g., so that you could load the Python module after installing), you can either load the extension package\u2019s environment module, or</p> <p><code>spack use &lt;extension package&gt;</code></p> <p>This only lasts for the current session, and is not of general interest. A more persistent option is to activate the extension:</p> <p><code>spack activate &lt;extension package&gt;</code></p> <p>This takes care of dependencies as well. The inverse operation is deactivation.</p> Command Description <code>spack deactivate &lt;extension package&gt;</code> deactivates extension alone. Will not deactivate if dependents exist <code>spack deactivate --force &lt;extension package&gt;</code> deactivates regardless of dependents <code>spack deactivate --all &lt;extension package&gt;</code> deactivates extension and all dependencies <code>spack deactivate --all &lt;parent&gt;</code> deactivates all extensions of parent (e.g., <code>&lt;python&gt;</code>)"},{"location":"Documentation/Development/Build_Tools/spack/#modules","title":"Modules","text":"<p>Spack can auto-create environment modulefiles for the packages that it builds, both in Tcl for \u201cenvironment modules\u201d per se, and in Lua for Lmod.  Auto-creation includes each dependency and option permutation, which can lead to excessive quantities of modulefiles.  Spack also uses the package hash as part of the modulefile name, which can be somewhat disconcerting to users.  These default behaviors can be treated in the active modules.yaml file, as well as practices used for support. Tcl modulefiles are created in <code>$SPACK_ROOT/share/spack/modules</code> by default, and the equivalent Lmod location is <code>$SPACK_ROOT/share/spack/lmod</code>.  Only Tcl modules are created by default.  You can modify the active modules.yaml file in the following ways to affect some example behaviors:</p>"},{"location":"Documentation/Development/Build_Tools/spack/#to-turn-lmod-module-creation-on","title":"To turn Lmod module creation on:","text":"<pre><code>modules:\n    enable:\n        - tcl\n        - lmod \n</code></pre>"},{"location":"Documentation/Development/Build_Tools/spack/#to-change-the-modulefile-naming-pattern","title":"To change the modulefile naming pattern:","text":"<pre><code>modules:\n    tcl:\n        naming_scheme: \u2018{name}/{version}/{compiler.name}-{compiler.version}\n</code></pre> <p>would achieve the Eagle naming scheme. </p>"},{"location":"Documentation/Development/Build_Tools/spack/#to-remove-default-variable-settings-in-the-modulefile-eg-cpath","title":"To remove default variable settings in the modulefile, e.g., CPATH:","text":"<pre><code>modules:\n    tcl:\n        all:\n            filter:\n                environment_blacklist: [\u2018CPATH\u2019]\n</code></pre> <p>Note that this would affect Tcl modulefiles only; if Spack also creates Lmod files, those would still contain default CPATH modification behavior.</p>"},{"location":"Documentation/Development/Build_Tools/spack/#to-prevent-certain-modulefiles-from-being-built-you-can-whitelist-and-blacklist","title":"To prevent certain modulefiles from being built, you can whitelist and blacklist:","text":"<pre><code>modules:\n    tcl:\n        whitelist: [\u2018gcc\u2019]\n        blacklist: [\u2018%gcc@4.8.5\u2019]\n</code></pre> <p>This would create modules for all versions of GCC built using the system compiler, but not for the system compiler itself. There are a great many further behaviors that can be changed, see https://spack.readthedocs.io/en/latest/module_file_support.html#modules for more.</p> <p>For general user support, it is not a bad idea to keep the modules that are publicly visible separate from the collection that Spack auto-generates. This involves some manual copying, but is generally not onerous as all rpaths are included in Spack-built binaries (i.e., you don\u2019t have to worry about satisfying library dependencies for Spack applications with an auto-built module, since library paths are hard-coded into the application binaries). This separation also frees one from accepting Spack\u2019s verbose coding formats within modulefiles, should you decide to maintain certain modulefiles another way.</p>"},{"location":"Documentation/Development/Build_Tools/spack/#configuration","title":"Configuration","text":"<p>Spack uses hierarchical customization files.  Every package is a Python class, and inherits from the top-level class Package.  Depending on the degree of site customization, you may want to fork the Spack repo to create your own customized Spack package. There are 4 levels of configuration. In order of increasing precedence,</p> <ol> <li>Default: <code>$SPACK_ROOT/etc/spack/default</code></li> <li>System-wide: <code>/etc/spack</code></li> <li>Site-wide: <code>$SPACK_ROOT/etc/spack</code></li> <li>User-specific: <code>$HOME/.spack</code></li> </ol> <p>Spack configuration uses YAML files, a subset of JSON native to Python. There are 5 main configuration files.</p> <ol> <li> <p><code>compilers.yaml</code>. Customizations to the Spack-known compilers for all builds</p> <p>i.  Use full path to compilers</p> <p>ii. Additional rpaths beyond the Spack repo</p> <p>iii.    Additional modules necessary when invoking compilers</p> <p>iv. Mixing toolchains</p> <p>v.  Optimization flags</p> <p>vi. Environment modifications</p> </li> <li> <p><code>config.yaml</code>. Base functionality of Spack itself</p> <p>i.  install_tree: where to install packages</p> <p>ii. build_stage: where to do compiles. For performance, can specify a local SSD or a RAMFS.</p> <p>iii.    modules_roots: where to install modulefiles</p> </li> <li> <p><code>modules.yaml</code>. How to create modulefiles</p> <p>i.  whitelist/blacklist packages from having their own modulefiles created</p> <p>ii. adjust hierarchies</p> </li> <li> <p><code>packages.yaml</code>. Specific optimizations, such as multiple hardware targets.</p> <p>i.  dependencies, e.g., don\u2019t build OpenSSL (usually want sysadmins to handle updates, etc.)</p> <p>ii. mark specific packages as non-buildable, e.g., vendor MPIs</p> <p>iii.    preferences, e.g., BLAS -&gt; MKL, LAPACK -&gt; MKL</p> </li> <li> <p><code>repos.yaml</code></p> <p>i.  Directory-housed, not remote</p> <p>ii. Specify other package locations</p> <p>iii.    Can then spec build in other configs (e.g., binary, don\u2019t build)</p> <p>iv. Precedence in YAML file order, but follows Spack precedence order (user &gt; site &gt; system &gt; default)</p> </li> </ol>"},{"location":"Documentation/Development/Build_Tools/spack/#variants-standard-adjustments-to-package-build","title":"Variants: standard adjustments to package build","text":"<p><code>spack edit \u2026</code>-- opens Python file for package, can easily write new variants</p>"},{"location":"Documentation/Development/Build_Tools/spack/#providers","title":"Providers","text":"<p><code>spack providers</code> -- virtual packages, e.g., blas, mpi, etc. Standards, not implementations. Abstraction of an implementation (blas/mkl, mpi/mpich, etc.)</p>"},{"location":"Documentation/Development/Build_Tools/spack/#mirrors","title":"Mirrors","text":"<ul> <li>mirrors.yaml: where packages are kept</li> <li>A repo is where build information is kept; a mirror is where code lives</li> </ul> <pre><code>MirrorTopLevel\n    package_a\n        package_a-version1.tar.gz\n        package_a-version2.tar.gz\n    package_b\n        \u22ee\n</code></pre> <p><code>spack mirror</code> to manage mirrors</p>"},{"location":"Documentation/Development/Build_Tools/spack/#repos","title":"Repos","text":"<ul> <li>Can take precedence from, e.g., a site repo</li> <li>Can namespace</li> </ul> <pre><code>packages\n    repo.yaml\n    alpha\n        hotfix-patch-ABC.patch\n        package.py\n        package.pyc\n    beta\n    theta\n</code></pre>"},{"location":"Documentation/Development/Build_Tools/spack/#kestrel-specific-configuration","title":"Kestrel specific configuration","text":"<p>In order to add HPE's and NREL's installed compilers to Kestrel, we can edit the <code>compilers.yaml</code> file as discussed earlier.  Similarly, we can add the HPE provided MPIs (Cray-MPICH) and specify variances for packages to be installed by editing the <code>packages.yaml</code>. Finally, if spack will be used for the creation of modulefiles, we can edit <code>module.yaml</code> to produce <code>tcl</code> or <code>LUA</code> modulefiles with the format and naming sought.</p> <p>The edited 3 files <code>compilers.yaml</code>, <code>packages.yaml</code> and <code>modules.yaml</code>; meant to be used on Kestrel; are hosted in the following locations: For CPU nodes installs ( <code>arch=linux-rhel8-sapphirerapids</code> ): <code>/nopt/nrel/apps/cpu_stack/software/spack_files</code> For GPU nodes installs ( <code>arch=linux-rhel8-zen4</code> ): <code>/nopt/nrel/apps/gpu_stack/software/spack_files</code> </p>"},{"location":"Documentation/Development/Compilers/rosetta_stone/","title":"Compiler Information","text":"<p>This document describes some of the important command line options for various compilers.  This includes gcc, gfortran, g++, Intel, Fortran, C, C++, as well as the Cray compilers. The infomation contained herein is not complete but only a small subset of what is available in man pages and full documentation.  For example, the man page for gcc is over 21,000 lines long.</p>"},{"location":"Documentation/Development/Compilers/rosetta_stone/#topics","title":"Topics","text":"<p>The topics covered include:</p> <ul> <li>Normal invocation</li> <li>Default optimization level</li> <li>Compiling for performance</li> <li>Compiling for debugging and related purposes</li> <li>Runtime checks</li> <li>Some File extensions</li> <li>Language standard settings (Dialect)</li> <li>Generating listing, if available</li> <li>Preprocessing</li> <li>OpenMP support</li> <li>OpenACC support</li> <li>UPC support (C++)</li> <li>Coarray support (Fortran)</li> <li>Important compiler specific options</li> <li>Notes</li> </ul>"},{"location":"Documentation/Development/Compilers/rosetta_stone/#compilers-covered","title":"Compilers covered","text":"<ul> <li>gcc</li> <li>gfortran</li> <li>Intel icc (Classic)<ul> <li>Moving to Intel's new icx compiler</li> </ul> </li> <li>Intel ifort (Fortran)<ul> <li>Moving to Intel's new ifx compiler</li> </ul> </li> <li>Cray C (Clang based)</li> <li>Cray Fortran (ftn)</li> </ul>"},{"location":"Documentation/Development/Compilers/rosetta_stone/#gccg","title":"gcc/g++","text":"<p>This discussion is for version 12.x. Most options are supported for recent versions of the compilers.  Also, most command line options for gcc and g++ are supported for each compiler.  It is recommended that C++ programs be compiled with g++ and C programs with gcc.</p>"},{"location":"Documentation/Development/Compilers/rosetta_stone/#normal-invocation","title":"Normal invocation","text":"<pre><code># Compile and link a program with the executable sent to the indicated\n  file\ngcc mycode.c -o myexec\ng++ mycode.C -o myexec\n\n# Compile a file but don't link \ngcc -c mycode.c \ng++ -c mycode.C \n</code></pre>"},{"location":"Documentation/Development/Compilers/rosetta_stone/#default-optimization","title":"Default optimization","text":"<p>The default optimization level is -O0 on most systems.  It is possible that a compiler might be configured to have a different default.  One easy way to determine the default is to build a simple application without specifying an optimization level and compare its size to a version compiled with optimization on.  </p>"},{"location":"Documentation/Development/Compilers/rosetta_stone/#compiling-for-performance","title":"Compiling for performance","text":"<pre><code>-O1 Optimize.  Optimizing compilation takes somewhat more time, and a\n    lot more memory for a large function.\n\n-O2 Optimize even more.  GCC performs nearly all supported\n    optimizations that do not involve a space-speed tradeoff.\n\n-O3 Optimize yet more.\n\n-Ofast Disregard strict standards compliance.  -Ofast enables all -O3\n    optimizations.  It also enables optimizations that are not valid\n    for all standard-compliant programs.  \n</code></pre> <p>You can discover which optimizations are at various levels of optimization as shown below.  The last command will show all potential optimization flags, over 250.</p> <pre><code>gcc -c -Q -O3 --help=optimizers &gt; /tmp/O3-opts\ngcc -c -Q -O2 --help=optimizers &gt; /tmp/O2-opts\ndiff /tmp/O2-opts /tmp/O3-opts | grep enabled\n\ngcc -Q  --help=optimizers \n</code></pre>"},{"location":"Documentation/Development/Compilers/rosetta_stone/#compiling-for-debugging-and-related-purposes","title":"Compiling for debugging and related purposes","text":"<p><pre><code>-Og Optimize debugging experience. Use instead of -O0. Does sopme\n    optimization but maintains debug information\n\n-g  Produce debugging information\n\ngcc -Og -g myprog.c\n\n-p,-pg Generate extra code to write profile information suitable for\n    the analysis program prof (for -p) or gprof\n</code></pre> There are many potential options  for profiling.  See the man page and search for -pg. </p>"},{"location":"Documentation/Development/Compilers/rosetta_stone/#some-file-extensions","title":"Some file extensions","text":"<pre><code>file.c\n   C source code that must be preprocessed.\n\nfile.i\n   C source code that should not be preprocessed.\n\nfile.ii\n   C++ source code that should not be preprocessed.\n\nfile.cc\nfile.cp\nfile.cxx\nfile.cpp\nfile.CPP\nfile.c++\nfile.C\n   C++ source code that must be preprocessed.  \n</code></pre> <p>You can specify explicitly the language for file indepenent of the extension using the -x option.  For example gcc -x c file.cc will complie the program as C instead of C++.</p>"},{"location":"Documentation/Development/Compilers/rosetta_stone/#language-standard-settings-dialect","title":"Language standard settings (Dialect)","text":"<pre><code>-ansi This is equivalent to -std=c90. In C++ mode, it is equivalent to -std=c++98.\n\n\n-std=\n\nc90\n   Support all ISO C90 programs \n\niso9899:199409\n   ISO C90 as modified in amendment 1.\n\nc99\n   ISO C99.  \n\nc11\n   ISO C11, the 2011 revision of the ISO C standard.  \n\nc18\n   ISO C17, the 2017 revision of the ISO C standard\n   (published in 2018).  \n\nc2x The next version of the ISO C standard, still under\n    development.  The support for this version is\n    experimental and incomplete.\n\n\nc++98 The 1998 ISO C++ standard plus the 2003 technical\n      corrigendum and some additional defect reports. Same as\n      -ansi for C++ code.\n\nc++11\n   The 2011 ISO C++ standard plus amendments.  \n\nc++14\n   The 2014 ISO C++ standard plus amendments.  \n\nc++17\n   The 2017 ISO C++ standard plus amendments. \n</code></pre> <p>This is a subset of all of the options.  There are \"gnu\" specific versions of many of these which give slight variations.  Also, some fo these can be specified  in various deprecated flags. The dialects available for the compilers are highly version dependent.  Older versions of compiler will not support newer dialects.</p>"},{"location":"Documentation/Development/Compilers/rosetta_stone/#preprocessing","title":"Preprocessing","text":"<p>Unless explicitly disabled by the file extension as described above files are preprocessed.  If you pass the -E option the file will be preprocessed only and will not be compiled.  The output is sent to the standard output </p>"},{"location":"Documentation/Development/Compilers/rosetta_stone/#openmp-support","title":"OpenMP support","text":"<pre><code>-fopenmp \n          Enable handling of OpenMP directives\n-fopenmp-simd\n          Enable handling of OpenMP's SIMD directives   \n-mgomp          \n          Generate code for use in OpenMP offloading \n</code></pre> <p>Offlading will not work on all platforms and may require additional options.</p>"},{"location":"Documentation/Development/Compilers/rosetta_stone/#openacc-support","title":"OpenACC support","text":"<pre><code> -fopenacc\n           Enable handling of OpenACC directives\n\n -fopenacc-dim=geom\n           Specify default compute dimensions for parallel offload\n           regions that do not explicitly specify\n ```\n\nOfflading will not work on all platforms and may require additional options.          \n</code></pre>"},{"location":"Documentation/Development/Compilers/rosetta_stone/#important-compiler-specific-options","title":"Important compiler specific options","text":"<p>-Wall      This enables all the warnings about constructions that some      users consider questionable, and that are easy to avoid (or      modify to prevent the warning)</p> <p>-Wextra       This enables some extra warning flags that are not enabled by      -Wall.</p>"},{"location":"Documentation/Development/Compilers/rosetta_stone/#gfortran","title":"gfortran","text":"<p>This discussion is for version 12.x. Most options are supported for recent versions of the compilers.  Also, most command line options for gcc and g++ are supported for gfortran.</p>"},{"location":"Documentation/Development/Compilers/rosetta_stone/#normal-invocation_1","title":"Normal invocation","text":"<pre><code># Compile and link a program with the executable sent to the indicated\n  file\ngfortran mycode.f90  -o myexec\n\n# Compile a file but don't link \ngfortran -c mycode.f90\n</code></pre>"},{"location":"Documentation/Development/Compilers/rosetta_stone/#default-optimization_1","title":"Default optimization","text":"<p>The default optimization level is -O0 on most systems.  It is possible that a compiler might be configured to have a different default.  One easy way to determine the default is to build a simple application without specifying an optimization level and compare its size to a version compiled with optimization on.  </p>"},{"location":"Documentation/Development/Compilers/rosetta_stone/#compiling-for-performance_1","title":"Compiling for performance","text":"<pre><code>-O1 Optimize.  Optimizing compilation takes somewhat more time, and a\n    lot more memory for a large function.\n\n-O2 Optimize even more.  GCC performs nearly all supported\n    optimizations that do not involve a space-speed tradeoff.\n\n-O3 Optimize yet more.\n\n-Ofast Disregard strict standards compliance.  -Ofast enables all -O3\n    optimizations.  It also enables optimizations that are not valid\n    for all standard-compliant programs.  \n</code></pre> <p>You can discover which optimizations are at various levels of optimization as shown below.  The last command will show all potential optimization flags, over 250.</p> <pre><code>gfortran -c -Q -O3 --help=optimizers &gt; /tmp/O3-opts\ngfortran -c -Q -O2 --help=optimizers &gt; /tmp/O2-opts\ndiff /tmp/O2-opts /tmp/O3-opts | grep enabled\n\ngfortran -Q  --help=optimizers \n</code></pre>"},{"location":"Documentation/Development/Compilers/rosetta_stone/#compiling-for-debugging-and-related-purposes_1","title":"Compiling for debugging and related purposes","text":"<p><pre><code>-Og Optimize debugging experience. Use instead of -O0. Does sopme\n    optimization but maintains debug information\n\n-g  Produce debugging information\n\n-fbacktrace Try to print a back trace on error\n\n-fcheck=&lt;all|array-temps|bits|bounds|do|mem|pointer|recursion&gt;.\n    Perform various runtime checks.  This will slow your program\n    down.\n\ngfortran -Og -g -fbacktrace -fcheck=all myprog.c\n\n-fcheck=&lt;all|array-temps|bits|bounds|do|mem|pointer|recursion&gt;\n    Perform various runtime checks\n\n-p,-pg Generate extra code to write profile information suitable for\n    the analysis program prof (for -p) or gprof\n</code></pre> There are many potential options  for profiling.  See the man page and search for -pg. </p>"},{"location":"Documentation/Development/Compilers/rosetta_stone/#some-file-extensions_1","title":"Some file extensions","text":"<pre><code>.F, .FOR, .FTN, .fpp, .FPP, .F90, .F95, .F03\n    preprocessor is run automatically   \n\n.f, .for, .ftn, .f90, .f95, .f03\n    preprocessor is not run automatically   \n</code></pre>"},{"location":"Documentation/Development/Compilers/rosetta_stone/#language-standard-settings-dialect_1","title":"Language standard settings (Dialect)","text":"<pre><code>f95, f2003, f2008, f2018 Specify strict conformance to the various\n    standards\n\ngnu 2018 with gnu extensions\n\nlegacy Older codes\n\n-ffree-form / -ffixed-form The source is in Free / Fixed form\n</code></pre>"},{"location":"Documentation/Development/Compilers/rosetta_stone/#language-standard-settings-save","title":"Language standard settings (Save)","text":"<p>The Fortran 90 standard does not indicate the status of variables that leave scope.  That is in general, a variable defined in a subroutine may or may not be defined when the subroutine is reentered.  There are exceptions for variables in common blocks and those defined in modules.</p> <p>For Fortran 95 and later local allocatable variables are automatically deallocated upon exit from a subroutine.</p> <p>The flags -fautomatic  and -fno-automatic change this behavior.</p> <pre><code>-fautomatic Automatically deallocate variables on exit independent of\n    standard setting\n\n-fno-automatic Do not automatically deallocate variables on exit\n    independent of standard setting\n\n-fmax-stack-var-size With this value set to some small value, say 1\n    it appears that variables are not deallocated.  A program\n    compiled with this option would in general be nonconformnet.\n</code></pre> <p>The above applies to allocatable arrays.  It is not clean what happens to scalers.</p>"},{"location":"Documentation/Development/Compilers/rosetta_stone/#language-standard-settings-argument-mismatch","title":"Language standard settings (argument mismatch)","text":"<p>Some code contains calls to external procedures with mismatches between the calls and the procedure definition, or with mismatches between different calls. Such code is non-conforming, and will usually be flagged with an error.  This options degrades the error to a warning, which can only be disabled by disabling all warnings via -w.  Only a single occurrence per argument is flagged by this warning. -fallow-argument-mismatch is implied by -std=legacy.</p> <p>It is recomended that source code be modified to have interfaces for routines that are called iwth various types of arguments.  Fortran 2018 allows for a generic type for such interfaces.  For example here is an interface for MPI_Bcast </p> <pre><code>module bcast\ninterface\n subroutine MPI_BCAST(BUF, COUNT, DATATYPE, DEST, COMM, IERROR)\n type(*),intent(inout) :: BUF\n !type(*), dimension(..), intent(in) :: BUF\n integer, intent(in) ::  COUNT, DATATYPE, DEST,  COMM\n integer, intent(out) :: IERROR\n end subroutine\nend interface\nend module\n</code></pre>"},{"location":"Documentation/Development/Compilers/rosetta_stone/#generating-listing","title":"Generating listing","text":"<p>Gfortran does not produce listings.</p>"},{"location":"Documentation/Development/Compilers/rosetta_stone/#preprocessing_1","title":"Preprocessing","text":"<p>Automatic preprocessing is determined by the file name extension as discussed above. You can manually turn it on/off via the options </p> <pre><code>-cpp - Preprocess\n-nocpp - Don't preprocess\n-cpp -E - Preprocess and send output to standard out. Don't compile\n</code></pre>"},{"location":"Documentation/Development/Compilers/rosetta_stone/#openmp-support_1","title":"OpenMP support","text":"<pre><code>-fopenmp        Enable handling of OpenMP directives\n-fopenmp-simd   Enable handling of OpenMP's SIMD directives   \n-mgomp          Generate code for use in OpenMP offloading \n</code></pre> <p>Offlading will not work on all platforms and may require additional options.</p>"},{"location":"Documentation/Development/Compilers/rosetta_stone/#openacc-support_1","title":"OpenACC support","text":"<p><pre><code> -fopenacc Enable handling of OpenACC directives\n\n -fopenacc-dim=geom Specify default compute dimensions for parallel offload\n     regions that do not explicitly specify\n</code></pre> Offlading will not work on all platforms and may require additional options.          </p>"},{"location":"Documentation/Development/Compilers/rosetta_stone/#important-compiler-specific-options_1","title":"Important compiler specific options","text":"<pre><code>-fimplicit-none \n            Produce and error message if there are explicitly typed variables.  \n\n-fdefault-real-8\n            Set the default real type to an 8 byte wide type.  This option also affects the kind of non-double real constants like 1.0. \n\n-pedantic \n            Issue warnings for uses of extensions to Fortran.\n\n       -fall-intrinsics\n           This option causes all intrinsic procedures (including the GNU-specific extensions) to be accepted.  This can\n           be useful with -std= to force standard-compliance but get access to the full range of intrinsics available\n           with gfortran.  \n</code></pre>"},{"location":"Documentation/Development/Compilers/rosetta_stone/#iccicpc","title":"icc/icpc","text":"<p>This discussion is for version 2021.6.0.  Icc and icpc will be replaced with clang based alternatives in the near future, icx and icpx.  In the Cray environment if PrgEnv-intel is loaded the \"cc\" maps to icc.</p>"},{"location":"Documentation/Development/Compilers/rosetta_stone/#normal-invocation_2","title":"Normal invocation","text":"<pre><code>Compile and link a program with the executable sent to the indicated\n  file\nicc mycode.c -o myexec\nicpc mycode.C -o myexec\n\nCompile a file but don't link \nicc -c mycode.c \nicpc -c mycode.C \n</code></pre> <p>NOTE: The icpc command uses the same compiler options as the icc command. Invoking the compiler using icpc compiles .c and .i files as C++. Invoking the compiler using icc compiles .c and .i files as C. Using icpc always  links in C++ libraries. Using icc only links in C++ libraries if C++ source is provided on the command line.</p>"},{"location":"Documentation/Development/Compilers/rosetta_stone/#default-optimization_2","title":"Default optimization","text":"<p>The default optimization level is -O2.  </p>"},{"location":"Documentation/Development/Compilers/rosetta_stone/#compiling-for-performance_2","title":"Compiling for performance","text":"<pre><code>-O0  Disables all optimizations.\n\n-O1  Enables optimizations for speed.\n\n-O2 Optimize even more. \n\n-O  Same ans -O2\n\n-O3 Optimize yet more.\n\n-Ofast -O3, -no-prec-div, and -fp-model\n\n-no-prec-div  enables optimizations that give slightly less precise\n    results than full IEEE division\n\n-fp-model slight decrease in the accuracy of math library functions\n\n-opt_report  Generate and optimization report\n</code></pre> <p>You can learn more about optimizations are at various levels of optimization as shown below.  </p> <pre><code>icc -V -help opt\n</code></pre>"},{"location":"Documentation/Development/Compilers/rosetta_stone/#compiling-for-debugging-and-related-purposes_2","title":"Compiling for debugging and related purposes","text":"<pre><code>-g[n] \n    0 Disables generation of symbolic debug information.\n    1 Produces minimal debug information for performing stack traces.\n    2 Produces complete debug information. This is the same as specifying -g with no n.\n    3 Produces extra information that may be useful for some tools.\n\n-Os Generate extra code to write profile information suitable for\n    the analysis program gprof\n</code></pre>"},{"location":"Documentation/Development/Compilers/rosetta_stone/#some-file-extensions_2","title":"Some file extensions","text":"<pre><code>file.c\n   C source code that must be preprocessed.\n\nfile.i\n   C source code that should not be preprocessed.\n\nfile.ii\n   C++ source code that should not be preprocessed.\n\nfile.cc\nfile.cp\nfile.cxx\nfile.cpp\nfile.CPP\nfile.c++\nfile.C\n   C++ source code that must be preprocessed.  \n</code></pre> <p>You can specify explicitly the language for file indepenent of the extension using the -x option.  For example icc -x c file.cc will complie the program as C instead of C++. <pre><code>#### Language standard settings (Dialect)\n</code></pre> -std=  enable language support for , as described below <p>c99     conforms to ISO/IEC 9899:1999 standard for C programs</p> <p>c11     conforms to ISO/IEC 9899:2011 standard for C programs</p> <p>c17     conforms to ISO/IEC 9899:2017 standard for C programs</p> <p>c18      conforms to ISO/IEC 9899:2018 standard for C programs</p> <p>c++11     enables C++11 support for C++ programs</p> <p>c++14     enables C++14 support for C++ programs</p> <p>c++17     enables C++17 support for C++ programs</p> <p>c++20     enables C++20 support for C++ programs</p> <p>c89     conforms to ISO/IEC 9899:1990 standard for C programs</p> <p>gnu89     conforms to ISO C90 plus GNU extensions</p> <p>gnu99      conforms to ISO C99 plus GNU extensions</p> <p>gnu++98      conforms to 1998 ISO C++ standard plus GNU extensions</p> <p>gnu++11     conforms to 2011 ISO C++ standard plus GNU extensions</p> <p>gnu++14     conforms to 2014 ISO C++ standard plus GNU extensions</p> <p>gnu++17      conforms to 2017 ISO C++ standard plus GNU extensions</p> <p>gnu++20 c     onforms to 2020 ISO C++ standard plus GNU extensions</p> <p>-strict-ansi      Implement a strict ANSI conformance dialect</p> <pre><code>```\n</code></pre>"},{"location":"Documentation/Development/Compilers/rosetta_stone/#preprocessing_2","title":"Preprocessing","text":"<p>Unless explicitly disabled by the file extension as described above files are preprocessed.  If you pass the -E option the file will be preprocessed only and will not be compiled.  The output is sent to the standard output </p>"},{"location":"Documentation/Development/Compilers/rosetta_stone/#openmp-support_2","title":"OpenMP support","text":"<pre><code>-fopenmp\n    Enable handling of OpenMP directives\n-qopenmp-stubs\n    Compile OpenMP programs in sequential mode \n-parallel          \n    Auto parallelize\n</code></pre>"},{"location":"Documentation/Development/Compilers/rosetta_stone/#openacc-support_2","title":"OpenACC support","text":"<pre><code>Not supported\n</code></pre> <p>Offlading will not work on all platforms and may require additional options.          </p> <ul> <li>Important compiler specific options</li> </ul> <pre><code>-Wall\n     This enables all the warnings about constructions that some\n     users consider questionable, and that are easy to avoid (or\n     modify to prevent the warning)\n\n-Wextra \n     This enables some extra warning flags that are not enabled by\n     -Wall.\n\n-help [category]   print full or category help message\n\nValid categories include\n       advanced        - Advanced Optimizations\n       codegen         - Code Generation\n       compatibility   - Compatibility\n       component       - Component Control\n       data            - Data\n       deprecated      - Deprecated Options\n       diagnostics     - Compiler Diagnostics\n       float           - Floating Point\n       help            - Help\n       inline          - Inlining\n       ipo             - Interprocedural Optimization (IPO)\n       language        - Language\n       link            - Linking/Linker\n       misc            - Miscellaneous\n       opt             - Optimization\n       output          - Output\n       pgo             - Profile Guided Optimization (PGO)\n       preproc         - Preprocessor\n       reports         - Optimization Reports\n\n       openmp          - OpenMP and Parallel Processing\n</code></pre>"},{"location":"Documentation/Development/Compilers/rosetta_stone/#moving-to-intels-new-compiler-icx","title":"Moving to Intel's new compiler icx","text":"<p>The Intel compilers icc and icpc are being retired and being replaced with icx and icpx. Other than the name change many people will not notice significant differences.  </p> <p>The document https://www.intel.com/content/www/us/en/developer/articles/guide/porting-guide-for-icc-users-to-dpcpp-or-icx.html  has details.  Here are some important blurbs from that page.</p> <p>ICX and ICC Classic use different compiler drivers. The Intel\u00ae C++ Compiler Classic  compiler drivers are icc, icpc, and icl. \u202fThe Intel\u00ae oneAPI DPC++/C++ Compiler drivers  are icx and icpx. Use icx to compile and link C programs, and icpx for C++ programs.</p> <p>Unlike the icc driver, icx does not use the file extension to determine whether to  compile as C or C+. Users must invoke icpx to compile C+ files. . In addition to  providing a core C++ Compiler, ICX/ICPX is also used to compile SYCL/DPC++ codes for the  Intel\u00ae oneAPI Data Parallel C++ Compiler when we pass an additional flag \u201c-fsycl\u201d.\u202f</p> <p>The major changes in compiler defaults are listed below:</p> <ul> <li>The Intel\u00ae oneAPI DPC++/C++ Compiler drivers are icx and icpx.</li> <li>Intel\u00ae C++ Compiler Classic uses icc, icpc or icl drivers but this compiler will be deprecated in the upcoming release.</li> <li>DPC++/SYCL users can use the icx/icpx driver along with the -fsycl flag which invokes ICX with SYCL extensions. </li> <li>Unlike Clang, the ICX Default floating point model was chosen to match ICC behavior and by default it is -fp-model=fast .</li> <li>MACRO naming is changing. Please be sure to check release notes for future macros to be included in ICX.</li> <li>No diagnostics numbers are listed for remarks, warnings, or notes. Every diagnostic is emitted with the corresponding compiler option to disable it. </li> <li>Compiler intrinsics cannot be automatically recognized without processor targeting options, unlike the behavior in Intel\u00ae C++ Compiler Classic. If you use intrinsics, read more on the documentation about intrinsic behavior changes. </li> </ul>"},{"location":"Documentation/Development/Compilers/rosetta_stone/#ifort","title":"ifort","text":"<p>This discussion is for version 2021.6.0.  Ifort  will be replaced with a clang backend based alternative in the near future, ifx.  Ifx will have most of the same options as ifort with some clang additions.  In the Cray environment if PrgEnv-intel is loaded the \"cc\" maps to icc.</p>"},{"location":"Documentation/Development/Compilers/rosetta_stone/#normal-invocation_3","title":"Normal invocation","text":"<pre><code># Compile and link a program with the executable sent to the indicated\n  file\nifort mycode.f90 -o myexec\n\n# Compile a file but don't link \nifort -c mycode.c \n</code></pre>"},{"location":"Documentation/Development/Compilers/rosetta_stone/#default-optimization_3","title":"Default optimization","text":"<p>The default optimization level is -O2.  </p>"},{"location":"Documentation/Development/Compilers/rosetta_stone/#compiling-for-performance_3","title":"Compiling for performance","text":"<pre><code>-O1 optimize for maximum speed, but disable some optimizations which\n    increase code size for a small speed benefit\n\n-O2 optimize for maximum speed (DEFAULT)\n\n-O3 optimize for maximum speed and enable more aggressive\n    optimizations that may not improve performance on some programs\n\n-O  same as -O2\n\n-Os  enable speed optimizations, but disable some optimizations which\n    increase code size for small speed benefit\n\n-O0  disable optimizations\n\n-Ofast  enable -O3 -no-prec-div -fp-model fast=2 optimizations\n\n-fno-alias  assume no aliasing in program\n\n-fno-fnalias  assume no aliasing within functions, but assume\n    aliasing across calls\n\n-fast  enable -xHOST -ipo -no-prec-div -O3 -static -fp-model=fast=2\n    optimizations\n\n-opt_report Generate and optimization report\n</code></pre> <p>You can learn more about optimizations are at various levels of optimization as shown below.  </p> <pre><code>ifort -V -help opt\n</code></pre>"},{"location":"Documentation/Development/Compilers/rosetta_stone/#compiling-for-debugging-and-related-purposes_3","title":"Compiling for debugging and related purposes","text":"<pre><code> -g[n] \n       0 Disables generation of symbolic debug information.\n       1 Produces minimal debug information for performing stack traces.\n       2 Produces complete debug information. This is the same as specifying -g with no n.\n       3 Produces extra information that may be useful for some tools.\n\nnone    Disables all check options.\n\narg_temp_created    Determines whether checking occurs for actual\n    arguments copied into temporary storage before routine calls.\n\nassume    Determines whether checking occurs to test that the\n    scalar-Boolean-expression in the ASSUME directive is true, or\n    that the addresses in the ASSUME_ALIGNED directive  are  aligned \n    on  the specified byte boundaries.\n\nbounds    Determines whether checking occurs for array subscript and\n    character s ubstring expressions.\n\ncontiguous    Determines whether the compiler checks pointer\n    contiguity at pointer-assignment time.\n\nformat    Determines whether checking occurs for the data type of an\n    item being formatted for output.\n\noutput_conversion    Determines whether checking occurs for the fit\n    of data items within a designated format descriptor field.\n\npointers    Determines whether checking occurs for certain\n    disassociated or uninitialized pointers or unallocated\n    allocatable objects.\n\nshape    Determines whether array conformance checking is performed.\n\nstack    Determines whether checking occurs on the stack frame.\n\nteams    Determines whether the run-time system diagnoses\n    non-standard coarray team usage.\n\nudio_iostat    Determines whether conformance checking occurs when\n    user-defined derived type input/output routines are executed.\n\nuninit     Determines whether checking occurs for uninitialized\n    variables.\n\n    all    Enables all check options.\n\n-Os Generate extra code to write profile information suitable for\n           the analysis program gprof\n</code></pre>"},{"location":"Documentation/Development/Compilers/rosetta_stone/#some-file-extensions_3","title":"Some file extensions","text":"<pre><code>Filenames with the suffix .f90 are interpreted as free-form Fortran\n    95/90 source files.\n\nFilenames with the suffix .f, .for, or .ftn are interpreted as\n    fixed-form Fortran source files.\n\nFilenames with the suffix .fpp, .F, .FOR, .FTN, or .FPP are\n    interpreted as fixed-form Fortran source files, which must be\n    preprocessed by the fpp preprocessor before being compiled.\n\nFilenames with the suffix .F90 are interpreted as free-form Fortran\n    source files, which must be pre-processed by the fpp preprocessor\n    before being compiled.\n</code></pre> <p>You can specify explicitly the language for file indepenent of the extension using the -x option.  For example icc -x c file.cc will complie the program as C instead of C++.</p>"},{"location":"Documentation/Development/Compilers/rosetta_stone/#language-standard-settings-dialect_2","title":"Language standard settings (Dialect)","text":"<pre><code>-stand \n\nnone    Tells the compiler to issue no messages for nonstandard\n    language elements. This is the same as specifying nostand.\n\nf90    Tells the compiler to issue messages for language elements\n    that are not standard in Fortran 90.\n\nf95    Tells the compiler to issue messages for language elements\n    that are not standard in Fortran 95.\n\nf03    Tells the compiler to issue messages for language elements\n    that are not standard in Fortran 2003.\n\nf08    Tells the compiler to issue messages for language elements\n    that are not standard in Fortran 2008.\n\nf18    Tells the compiler to issue messages for language elements\n    that are not standard in Fortran 2018. This option is set if you\n    specify warn stderrors.\n</code></pre>"},{"location":"Documentation/Development/Compilers/rosetta_stone/#generate-listing","title":"Generate Listing","text":"<pre><code>-list\n</code></pre>"},{"location":"Documentation/Development/Compilers/rosetta_stone/#preprocessing_3","title":"Preprocessing","text":"<p>Unless explicitly enabled by the file extension as described above files are not preprocessed.  If you pass the -E option the file will be preprocessed only and will not be compiled.  The output is sent to the standard output.  The option  **-fpp ** will force running the preprocessor.</p>"},{"location":"Documentation/Development/Compilers/rosetta_stone/#openmp-support_3","title":"OpenMP support","text":"<pre><code>-fopenmp\n    Enable handling of OpenMP directives\n-qopenmp-stubs\n    Compile OpenMP programs in sequential mode \n-parallel          \n    Auto parallelize\n</code></pre>"},{"location":"Documentation/Development/Compilers/rosetta_stone/#openacc-support_3","title":"OpenACC support","text":"<pre><code>Not supported\n</code></pre>"},{"location":"Documentation/Development/Compilers/rosetta_stone/#coarray-fortran","title":"Coarray Fortran","text":"<pre><code>-coarray[=keyword] Enables the coarray feature where keyword\n    Specifies the memory system where the coarrays will be\n    implemented. Possible values are:\n\nshared    Indicates a shared memory system. This is the default.\n\ndistributed    Indicates a distributed memory system.\n\nsingle     Indicates a configuration where the image does not\n    contain self-replication code. This results in an executable with\n    a single running image. This configuration can be useful for\n    debugging purposes, even though there are no inter-image\n    interactions.\n</code></pre> <ul> <li>Important compiler specific options</li> </ul> <pre><code>-save    Causes variables to be placed in static memory.\n\n\nDefault:    This option saves all variables in static allocation\n    except local variables within a recursive routine and variables\n    declared as AUTOMATIC.\n\n-auto-scalar    Scalar variables of intrinsic types INTEGER, REAL,\n    COMPLEX, and LOGICAL are allocated  to the run-time stack unless\n    the routine is recursive of OpenMP For Fortran 95 and later\n    variables are not saved by default and allocatable arrays are\n    deallocated.  This appears to be true ifort even if the standard\n    is set to f90.  However, it is poor practice to rely on this\n    behavior.\n\n\n-Wall.   This enables all the warnings about constructions that some\n    users consider questionable, and that are easy to avoid (or\n    modify to prevent the warning)\n\n-warn declarations    Generate warnings for variables that are not\n    explicitly typed.\n\n-Wextra     This enables some extra warning flags that are not\n    enabled by -Wall.\n-save    Causes variables to be placed in static memory.\n\n\nDefault:    This option saves all variables in static allocation\n    except local variables within a recursive routine and variables\n    declared as AUTOMATIC.\n\n-auto-scalar    Scalar variables of intrinsic types INTEGER, REAL,\n    COMPLEX, and LOGICAL are allocated  to the run-time stack unless\n    the routine is recursive of OpenMP For Fortran 95 and later\n    variables are not saved by default and allocatable arrays are\n    deallocated.  This appears to be true ifort even if the standard\n    is set to f90.  However, it is poor practice to rely on this\n    behavior.\n\n\n-Wall.   This enables all the warnings about constructions that some\n    users consider questionable, and that are easy to avoid (or\n    modify to prevent the warning)\n\n-warn declarations    Generate warnings for variables that are not\n    explicitly typed.\n\n-Wextra     This enables some extra warning flags that are not\n    enabled by -Wall.\n\n\n-help [category]    print full or category help message\n\nValid categories include\n       advanced        - Advanced Optimizations\n       codegen         - Code Generation\n       compatibility   - Compatibility\n       component       - Component Control\n       data            - Data\n       deprecated      - Deprecated Options\n       diagnostics     - Compiler Diagnostics\n       float           - Floating Point\n       help            - Help\n       inline          - Inlining\n       ipo             - Interprocedural Optimization (IPO)\n       language        - Language\n       link            - Linking/Linker\n       misc            - Miscellaneous\n       opt             - Optimization\n       output          - Output\n       pgo             - Profile Guided Optimization (PGO)\n       preproc         - Preprocessor\n       reports         - Optimization Reports\n\n       openmp          - OpenMP and Parallel Processing\n</code></pre>"},{"location":"Documentation/Development/Compilers/rosetta_stone/#moving-to-intels-new-compiler-ifx","title":"Moving to Intel's new compiler ifx","text":"<p>Intel\u00ae Fortran Compiler Classic (ifort) is now deprecated and will be discontinued in late 2024.  Intel recommends that customers transition now to using the LLVM-based Intel\u00ae Fortran Compiler (ifx). Other  than the name change some people will not notice significant differences.  The new compiler supports offloading to Intel GPU. Kestrel and Swift do not have Intel GPUs so this is not at NREL.  </p> <p>One notable deletion from the new compiler is dropping of auto-parilization.  With ifort the  -parallel compiler option auto-parallelization is enabled. That is not true for ifx; there  is no auto-parallelization feature with ifx.</p> <p>For complete details please see: https://www.intel.com/content/www/us/en/developer/articles/guide/porting-guide-for-ifort-to-ifx.html</p>"},{"location":"Documentation/Development/Compilers/rosetta_stone/#cray-cc","title":"Cray CC","text":"<p>In the Cray environment cc is a generic call for several different compilers.  The compile actually called is determined by the modules loaded.  Here we discuss Cray C : Version 14.0.4.  cc will detect if the program being compiled calls MPI routines.  If so, it will call the program as MPI.  Cray C : Version 14.0.4 is clang based with Cray enhancements</p>"},{"location":"Documentation/Development/Compilers/rosetta_stone/#normal-invocation_4","title":"Normal invocation","text":"<pre><code># Compile and link a program with the executable sent to the indicated\n  file\ncc mycode.c  -o myexec\n\n# Compile a file but don't link \ncc -c mycode.c \n</code></pre>"},{"location":"Documentation/Development/Compilers/rosetta_stone/#default-optimization_4","title":"Default optimization","text":"<p>The default optimization level is -O0.</p>"},{"location":"Documentation/Development/Compilers/rosetta_stone/#compiling-for-performance_4","title":"Compiling for performance","text":"<p><pre><code>-O0, -O1, -O2, -O3, -Ofast, -Os, -Oz, -Og, -O, -O4 Specify which\n    optimization level to use: \n\n-O0    Means \"no optimization\": this\n    level compiles the fastest and generates the most debuggable\n    code.\n\n-O1    Somewhere between -O0 and -O2.\n\n-O2    Moderate level of optimization which enables most\n    optimizations.\n\n-O3     Like -O2, except that it enables optimizations that take\n    longer to perform or that may generate larger code (in an attempt\n    to make the program run faster).\n\n-Ofast     Enables all the optimizations from -O3 along with other\n    aggressive optimizations that may violate strict compliance with\n    language standards.\n\n-Os     Like -O2 with extra optimizations to reduce code size.\n\n-Oz    Like -Os (and thus -O2), but reduces code size further.\n\n-Og    Like -O1. In future versions, this option might disable\n    different optimizations in order to improve debuggability.\n\n-O    Equivalent to -O1.\n\n-O4    and higher Currently equivalent to -O3\n</code></pre> For best performance, -Ofast with -flto is recommended where -flot = Generate  output  files  in  LLVM  formats,  suitable for link time optimization.  The performance improvement with high levels of optimmization.  Here are the run times for a simple finite difference code at various levels of optimization.</p> <pre><code>Option       Run Time (sec)\n-O0            10.30\n-O1             3.19\n-O2             2.99\n-O3             2.04\n-Ofast          1.88\n-Ofast -flto    1.49\n-Os             3.19\n-Oz             3.31\n-Og             3.19\n-O              3.20\n</code></pre>"},{"location":"Documentation/Development/Compilers/rosetta_stone/#compiling-for-debugging-and-related-purposes_4","title":"Compiling for debugging and related purposes","text":"<pre><code>-fstandalone-debug \n      Turn off the stripping of some debug information that might be useful to some debuggers\n\n-feliminate-unused-debug-types\n      By default, Clang does not emit type information for types that are defined but not \n      used in a program. To retain the debug info for these unused types, the negation \n      -fno-eliminate-unused-debug-types can be used.\n\n-fexceptions\n      Enable generation of unwind information. This allows exceptions to be thrown through \n      Clang compiled stack frames.  This is on by default in x86-64.\n\n-ftrapv\n      Generate code to catch integer overflow errors.  Signed integer overflow is undefined \n      in C. With this flag, extra code is generated to detect this and abort when it happens.\n</code></pre>"},{"location":"Documentation/Development/Compilers/rosetta_stone/#some-file-extensions_4","title":"Some file extensions","text":"<pre><code>file.c\n   C source code that must be preprocessed.\n\nfile.i\n   C source code that should not be preprocessed.\n\nfile.ii\n   C++ source code that should not be preprocessed.\n\nfile.cc\nfile.cp\nfile.cxx\nfile.cpp\nfile.CPP\nfile.c++\nfile.C\n   C++ source code that must be preprocessed.  \nfile.upc\n   UPC\n</code></pre>"},{"location":"Documentation/Development/Compilers/rosetta_stone/#language-standard-settings-dialect_3","title":"Language standard settings (Dialect)","text":"<p>Standards are determined by the file extension as given above.  Some addttional checks can be performed.</p> <pre><code>  -std=&lt;standard&gt;\n</code></pre> <p>Specify the language standard to compile for.</p>"},{"location":"Documentation/Development/Compilers/rosetta_stone/#supported-values-for-the-c-language-are","title":"Supported values for the C language are:","text":"<ul> <li> <p>ISO C 1999 with GNU extensions</p> <ul> <li>c89</li> <li>c90</li> </ul> </li> <li> <p>iso9899:1990</p> </li> <li> <p>ISO C 2011</p> <ul> <li>c11</li> <li>iso9899:2011</li> </ul> </li> <li> <p>ISO C 2011 with GNU extensions</p> <ul> <li>gnu11</li> </ul> </li> <li> <p>ISO C 2017</p> <ul> <li>iso9899:2017</li> <li>c17</li> </ul> </li> <li> <p>ISO C 2017 with GNU extensions</p> <ul> <li>gnu17</li> </ul> </li> </ul> <p>The default C language standard is gnu17</p>"},{"location":"Documentation/Development/Compilers/rosetta_stone/#supported-values-for-the-c-language-are_1","title":"Supported values for the C++ language are:","text":"<ul> <li> <p>ISO C++ 1998 with amendments</p> <ul> <li>c++98</li> <li>c++03</li> </ul> </li> <li> <p>ISO C++ 1998 with amendments and GNU extensions</p> <ul> <li>gnu++98</li> <li>gnu++03</li> </ul> </li> <li> <p>ISO C++ 2011 with amendments</p> <ul> <li>c++11</li> </ul> </li> <li> <p>ISO C++ 2011 with amendments and GNU extensions</p> <ul> <li>gnu++11</li> </ul> </li> <li> <p>ISO C++ 2014 with amendments</p> <ul> <li>c++14</li> </ul> </li> <li> <p>ISO C++ 2014 with amendments and GNU extensions</p> <ul> <li>gnu++14</li> </ul> </li> <li> <p>ISO C++ 2017 with amendments</p> <ul> <li>c++17</li> </ul> </li> <li> <p>ISO C++ 2017 with amendments and GNU extensions</p> <ul> <li>gnu++17</li> </ul> </li> <li> <p>Working draft for ISO C++ 2020</p> <ul> <li>c++2a</li> </ul> </li> <li> <p>Working draft for ISO C++ 2020 with GNU extensions</p> <ul> <li>gnu++2a</li> </ul> </li> <li> <p>The default OpenCL language standard is cl1.0.</p> <ul> <li>OpenCL</li> </ul> </li> </ul>"},{"location":"Documentation/Development/Compilers/rosetta_stone/#supported-values-for-the-cuda-language-are","title":"Supported values for the CUDA language are:","text":"<ul> <li>cuda</li> </ul>"},{"location":"Documentation/Development/Compilers/rosetta_stone/#generating-listing_1","title":"Generating listing","text":"<pre><code>-fsave-loopmark    Generate a loopmark listing file (.lst) that shows which optimizations \n    were applied to which parts of the source code.\n\n-floopmark-style=&lt;style&gt;    Specifies the style of the loopmark listing file.\n\n    Valid values for &lt;style&gt; are:\n        ''grouped''         Places all messages at the end of the listing.\n        ''interspersed''    Places each message after the relevant source code line.\n</code></pre>"},{"location":"Documentation/Development/Compilers/rosetta_stone/#preprocessing_4","title":"Preprocessing","text":"<p>Automatic preprocessing is determined by the file name extension as discussed above. You can manually turn it on/off via the options</p> <pre><code> -E    with output going to standard out\n</code></pre> <p>The compiler predefines the macro cray in addition to all of the usual Clang predefined macros.</p>"},{"location":"Documentation/Development/Compilers/rosetta_stone/#openmp-support_4","title":"OpenMP support","text":"<pre><code>-fopenmp    Enables OpenMP and links in OpenMP libraries\n</code></pre>"},{"location":"Documentation/Development/Compilers/rosetta_stone/#openacc-support_4","title":"OpenACC support","text":"<pre><code>Not suported    \n</code></pre>"},{"location":"Documentation/Development/Compilers/rosetta_stone/#important-compiler-specific-options_2","title":"Important compiler specific options","text":"<pre><code>   Unified Parallel C (UPC) Options\n-hupc, -hdefault -hupc    Configures the compiler driver to expect\n    UPC source code.  Source files with a .upc extension are\n    automatically treated as UPC code, but this option permits a file\n    with any other extension (typically .c) to be understood as UPC\n    code.  -hdefault cancels this behavior; if both -hupc and\n    -hdefault appear in a command line, whichever appears last takes\n    precedence and applies to all source files in the command line.\n\n-fupc-auto-amo, -fno-upc-auto-amo    Automatically use network\n    atomics for remote updates to reduce latency.  For example, x +=\n    1 can be performed as a remote atomic add.  If an update is\n    recognized as local to the current  thread,  then  no  atomic  is\n    used.  These atomics are intended as a performance optimization\n    only and shall not be relied upon to prevent race conditions. \n    Enabled at -O1 and above.\n\n-fupc-buffered-async, -fno-upc-buffered-async    Set aside memory in\n    the UPC runtime library for aggregating random remote accesses\n    designated with \"#pragma pgas buffered_async\".  Disabled by\n    default.\n\n-fupc-pattern, -fno-upc-pattern    Identify simple communication\n    loops and aggregate the remote accesses into a single function\n    call which replaces the loop.  Enabled at -O1 and above.\n\n-fupc-threads=&lt;N&gt;    Set  the number of threads for a static THREADS\n    translation.  This option causes __UPC_STATIC_THREADS__ to be\n    defined instead of __UPC_DYNAMIC_THREADS__ and replaces all uses\n    of the UPC keyword THREADS with the value N.\n</code></pre>"},{"location":"Documentation/Development/Compilers/rosetta_stone/#cray-ftn","title":"Cray ftn","text":"<p>In the Cray environment ftn is a generic call for several different compilers.  The compile actually called is determined by the modules loaded.  Here we discuss Cray Fortran : Version 14.0.4.  Ftn will detect if the program being compiled calls MPI routines.  If so, it will call the program as MPI.</p>"},{"location":"Documentation/Development/Compilers/rosetta_stone/#normal-invocation_5","title":"Normal invocation","text":"<pre><code># Compile and link a program with the executable sent to the indicated\n  file\nftn mycode.f90  -o myexec\n\n# Compile a file but don't link \nftn -c mycode.f90\n</code></pre>"},{"location":"Documentation/Development/Compilers/rosetta_stone/#default-optimization_5","title":"Default optimization","text":"<p>The default optimization level is -O 2.</p>"},{"location":"Documentation/Development/Compilers/rosetta_stone/#compiling-for-performance_5","title":"Compiling for performance","text":"<pre><code>-O\n\n0      Specifies  no  automatic  cache  management;  all memory\n    references are allocated to cache.  Both automatic cache blocking\n    and manual cache blocking (by use of the BLOCKABLE directive) are\n    shut off. Characteristics include low compile time.  This option\n    is compatible with all optimization levels.\n\n1      Specifies conservative automatic cache management.\n    Characteristics include moderate compile time.  Symbols are\n    placed in the cache when the possibility of cache reuse exists\n    and the predicted cache footprint of the symbol in isolation is\n    small enough to experience reuse.\n\n2      Specifies  moderately  aggressive automatic cache management. \n    Characteristics include moderate compile time.  Symbols are\n    placed in the cache when the possibility of cache reuse exists\n    and the pre\u00e2\u20ac\u0090 dicted state of the cache model is such that the\n    symbol will be reused. (Default)\n\n3      Specifies aggressive automatic cache management.\n    Characteristics include potentially high compile time.  Symbols\n    are placed in the cache when the possibility of cache reuse\n    exists and the  allocation of the symbol to the cache is\n    predicted to increase the number of cache hits.\n\nfast    Same as 3.\n</code></pre>"},{"location":"Documentation/Development/Compilers/rosetta_stone/#compiling-for-debugging-and-related-purposes_5","title":"Compiling for debugging and related purposes","text":"<pre><code>-G (level)\n\n    0      Full   information is available for debugging, but at the cost\n        of a slower and larger executable.  Breakpoints can be set at\n        each line.  Most optimizations are disabled.\n\n    1      Most  information is available with partial optimization. Some\n        optimizations make tracebacks and limited breakpoints available\n        in the debugger.  Some scalar optimizations and  all  loop  nest\n        re\u00e2\u20ac\u0090 structuring  is  disabled,  but  the source code will be\n        visible and most symbols will be available.\n\n    2      Partial information.  Most optimizations, tracebacks and very\n        limited breakpoints are available in the debugger.  The source\n        code will be visible and some symbols will be  available.\n\n\n-R runchk Specifies any of a group of runtime checks for your\n    program.  To specify more than one type of checking, specify\n    consecutive runchk arguments, as follows: -R bs.\n\n\n    b      Enables checking of array bounds.  Bounds checking is not\n        performed on arrays dimensioned as (1).  Enables -Ooverindex.\n\n    c      Enables conformance checking of array operands in array\n        expressions.\n\n    d      Enables a run time check for the !dir$ collapse directive and\n        checks the validity of the loop_info count information.\n\n    p      Generates run time code to check the association or allocation\n        status of referenced POINTER variables, ALLOCATABLE arrays, or\n        assumed-shape arrays.\n\n    s      Enables checking of character substring bounds.\n</code></pre>"},{"location":"Documentation/Development/Compilers/rosetta_stone/#some-file-extensions_5","title":"Some file extensions","text":"<p>The default is fixed for source files that have .f, .F, .for, or .FOR</p> <p>The default is free for source files that have .f90, .F90, .f95, .F95, .f03, .F03, .f08, .F08, .f18, .F18,  .ftn,  or  .FTN</p> <p>The upper-case file extensions, .F, .FOR, .F90, .F95, .F03, .F08, .F18, or .FTN, will enable source preprocessing by default.</p>"},{"location":"Documentation/Development/Compilers/rosetta_stone/#language-standard-settings-dialect_4","title":"Language standard settings (Dialect)","text":"<p>Standards are determined by the file extension as given above.  Some addttional checks can be performed.</p> <pre><code>-e enable\n\n      b      If enabled, issue a warning message rather than an error\n        message when the compiler detects a call to a procedure\n        with one or more dummy arguments having the TARGET,\n        VOLATILE or ASYNCHRONOUS attribute and there is not an\n        explicit interface definition.\n\n\n      c      Interface checking: use Cray system modules to check\n        library calls in a compilation.  If you have a procedure\n        with the same name as one in the library, you will get\n        errors, as the compiler does not skip  user- specified\n        procedures when performing checks.\n\n\n      C      Enable/disable some types of standard call site\n        checking.  The current Fortran standard requires that the\n        number and types of arguments must agree between the caller\n        and callee.  These constraints are enforced in cases where\n        the compiler can detect them, however, specifying -dC\n        disables some of this error-checking, which may be\n        necessary in order to get some older Fortran codes to\n        compile.\n\n-f source_form free or fixed\n</code></pre>"},{"location":"Documentation/Development/Compilers/rosetta_stone/#language-standard-settings-save_1","title":"Language standard settings (Save)","text":"<pre><code>    -e v    Allocate  variables to static storage.  These variables\n            are treated as if they had appeared in a SAVE statement.  Variables\n            that are explicitly or implicitly defined as automatic variables are\n            not allocated to static storage. The following types of variables are\n            not allocated to static storage: automatic variables (explicitly or\n            implicitly stated), variables declared with the AUTOMATIC attribute,\n            variables allocated in  an  ALLOCATE statement, and local\n            variables in explicit recursive procedures.  Variables with the\n            ALLOCATABLE attribute remain allocated upon procedure exit, unless\n            explicitly deallocated, but they are not allocated in static memory. \n            Variables in explicit recursive procedures consist of those in\n            functions, in subroutines, and in internal procedures within\n            functions and subroutines that have been defined with the RECURSIVE \n            attribute.  The STACK compiler directive overrides this option.\n</code></pre>"},{"location":"Documentation/Development/Compilers/rosetta_stone/#generating-listing_2","title":"Generating listing","text":"<p>-h list=a</p>"},{"location":"Documentation/Development/Compilers/rosetta_stone/#preprocessing_5","title":"Preprocessing","text":"<p>Automatic preprocessing is determined by the file name extension as discussed above. You can manually turn it on/off via the options</p> <p><pre><code> -E    Preprocess and compile\n -eZ   Preprocess and compile\n -eP   Preprocess don' compile\n</code></pre> The Cray Fortran preprocessor has limited functionality.  In particular it does not remove C style comments which can cause compile errors.  You might want to use the gnu preprocessor instead.  </p> <pre><code>gfortran -cpp -E file.F90 &gt; file.f90\nftn file.f80\n</code></pre>"},{"location":"Documentation/Development/Compilers/rosetta_stone/#openmp-support_5","title":"OpenMP support","text":"<pre><code>-homp    Enables OpenMP and links in OpenMP libraries when possible\n    using CCE-Classic.\n\n-hnoomp    Disables OpenMP and links in non-OpenMP libraries when\n    using CCE-classic.\n\nTHE FOLLOWING APPLIE IF THE BACKEND COMPILER IS NOT CRAY FORTRAN.\n\n-fopenmp   Enables OpenMP and links in OpenMP libraries when possible\n    using CCE, AOCC, and GNU.\n\n-openmp    Enables OpenMP and links in OpenMP libraries when\n    possible.\n\n-noopenmp       Disables OpenMP.\n\n-mp        Enables OpenMP and links in OpenMP libraries when\n    possible using PGI.\n\n-Mnoopenmp  Disables OpenMP and links in non-OpenMP libraries when\n    using PGI.\n\n-qopenmp     Enables OpenMP and links in OpenMP libraries when\n    possible when using Intel.\n\n-qno-openmp  Disables OpenMP and links in non-OpenMP libraries\n    when possible when using Intel.\n</code></pre>"},{"location":"Documentation/Development/Compilers/rosetta_stone/#openacc-support_5","title":"OpenACC support","text":"<pre><code> -h acc         \n</code></pre>"},{"location":"Documentation/Development/Compilers/rosetta_stone/#coarray","title":"Coarray","text":"<p>The -h pgas_runtime option directs the compiler driver to link with the runtime libraries required when linking programs that use UPC or coarrays.  In general, a resource manager job launcher such as aprun  or                      srun must be used to launch the resulting executable.</p>"},{"location":"Documentation/Development/Compilers/rosetta_stone/#important-compiler-specific-options_3","title":"Important compiler specific options","text":"<pre><code>-e I      Treat all variables as if an IMPLICIT NONE statement had been specified. \n</code></pre>"},{"location":"Documentation/Development/Containers/","title":"Introduction to Software Containerization","text":""},{"location":"Documentation/Development/Containers/#what-are-software-imagescontainers","title":"What are software images/containers?","text":"<p>Software images provide a method of packaging your code so that its container can be run anywhere you have a container runtime environment. This enables you to create an image on your local laptop and then run it on an HPC system or other computing resource. Software containerization provides an alternative, more robust method of isolating and packaging your code compared to solutions such as Conda virtual environments. </p> <p>A note on terminology: A software container is considered an instance of an image, meaning the former gets created during the runtime of the latter. In other words, a software image is what you build and distribute, whereas the container is what gets executed from a given image.</p>"},{"location":"Documentation/Development/Containers/#docker-vs-apptainer","title":"Docker vs. Apptainer","text":"<p>The most common container runtime environment (outside of HPC) is Docker. Due to the fact that it requires root-level permissions to build its associated images and run containers, Docker is not suited for HPC environments and is therefore not available on NREL's systems currently. Apptainer is an alternative containerization tool that can be used in HPC environments because running it does not require root. However, you can use Docker to build images locally and convert them to the Apptainer format for use with HPC (described in more detail here).</p>"},{"location":"Documentation/Development/Containers/#compatibility","title":"Compatibility","text":"<p>Apptainer is able to run most Docker images, but Docker is unable to run Apptainer images. A key consideration when deciding to containerize an application is which container engine to build with. A suggested best practice is to build images with Docker whenever possible, as this provides more flexibility. However, if this is not feasible, you may have to build with Apptainer or maintain separate images for each container engine.</p>"},{"location":"Documentation/Development/Containers/#advantages-to-software-containerization","title":"Advantages to software containerization","text":"<ul> <li>Portability: Containers can be run on HPC, locally, and on cloud infrastructure used at NREL. </li> <li>Reproducibility: Containers are one option to ensure reproducible research by packaging all necessary software to reproduce an analysis. Containers are also easily versioned using a hash.</li> <li>Modularity: Images are composed of cacheable \"layers\" of other images or build commands, facilitating the image building process.</li> <li>Workflow integration: Workflow management systems such as Airflow, Nextflow, Luigi, and others provide built-in integration with container engines. </li> </ul>"},{"location":"Documentation/Development/Containers/#accessing-hpc-hardware-from-software-containers","title":"Accessing HPC hardware from software containers","text":"<p>Both Apptainer and Docker provide the ability to use hardware based features on the HPC systems such as GPUs. A common usage of containers is packaging GPU-enabled tools such as TensorFlow. Apptainer natively provides access to the GPU and driver on the host. Please visit our documentation on accessing GPUs from Apptainer images for more information. In principle, the MPI installations can be also be accessed from correctly configured containers, but care is also needed to ensure compatibility between the libraries on the host and container.</p>"},{"location":"Documentation/Development/Containers/#building-software-images","title":"Building software images","text":"<p>Regardless of the runtime platform, images are built from a special configuration file. A <code>Dockerfile</code> is such a configuration for Docker, while Apptainer uses a \"Definition File\" (with a <code>.def</code> extension). These files specify the installation routines necessary to create the desired application, as well as any additional software packages to install and configure in this environment that may be required. You can think of these files as \"recipes\" for installing a given application you wish to containerize.</p> <p>Building Docker or Apptainer images requires root/admin privileges and cannot be done directly by users of HPC systems. Docker is available on most platforms, and users with admin privileges on a local machine (such as your laptop) can build Docker images locally. The Docker image file can then be pushed to a registry and pulled on the HPC system using Apptainer as described here, or a tool such as Docker2Singularity may be used to convert the image to the Apptainer format. Alternatively, users with admin privileges on a Linux system can run Apptainer locally to build images. Another option is to use Sylab's remote building Container Service, which provides free accounts with a limited amount of build time for Apptainer-formatted images.</p>"},{"location":"Documentation/Development/Containers/#example-docker-build-workflow-for-hpc-users","title":"Example Docker build workflow for HPC users","text":"<p>Because of the permission limitations described above, it is recommended that HPC users start with building a Docker image locally, e.g., on your laptop. If you are a researcher at NREL and plan to regularly containerize applications, you can request Docker to be installed at the admin-level on your work computer from the IT Service Portal. This section will describe a simple workflow for building a Docker image locally, exporting it as a <code>.tar</code> file, uploading it to Kestrel, and converting it to an Apptainer image for execution on HPC.</p>"},{"location":"Documentation/Development/Containers/#1-local-docker-build","title":"1. Local Docker build","text":"<p>The following Dockerfile illustrates the build steps to create a small image. In this example, we simply install <code>python3</code> into an image based on the Ubuntu operating system (version 22.04):</p> <pre><code># Docker example: save as `Dockerfile` in your working directory\n\nFROM ubuntu:22.04\n\nRUN apt-get update -y &amp;&amp; apt-get install python3 -y\n</code></pre> <p>Images are normally built (or \"bootstrapped\") from a base image indicated by <code>FROM</code>. This base image is composed of one or more layers that will be pulled from the appropriate container registry during buildtime. In this example, version 22.04 of the Ubuntu operating system is specified as the base image. Docker pulls from Ubuntu's DockerHub container registry by default. The ability to use a different base image provides a way to use packages which may work more easily on a specific operating system distribution. For example, the Linux distribution on Kestrel is Red Hat, so building the above image would allow the user to install packages from Ubuntu repositories.</p> <p>The <code>RUN</code> portion of the above Dockerfile indicates the command to run during the image's buildtime. In this example, it installs the Python 3 package. Additional commands such as <code>COPY</code>, <code>ENV</code>, and others enable the customization of your image to suit your compute environment requirements. </p> <p>To build an image from the above Dockerfile (we will call it \"simple_python3\"), copy its contents to a file named <code>Dockerfile</code> in your current working directory and run the following:</p> <pre><code>docker build . -t simple_python3 --platform=linux/amd64\n</code></pre> <p>It is important to note that without the <code>--platform</code> option, <code>docker build</code> will create an image that matches your local machine's CPU chip architecture by default. If you have a machine running on <code>x86-64</code>/<code>amd64</code>, the container's architecture will be compatible NREL's HPC systems. If your computer does not use chips like these (such as if you have a Mac computer that runs on \"Apple Silicon\", which uses <code>arm64</code>), your image's architecture will not match what is found on NREL's HPC systems, causing performance degradation of its containers (at best) or fatal errors (at worst) during runtime on Kestrel, Swift, or Vermillion. Regardless of your local machine, as a best practice, you should explicitly specify your image's desired platform during buildtime with <code>--platform=linux/amd64</code> to ensure compatibility on NREL's HPC systems.</p>"},{"location":"Documentation/Development/Containers/#2-export-docker-image-to-tar","title":"2. Export Docker image to .tar","text":"<p>Coming soon: a centralized software image registry/repository for NREL users, which will simplify the following steps. In the meantime, please follow steps 2 and 3 as written. </p> <p>Once the Docker image is built, you can export it to a <code>.tar</code> archive with the following command:</p> <pre><code>docker image save simple_python3 -o simple_python3.tar\n</code></pre> <p>Depending on the specific application you are building, exported images can be relatively large (up to tens of GB). For this reason, you may wish to gzip/compress the <code>.tar</code> to a <code>.tar.gz</code>, which will save network bandwidth and ultimately reduce total transfer time:</p> <pre><code>tar czf simple_python3.tar.gz simple_python3.tar\n</code></pre>"},{"location":"Documentation/Development/Containers/#3-upload-exported-image-in-targz-format-to-hpc-system","title":"3. Upload exported image in <code>.tar.gz</code> format to HPC system","text":"<p>Now that the exported Docker image is compressed to <code>.tar.gz</code> format, you will need to transfer it to one of NREL's HPC systems. Considering the scratch space of Kestrel as an example destination, we will use <code>rsync</code> as the transfer method. Be sure to replace <code>USERNAME</code> with your unique HPC username:</p> <pre><code>rsync -aP --no-g simple_python3.tar.gz USERNAME@kestrel.hpc.nrel.gov:/scratch/USERNAME/\n</code></pre> <p>For more information on alternatives to <code>rsync</code> (such as FileZilla or Globus), please refer to our documentation regarding file transfers.</p>"},{"location":"Documentation/Development/Containers/#4-convert-tar-to-apptainer-image","title":"4. Convert .tar to Apptainer image","text":"<p>Once <code>rsync</code> finishes, you should find the following file (roughly 72MB in size) in your personal scratch folder on Kestrel (i.e., <code>/scratch/$USER</code>):</p> <pre><code>[USERNAME@kl1 USERNAME]$ ls -lh /scratch/$USER/simple_python3.tar.gz\n-rw-r--r-- 1 USERNAME USERNAME 72M Mar 20 15:39 /scratch/USERNAME/simple_python3.tar.gz\n</code></pre> <p>The next step is to convert this \"Docker archive\" to an Apptainer-compatible image. Especially for larger images, this can be a memory-intensive process, so we will first request a job from Slurm, e.g.:</p> <pre><code>salloc -A &lt;account&gt; -p &lt;partition&gt; -t &lt;time&gt; ...\n</code></pre> <p>You can now convert the Docker image archive to an Apptainer <code>.sif</code> image on Kestrel with the following <code>build</code> command. Be sure to first unzip the <code>.tar.gz</code> archive, and prefix the resulting <code>.tar</code> with <code>docker-archive://</code>:</p> <pre><code>cd /scratch/$USER\nmodule load apptainer/1.1.9\ntar xzf simple_python3.tar.gz simple_python3.tar\napptainer build simple_python3.sif docker-archive://simple_python3.tar\n</code></pre> <p>Once this finishes, you can invoke the container with <code>apptainer exec simple_python3.sif &lt;command&gt;</code>. Anything that follows the name of the image will be executed from the container, even if the same command is found on the host system. To illustrate, if we examine the location of the <code>python3</code> binary within the <code>simple_python3.sif</code> image and the host system (Kestrel), we see they are both called from the location <code>/usr/bin/python3</code>:</p> <pre><code># host's Python3\n[USERNAME@COMPUTE_NODE USERNAME]$ which python3\n/usr/bin/python3\n\n# container's Python3\n[USERNAME@COMPUTE_NODE USERNAME]$ apptainer exec simple_python3.sif which python3\n/usr/bin/python3\n</code></pre> <p>However, the <code>apt-get install python3</code> command in the Dockerfile should have installed the most up-to-date Python3 library from Ubuntu's package manager, which is 3.10.12 at the time this is written. By contrast, the Python3 library installed on the host is older (version 3.6.8). In this way, we can confirm that the <code>python3</code> executed with <code>apptainer exec ...</code> is indeed originating from <code>simple_python3.sif</code>:</p> <pre><code># host Python3\n[USERNAME@COMPUTE_NODE USERNAME]$ python3 --version\nPython 3.6.8\n\n# container Python3\n[USERNAME@COMPUTE_NODE USERNAME]$ apptainer exec simple_python3.sif python3 --version\nPython 3.10.12\n</code></pre> <p>For more specific information on and best practices for using Apptainer on NREL's HPC systems, please refer to its dedicated documentation page.</p>"},{"location":"Documentation/Development/Containers/#5-a-more-involved-dockerfile-example-cuda-124","title":"5. A more involved Dockerfile example (CUDA 12.4)","text":"<p>For an example of an image you can build to provide everything needed for CUDA v.12.4, please refer to this Dockerfile.</p>"},{"location":"Documentation/Development/Containers/#using-apptainer-as-build-alternatives-to-docker","title":"Using Apptainer as build alternatives to Docker","text":"<p>Given Docker's popularity, support, and its widespread compatibility with other container runtimes, it is recommended to start your containerization journey with the steps outlined in the previous section. However, there could be rare cases in which you need to directly build an image with Apptainer. Instead of \"Dockerfiles\", these container runtimes use \"Definition Files\" for image building that have a similar, yet distinct format. Please refer to the respective link for more information. We also provide an Apptainer image build example in our documentation, which can be remotely built via the Singularity Container Service from Sylabs, the developer of Apptainer.</p>"},{"location":"Documentation/Development/Containers/apptainer/","title":"Apptainer","text":"<p>Note</p> <p>Singularity has been deprecated in favor of a new container runtime environment called Apptainer, which is its direct decendent. Apptainer will run Singularity containers and it supports Singularity commands by default. Since Singularity is deprecated, it is advised to use Apptainer when building new images. More information about Apptainer can be found at https://apptainer.org. </p>"},{"location":"Documentation/Development/Containers/apptainer/#how-to-use-apptainer","title":"How to use Apptainer","text":"<p>On NREL HPC systems, Apptainer is accessed via a module named <code>apptainer</code> (you can check the current default module via <code>ml -d av apptainer</code>). On Kestrel specifically, the directory <code>/nopt/nrel/apps/software/apptainer/1.1.9/examples</code> holds a number of images (<code>*.sif</code>) and an example script (<code>script</code>) that shows how to run containers hosting MPI programs across multiple nodes. The <code>script</code> can also be accessed from our GitHub repository.</p> <p>Before we get to the more complicated example from <code>script</code>, we'll first look at downloading (or pulling) and working with a simple image. The following examples assume you are logged into Kestrel, but the concepts demonstrated are still valid for any host system on which you wish to execute a container.</p> <p>Input commands are preceded by a <code>$</code>.</p> <p>Note</p> <p>If you wish to containerize your own application, it may be worth starting with building a local Docker image and transferring it to Kestrel before attempting to directly create your own Apptainer image, since you do not have root access on HPC systems.</p>"},{"location":"Documentation/Development/Containers/apptainer/#apptainer-runtime-examples","title":"Apptainer runtime examples","text":""},{"location":"Documentation/Development/Containers/apptainer/#run-hello-world-ubuntu-image","title":"Run hello-world Ubuntu image","text":""},{"location":"Documentation/Development/Containers/apptainer/#allocate-a-compute-node","title":"Allocate a compute node.","text":"<pre><code>$ ssh USERNAME@kestrel.hpc.nrel.gov\n[USERNAME@kl1 ~]$ salloc --exclusive --mem=0 --tasks-per-node=104 --nodes=1 --time=01:00:00 --account=MYACCOUNT --partition=debug\n[USERNAME@x1000c0s0b0n0 ~]$ cat /etc/redhat-release\nRed Hat Enterprise Linux release 8.6 (Ootpa)\n</code></pre>"},{"location":"Documentation/Development/Containers/apptainer/#load-the-apptainer-module","title":"Load the apptainer module","text":"<p><pre><code>[USERNAME@x1000c0s0b0n0 ~]$ module purge\n[USERNAME@x1000c0s0b0n0 ~]$ ml -d av apptainer\n------------------------------------ /nopt/nrel/apps/modules/default/application ------------------------------------\n   apptainer/1.1.9\n[USERNAME@x1000c0s0b0n0 ~]$ module load apptainer/1.1.9\n</code></pre> Note: at the time of writing, <code>apptainer/1.1.9</code> is the default Apptainer module on Kestrel as determined by running <code>ml -d av apptainer</code>.</p>"},{"location":"Documentation/Development/Containers/apptainer/#retrieve-hello-world-image-be-sure-to-use-scratch-as-images-are-typically-large","title":"Retrieve hello-world image.  Be sure to use /scratch as images are typically large","text":"<pre><code>[USERNAME@x1000c0s0b0n0 ~]$ cd /scratch/$USER\n[USERNAME@x1000c0s0b0n0 USERNAME]$ mkdir -p apptainer-images\n[USERNAME@x1000c0s0b0n0 USERNAME]$ cd apptainer-images\n[USERNAME@x1000c0s0b0n0 apptainer-images]$ apptainer pull --name hello-world.simg shub://vsoch/hello-world\nProgress |===================================| 100.0%\n</code></pre>"},{"location":"Documentation/Development/Containers/apptainer/#explore-image-details","title":"Explore image details","text":"<pre><code>[USERNAME@x1000c0s0b0n0 apptainer-images]$ apptainer inspect hello-world.simg # Shows labels\n{\n    \"org.label-schema.usage.apptainer.deffile.bootstrap\": \"docker\",\n    \"MAINTAINER\": \"vanessasaur\",\n    \"org.label-schema.usage.apptainer.deffile\": \"apptainer\",\n    \"org.label-schema.schema-version\": \"1.0\",\n    \"WHATAMI\": \"dinosaur\",\n    \"org.label-schema.usage.apptainer.deffile.from\": \"ubuntu:14.04\",\n    \"org.label-schema.build-date\": \"2017-10-15T12:52:56+00:00\",\n    \"org.label-schema.usage.apptainer.version\": \"2.4-feature-squashbuild-secbuild.g780c84d\",\n    \"org.label-schema.build-size\": \"333MB\"\n}\n[USERNAME@x1000c0s0b0n0 apptainer-images]$ apptainer inspect -r hello-world.simg # Shows the script run\n#!/bin/sh\n\nexec /bin/bash /rawr.sh\n</code></pre>"},{"location":"Documentation/Development/Containers/apptainer/#run-image-default-script","title":"Run image default script","text":"<pre><code>[USERNAME@x1000c0s0b0n0 apptainer-images]$ apptainer run hello-world.simg\nRaawwWWWWWRRRR!! Avocado!\n</code></pre>"},{"location":"Documentation/Development/Containers/apptainer/#run-images-containing-mpi-programs-on-multiple-nodes","title":"Run images containing MPI programs on multiple nodes","text":"<p>As mentioned above, there is a script in the apptainer directory that shows how MPI applications built inside an image can be run on multiple nodes. We'll run 5 containers with different versions of MPI. Each container has two MPI programs installed, a glorified Hello World (<code>phostone</code>) and PingPong (<code>ppong</code>). The 5 versions of MPI are:</p> <ol> <li>openmpi</li> <li>IntelMPI</li> <li>MPICH - with ch4</li> <li>MPICH - with ch4 with different compile options</li> <li>MPICH - with ch3</li> </ol> <p>\"ch*\" can be thought as a \"lower level\" communications protocol. A MPICH container might be built with either but we have found that ch4 is considerably faster on Kestrel. </p> <p>The script can be found at <code>/nopt/nrel/apps/software/apptainer/1.1.9/examples/script</code>, as well as our GitHub repository.</p> <p>Here is a copy:</p> Sample job script: Running MPI-enabled Apptainer containers <pre><code>#!/bin/bash \n#SBATCH --job-name=\"apptainer\"\n#SBATCH --nodes=2\n#SBATCH --ntasks-per-node=2\n#SBATCH --exclusive\n#SBATCH --export=ALL\n#SBATCH --time=02:00:00\n#SBATCH --output=apptainer.log\n#SBATCH --mem=0\n\nexport STARTDIR=`pwd`\nexport CDIR=/nopt/nrel/apps/software/apptainer/1.1.9/examples\nmkdir $SLURM_JOB_ID\ncd $SLURM_JOB_ID\n\ncat $0 &gt;   script\nprintenv &gt; env\n\ntouch warnings\ntouch output\n\nmodule load apptainer\nwhich apptainer &gt;&gt; output\n\necho \"hostname\" &gt;&gt; output\nhostname        &gt;&gt; output\n\necho \"from alpine.sif\" &gt;&gt; output\n          apptainer exec $CDIR/alpine.sif hostname  &gt;&gt; output\necho \"from alpine.sif with srun\" &gt;&gt; output\nsrun -n 1 --nodes=1 apptainer exec $CDIR/alpine.sif cat /etc/os-release  &gt;&gt; output\n\n\nexport OMP_NUM_THREADS=2\n\n$CDIR/tymer times starting\n\nMPI=pmix\nfor v in openmpi intel mpich_ch4 mpich_ch4b  mpich_ch3; do\n  srun  --mpi=$MPI   apptainer  exec   $CDIR/$v.sif  /opt/examples/affinity/tds/phostone -F &gt;  phost.$v  2&gt;&gt;warnings\n  $CDIR/tymer times $v\n  MPI=pmi2\n  unset PMIX_MCA_gds\ndone\n\nMPI=pmix\n#skip mpich_ch3 because it is very slow\nfor v in openmpi intel mpich_ch4 mpich_ch4b           ; do\n  srun  --mpi=$MPI   apptainer  exec   $CDIR/$v.sif  /opt/examples/affinity/tds/ppong&gt;  ppong.$v  2&gt;&gt;warnings\n  $CDIR/tymer times $v\n  MPI=pmi2\n  unset PMIX_MCA_gds\ndone\n\n$CDIR/tymer times finished\n\nmv $STARTDIR/apptainer.log .\n</code></pre> <p>We set the variable <code>CDIR</code> which points to the directory from which we will get our containers.</p> <p>We next create a directory for our run and go there. The <code>cat</code> and <code>printenv</code> commands give us a copy of our script and the environment in which we are running. This is useful for debugging.</p> <p>Before we run the MPI containers, we run the command <code>hostname</code> from inside a very simple container <code>alpine.sif</code>. We show containers can be run without/with <code>srun</code>. In the second instance we <code>cat /etc/os-release</code> to show we are running a different OS.  </p> <p>Then we get into the MPI containers. This is done in a loop over containers containing the MPI versions: <code>openmpi</code>, <code>intelmpi</code>, <code>mpich_ch4</code>, <code>mpich_ch4b</code>, and <code>mpich_ch3</code>. </p> <p>The application <code>tymer</code> is a simple wall clock timer.  </p> <p>The <code>--mpi=</code> option on the srun line instructs slurm how to launch jobs. The normal option is <code>--mpi=pmi2</code>. However, containers using OpenMPI might need to use the option <code>--mpi=pmix</code> as we do here.</p> <p>The first loop just runs a quick \"hello world\" example. The second loop runs a pingpong test. We skip the <code>mpich_ch3</code> pingpong test because it runs very slowly.</p> <p>You can see example output from this script in the directory:</p> <pre><code>/nopt/nrel/apps/software/apptainer/1.1.9/examples/output/\n</code></pre> <p>Within <code>/nopt/nrel/apps/software/apptainer/1.1.9/examples</code>, the subdirectory <code>defs</code> contains the recipes for the images in <code>examples</code>. The images <code>apptainer.sif</code> and <code>intel.sif</code> were built in two steps using <code>app_base.def</code> - apptainer.def and mods_intel.def - intel.def. They can also be found in the HPC code examples repository.</p> <p>The script <code>sif2def</code> can be used to generate a <code>.def</code> recipe from a <code>.sif</code> image. It has not been extensively tested, so it may not work for all images and is provided here \"as is.\"</p>"},{"location":"Documentation/Development/Containers/apptainer/#apptainer-buildtime-examples","title":"Apptainer buildtime examples","text":""},{"location":"Documentation/Development/Containers/apptainer/#create-ubuntu-based-image-with-mpi-support","title":"Create Ubuntu-based image with MPI support","text":"<p>Apptainer images can be generated from a <code>.def</code> recipe. </p> <p>This example shows how to create an Apptainer image running on the Ubuntu operating system with openmpi installed. The recipe is shown in pieces to make it easier to describe what each section does. The complete recipe can be found in the <code>defs</code> subdirectory of <code>/nopt/nrel/apps/software/apptainer/1.1.9/examples</code>. Building images requires root/admin privileges, so the build process must be run on a user's computer with apptainer installed or via the Singularity Container Service. After creation, the image can be copied to Kestrel and run.</p>"},{"location":"Documentation/Development/Containers/apptainer/#create-a-new-recipe-based-on-ubuntulatest","title":"Create a new recipe based on ubuntu:latest","text":"<pre><code>Bootstrap: docker\nfrom: ubuntu:latest\n</code></pre>"},{"location":"Documentation/Development/Containers/apptainer/#add-ld_library_path-usrlocallib-used-by-openmpi","title":"Add LD_LIBRARY_PATH /usr/local/lib used by OpenMPI","text":"<pre><code>%environment\n    export LD_LIBRARY_PATH=/usr/local/lib:$LD_LIBRARY_PATH\n    export PMIX_MCA_gds=^ds12\n</code></pre>"},{"location":"Documentation/Development/Containers/apptainer/#install-development-tools-after-bootstrap-is-created","title":"Install development tools after bootstrap is created","text":"<pre><code>%post\n    echo \"Installing basic development packages...\"\n    export DEBIAN_FRONTEND=noninteractive\n    apt-get update\n    apt-get install -y bash gcc g++ gfortran make curl python3\n</code></pre>"},{"location":"Documentation/Development/Containers/apptainer/#download-compile-and-install-openmpi","title":"Download, compile and install openmpi.","text":"<pre><code>    echo \"Installing OPENMPI...\"\n    curl https://download.open-mpi.org/release/open-mpi/v4.1/openmpi-4.1.5.tar.gz --output openmpi-4.1.5.tar.gz\n    mkdir -p /opt/openmpi/src\n    tar -xzf openmpi-4.1.5.tar.gz -C /opt/openmpi/src\n    cd /opt/openmpi/src/*\n    ./configure \n    make install\n</code></pre>"},{"location":"Documentation/Development/Containers/apptainer/#compile-and-install-example-mpi-application","title":"Compile and install example MPI application","text":"<pre><code>    echo \"Build OPENMPI example...\"\n    export LD_LIBRARY_PATH=/usr/local/lib:$LD_LIBRARY_PATH\n    cd /opt/openmpi/src/*/examples\n    mpicc ring_c.c -o /usr/bin/ring\n</code></pre>"},{"location":"Documentation/Development/Containers/apptainer/#set-default-script-to-run-ring","title":"Set default script to run ring","text":"<pre><code>  /usr/bin/ring\n</code></pre>"},{"location":"Documentation/Development/Containers/apptainer/#example-build-image-command-must-have-root-access","title":"Example Build image command (must have root access)","text":"<pre><code>sudo $(type -p apptainer) build small.sif  ubuntu-mpi.def\n</code></pre>"},{"location":"Documentation/Development/Containers/apptainer/#test-image","title":"Test image","text":"<pre><code>[kuser@kl1 ~]$ salloc --exclusive --mem=0 --tasks-per-node=104 --nodes=2 --time=01:00:00 --account=MYACCOUNT --partition=debug\nsalloc: Granted job allocation 90367\nsalloc: Waiting for resource configuration\nsalloc: Nodes x3000c0s25b0n0,x3000c0s27b0n0 are ready for job\n[kuser@x3000c0s25b0n0 ~]$ module load apptainer \n[kuser@x3000c0s25b0n0 ~]$ srun -n 8 --tasks-per-node=4 --mpi=pmix apptainer run small.sif\nProcess 2 exiting\nProcess 3 exiting\nProcess 0 sending 10 to 1, tag 201 (8 processes in ring)\nProcess 0 sent to 1\nProcess 0 decremented value: 9\nProcess 0 decremented value: 8\nProcess 0 decremented value: 7\nProcess 0 decremented value: 6\nProcess 0 decremented value: 5\nProcess 0 decremented value: 4\nProcess 0 decremented value: 3\nProcess 0 decremented value: 2\nProcess 0 decremented value: 1\nProcess 0 decremented value: 0\nProcess 0 exiting\nProcess 1 exiting\nProcess 5 exiting\nProcess 6 exiting\nProcess 7 exiting\nProcess 4 exiting\n[kuser@x3000c0s25b0n0 ~]$\n</code></pre>"},{"location":"Documentation/Development/Containers/apptainer/#utilizing-gpu-resources-with-apptainer-images","title":"Utilizing GPU resources with Apptainer images","text":"<p>GPU-accelerated software often have complex software and hardware requirements to function properly, making containerization a particularly attractive option for deployment and use. These requirements manifest themselves as you are building your image (buildtime) and when you run a container (runtime). This section describes key components of software images that are successfully GPU-enabled with a Tensorflow container example. For more detailed documentation on the subject, visit Apptainer's dedicated GPU Support page.</p>"},{"location":"Documentation/Development/Containers/apptainer/#tensorflow-container-example","title":"Tensorflow Container Example","text":""},{"location":"Documentation/Development/Containers/apptainer/#1-pull-a-compatible-version-of-gpu-enabled-tensorflow-from-dockerhub","title":"1. Pull a compatible version of GPU-enabled Tensorflow from DockerHub","text":"<p>There are several versions (tags) of Tensorflow images available from the DockerHub container registry, each with different versions of GPU drivers and CUDA. You can obtain this information from the host by running the command <code>nvidia-smi</code> after allocating a GPU within a Slurm job on your desired system. Alternatively, you could simply consult the table below. If your running container is installed with a different GPU driver/CUDA version than what is listed below for your target system, you will either run into a fatal error, or the software will bypass the GPU and run on the CPU, slowing computation.</p> System Partition name GPU type(cards per node) <code>nvidia-smi</code>GPU driver version CUDA Version Kestrel gpu-h100 H100 (4) 550.54.15 12.4 Swift gpu A100 (4) 550.54.15 12.4 Vermilion gpu A100 (1) 460.106.00 11.2 <p>Kestrel's H100 GPUs run with CUDA 12.4 with a GPU driver version of 550.54.15. Most GPU-enabled applications are compatible with a given major version release of CUDA; for example, if an application requires CUDA/12.4, it will more than likely work with other versions of CUDA &gt;= 12.0. So for this example on Kestrel, we are looking for a Tensorflow image tag that includes as close to CUDA/12.4 as we can. On DockerHub, we see from consulting the layers of <code>tensorflow:2.15.0-gpu</code> that this image fits our requirements (note line 14: <code>ENV CUDA_VERSION=12.3.0</code>). At the time of writing, a Tensorflow image with CUDA/12.4 is not yet available from this DockerHub repository.</p> <p>First, allocate a Kestrel GPU compute node:</p> <pre><code>salloc -A &lt;YOUR-ACCOUNT&gt; -t 1:00:00 --gpus=1 -N 1 -n 1 --mem-per-cpu=8G\n</code></pre> <p>Note</p> <p>We are only requesting 1 GPU card (<code>--gpus=1</code>) of the 4 available per node, and subsequently 1 task (<code>-n 1</code>). Though we are automatically given access to all of the GPU memory on the node, we request 8G of CPU memory from <code>salloc</code>. This is because our Tensorflow example will require a decent amount of CPU memory as it copies data to and from the GPU device. If such CPU memory is a bottleneck in a real-world example, you may want to consider replacing <code>-n 1 --mem-per-cpu=8G</code> with <code>--exclusive --mem=350G</code> to request all of the node's CPU resources, even if you are only using a single GPU card. You can request up to 700G of CPU RAM. </p> <p>Once we are allocated a node, we will load the Apptainer module, and then pull <code>tensorflow:2.15.0-gpu</code> from DockerHub to a personal scratch location on Kestrel.</p> <pre><code>module load apptainer\napptainer pull /scratch/$USER/tensorflow-2.15.0.sif docker://tensorflow/tensorflow:2.15.0-gpu\n</code></pre> <p>Once the image finishes pulling, we can see a new <code>.sif</code> file in <code>/scratch/$USER</code>:</p> <pre><code>ls -lh /scratch/$USER/tensorflow-2.15.0.sif\n-rwxrwxr-x 1 USERNAME USERNAME 3.4G Apr  3 11:49 /scratch/USERNAME/tensorflow-2.15.0.sif\n</code></pre> <p>Note</p> <p>We recommend saving <code>.sif</code> files to <code>/scratch</code> or <code>/projects</code> whenever feasible, as these images tend to be large, sometimes approaching tens of GB.</p>"},{"location":"Documentation/Development/Containers/apptainer/#2-verify-gpu-device-is-found","title":"2. Verify GPU device is found","text":""},{"location":"Documentation/Development/Containers/apptainer/#recognizing-gpu-device-from-slurm","title":"Recognizing GPU device from Slurm","text":"<p>As a reminder, we only requested 1 GPU card in our <code>salloc</code> command above. On the Slurm side of things, we can verify this device is accessible to our computing environment by examining the contents of the <code>SLURM_GPUS_ON_NODE</code> (the number of allocated GPU cards) and <code>SLURM_JOB_GPUS</code> (the device's ID). By grepping for <code>GPU</code> from our list of environmental variables, we can see that Slurm indeed recognizes a single GPU device with ID <code>0</code>:</p> <pre><code>env | grep GPU\nSLURM_GPUS_ON_NODE=1\nSLURM_JOB_GPUS=0\n</code></pre>"},{"location":"Documentation/Development/Containers/apptainer/#recognizing-gpu-device-from-the-container","title":"Recognizing GPU device from the container","text":"<p>It is important to note that just because Slurm has allocated this device, it doesn't necessarily mean that the Tensorflow container can recognize it. Let's now verify that a GPU is accessible on the containerized Python side of things. We will invoke <code>/scratch/$USER/tensorflow-2.15.0.sif</code> to see whether Tensorflow itself can use the GPU allocated by Slurm:</p> <pre><code>apptainer exec /scratch/$USER/tensorflow-2.15.0.sif python -c \"import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))\"\n</code></pre> <p>Oh no! You should see this error as the output from the above command:</p> <pre><code>libcuda reported version is: NOT_FOUND: was unable to find libcuda.so DSO loaded into this program\n</code></pre> <p>What happened here - didn't we pull a Tensorflow image that contains CUDA/12.3? We did, but whenever you run GPU-enabled Apptainers, it is critical to supply the <code>--nv</code> flag after <code>exec</code>, otherwise the GPU device(s) will not be found. You can read more about what <code>--nv</code> does here.</p> <p>Let's try finding this device from Python again, this time after supplying <code>--nv</code> to the container runtime:</p> <pre><code>apptainer exec --nv /scratch/$USER/tensorflow-2.15.0.sif python -c \"import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))\"\n</code></pre> <p>That's better! We can now see that GPU device <code>0</code> as allocated by Slurm is accessible to Tensorflow:</p> <pre><code>[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n</code></pre>"},{"location":"Documentation/Development/Containers/apptainer/#3-run-example-tensorflow-training-script","title":"3. Run example Tensorflow training script","text":"<p>Now we are ready to run the Tensorflow container. We will run the script below, which is based on Tensorflow's advanced quickstart example. This script tests a model that is trained on the mnist example dataset.</p> Python script: Simple GPU Tensorflow train and test <pre><code>import time\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Flatten, Conv2D\nfrom tensorflow.keras import Model\n\n# source: https://www.tensorflow.org/tutorials/quickstart/advanced\n\n### load mnist dataset\nmnist = tf.keras.datasets.mnist\n\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\nx_train, x_test = x_train / 255.0, x_test / 255.0\n\n# Add a channels dimension\nx_train = x_train[..., tf.newaxis].astype(\"float32\")\nx_test = x_test[..., tf.newaxis].astype(\"float32\")\n\n### Use tf.data to batch and shuffle the dataset\ntrain_ds = tf.data.Dataset.from_tensor_slices(\n    (x_train, y_train)).shuffle(10000).batch(32)\n\ntest_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(32)\n\nclass MyModel(Model):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = Conv2D(32, 3, activation='relu')\n        self.flatten = Flatten()\n        self.d1 = Dense(128, activation='relu')\n        self.d2 = Dense(10)\n\n    def call(self, x):\n        x = self.conv1(x)\n        x = self.flatten(x)\n        x = self.d1(x)\n        return self.d2(x)\n\n# Create an instance of the model\nmodel = MyModel()\n\n### optimizer/loss function\nloss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\noptimizer = tf.keras.optimizers.Adam()\n\n### Select metrics to measure the loss and the accuracy of the model. These metrics accumulate the values over epochs and then print the overall result.\ntrain_loss = tf.keras.metrics.Mean(name='train_loss')\ntrain_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n\ntest_loss = tf.keras.metrics.Mean(name='test_loss')\ntest_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')\n\n### train the model\n@tf.function\ndef train_step(images, labels):\n    with tf.GradientTape() as tape:\n        # training=True is only needed if there are layers with different\n        # behavior during training versus inference (e.g. Dropout).\n        predictions = model(images, training=True)\n        loss = loss_object(labels, predictions)\n    gradients = tape.gradient(loss, model.trainable_variables)\n    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n\n    train_loss(loss)\n    train_accuracy(labels, predictions)\n\n# test the model\n@tf.function\ndef test_step(images, labels):\n    # training=False is only needed if there are layers with different\n    # behavior during training versus inference (e.g. Dropout).\n    predictions = model(images, training=False)\n    t_loss = loss_object(labels, predictions)\n\n    test_loss(t_loss)\n    test_accuracy(labels, predictions)\n\n\nt0 = time.time()\nEPOCHS = 10\nfor epoch in range(EPOCHS):\n    # Reset the metrics at the start of the next epoch\n    train_loss.reset_states()\n    train_accuracy.reset_states()\n    test_loss.reset_states()\n    test_accuracy.reset_states()\n\n    for images, labels in train_ds:\n        train_step(images, labels)\n\n    for test_images, test_labels in test_ds:\n        test_step(test_images, test_labels)\n\n    print(\n        f'Epoch {epoch + 1}, '\n        f'Loss: {train_loss.result()}, '\n        f'Accuracy: {train_accuracy.result() * 100}, '\n        f'Test Loss: {test_loss.result()}, '\n        f'Test Accuracy: {test_accuracy.result() * 100}'\n    )\nt1 = time.time()\nprint(f'A total of {EPOCHS} epochs took {t1-t0} seconds')\n</code></pre> <p>Save this script as <code>tensor_test.py</code> into your current working directory and run the following command:</p> <pre><code>apptainer exec --nv /scratch/$USER/tensorflow-2.15.0.sif python tensor_test.py\n</code></pre> <p>Assuming you made the same <code>salloc</code> request above, it should take ~26 seconds to run through 10 training/testing epochs on a single GPU. This particular container is sophisticated enough to automatically switch between CPU and GPU computation depending on the availability of a GPU device. If you'd like to compare the time it takes for this script to run purely on a CPU, simply omit the <code>--nv</code> flag from your call to <code>apptainer</code> above and run the command on the same node. You should observe that the runtime jumps to ~252 seconds, meaning that the GPU computation is almost 10 times faster than the CPU!</p>"},{"location":"Documentation/Development/Containers/apptainer/#best-practices-and-recommendations","title":"Best practices and recommendations","text":"<p>This section describes general recommendations and best practices for Apptainer users across NREL's HPC systems.</p>"},{"location":"Documentation/Development/Containers/apptainer/#change-apptainer-cache-location-to-scratchuser","title":"Change Apptainer cache location to <code>/scratch/$USER</code>","text":"<p>By default, Apptainer will cache image layers to your <code>$HOME</code> folder when you pull or build <code>.sif</code> images, which is not ideal as users have a limited storage quota in <code>/home</code>. As you continue to use Apptainer, this cache folder can become quite large and can easily fill your <code>$HOME</code>. Fortunately, the location of this cache folder can be controlled through the <code>APPTAINER_CACHEDIR</code> environmental variable. To avoid overfilling your <code>$HOME</code> with unnecessary cached data, it is recommended to add an <code>APPTAINER_CACHEDIR</code> location to your <code>~/.bashrc</code> file. You can accomplish this with the following command, which will direct these layers to save to a given system's scratch space:</p> <p><code>echo \"export APPTAINER_CACHEDIR=/scratch/$USER/.apptainer\" &gt;&gt; ~/.bashrc</code></p> <p>Note that you will either need to log out and back into the system, or run <code>source ~/.bashrc</code> for the above change to take effect.</p>"},{"location":"Documentation/Development/Containers/apptainer/#save-def-files-to-home-folder-and-images-to-scratch-or-projects","title":"Save <code>.def</code> files to home folder and images to /scratch or /projects","text":"<p>An Apptainer definition file (<code>.def</code>) is a relatively small text file that contains much (if not all) of the build context for a given image. Since your <code>$HOME</code> folders on NREL's HPC systems are regularly backed up, it is strongly recommended to save this file to your home directory in case it accidentally gets deleted or otherwise lost. Since <code>.sif</code> images themselves are 1. typically large and 2. can be rebuilt from the <code>.def</code> files, we recommend saving them to a folder outside of your <code>$HOME</code>, for similar reasons described in the previous section. If you intend to work with an image briefly or intermittantly, it may make sense to save the <code>.sif</code> to your <code>/scratch</code> folder, from which files can be purged if they haven't been accessed for 28 days. If you plan to use an image frequently over time or share it with other users in your allocation, saving it in a <code>/projects</code> location you have access to may be better.</p>"},{"location":"Documentation/Development/Containers/apptainer/#bind-mounting-directories","title":"Bind Mounting Directories","text":"<p>By default, most containers only mount your <code>$HOME</code> folder, current working directory, and a handful of other common folders. If a host directory isn't in this list and isn't explicitly provided during runtime, you may get a \"File not found\" error. For example, if you are running a container from <code>/scratch/$USER</code> and want to write a result file to a <code>/projects</code> location, you will need to provide the mount path with the <code>-B &lt;/path/on/host&gt;:&lt;/path/in/container&gt;</code> option:</p> <pre><code>apptainer -B /projects:/projects --nv exec IMAGE.sif COMMAND &gt; /projects/my-project/result.txt\n</code></pre>"},{"location":"Documentation/Development/Containers/apptainer/#provide-the-nv-flag-to-apptainer-runtime-gpu","title":"Provide the <code>--nv</code> flag to Apptainer Runtime (GPU)","text":"<p>Once you allocate at least one GPU card in your job, you then need to make Apptainer recognize the GPU resources you wish to use. To accomplish this, you can supply the <code>--nv</code> flag to an <code>apptainer shell ...</code> or <code>apptainer exec ...</code> command. Using a generic <code>gpu_accelerated_tensorflow.sif</code> image as an example:</p> <pre><code>apptainer exec --nv gpu_accelerated_tensorflow.sif python tensorflow.py\n</code></pre>"},{"location":"Documentation/Development/Containers/apptainer/#providing-cuda-to-host-environment-gpu","title":"Providing CUDA to Host Environment (GPU)","text":"<p>In the Tensorflow example above, the container included all of the necessary software to run on a GPU, including CUDA. However, depending on the specific software container you are trying to run, its image may or may not include a working version of CUDA. If you encounter CUDA- or GPU-driver errors, try loading version 12.4 of the CUDA module before running the container:</p> <pre><code>module load cuda/12.4\n</code></pre>"},{"location":"Documentation/Development/Containers/registries/","title":"Container registries at NREL","text":""},{"location":"Documentation/Development/Containers/registries/#introduction","title":"Introduction","text":"<p>Container registries enable users to store container images. An overview of the steps to use each fo the main container registries available to NREL users is provided below. Registries can enable reproducibility by storing tagged versions of containers, and also facilitate transferring images easily between different computational resources. </p>"},{"location":"Documentation/Development/Containers/registries/#create-docker-images","title":"Create Docker images","text":"<p>Docker is not supported on NREL's HPC systems, including Kestrel. Instead, Apptainer is the container engine provided as a module. Apptainer is able to pull Docker images and convert them to Apptainer-formatted images. We generally recommend building Docker images to ensure portability between compute resources and using Apptainer to convert the image when running on an HPC system. </p>"},{"location":"Documentation/Development/Containers/registries/#accessibility","title":"Accessibility","text":"Registry Kestrel Access AWS Access Docker Support Apptainer Support Harbor No** No Yes Yes AWS ECR Yes Yes Yes No* DockerHub Yes Yes Yes No* *for DockerHub and AWS ECR it may be possible to push images using ORAS, but this was not found to be a streamlined process in testing. **Harbor was originally set up for Kestrel's predecessor, Eagle. A replacement is being identified."},{"location":"Documentation/Development/Containers/registries/#aws-ecr","title":"AWS ECR","text":"<p>AWS ECR can be utilized by projects with a cloud allocation to host containers. ECR primarily can be used with Docker containers, although Apptainer should also be possible. </p>"},{"location":"Documentation/Development/Containers/registries/#harbor","title":"Harbor","text":"<p>NREL's Harbor is a registry hosted by ITS that supports both Docker and Apptainer containers. Harbor was originally set up for Kestrel's predecessor, Eagle, which also used Apptainer's predecessor, Singularity. **NREL ITS is currently evaluating a replacement to internally hosted Harbor (likely moving to Enterprise DockerHub) The following information is archived until such a replacement is identified for Kestrel.</p>"},{"location":"Documentation/Development/Containers/registries/#docker","title":"Docker","text":""},{"location":"Documentation/Development/Containers/registries/#login","title":"Login","text":"<p>On your local machine to push a container to the registry.  <pre><code>docker login harbor.nrel.gov\n</code></pre></p>"},{"location":"Documentation/Development/Containers/registries/#prepare-image-for-push","title":"Prepare image for push","text":"<pre><code>docker tag SOURCE_IMAGE[:TAG] harbor.nrel.gov/REPO/IMAGE[:TAG]\n</code></pre> <pre><code>docker push harbor.nrel.gov/REPO/IMAGE[:TAG]\n</code></pre>"},{"location":"Documentation/Development/Containers/registries/#pull-docker-image-on-eagle","title":"Pull Docker image on Eagle","text":"<p>Pull and convert container to Singularity on Eagle.</p> <p>Note: <code>--nohttps</code> is not optimal but need to add certs for NREL otherwise there is a cert error.  <pre><code>apptainer pull --nohttps --docker-login docker://harbor.nrel.gov/REPO/IMAGE[:TAG]\n</code></pre></p> <p>The container should now be downloaded and usable as usual</p>"},{"location":"Documentation/Development/Containers/registries/#singularity","title":"Singularity","text":""},{"location":"Documentation/Development/Containers/registries/#login-information","title":"Login information","text":"<p>Under your User Profile in Harbor obtain and export the following information <pre><code>export SINGULARITY_DOCKER_USERNAME=&lt;harbor username&gt;\nexport SINGULARITY_DOCKER_PASSWORD=&lt;harbor CLI secret&gt;\n</code></pre></p>"},{"location":"Documentation/Development/Containers/registries/#push-a-singularity-image","title":"Push a Singularity image","text":"<pre><code>singularity push &lt;image&gt;.sif oras://harbor.nrel.gov/&lt;PROJECT&gt;/&lt;IMAGE&gt;:&lt;TAG&gt;\n</code></pre>"},{"location":"Documentation/Development/Containers/registries/#pull-a-singularity-image","title":"Pull a Singularity image","text":"<pre><code>singularity pull oras://harbor.nrel.gov/&lt;PROJECT&gt;/&lt;IMAGE&gt;:&lt;TAG&gt;\n</code></pre>"},{"location":"Documentation/Development/Containers/registries/#dockerhub","title":"DockerHub","text":"<p>An enterprise version of DockerHub is being evaluated and is currently unavailable. However, NREL HPC users are free to pull Docker images with Apptainer directly from the public version of DockerHub. For example, this pulls the official Ubuntu v.22.04 image from DockerHub and converts it to the Apptainer-formatted <code>ubuntu-22.04.sif</code> image:</p> <pre><code>apptainer pull ubuntu-22.04.sif docker://ubuntu:22.04\n</code></pre> <p>Note</p> <p>DockerHub maintains a series of \"official\" images that follow the syntax <code>apptainer pull &lt;name of SIF&gt; docker://&lt;image name&gt;:&lt;image version&gt;</code> when pulling with Apptainer. For all other images that are not listed in the link, you should instead use the syntax <code>apptainer pull &lt;name of SIF&gt; docker://&lt;image repo name&gt;/&lt;image name&gt;:&lt;image version&gt;</code>.</p>"},{"location":"Documentation/Development/Containers/registries/#dockerhub-enterprise-credentials","title":"DockerHub Enterprise Credentials","text":"<p>To get the needed credentials for NREL Dockerhub, select your username in the top right -&gt; Account -&gt; Security -&gt; Create a new access token.</p> <p>The dialog box will describe how to use the security token with <code>docker login</code> to enable pulling and pushing containers. </p>"},{"location":"Documentation/Development/Containers/singularity/","title":"Singularity","text":"<p>Singularity is a platform designed specifically for running containers on HPC systems. </p> <p>Note</p> <p>Singularity has been deprecated in favor of a new container application called Apptainer, and NREL clusters now exclusively use Apptainer. This page has subsequently been deprecated. For more information about Apptainer and using it on NREL HPC systems, see Apptainer.</p>"},{"location":"Documentation/Development/Debug_Tools/ddt/","title":"DDT (Linaro Debugger)","text":"<p>DDT is Linaro's (formally ARM's) parallel GUI based debugger</p> <p>ddt is a GUI based parallel debugger that supports MPI, OpenMP, Cuda. It can be used with C, C++, Fortran and Python.  It shares much of its infrastructure with Linaro's map and profiling tools.  See the Linaro-Forge page for additional information.</p>"},{"location":"Documentation/Development/Debug_Tools/gdb/","title":"GDB (GNU Debugger)","text":"<p>Documentation: GDB</p> <p>GDB is GNU's command line interface debugging tool.</p>"},{"location":"Documentation/Development/Debug_Tools/gdb/#getting-started","title":"Getting started","text":"<p>GDB is available on NREL machines and supports a number of languages, including C, C++, and Fortran. </p> <p>When using GDB, make sure the program you are attempting to debug has been compiled with the <code>-g</code> debug flag and with the <code>-O0</code> optimization flag to achieve the best results.</p> <p>Run GDB with the following command: <code>gdb --args my_executable arg1 arg 2 arg3</code> This will launch gdb running <code>my_executable</code>, and passes arguments <code>arg1</code>, <code>arg2</code>, and <code>arg3</code> to <code>my_executable</code>.</p> <p>For links to in-depth tutorials and walkthroughs of GDB features, please see Resources.</p>"},{"location":"Documentation/Development/Debug_Tools/gdb/#availability","title":"Availability","text":"Kestrel Swift Vermilion gdb/12.1, gdb/11.2* gdb/7.12.1, gdb/14.2, gdb/8.2* gdb/8.2* <p>* In path by default. Do not need to use <code>module load</code>.</p>"},{"location":"Documentation/Development/Debug_Tools/gdb/#resources","title":"Resources","text":"<ul> <li> <p>Sample GDB session</p> </li> <li> <p>\"Print statement\"-style debugging with GDB</p> </li> </ul>"},{"location":"Documentation/Development/Jupyter/","title":"Introduction to Jupyter","text":""},{"location":"Documentation/Development/Jupyter/#what-is-jupyter","title":"What is Jupyter?","text":"<p>A web app for interactive Python in a browser </p> <p>Jupyter offers a number of benefits for researchers in many fields, including:</p> <ul> <li>Live coding: Make changes and see the effects in real-time.</li> <li>Instant visualization: Charts and graphics render quickly in a browser window.</li> <li>Sharable: Notebooks can be copied and sent to others, or multiple users can edit a single shared notebook.</li> <li>Reproducible: Create a shareable environment with pinned Python and scientific library versions.</li> <li>Customizable: Many configuration options, extensions, and libraries are available.</li> <li>Not just for Python: Supports many other languages (including R, Julia, and many others.)   <ul> <li>See https://github.com/jupyter/jupyter/wiki/Jupyter-kernels for examples.</li> </ul> </li> </ul>"},{"location":"Documentation/Development/Jupyter/#example-notebook-code","title":"Example Notebook Code","text":"<p>With the appropriate libraries installed into the Jupyter environment, the following code can be placed in one cell in a notebook, or split across multiple cells, and executed to produce quick graphs:</p> <pre><code>import chart_studio.plotly as py\nimport plotly.figure_factory as ff\nimport pandas as pd\nimport numpy as np\nimport matplotlib\nimport matplotlib.pyplot as plt\nx = np.linspace(0, 5, 10)\ny = x ** 2\nn = np.array([0,1,2,3,4,5])\nxx = np.linspace(-0.75, 1., 100)\n\nfig, axes = plt.subplots(1, 4, figsize=(12,3))\n\naxes[0].scatter(xx, xx + 1.25*np.random.randn(len(xx)))\n#axes[0].scatter(xx, xx + 0.25*np.random.randn(len(xx)))\naxes[0].set_title(\"scatter\")\n\naxes[1].step(n, n**2.0, lw=2)\naxes[1].set_title(\"step\")\n\naxes[2].bar(n, n**2, align=\"center\", width=0.5, alpha=0.5)\naxes[2].set_title(\"bar\")\n\naxes[3].fill_between(x, x**2.5, x**3, color=\"green\", alpha=0.5);\naxes[3].set_title(\"fill_between\");\n</code></pre> <p></p>"},{"location":"Documentation/Development/Jupyter/#jupyter-terminology","title":"Jupyter Terminology","text":""},{"location":"Documentation/Development/Jupyter/#jupyterhub","title":"Jupyterhub","text":"<p>This is the multi-user \"backend\" server. The \"Hub\" allows users to login, then launches the single-user Jupyter server for them. Hubs are usually installed and managed by system administrators, not Jupyter users.</p> <p>A Jupyterhub server (kestrel-jhub) is available on Kestrel for use with your HPC data. More on KJHub later in this document.</p>"},{"location":"Documentation/Development/Jupyter/#jupyterjupyter-servernotebook-server","title":"Jupyter/Jupyter Server/Notebook server","text":"<p>The single-user server/web interface. Use to create, save, or load .ipynb notebook files. This is what users generally interact with.</p>"},{"location":"Documentation/Development/Jupyter/#jupyter-notebook","title":"Jupyter Notebook","text":"<p>A Notebook is an individual .pynb file. It contains your Python code and visualizations, and is sharable/downloadable.</p>"},{"location":"Documentation/Development/Jupyter/#jupyter-lab","title":"Jupyter Lab","text":"<p>A redesigned web interface for your Jupyter Notebook Server - \"Notebooks 2.0\". Preferred by some, and promoted as the next evolution of Notebooks. Lab has many new and different extensions, but many are also not compatible between Notebook and Lab. Lab is still under development, so is lacking some features of \"classic\" notebooks.</p>"},{"location":"Documentation/Development/Jupyter/#kernel","title":"Kernel","text":"<p>Kernels define the Python environments used by your notebooks. Derived from ipykernel, a predecessor project to Jupyter, and you may see Jupyter kernels referred to as \"ipykernels\". Custom kernels require the \"ipykernel\" package installed in your Jupyter conda environment.</p> <p>More on kernels later.</p>"},{"location":"Documentation/Development/Jupyter/#jupyterhub-service-on-kestrel-kjhub","title":"JupyterHub Service on Kestrel (KJHub)","text":"<p>The NREL HPC team runs a JupyterHub service for HPC users to quickly access notebooks and data stored on Kestrel, Kestrel-JHub (KJHub.)</p> <p>KJHub is available from the NREL VPN (onsite or offsite) for internal NREL users.</p> <p>This service is not directly accessible externally for non-NREL HPC users. However, it may be reached by using the HPC VPN, or by using a FastX Remote Desktop session via the DAV nodes.</p> <p>The JupyterHub service is accessible via web browser at https://kestrel-jhub.hpc.nrel.gov</p>"},{"location":"Documentation/Development/Jupyter/#jupyterhub-advantages","title":"JupyterHub Advantages:","text":"<ul> <li>Fast and easy access to notebooks with no setup. </li> <li>Use regular Kestrel credentials to log in.</li> <li>Great for simple tasks, including light to moderate data processing, code debugging/testing, and/or visualization using basic scientific and visualization libraries.</li> </ul>"},{"location":"Documentation/Development/Jupyter/#jupyterhub-disadvantages","title":"JupyterHub Disadvantages:","text":"<ul> <li>Limited resources: KJHub is a single node with 128 CPU cores and 512GB RAM.</li> <li>Managed usage: Up to 8 cores/100GB RAM per user before automatic throttling will greatly slow down processing.</li> <li>Competition: Your notebook competes with other users for CPU and RAM on the KJHub node.</li> <li>Slow updates: A limited list of basic scientific libraries are available in the default notebook kernel/environment.</li> </ul>"},{"location":"Documentation/Development/Jupyter/#simple-instructions-to-access-jupyterhub","title":"Simple Instructions to access JupyterHub:","text":"<ul> <li>Visit https://kestrel-jhub.hpc.nrel.gov in a web browser and log in using your HPC credentials.</li> </ul> <p>KJHub opens a standard JupyterLab interface by default. Change the url ending from \"/lab\" to \"/tree\" in your web browser to switch to the classic Notebooks interface.</p>"},{"location":"Documentation/Development/Jupyter/#using-a-compute-node-to-run-your-own-jupyter-notebooks","title":"Using a Compute Node to Run Your Own Jupyter Notebooks","text":"<p>Kestrel supports running your own Jupyter Notebook server on a compute node. This is highly recommended over KJHub for advanced Jupyter use and heavy computational processing.</p>"},{"location":"Documentation/Development/Jupyter/#advantages","title":"Advantages:","text":"<ul> <li>Custom conda environments to load preferred libraries.</li> <li>Full node usage: Exclusive access to the resources of the node your job is reserved on, including up to 104 CPU cores and up to 240GB RAM on Kestrel CPU nodes and up to 2TB RAM on Kestrel bigmem nodes. (See the system specifications page for more information on the types of nodes available on Kestrel.)</li> <li>No competing with other users for CPU cores and RAM, and no Arbiter2 process throttling.</li> <li>Less than a whole node may be requested via the shared node queue, to save AUs.</li> </ul>"},{"location":"Documentation/Development/Jupyter/#disadvantages","title":"Disadvantages:","text":"<ul> <li>Must compete with other users for a node via the job queue.</li> <li>Costs your allocation AU. </li> </ul>"},{"location":"Documentation/Development/Jupyter/#launching-your-own-jupyter-server-on-an-hpc-system","title":"Launching Your Own Jupyter Server on an HPC System","text":"<p>Before you get started, we recommend installing your own Jupyter inside of a conda environment. The default conda/anaconda3 module contains basic Jupyter Notebook packages, but you will likely want your own Python libraries, notebook extensions, and other features. Basic directions are included later in this document.</p> <p>Internal (NREL) HPC users on the NREL VPN, or external users of the HPC VPN, may use the instructions below.</p> <p>External (non-NREL) HPC users may follow the same instructions, but please use <code>kestrel.nrel.gov</code> in place of <code>kestrel.hpc.nrel.gov</code>.</p>"},{"location":"Documentation/Development/Jupyter/#using-a-compute-node-to-run-jupyter-notebooks","title":"Using a Compute Node to run Jupyter Notebooks","text":"<p>Connect to a login node and request an interactive job using the <code>salloc</code> command.</p> <p>The examples below will start a 2-hour job. Edit the <code>&lt;account&gt;</code> to the name of your allocation, and adjust the time accordingly. Since these are interactive jobs, they will get some priority, especially if they're shorter, so only book as much time as you will be actively working on the notebook.</p>"},{"location":"Documentation/Development/Jupyter/#on-kestrel","title":"On Kestrel:","text":"<p>Connect to the login node and launch an interactive job:</p> <p><code>[user@laptop:~]$ ssh kestrel.hpc.nrel.gov</code></p> <p><code>[user@kl1:~]$ salloc -A &lt;account&gt; -t 02:00:00</code></p>"},{"location":"Documentation/Development/Jupyter/#starting-jupyter-inside-the-job","title":"Starting Jupyter Inside the Job","text":"<p>Once the job starts and you are allocated a compute node, load the appropriate modules, activate your Jupyter environment, and launch the Jupyter server.</p> <p><code>[user@x1000c0s0b0n1:~]$ module load anaconda3</code></p> <p><code>[user@x1000c0s0b0n1:~]$ source activate myjupenv</code></p> <p><code>[user@x1000c0s0b0n1:~]$ jupyter-notebook --no-browser --ip=$(hostname -s)</code></p> <p>Take note of the node name that your job is assigned. (x1000c0s0b0n1 in the above example.)</p> <p>Also note the url that Jupyter displays when starting up, e.g. <code>http://127.0.0.1:8888/?token=&lt;alphabet soup&gt;</code>.</p> <p>The <code>&lt;alphabet soup&gt;</code> is a long string of letters and numbers. This is a unique authorization token for your Jupyter session. you will need it, along with the full URL, for a later step.</p>"},{"location":"Documentation/Development/Jupyter/#on-your-own-computer","title":"On Your Own Computer:","text":"<p>Next, open an SSH tunnel through a login node to the compute node. Log in when prompted using your regular HPC credentials, and put this terminal to the side or minimize it, but leave it open until you are done working with Jupyter for this session.</p> <p><code>[user@laptop:~]$ ssh -N -L 8888:&lt;nodename&gt;:8888 username@kestrel.hpc.nrel.gov</code></p>"},{"location":"Documentation/Development/Jupyter/#open-a-web-browser","title":"Open a Web Browser","text":"<p>Copy the full url and token from Jupyter startup into your web browser. For example:</p> <p><code>http://127.0.0.1:8888/?token=&lt;alphabet soup&gt;</code></p>"},{"location":"Documentation/Development/Jupyter/#using-a-compute-node-the-easy-way","title":"Using a Compute Node - The Easy Way","text":"<p>Scripted assistance with launching a Jupyter session on Kestrel is available.</p>"},{"location":"Documentation/Development/Jupyter/#internal-nrel-users-only-pyeagle","title":"Internal NREL Users Only: pyeagle","text":"<p>The pyeagle package is available for internal users to handle launching and monitoring a jupyter server on a compute node. This package is maintained by an NREL HPC user group and was originally written for use with Eagle, but now supports Kestrel.</p>"},{"location":"Documentation/Development/Jupyter/#auto-launching-on-kestrel-with-an-sbatch-script","title":"Auto-launching on Kestrel with an sbatch Script","text":"<p>There are scripts written for launching a Jupyter session inside of a slurm job.</p> <p>Full directions and scripts included in the Jupyter repo.</p> Standard Jupyter session launch with full CPU request <p>Download sbatch_jupyter.sh and auto_launch_jupyter.sh</p> <p>Edit sbatch_jupyter.sh to change: <pre><code>--time=&lt;time_request&gt;\n--account=&lt;project_handle&gt;\n...\n...\nsource activate /home/$USER/.conda-envs/&lt;MY_ENVIRONMENT&gt; # Replace &lt;MY_ENVIRONMENT&gt; with the name of your conda environment\n</code></pre></p> <p>Edit auto_launch_jupyter.sh to include your sbatch_jupyter script: <pre><code>RES=$(sbatch sbatch_jupyter.sh)\n</code></pre></p> <p>Run auto_launch_jupyter.sh and follow the directions that come up on your terminal window.</p> Standard Jupyter session launch in the shared partition with partial CPU request <p>Download shared_sbatch_jupyter.sh and auto_launch_jupyter.sh</p> <p>Edit shared_sbatch_jupyter.sh to change: <pre><code>--time=&lt;time_request&gt;\n--account=&lt;project_handle&gt;\n...\n--cpus-per-task=&lt;CPUs_request&gt;\n--mem-per-cpu=&lt;CPU_memory_request&gt;                 # Default is 1G per core\n...\n...\nsource activate /home/$USER/.conda-envs/&lt;MY_ENVIRONMENT&gt;  # Replace &lt;MY_ENVIRONMENT&gt; with the name of your conda environment\n</code></pre></p> <p>Edit auto_launch_jupyter.sh to include your sbatch_jupyter script: <pre><code>RES=$(sbatch shared_sbatch_jupyter.sh)\n</code></pre></p> <p>Run auto_launch_jupyter.sh and follow the directions that come up on your terminal window.</p> Standard Jupyter session launch with GPU request <p>Download gpu_sbatch_jupyter.sh and auto_launch_jupyter.sh</p> <p>Edit gpu_sbatch_jupyter.sh to change: <pre><code>--time=&lt;time_request&gt;\n--account=&lt;project_handle&gt;\n...\n--cpus-per-task=&lt;CPU_request&gt;\n--gres=gpu:&lt;GPU_request&gt;\n\nexport CUDA_VISIBLE_DEVICES=0  # if GPUs request is 1, then set =0\n                               # if GPUs request is 2, then set =0,1\n                               # if GPUs request is 3, then set =0,1,2\n                               # if GPUs request if 4, then set =0,1,2,3\n...\n...\nsource activate /home/$USER/.conda-envs/&lt;MY_ENVIRONMENT&gt;  # Replace &lt;MY_ENVIRONMENT&gt; with the name of your conda environment\n</code></pre></p> <p>Edit auto_launch_jupyter.sh to include your sbatch_jupyter script: <pre><code>RES=$(sbatch gpu_sbatch_jupyter.sh)\n</code></pre></p> <p>Run auto_launch_jupyter.sh and follow the directions that come up on your terminal window.</p>"},{"location":"Documentation/Development/Jupyter/#reasons-to-not-run-jupyter-directly-on-a-login-node","title":"Reasons to Not Run Jupyter Directly on a Login Node","text":"<p>Data processing and visualization should be done via either KJHub or a compute node.</p> <p>Login nodes are highly shared and limited resources. There will be competition for CPU, RAM, and network I/O for storage, and Arbiter2 software will automatically throttle moderate to heavy usage on login nodes, greatly slowing down your processing.</p>"},{"location":"Documentation/Development/Jupyter/#custom-conda-environments-and-jupyter-kernels","title":"Custom Conda Environments and Jupyter Kernels","text":"<p>On Kestrel, the module 'anaconda3' is available to run the conda command and manage your environments. </p> <p>As an alternative, the module 'mamba' is available instead. Mamba is a conda-compatible environment manager with very similar usage. Most conda commands in this documentation may be used with mamba instead and they may generally be considered interchangeable.</p>"},{"location":"Documentation/Development/Jupyter/#creating-a-conda-environment","title":"Creating a Conda Environment","text":"<p>To add your own packages to conda on Kestrel:</p> <p>Create an environment and install the base jupyter packages. Then activate the environment and install other libraries that you want to use, e.g. scipy, numpy, and so on.</p> <p><code>conda create -n myjupyter -c conda-forge jupyter ipykernel</code></p> <p><code>source activate myjupyter</code></p> <p><code>conda install -c conda-forge scipy numpy matplotlib</code></p>"},{"location":"Documentation/Development/Jupyter/#add-custom-ipykernel","title":"Add Custom iPykernel","text":"<p>A kernel is what allows Jupyter to use your customized conda environment inside Jupyter, in a notebook. Use ipykernel to build your kernel. Inside your custom conda environment, run:</p> <p><code>python -m ipykernel install --user --name=myjupyter</code></p> <p>If you already have a Jupyter server running, restart it to load the new kernel.</p> <p>The new kernel will appear in the drop-down as an option to open a new notebook.</p> <p>You can have multiple kernels, allowing you to load different conda environments for your different projects into Notebooks.</p>"},{"location":"Documentation/Development/Jupyter/#jupyter-kernel-management","title":"Jupyter Kernel Management","text":"<p>Use the kernelspec list command inside your Jupyter conda environment to see what ipykernels you have installed:</p> <p><code>jupyter kernelspec list</code></p> <p>To remove an old kernel, use the kernelspec remove command:</p> <p><code>jupyter kernelspec remove myoldjupyter</code></p>"},{"location":"Documentation/Development/Jupyter/#magic-commands","title":"Magic Commands","text":"<p>Magic commands are \"meta commands\" that add extra functionality to Jupyter.</p> <p>Magic commands begin with % or %%.</p>"},{"location":"Documentation/Development/Jupyter/#example-magic-commands","title":"Example Magic Commands","text":"<pre><code>* %lsmagic - list all magic commands\n* %run _file.py_ - run an external python script\n* %%time - placed at top of cell, prints execution time\n* %who - list all defined variables in notebook\n</code></pre> <pre><code>%lsmagic\n</code></pre> <pre><code>Available line magics:\n%alias  %alias_magic  %autoawait  %autocall  %automagic  %autosave  %bookmark  %cat  %cd  %clear  %colors  %conda  %config  %connect_info  %cp  %debug  %dhist  %dirs  %doctest_mode  %ed  %edit  %env  %gui  %hist  %history  %killbgscripts  %ldir  %less  %lf  %lk  %ll  %load  %load_ext  %loadpy  %logoff  %logon  %logstart  %logstate  %logstop  %ls  %lsmagic  %lx  %macro  %magic  %man  %matplotlib  %mkdir  %more  %mv  %notebook  %page  %pastebin  %pdb  %pdef  %pdoc  %pfile  %pinfo  %pinfo2  %pip  %popd  %pprint  %precision  %prun  %psearch  %psource  %pushd  %pwd  %pycat  %pylab  %qtconsole  %quickref  %recall  %rehashx  %reload_ext  %rep  %rerun  %reset  %reset_selective  %rm  %rmdir  %run  %save  %sc  %set_env  %store  %sx  %system  %tb  %time  %timeit  %unalias  %unload_ext  %who  %who_ls  %whos  %xdel  %xmode\n\nAvailable cell magics:\n%%!  %%HTML  %%SVG  %%bash  %%capture  %%debug  %%file  %%html  %%javascript  %%js  %%latex  %%markdown  %%perl  %%prun  %%pypy  %%python  %%python2  %%python3  %%ruby  %%script  %%sh  %%svg  %%sx  %%system  %%time  %%timeit  %%writefile\n\nAutomagic is ON, % prefix IS NOT needed for line magics.\n</code></pre>"},{"location":"Documentation/Development/Jupyter/#shell-commands","title":"Shell Commands","text":"<p>You can also run shell commands inside a cell. For example:</p> <p><code>!conda list</code> - see the packages installed in the environment you're using</p> <pre><code>!pwd\n!ls\n</code></pre> <pre><code>/home/username/jup\nauto_launch_jupyter.sh    Jupyter Presentation.ipynb  slurm-6445885.out\ngeojsondemo.ipynb         old                         sshot1.png\nInteresting Graphs.ipynb  sbatch_jupyter.sh           sshot2.png\njup-logo.png              slurm\n</code></pre>"},{"location":"Documentation/Development/Jupyter/#interestinguseful-notebooks-extensions-and-learning-resources","title":"Interesting/Useful Notebooks, Extensions, and Learning Resources","text":"<p>Awesome Jupyter</p> <p>Awesome Jupyterlab</p> <p>Plotting with matplotlib</p> <p>Python for Data Science</p> <p>Numerical Computing in Python</p> <p>The Sound of Hydrogen</p> <p>Plotting Pitfalls</p> <p>GeoJSON Extension</p>"},{"location":"Documentation/Development/Languages/bash/","title":"An Introduction to Bash Scripting","text":"<p>Bash (Bourne Again Shell) is one of the most widely available and used command line shell applications. Along with basic shell functionality, it offers a wide variety of features which, if utilized thoughtfully, can create powerful automated execution sequences that run software, manipulate text and files, parallelize otherwise single-process software, or anything else you may want to do from the command line. </p> <p>Shell scripts are also one of the most common ways our HPC community submits jobs, and running a large parallel workload often requires some initialization of the software environment before meaningful computations can begin. This typically involves tasks such as declaring environment\u00a0variables, preparing input files or staging directories for data, loading modules and libraries that the software needs to run, preparing inputs, manipulating datasets, and so on. Bash can even be used to launch several single-core jobs, effectively taking on the role of an ad hoc batch executor, as well. </p> <p>This article provides a brief introduction to bash, as well as a list of tips, tricks, and good practices when it comes to writing effective bash scripts that can apply widely in both HPC and non-HPC environments. We will also provide links to some additional resources to help further your bash scripting skills.</p>"},{"location":"Documentation/Development/Languages/bash/#executinginvoking-scripts","title":"Executing/Invoking Scripts","text":"<p>All of bash commands work at the command prompt \"live\", i.e. interpreted line-by-line as you type commands and press enter. A bash \"script\" may be regarded as a list of bash commands that have been saved to a file for convenience, usually with some basic formatting, and possibly comments, for legibility.</p> <p>All bash scripts must begin with a special character combination, called the \"shebang\" or <code>#!</code> character, followed by the name of an interpreter:</p> <p><code>#!/bin/bash</code></p> <p>This declares that the contents of the file that follow are to be interpreted as commands, using <code>/bin/bash</code> as the interpreter. This includes commands, control structures, and comments.</p> <p>Plenty of other interpreters exist. For example, Python scripts begin with: <code>#!/usr/bin/python</code> or <code>/usr/bin/env python</code>, perl scripts: <code>#!/usr/bin/perl</code>, and so on.</p>"},{"location":"Documentation/Development/Languages/bash/#bash-scripting-syntax","title":"Bash Scripting Syntax","text":"<p>If you read a bash script, you may be tempted to default to your usual understanding of how code generally works. For example, with most languages, typically there is a binary or kernel which digests the code you write (compilers/gcc for C, the python interpreter/shell, Java Virtual Machine for Java, and so on.) The binary/kernel/interpreter then interprets the text into some sort of data structure which enforces the priority of certain commands over others, and finally generates some execution of operations based on that data structure.  </p> <p>Bash isn't too far off from this model, and in some respects functions as any other interpreted language: you enter a command (or a control structure) and it is executed. </p> <p>However, as a shell that also serves as your major interface to the underlying operating system, it does have some properties and features that may blur the lines between what you think of as 'interpreted' versus 'compiled'.</p> <p>For instance, many aspects of the bash \"language\" are actually just the names of pre-compiled binaries which do the heavy lifting. Much the same way you can run <code>python</code>\u00a0or <code>ssh</code>\u00a0in a command line, under the hood normal bash operations such as <code>if</code>, <code>echo</code>, and <code>exit</code>\u00a0are actually just programs that expect a certain cadence for the arguments you give it. A block such as:</p> <p><pre><code>if true; then echo \"true was true\"; fi\n</code></pre> This is really just a sequence of executing many compiled applications or shell built-ins with arguments; the names of these commands were just chosen to read as a typical programming grammar. </p> <p>A good example is the program <code>[</code> which is just an oddly-named command you can invoke. Try running <code>which [</code> at a command prompt. The results may surprise you: <code>/usr/bin/[</code> is actually a compiled program on disk, not a \"built-in\" function!</p> <p>This is why you need to have a space between the brackets\u00a0and your conditional, because the conditional itself is passed as an argument to the command <code>[</code>. In languages like C it's common to write the syntax as <code>if (conditional)\u00a0{ ...; }</code>. However, in bash, if you try to run <code>if\u00a0[true]</code>\u00a0you will likely get an error saying there isn't a command called <code>[true]</code>\u00a0that you can run. This is also why you often see stray semicolons that seem somewhat arbitrary, as semicolons separate the execution of two binaries. Take this snippet for example: <pre><code>echo \"First message.\" ; echo \"Second message.\"\n</code></pre> This is equivalent to: <pre><code>echo \"First message.\"\necho \"Second message.\"\n</code></pre> In the first snippet, if the semicolon was not present, the second <code>echo</code>\u00a0would be interpreted as an argument to the first echo and would end up outputting:\u00a0<code>First message. echo Second message.</code></p> <p>Bash interprets <code>;</code> and <code>\\n</code> (newline) as separators. If you need to pass these characters into a function (for example, common in <code>find</code>'s <code>-exec</code> flag) you need to escape them with a <code>\\</code>. This is useful for placing arguments on separate lines to improve readability like this example: <pre><code>chromium-browser \\\n--start-fullscreen \\\n--new-window \\\n--incognito \\\n'https://google.com'\n</code></pre></p> <p>Similarly, normal if-then-else control flow that you would expect of any programming/scripting language has the same caveats. Consider this snippet: <pre><code>if\u00a0true\nthen\n  echo \"true is true\"\nelse\n  echo \"false is true?\"\nfi\n</code></pre> If we break down what's essentially happening here (omitting some of the technical details):</p> <ul> <li><code>if</code>\u00a0invokes the command <code>true</code> which always exits with a successful exit code (<code>0</code>)</li> <li><code>if</code> interprets a success exit code (<code>0</code>) as a truism and runs the <code>then</code>.</li> <li>the <code>then</code> command will execute anything it's given until <code>else</code>, <code>elif</code>, or <code>fi</code></li> <li>the <code>else</code> command is the same as <code>then</code> but will only execute if <code>if</code>\u00a0returned an erroneous exit code.</li> <li>the <code>fi</code> command\u00a0indicates that no more conditional branches exist relative to the logical expression given to the original <code>if</code>.</li> </ul> <p>All this to say, this is why you often see if-then-else blocks written succinctly as <code>if\u00a0[ &lt;CONDITIONAL&gt; ]; then &lt;COMMANDS&gt;; fi</code>\u00a0with seemingly arbitrary semicolons and spaces. It is exactly why things work this way that bash is able to execute arbitrary executables (some of which you may end up writing) and not require something like Python's subprocess module.</p> <p>This is just to give you an understanding for\u00a0why some of the syntax you will encounter is the way it is. Everything in bash is either a command or an argument to a command.</p>"},{"location":"Documentation/Development/Languages/bash/#parentheses-braces-and-brackets","title":"Parentheses, Braces, and Brackets","text":"<p>Bash utilizes many flavors of symbolic enclosures. A complete guide is beyond the scope of this document, but you may see the following:</p> <ul> <li><code>( )</code> - Single parentheses: run enclosed commands in a subshell<ul> <li><code>a='bad';(a='good'; mkdir $a); echo $a</code>  result: directory \"good\" is made, echoes \"bad\" to screen</li> </ul> </li> <li><code>$( )</code> - Single parentheses with dollar sign: subshell output to string(command substitution) (preferred method)<ul> <li><code>echo \"my name is $( whoami )\"</code> result: prints your username</li> </ul> </li> <li><code>&lt;( )</code> - Parentheses with angle bracket: process substitution<ul> <li><code>sort -n -k 5 &lt;( ls -l ./dir1) &lt;(ls -l ./dir2)</code> result: sorts ls -l results of two directories by column 5 (size)</li> </ul> </li> <li><code>[ ]</code> - Single Brackets: truth testing with filename expansion or word splitting<ul> <li><code>if [ -e myfile.txt ]; then echo \"yay\"; else echo \"boo\"; fi</code> result: if myfile.txt exists, celebrate</li> </ul> </li> <li><code>{ }</code> - Single Braces/curly brackets: expansion of a range</li> <li><code>${ }</code> - Single braces with dollar sign: expansion with interpolation</li> <li><code>` `</code> - Backticks: command/process substitution</li> <li><code>(( ))</code> - Double parentheses: integer arithmetic </li> <li><code>$(( ))</code> - Double parentheses with dollar sign: integer arithmatic to string</li> <li><code>[[ ]]</code> - Double brackets: truth testing with regex </li> </ul>"},{"location":"Documentation/Development/Languages/bash/#additional-notes-on-single-parentheses","title":"Additional Notes on <code>( )</code> (Single Parentheses)","text":"<p>There are 3 features in Bash which are denoted by a pair of parentheses, which are Bash subshells, Bash array declarations, and Bash function declarations. See the table below for when each feature is enacted:</p> Syntax Bash Feature Command/line begins with <code>(</code> Run the contained expression(s) in a subshell. This will pass everything until a closing <code>)</code> to a child-fork of Bash that inherits the environment from the invoking Bash instance, and exits with the exit code of the last command the subshell exitted with. See the section on subshells for more info. A valid Bash identifier is set equal to a parnethetically enclosed list of items(.e.g. <code>arr=(\"a\" \"b\" \"c\")</code> ) Creates a Bash array with elements enclosed by the parentheses. The default indexing of the elements is numerically incremental from 0 in the given order, but this order can be overridden or string-based keys can be used. See the section on arrays for more info. A valid Bash identifier is followed by <code>()</code> and contains some function(s) enclosed by <code>{ }</code>(i.e. <code>func() { echo \"test\"; }</code> ) Declare a function which can be re/used throughout a Bash script. See the either of \"<code>{ }</code>\" or functions for more info."},{"location":"Documentation/Development/Languages/bash/#examples-of-enclosure-usage","title":"Examples of Enclosure Usage","text":"<p>Note that whitespace is required, prohibited, or ignored in certain situations. See this block for specific examples of how to use whitespace in the various contexts of parantheses. <pre><code>### Subshells\n(echo hi)   # OK\n( echo hi)  # OK\n(echo hi )  # OK\n( echo hi ) # OK\n\n### Arrays\narr=(\"a\" \"b\" \"c\")   # Array of 3 strings\narr =(\"a\" \"b\" \"c\")    # ERROR\narr= (\"a\" \"b\" \"c\")    # ERROR\narr = (\"a\" \"b\" \"c\")   # ERROR\narr=(\"a\"\"b\"\"c\")     # Array of one element that is \"abc\"\narr=(\"a\",\"b\",\"c\")   # Array of one element that is \"a,b,c\"\narr=(\"a\", \"b\", \"c\") # ${arr[0]} == \"a,\"\n\n### Functions \nfunc(){echo hi;} # ERROR\nfunc(){ echo hi;}     # OK\nfunc (){ echo hi;}    # OK\nfunc () { echo hi;}   # OK\nfunc () { echo hi; }  # OK\n</code></pre></p> Command Behavior <code>(ls -1 | head -n 1)</code> Run the command in a subshell. This will return the exit code of the last process that was ran. <code>test_var=(ls -1)</code> Create a bash array with the elements <code>ls</code> and <code>-1</code>, meaning <code>${test_var[1]}</code> will evaluate to <code>-1</code>. <code>test_var=$(ls -1)</code> Evaluate <code>ls -1</code> and capture the output as a string. <code>test_var=(`ls -1`)</code> or <code>test_var=($(ls -1))</code> Evaluate <code>ls -1</code> and capture the output as an array."},{"location":"Documentation/Development/Languages/bash/#bracket-usage","title":"Bracket Usage:","text":"<p>Correct:</p> <ul> <li> <p><code>[ cmd ]</code> - There must be spaces or terminating characters (<code>\\n</code> or <code>;</code>) surrounding any brackets. </p> </li> <li> <p>Like many common bash commands, \"[\" is actually a standalone executable, usually located at <code>/usr/bin/[</code>, so it requires spaces to invoke correctly. </p> </li> </ul> <p>Erroneous:</p> <ul> <li><code>[cmd]</code>   - tries to find a command called <code>[cmd]</code> which likely doesn't exist</li> <li><code>[cmd ]</code>  - tries to find a command called <code>[cmd</code> and pass <code>]</code> as an argument to it</li> <li><code>[ cmd]</code>  - tries to pass <code>cmd]</code> as an argument to <code>[</code> which expects an argument of <code>]</code> that isn't technically provided.</li> </ul> <p>There are many other examples of using enclosures in bash scripting beyond the scope of this introduction. Please see the resources section for more information.</p>"},{"location":"Documentation/Development/Languages/bash/#variables","title":"Variables","text":"<p>Variable assignment in bash is simply to assign a value to a string of characters. All subsequent references to that variable must be prefixed by <code>$</code>:</p> <pre><code>$ MYSTRING=\"a string\"\n$ echo $MYSTRING\na string\n$ MYNUMBER=\"42\"\n$ echo $MYNUMBER\n42\n</code></pre>"},{"location":"Documentation/Development/Languages/bash/#exporting-variables","title":"Exporting Variables","text":"<p>When you declare a variable in bash, that variable is only available in the shell in which it is declared; if you spawn a sub-shell, the variable will not be accessible. Using the <code>export</code> command, you can essentially declare the variable to be inheritable.</p> <pre><code># without exporting:\n$ TESTVAR=100  \n$ echo $TESTVAR\n100     # returns a result\n$ bash  # spawn a sub-shell\n$ echo $TESTVAR\n        # no result\n$ exit  # exit the subshell\n# with exporting: \n$ export TESTVAR=100\n$ echo $TESTVAR\n100     # returns a result \n$ bash  # spawn a sub-shell\n$ echo $TESTVAR  \n100     # value is passed into the subshell\n$ exit  # exit the subshell\n$\n</code></pre>"},{"location":"Documentation/Development/Languages/bash/#sourcing-variables","title":"Sourcing Variables","text":"<p>\"Source\" (shortcut: <code>.</code>) is a built-in bash command that takes a bash script as an argument. Bash will execute the contents of that file in the current shell, instead of spawning a sub-shell. This will load any variables, function declarations, and so on into your current shell. </p> <p>A common example of using the <code>source</code> command is when making changes to your <code>~/.bashrc</code>, which is usually only parsed once upon login. Rather than logging out and logging back in every time you wish to make a change, you can simply run <code>source ~/.bashrc</code> or <code>. ~/.bashrc</code> and the changes will take effect immediately.</p>"},{"location":"Documentation/Development/Languages/bash/#declaring-variables","title":"Declaring Variables","text":"<p>Variable typing in bash is implicit, and the need to declare a type is rare, but the <code>declare</code> command can be used when necessary: <pre><code>$ declare -i MYNUMBER # set type as an integer\n$ echo $MYNUMBER\n0\n$ declare -l MYWORD=\"LOWERCASE\" # set type as lowercase \n$ echo $MYWORD\nlowercase\n$\n</code></pre> see <code>help declare</code> at the command line for more information on types that can be declared.</p>"},{"location":"Documentation/Development/Languages/bash/#further-resources","title":"Further Resources","text":"<p>NREL HPC Github - User-contributed bash script and examples that you can use on HPC systems.</p> <p>BASH cheat sheet - A concise and extensive list of example commands, built-ins, control structures, and other useful bash scripting material.</p>"},{"location":"Documentation/Development/Languages/c%2B%2B/","title":"C++","text":"<p>\"C++ is a general-purpose programming language providing a direct and efficient model of hardware combined with facilities for defining lightweight abstractions.\"   - Bjarne Stroustrup, \"The C++ Programming Language, Fourth Edition\"</p>"},{"location":"Documentation/Development/Languages/c%2B%2B/#getting-started","title":"Getting Started","text":"<p>This section illustrates the process to compile and run a basic C++ program on the HPC systems.</p>"},{"location":"Documentation/Development/Languages/c%2B%2B/#hello-world","title":"Hello World","text":"<p>Begin by creating a source file named <code>hello.cpp</code> with the following contents:</p> <pre><code>#include &lt;iostream&gt;\n\nint main(void) {\n  std::cout &lt;&lt; \"Hello, World!\\n\";\n  return 0;\n}\n</code></pre> <p>Next, we must select the compiler to use for compiling our program.  We can choose among GNU, Intel, and Cray compilers, depending on the system that we are using (see Compilers and Toolchains).  To see available modules and versions, use <code>module avail</code>.  For this example, we will use the <code>g++</code> compiler, which is part of GNU's <code>gcc</code> package.  We will load the default version of the compiler, which in this case is gcc 10.1:</p> <pre><code>$ module load gcc\n$ module list\nCurrently Loaded Modules:\n  1) gcc/10.1.0\n$ gcc --version | head -1\ngcc (Spack GCC) 10.1.0\n</code></pre> <p>With the <code>gcc</code> package, the C++ compiler is provided by the <code>g++</code> command.  To compile the program, run:</p> <pre><code>$ g++ hello.cpp -o hello\n</code></pre> <p>This creates an executable named <code>hello</code>.  Now run the program and observe the output:</p> <pre><code>$ ./hello\nHello, World!\n</code></pre>"},{"location":"Documentation/Development/Languages/c%2B%2B/#compilers-and-toolchains","title":"Compilers and Toolchains","text":"<p>The following is a summary of available compilers and toolchains.  User are encouraged to run <code>module avail</code> to check for the most up-to-date information on a particular system.</p> Toolchain C++ Compiler Module Systems gcc <code>g++</code> <code>gcc</code> All Intel <code>icpc</code> <code>intel-oneapi-compilers</code> Swift, Vermilion, Kestrel Cray <code>CC</code> <code>PrgEnv-cray</code> Kestrel <p>Note that Kestrel also provides the <code>PrgEnv-intel</code> and <code>PrgEnv-gnu</code> modules, which combine the Intel or gcc compilers together with Cray MPICH.  Please refer to Kestrel Programming Environments Overview for details about the programming environments available on Kestrel.</p> <p>For information specific to compiling MPI applications, refer to MPI.</p>"},{"location":"Documentation/Development/Languages/r/","title":"Running R Statistical Computing Environment Software","text":"<p>Learn how to run the R statistical computing environment software.</p>"},{"location":"Documentation/Development/Languages/r/#what-is-r","title":"What Is R?","text":"<p>R is an open-source programming language designed for statistical computing and graphics. It is the current standard for the development of new statistical methodologies and enjoys a large user base.</p> <p>For more information related to the R project, see the R website.</p>"},{"location":"Documentation/Development/Languages/r/#accessing-r","title":"Accessing R","text":"<p>The supported method for using R on NREL HPC systems is via Anaconda/mamba. In order to install R, first load the mamba module. On Kestrel or Swift, this is <code>module load mamba</code>. Next, create a new conda environment that contains at least the <code>r-base</code> package, which installs R itself. Optionally, install the <code>r-essentials</code> bundle, which provides many of the most popular R packages for data science, such as the tidyverse family of packages.</p> <p>For example, to create and activate a new environment named <code>r_env</code> in your current directory that includes the <code>r-essentials</code> bundle:</p> <pre><code>module load mamba\nconda create --prefix=./r_env r-essentials r-base\nconda activate ./r_env\n</code></pre> <p>Note</p> <p>We install the <code>r_env</code> conda environment in our current directory with the <code>--prefix</code> option for a number of reasons. Note that if you create an environment with the <code>-n</code> or <code>--name</code> option, it will install into your home directory by default, which is not ideal due to its limited storage. The <code>--prefix</code> option can also accept an absolute path, such as a dedicated <code>/projects</code> directory. Please see our dedicated conda documentation for more information.</p> <p>For more information about using R in the Anaconda framework, see Using R language with Anaconda.</p>"},{"location":"Documentation/Development/Languages/r/#running-r-interactively","title":"Running R Interactively","text":"<p>R is most commonly used via an interactive shell. To do this, first request an interactive compute node (see running interactive jobs) using the <code>salloc</code> command. Alternatively, R can be used through Jupyterhub.</p> <p>Once on a compute node, R environments can be accessed through Anaconda as described above. To access the R interactive console, type R at the command line. You will be prompted with the familiar R console in your terminal window: </p> R Terminal <pre><code>$ R\n\nR version 4.4.2 (2024-10-31) -- \"Pile of Leaves\"\nCopyright (C) 2024 The R Foundation for Statistical Computing\nPlatform: x86_64-conda-linux-gnu\n\nR is free software and comes with ABSOLUTELY NO WARRANTY.\nYou are welcome to redistribute it under certain conditions.\nType 'license()' or 'licence()' for distribution details.\n\nNatural language support but running in an English locale\n\nR is a collaborative project with many contributors.\nType 'contributors()' for more information and\n'citation()' on how to cite R or R packages in publications.\n\nType 'demo()' for some demos, 'help()' for on-line help, or\n'help.start()' for an HTML browser interface to help.\nType 'q()' to quit R.\n</code></pre> <p>Note</p> <p>You can run individual R commands directly from the command line via <code>R -e 'COMMAND'</code>, such as <code>R -e 'print(\"Hello, world\")'</code>.</p>"},{"location":"Documentation/Development/Languages/r/#running-r-scripts","title":"Running R Scripts","text":"<p>Since running R programs line by line in the interactive console can be a little tedious, it is often better to combine R commands into a single script and have R execute them all at once. R scripts are text files containing R commands with file extension .R: </p> <p>hello_world.R</p> <pre><code>message = \"Hi there!\"\nnums = sample(1:100, 5)\ncat(message, \"\\n\")\ncat(\"Here are some random numbers: \", paste(nums, sep = \", \"),\"\\n\")\n</code></pre> <p>There are several options for running R scripts:</p> source() <p>The source()  function will execute R scripts from inside the interactive console.</p> <pre><code>&gt; source(\"hello_world.R\")\n  Hi there! \n  Here are some random numbers:  100 41 14 82 63 \n</code></pre> Rscript <p>The Rscript command can be used to run R scripts from the command line. Output is piped to the stdout.</p> <pre><code>$ Rscript hello_world.R\nHi there! \nHere are some random numbers:  71 37 50 24 90 \n</code></pre> R CMD BATCH <p>R CMD BATCH is an older function that behaves similar to Rscript. All output is piped to a corresponding .Rout file.</p> <pre><code>$ R CMD BATCH --no-site-file hello_world.R\n$ cat hello_world.Rout \n\n&gt; #hello_world.R\n&gt; \n&gt; message = \"Hi there!\"\n&gt; nums = sample(1:100, 5)\n&gt; cat(message, \"\\n\")\nHi there! \n&gt; cat(\"Here are some random numbers: \", paste(nums, sep = \", \"),\"\\n\")\nHere are some random numbers:  41 51 61 70 43 \n&gt; \n&gt; proc.time()\n   user  system elapsed \n  0.188   0.024   0.277 \n</code></pre>"},{"location":"Documentation/Development/Languages/r/#submitting-jobs","title":"Submitting Jobs","text":"<p>Another option for using R on the HPC systems is to submit batch jobs to be run on non-interactive nodes. An example job script for running the hello_world.R example is below. Ensure you update your allocation name as well as the path of the conda environment where R has been installed. </p> <p>Note</p> <p>The following example script is submitted to the shared partition on Kestrel, which allows nodes to run multiple jobs at once. This is because without invoking any special parallel packages in the <code>hello_world.R</code> script, R is limited to using a single core. Refer to the Parallel Programming in R section below for information on how to use more than one core from your R code.</p> <pre><code>#! /bin/bash\n#SBATCH --job-name=helloworld\n#SBATCH --nodes=1\n#SBATCH --partition=shared\n#SBATCH -n 1 \n#SBATCH -N 1\n#SBATCH --mem-per-cpu=1G\n#SBATCH --time=00:05:00\n#SBATCH --account=&lt;your_allocation_id&gt;\n\nmodule load mamba\nconda activate /path/to/r_env\nRscript hello_world.R\n</code></pre>"},{"location":"Documentation/Development/Languages/r/#versions-and-packages","title":"Versions and Packages","text":"<p>R is a popular open-source language with an active development community. New versions of R are frequently released. Any version can be installed into a custom anaconda environment. Commands for using other versions is shown below: </p> Custom Installation with Conda <pre><code>$ mamba search r-essentials\nLoading channels: done\n# Name                       Version           Build  Channel\nr-essentials                   1.5.2        r3.3.2_0  conda-forge\nr-essentials                   3.4.1        r3.4.1_0  conda-forge\nr-essentials                   3.5.1       r351_1000  conda-forge\nr-essentials                   3.5.1       r351_2000  conda-forge\nr-essentials                   3.5.1        r35_2001  conda-forge\nr-essentials                     3.6        r36_2001  conda-forge\nr-essentials                     3.6        r36_2002  conda-forge\nr-essentials                     4.0        r40_2002  conda-forge\nr-essentials                     4.0 r40hd8ed1ab_2002  conda-forge\nr-essentials                     4.1 r41hd8ed1ab_2002  conda-forge\nr-essentials                     4.1 r41hd8ed1ab_2003  conda-forge\nr-essentials                     4.2 r42hd8ed1ab_2003  conda-forge\nr-essentials                     4.2 r42hd8ed1ab_2004  conda-forge\nr-essentials                     4.3 r43hd8ed1ab_2004  conda-forge\nr-essentials                     4.3 r43hd8ed1ab_2005  conda-forge\nr-essentials                     4.4 r44hd8ed1ab_2005  conda-forge\n\n$ mamba create --prefix=./test-R r-essentials==4.3\n&lt;Text&gt;\n$ conda activate ./test-R\n(test-R) $ R --version\nR version 4.4.2 (2024-10-31) -- \"Pile of Leaves\"\nCopyright (C) 2024 The R Foundation for Statistical Computing\nPlatform: x86_64-conda-linux-gnu\n\nR is free software and comes with ABSOLUTELY NO WARRANTY.\nYou are welcome to redistribute it under the terms of the\nGNU General Public License versions 2 or 3.\nFor more information about these matters see\nhttps://www.gnu.org/licenses/.\n</code></pre>"},{"location":"Documentation/Development/Languages/r/#installing-new-packages","title":"Installing New Packages","text":"<p>The <code>install.packages()</code> command in R will download new packages from the CRAN source directory and install them for your account. If you are running R from within a custom Anaconda environment, they will be specific to that environment. In either case, these packages will not be visible to other users.</p>"},{"location":"Documentation/Development/Languages/r/#checking-installed-packages","title":"Checking Installed Packages","text":"<p>The command <code>installed.packages()</code> in R list details about all packages that are loaded and visible to current R session.</p>"},{"location":"Documentation/Development/Languages/r/#loading-packages","title":"Loading Packages","text":"<p>Packages are loaded into the current R environment through the <code>library()</code> function.</p>"},{"location":"Documentation/Development/Languages/r/#graphics","title":"Graphics","text":"<p>R is commonly used to produce high-quality graphics based on data. This capability is built-in and can be extended through the use of packages such as ggplot2. To produce graphics on the HPC systems, the easiest method is to output graphical displays to an appropriate filetype (pdf, jpeg, etc.). Then this file can be moved to your local machine using command line tools such as scp or rsync.</p> Example R Script for Graphics Output <pre><code>library(ggplot2)\nset.seed(8675309)\nnumbers = rnorm(200, sd = 2)\nmore.numbers = rnorm(100, mean = 10, sd = 2)\n\ndf = data.frame(values = c(numbers, more.numbers))\n\np = ggplot(df, aes(x = values, y = ..density..)) +\n    geom_histogram(fill = \"dodgerblue\",\n                   colour = \"black\",\n                   alpha = .5,\n                   binwidth = .5) +\n    geom_density(size = 1.5) +\n    labs(y = \"Density\", x = \"Value\",\n         title = \"Histogram Example\")\n\npng(file = \"histogram_example.png\")\nprint(p)\ndev.off()\n</code></pre>"},{"location":"Documentation/Development/Languages/r/#parallel-programming-in-r","title":"Parallel Programming in R","text":"<p>Programming in R on the HPC systems has two distinct advantages. First, running jobs on a remote system means you do not have to tie up your local machine. This can be particularly useful for jobs that take considerable time and resources to run. Secondly, the increased computational capabilities of the HPC system provide an opportunity to improve performance through parallel processing. R code, like many programming languages, is typically written and executed serially. This means that the added benefits of having multiple processing cores available are typically lost.</p> <p>A major goal of the R community in recent years has been the development of specialized libraries and programming paradigms to better leverage modern HPC systems. The CRAN Task View: High-Performance and Parallel Computing with R contains a detailed list of packages that address various aspects of these problems. </p> <p>Notable examples are:</p> <ul> <li>Parallel</li> <li>Foreach</li> <li>Multicore</li> <li>Snow</li> <li>pbdR</li> <li>Rmpi</li> </ul> <p>Each package includes in-depth documentation and examples for how to implement parallel processing in R code. Learning these packages does require a moderate amount of time, but for many large problems the improvements in computational efficiency dramatically outweighs the initial investment.</p> <p>Most of these packages will have to be installed in a custom environment as many dependencies are incompatible with the version of openmpi installed in conda. </p> Using the pbdR Project on Kestrel <p>The pbdR project \"enables high-level distributed data parallelism in R, so that it can easily utilize large HPC platforms with thousands of cores, making the R language scale to unparalleled heights.\" There are several packages within this project: pbdMPI for easy MPI work, pbdDMAT for distributed data matrices and associated functions, and pbdDEMO for a tutorial/vignette describing most of the project's details.</p> <p>The <code>pbdMPI</code> package provides the MPI interface, which requires Open MPI.  Note that the Open MPI module must be loaded prior to installing the package. For example, on Kestrel:</p> <pre><code>$ module load openmpi/5.0.3-gcc\n$ R\n&gt; install.packages(\"pbdMPI\")\n</code></pre> <p>The following script is a ranknode.R example using the pbdMPI package:</p> <pre><code>library(pbdMPI, quiet = TRUE)\ninit()\n.comm.size &lt;- comm.size()\n.comm.rank &lt;- comm.rank()\n.hostname &lt;- Sys.info()[\"nodename\"]\nmsg &lt;- sprintf(\"I am %d of %d on %s.\\n\", .comm.rank, .comm.size, .hostname)\ncomm.cat(msg, all.rank = TRUE, quiet = TRUE)\ncomm.cat(msg, rank.print = sample(0:.comm.size, size = 1))\ncomm.cat(msg, rank.print = sample(0:.comm.size, size = 1), quiet = TRUE)\nfinalize()\n</code></pre> <p>You could run this interactively from a compute node or by submitting it to the job scheduling using a shell script similar to the one given below. For example, you would submit this job to a shared node on Kestrel via <code>sbatch ranknode.sh</code> from a login node provided you name the script appropriately: </p> <pre><code>#!/bin/bash\n#SBATCH --nodes=2\n#SBATCH --ntasks-per-node=24\n#SBATCH --mem-per-cpu=1G\n#SBATCH --time=00:05:00\n#SBATCH --partition=shared\n#SBATCH --account=&lt;your_allocation_id&gt;\n\nmodule load mamba\nmodule load openmpi/5.0.3-gcc\nconda activate /path/to/renv\n\nINPUT_BASENAME=ranknode # JOB NAME - USER INPUT PARAMETER\nJOB_FILE=$INPUT_BASENAME.R\nOUT_FILE=$INPUT_BASENAME.Rout\nsrun -n 48 Rscript $JOB_FILE &gt; $OUT_FILE\n</code></pre> <p>In either case (interactive or queue submission), the output produced from the ranknode.R script should look like this:</p> <pre><code>I am 0 of 48 on x1004c0s2b0n0.\nI am 1 of 48 on x1004c0s2b0n0.\nI am 2 of 48 on x1004c0s2b0n0.\n...\nI am 46 of 48 on x1004c0s2b0n1.\nI am 47 of 48 on x1004c0s2b0n1.\nI am 42 of 48 on x1004c0s2b0n1.\nI am 45 of 48 on x1004c0s2b0n1.\n</code></pre>"},{"location":"Documentation/Development/Languages/r/#contacts","title":"Contacts","text":"<p>For questions on the R software environment itself or advanced R package questions, please contact HPC-Help@nrel.gov.</p> <p>Additionally, NREL has an internal R Users Group that meets periodically to highlight interesting packages, problems, and share experiences related to R programming. For more details, contact Daniel Inman. </p>"},{"location":"Documentation/Development/Languages/r/#references","title":"References","text":"<ul> <li>Rmpi: Interface Wrapper to MPI (Message-Passing Interface)</li> <li>University of Western Ontario \u2013 Rmpi News</li> <li>State of the Art in Parallel Computing with R</li> </ul>"},{"location":"Documentation/Development/Languages/Fortran/","title":"Fortran","text":"<p>Despite its age, Fortran is still a common language in scientific computing on account of its speed and ease of use in writing numerical computing-centric code.</p>"},{"location":"Documentation/Development/Languages/Fortran/#getting-started","title":"Getting Started","text":"<p>This section walks through how to compile and run a basic Fortran code, and then a basic Fortran MPI code, adapted from here. See Compilers and Toolchains for compiler and programming environment information on NREL HPC systems. For an extensive guide to Fortran 90, see our page on Advanced Fortran. See External Resources for general Fortran language tutorials and Fortran-MPI tutorials.  </p>"},{"location":"Documentation/Development/Languages/Fortran/#hello-world","title":"Hello World","text":"<p>Create a file named hello.f90, and save the following text to the file:</p> <pre><code>PROGRAM hello\n\nwrite(*,*) \"Hello World\"\n\nEND PROGRAM hello\n</code></pre> <p>Now, we must choose the compiler with which to compile our program. We can choose between the GNU, Intel, Nvidia, and Cray compilers, depending on which system we're on (see Compilers and Toolchains). </p> <p>To see available versions of a chosen compiler, use <code>module avail</code>. For this example, we'll use gfortran, which is part of GNU's <code>gcc</code> package:</p> <pre><code>module avail gcc \n   gcc/10.3.0          gcc/11.2.0          gcc/12.1.0(default)\n</code></pre> <p>We'll use gcc/12.1.0:</p> <pre><code>module load gcc/12.1.0\n</code></pre> <p>Now, we can compile the program with the following command:</p> <p><code>gfortran hello.f90 -o hello</code></p> <p>This creates an executable named <code>hello</code>. Execute it by typing the following into your terminal:</p> <p><code>./hello</code></p> <p>It should return the following output:</p> <p><code>Hello World</code></p>"},{"location":"Documentation/Development/Languages/Fortran/#hello-world-in-mpi-parallel","title":"Hello World in MPI Parallel","text":"<p>The purpose of Fortran today is to run large scale computations fast. For the \"large scale\" part, we use MPI. Now that we have a working Hello World program, let's modify it to run on multiple MPI tasks.</p> <p>On Kestrel, there are multiple implementations of MPI available. We can choose between OpenMPI, Intel MPI, MPICH, and Cray MPICH. These MPI implementations are associated with an underlying Fortran compiler. For example, if we type:</p> <p><code>module avail openmpi</code></p> <p>we find that both <code>openmpi/4.1.4-gcc</code> and <code>openmpi/4.1.4-intel</code> are available.</p> <p>Let's choose the openmpi/gcc combination:</p> <p><code>module load openmpi/4.1.4-gcc</code></p> <p>Now, create a new file named <code>hello_mpi.f90</code> and save the following contents to the file:</p> <pre><code>PROGRAM hello_mpi\ninclude 'mpif.h'\n\ninteger :: ierr, my_rank, number_of_ranks\n\ncall MPI_INIT(ierr)\ncall MPI_COMM_SIZE(MPI_COMM_WORLD, number_of_ranks, ierr)\ncall MPI_COMM_RANK(MPI_COMM_WORLD, my_rank, ierr)\n\nwrite(*,*) \"Hello World from MPI task: \", my_rank, \"out of \", number_of_ranks\n\ncall MPI_FINALIZE(ierr)\n\nEND PROGRAM hello_mpi\n</code></pre> <p>To compile this program, type:</p> <p><code>mpif90 hello_mpi.f90 -o hello_mpi</code></p> <p>To run this code on the login node, type:</p> <p><code>mpirun -n 4 ./hello_mpi</code></p> <p>You should receive a similar output to the following (the rank ordering may differ):</p> <pre><code> Hello World from MPI task:            1 out of            4\n Hello World from MPI task:            2 out of            4\n Hello World from MPI task:            3 out of            4\n Hello World from MPI task:            0 out of            4\n</code></pre> <p>Generally, we don't want to run MPI programs on the login node! Let's submit this as a job to the scheduler. Create a file named <code>job.in</code> and modify the file to contain the following:</p> <p><pre><code>#!/bin/bash\n\n#SBATCH --time=00:01:00\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=4\n#SBATCH --partition=standard\n#SBATCH --account=&lt;your account here&gt;\n\nmodule load openmpi/4.1.4-gcc\n\nsrun -n 4 ./hello_mpi &amp;&gt; hello.out\n</code></pre> Be sure to replace the <code>&lt;your account here&gt;</code> with your account name.</p> <p>Submit the job:</p> <p><code>sbatch job.in</code></p> <p>When the job is done, the file hello.out should contain the same output as you found before (the ordering of ranks may differ).</p>"},{"location":"Documentation/Development/Languages/Fortran/#compilers-and-toolchains","title":"Compilers and Toolchains","text":""},{"location":"Documentation/Development/Languages/Fortran/#fortran-compilers","title":"Fortran compilers","text":"Compiler Compiler Executable Module Avail Systems available on gcc gfortran gcc Kestrel, Swift, Vermilion intel ifort intel-oneapi Kestrel, Swift, Vermilion intel ifort intel-classic Kestrel"},{"location":"Documentation/Development/Languages/Fortran/#fortran-mpi-toolchains","title":"Fortran-MPI Toolchains","text":"Compiler MPI Compiler Executable Module Avail Systems available on gcc openmpi mpifort openmpi Kestrel, Swift, Vermilion intel openmpi mpifort openmpi/4.1.x-intel Kestrel intel intel mpiifort intel-oneapi-mpi Kestrel, Swift, Vermilion gcc MPICH mpifort mpich Kestrel, Swift, Vermilion intel MPICH mpifort mpich/4.0.2-intel Kestrel only cray MPICH ftn cray-mpich Kestrel only"},{"location":"Documentation/Development/Languages/Fortran/#external-resources","title":"External Resources","text":"<ul> <li>Comprehensive treatise on Fortran 90</li> <li>Basic Fortran Tutorial</li> <li>Detailed Fortran Tutorial</li> <li>Fortran/MPI on an HPC Tutorial</li> </ul>"},{"location":"Documentation/Development/Languages/Fortran/f90_advanced/","title":"Advanced Fortran 90","text":"<p>This document is derived from an HTML page written at the San Diego Supercomper Center many years ago. Its purpose is to Introduce Fortran 90 concepts to Fortran 77 programers.  It does this by presenting an example program and introducing concepts as various routines of the program are presented.  The original web page has been used over the years and has been translated into several languages. </p> <p>Note: See our Fortran Overview page for basic getting started instructions and compiler/toolchain information.</p>"},{"location":"Documentation/Development/Languages/Fortran/f90_advanced/#format-for-our-presentation","title":"Format for our presentation","text":"<ul> <li>We will \"develop\" an application<ul> <li>Incorporate f90 features</li> <li>Show source code</li> <li>Explain what and why as we do it</li> </ul> </li> <li>Application is a genetic algorithm<ul> <li>Easy to understand and program</li> <li>Offers rich opportunities for enhancement</li> </ul> </li> <li>We also provide an summary of F90 syntax, key words, operators, constants, and functions</li> </ul>"},{"location":"Documentation/Development/Languages/Fortran/f90_advanced/#what-was-in-mind-of-the-language-writers-what-were-they-thinking","title":"What was in mind of the language writers? What were they thinking?","text":"<ul> <li>Enable portable codes<ul> <li>Same precision</li> <li>Include many common extensions</li> </ul> </li> <li>More reliable programs</li> <li>Getting away from underlying hardware</li> <li>Move toward parallel programming</li> <li>Run old programs</li> <li>Ease of programming<ul> <li>Writing</li> <li>Maintaining</li> <li>Understanding</li> <li>Reading</li> </ul> </li> <li>Recover C and C++ users</li> </ul>"},{"location":"Documentation/Development/Languages/Fortran/f90_advanced/#why-fortran","title":"Why Fortran?","text":"<p>Famous Quote: \"I don't know what the technical characteristics of  the standard language for scientific and engineering  computation in the year 2000 will be... but I know it  will be called Fortran.\" John Backus.</p> <p>Note: He claimed that he never said this.</p> <ul> <li>Language of choice for Scientific programming</li> <li>Large installed user base.</li> <li>Fortran 90 has most of the features of C . . . and then some</li> <li>The compilers produce better programs</li> </ul>"},{"location":"Documentation/Development/Languages/Fortran/f90_advanced/#justification-of-topics","title":"Justification of topics","text":"<ul> <li>Enhance performance</li> <li>Enhance portability</li> <li>Enhance reliability</li> <li>Enhance maintainability</li> </ul>"},{"location":"Documentation/Development/Languages/Fortran/f90_advanced/#classification-of-topics","title":"Classification of topics","text":"<ul> <li>New useful features</li> <li>Old tricks</li> <li>Power features</li> <li>Overview of F90</li> </ul>"},{"location":"Documentation/Development/Languages/Fortran/f90_advanced/#what-is-a-genetic-algorithm","title":"What is a Genetic Algorithm","text":"<ul> <li>A \"suboptimization\" system<ul> <li>Find good, but maybe not optimal, solutions to difficult problems</li> <li>Often used on NP-Hard or combinatorial optimization problems</li> </ul> </li> <li>Requirements<ul> <li>Solution(s) to the problem represented as a string</li> <li>A fitness function<ul> <li>Takes as input the solution string</li> <li>Output the desirability of the solution</li> </ul> </li> <li>A method of combining solution strings to generate new solutions</li> </ul> </li> <li>Find solutions to problems by Darwinian evolution<ul> <li>Potential solutions ar though of as living entities in a population</li> <li>The strings are the genetic codes for the individuals</li> <li>Fittest individuals are allowed to survive to reproduce</li> </ul> </li> </ul>"},{"location":"Documentation/Development/Languages/Fortran/f90_advanced/#simple-algorithm-for-a-ga","title":"Simple algorithm for a GA","text":"<ul> <li>Generate a initial population, a collection of strings</li> <li>do for some time<ul> <li>evaluate each individual (string) of the population using the fitness function</li> <li>sort the population with fittest coming to the top</li> <li>allow the fittest individuals to \"sexually\" reproduce replacing the old     population</li> <li>allow for mutation</li> </ul> </li> <li>end do</li> </ul>"},{"location":"Documentation/Development/Languages/Fortran/f90_advanced/#our-example-problem","title":"Our example problem","text":"<ul> <li>Instance:Given a map of the N  states or countries and a fixed number of colors</li> <li>Find a coloring of the map, if it exists, such that no two states that share a boarder have the same color</li> <li>Notes         - In general, for a fixed number of colors and an arbitrary map the only                 known way to find if there is a valid coloring is a brute force search                 with the number of combinations = (NUMBER_OF_COLORS)**(NSTATES)<ul> <li>The strings of our population are integer vectors represent the coloring</li> <li>Our fitness function returns the number of boarder violations</li> <li>The GA searches for a mapping with few, hopefully 0 violations</li> <li>This problem is related to several important NP_HARD problems in computer science<ul> <li>Processor scheduling</li> <li>Communication and grid allocation for parallel computing</li> <li>Routing</li> </ul> </li> </ul> </li> </ul> <p>Start of real Fortran 90 discussion</p>"},{"location":"Documentation/Development/Languages/Fortran/f90_advanced/#comparing-a-fortran-77-routine-to-a-fortran-90-routine","title":"Comparing a FORTRAN 77 routine to a Fortran 90 routine","text":"<ul> <li>The routine is one of the random number generators from:  Numerical Recipes, The Art of Scientific Computing. Press, Teukolsky, Vetterling and Flannery.  Cambridge University Press 1986.</li> <li>Changes<ul> <li>correct bugs</li> <li>increase functionality</li> <li>aid portability</li> </ul> </li> </ul>"},{"location":"Documentation/Development/Languages/Fortran/f90_advanced/#original","title":"Original","text":"<pre><code>    function ran1(idum)\n        real ran1\n        integer idum\n        real r(97)\n        parameter ( m1=259200,ia1=7141,ic1=54773)\n        parameter ( m2=134456,ia2=8121,ic2=28411)\n        parameter ( m3=243000,ia3=4561,ic3=51349)\n        integer j\n        integer iff,ix1,ix2,ix3\n        data iff /0/\n        if (idum.lt.0.or.iff.eq.0)then\n            rm1=1.0/m1\n            rm2=1.0/m2\n            iff=1\n            ix1=mod(ic1-idum,m1)\n            ix1=mod(ia1*ix1+ic1,m1)\n            ix2=mod(ix1,m2)\n            ix1=mod(ia1*ix1+ic1,m1)\n            ix3=mod(ix1,m3)\n            do 11 j=1,97\n                ix1=mod(ia1*ix1+ic1,m1)\n                ix2=mod(ia2*ix2+ic2,m2)\n                r(j)=(real(ix1)+real(ix2)*rm2)*rm1\n 11           continue\n            idum=1\n        endif\n        ix1=mod(ia1*ix1+ic1,m1)\n        ix2=mod(ia2*ix2+ic2,m2)\n        ix3=mod(ia3*ix3+ic3,m3)\n        j=1+(97*ix3)/m3\n        if(j.gt.97.or.j.lt.1)then\n            write(*,*)' error in ran1 j=',j\n            stop\n        endif\n        ran1=r(j)\n        r(j)=(real(ix1)+real(ix2)*rm2)*rm1\n        return\n     end \n</code></pre>"},{"location":"Documentation/Development/Languages/Fortran/f90_advanced/#fortran-90","title":"Fortran 90","text":"<pre><code>module ran_mod\ncontains\n     function ran1(idum)\n        use numz\n        implicit none  !note after use statement\n        real (b8) ran1\n        integer , intent(inout), optional ::  idum\n        real (b8) r(97),rm1,rm2\n        integer , parameter :: m1=259200,ia1=7141,ic1=54773\n        integer , parameter :: m2=134456,ia2=8121,ic2=28411\n        integer , parameter :: m3=243000,ia3=4561,ic3=51349\n        integer j\n        integer iff,ix1,ix2,ix3\n        data iff /0/\n        save ! corrects a bug in the original routine\n        if(present(idum))then\n          if (idum.lt.0.or.iff.eq.0)then\n            rm1=1.0_b8 m1\n            rm2=1.0_b8 m2\n            iff=1\n            ix1=mod(ic1-idum,m1)\n            ix1=mod(ia1*ix1+ic1,m1)\n            ix2=mod(ix1,m2)\n            ix1=mod(ia1*ix1+ic1,m1)\n            ix3=mod(ix1,m3)\n            do j=1,97\n                ix1=mod(ia1*ix1+ic1,m1)\n                ix2=mod(ia2*ix2+ic2,m2)\n                r(j)=(real(ix1,b8)+real(ix2,b8)*rm2)*rm1\n            enddo\n            idum=1\n          endif\n        endif\n        ix1=mod(ia1*ix1+ic1,m1)\n        ix2=mod(ia2*ix2+ic2,m2)\n        ix3=mod(ia3*ix3+ic3,m3)\n        j=1+(97*ix3)/m3\n        if(j.gt.97.or.j.lt.1)then\n            write(*,*)' error in ran1 j=',j\n            stop\n        endif\n        ran1=r(j)\n        r(j)=(real(ix1,b8)+real(ix2,b8)*rm2)*rm1\n        return\n     end function ran1\n</code></pre>"},{"location":"Documentation/Development/Languages/Fortran/f90_advanced/#comments","title":"Comments","text":"<ol> <li>Modules are a way of encapsulating functions an data.  More below.</li> <li>The use numz line is similar to an include file.  In this case it defines our real data type.</li> <li>real (b8)  is a new way to specify percision for data types in a portable way.</li> <li>integer , intent(inout), optional ::  idum we are saying idum is an optional input parameter</li> <li>integer , parameter :: just a different syntax</li> <li>The save statement is needed for program correctness</li> <li>present(idum) is a function to determine if ran1 was called with the optional parameter</li> </ol>"},{"location":"Documentation/Development/Languages/Fortran/f90_advanced/#obsolescent-features","title":"Obsolescent features","text":"<p>The following are available in Fortran 90. On the other hand, the concept of \"obsolescence\" is introduced. This means that some constructs may be removed in the future.</p> <ul> <li>Arithmetic IF-statement</li> <li>Control variables in a DO-loop which are floating point or double-precision floating-point</li> <li>Terminating several DO-loops on the same statement</li> <li>Terminating the DO-loop in some other way than with CONTINUE or END DO</li> <li>Alternate return</li> <li>Jump to END IF from an outer block</li> <li>PAUSE</li> <li>ASSIGN and assigned GOTO and assigned FORMAT , that is the whole \"statement number variable\" concept.</li> <li>Hollerith editing in FORMAT.</li> </ul>"},{"location":"Documentation/Development/Languages/Fortran/f90_advanced/#new-source-form-and-related-things","title":"New source form and related things","text":""},{"location":"Documentation/Development/Languages/Fortran/f90_advanced/#summary","title":"Summary","text":"<ul> <li>! now indicates the start of a comment</li> <li>&amp; indicates the next line is a continuation</li> <li>Lines can be longer than 72 characters</li> <li>Statements can start in any column</li> <li>Use ; to put multiple statements on one line</li> <li>New forms for the do loop</li> <li>Many functions are generic</li> <li>32 character names</li> <li>Many new array assignment techniques</li> </ul>"},{"location":"Documentation/Development/Languages/Fortran/f90_advanced/#features","title":"Features","text":"<ul> <li>Flexibility can aid in program readability</li> <li>Readability decreases errors</li> <li>Got ya!<ul> <li>Can no longer use C to start a comment</li> <li>Character in column 5 no longer is continue</li> <li>Tab is not a valid character (may produce a warning)</li> <li>Characters past 72 now count</li> </ul> </li> </ul> <pre><code>program darwin\n     real a(10), b(10), c(10), d(10), e(10), x, y\n     integer odd(5),even(5)\n! this line is continued by using \"&amp;\"\n     write(*,*)\"starting \",&amp;  \n                \"darwin\" ! this line in a continued from above\n! multiple statement per line --rarely a good idea\n     x=1; y=2; write(*,*)x,y  \n     do i=1,10    ! statement lable is not required for do\n        e(i)=i\n     enddo\n     odd= (/ 1,3,5,7,9 /)  ! array assignment\n     even=(/ 2,4,6,8,10 /) ! array assignment\n     a=1          ! array assignment, every element of a = 1\n     b=2\n     c=a+b+e      ! element by element assignment\n     c(odd)=c(even)-1  ! can use arrays of indices on both sides\n     d=sin(c)     ! element by element application of intrinsics\n     write(*,*)d\n     write(*,*)abs(d)  ! many intrinsic functions are generic\n a_do_loop : do i=1,10\n               write(*,*)i,c(i),d(i)\n             enddo a_do_loop\n     do\n        if(c(10) .lt. 0.0 ) exit\n        c(10)=c(10)-1\n     enddo\n     write(*,*)c(10)\n     do while (c(9) .gt. 0)\n        c(9)=c(9)-1\n     enddo\n     write(*,*)c(9)\nend program\n</code></pre>"},{"location":"Documentation/Development/Languages/Fortran/f90_advanced/#new-data-declaration-method","title":"New data declaration method","text":"<ul> <li> <p>Motivation</p> <ul> <li>Variables can now have attributes such as         - Parameter             - Save             - Dimension</li> <li>Attributes are assigned in the variable declaration statement</li> </ul> </li> <li> <p>One variable can have several attributes</p> </li> <li>Requires Fortran 90 to have a new statement form</li> </ul> <p><pre><code>integer,parameter :: in2 = 14\n    real, parameter :: pi = 3.141592653589793239\n    real, save, dimension(10) :: cpu_times,wall_times\n!****    the old way of doing the same    ****!\n!****    real cpu_times(10),wall_times(10) ****!\n!****    save cpu_times, wall_times        ****!\n</code></pre> - Other Attributes     -  allocatable     -  public     -  private     -  target     -  pointer     -  intent     -  optional</p>"},{"location":"Documentation/Development/Languages/Fortran/f90_advanced/#kind-facility","title":"Kind facility","text":"<ul> <li>Motivation<ul> <li>Assume we have a program that we want to run on two different machines</li> <li>We want the same representation of reals on both machines (same number     of significant digits)</li> <li>Problem: different machines have different representations for reals</li> </ul> </li> </ul>"},{"location":"Documentation/Development/Languages/Fortran/f90_advanced/#digits-of-precision-for-some-old-machines-and-data-type","title":"Digits of precision for some (old) machines and data type","text":"Machine Real Double Precision IBM (SP) 6 15 Cray (T90) 15 33 Cray (T3E) 15 15"},{"location":"Documentation/Development/Languages/Fortran/f90_advanced/#or","title":"* or *","text":"<ul> <li>We may want to run with at least 6 digits today and at least 14 digits tomorrow</li> <li>Use the Select_Real_Kind(P) function to create a data type with P digits of precision</li> </ul> <pre><code>program darwin\n! e has at least 4 significant digits\n  real(selected_real_kind(4))e\n! b8 will be used to define reals with 14 digits\n  integer, parameter:: b8 = selected_real_kind(14)\n  real(b8), parameter :: pi = 3.141592653589793239_b8 ! note usage of _b8\n! with  a constant\n! to force precision\n e= 2.71828182845904523536\n  write(*,*)\"starting \",&amp;  ! this line is continued by using \"&amp;\"\n            \"darwin\"       ! this line in a continued from above\n  write(*,*)\"pi has \",precision(pi),\" digits precision \",pi\n  write(*,*)\"e has   \",precision(e),\" digits precision \",e\nend program\n</code></pre>"},{"location":"Documentation/Development/Languages/Fortran/f90_advanced/#example-output","title":"Example output","text":"<pre><code>  sp001  % darwin\n starting darwin\n pi has  15  digits precision  3.14159265358979312\n e has    6  digits precision  2.718281746\nsp001 %\n</code></pre> <ul> <li>Can convert to/from given precision for all variables created using \"b8\" by changing definition of \"b8\"</li> <li>Use the Select_Real_Kind(P,R) function to create a data type with P digits of precision and exponent range of R</li> </ul>"},{"location":"Documentation/Development/Languages/Fortran/f90_advanced/#modules","title":"Modules","text":"<ul> <li> <p>Motivation:</p> <ul> <li>Common block usage is prone to error</li> <li>Provide most of capability of common blocks but safer</li> <li>Provide capabilities beyond common blocks</li> </ul> </li> <li> <p>Modules can contain:</p> <ul> <li>Data definitions</li> <li>Data to be shared much like using a labeled common</li> <li>Functions and subroutines</li> <li>Interfaces (more on this later)</li> </ul> </li> <li> <p>You \"include\" a module with a \"use\" statement</p> </li> </ul> <pre><code>module numz\n  integer,parameter:: b8 = selected_real_kind(14)\n  real(b8),parameter :: pi = 3.141592653589793239_b8\n  integergene_size\nend module\n program darwin\n    use numz\n    implicit none    ! now part of the standard, put it after the use statements\n   write(*,*)\"pi has \",precision(pi),\"\ndigits precision \",pi\n   call set_size()\n   write(*,*)\"gene_size=\",gene_size\n end program\nsubroutine set_size\n  use numz\n  gene_size=10\nend subroutine\n</code></pre>"},{"location":"Documentation/Development/Languages/Fortran/f90_advanced/#an-example-run","title":"An example run","text":"<pre><code>  pi has  15  digits precision  3.14159265358979312\n  gene_size=10\n</code></pre>"},{"location":"Documentation/Development/Languages/Fortran/f90_advanced/#module-functions-and-subroutines","title":"Module functions and subroutines","text":"<ul> <li> <p>Motivation:</p> <ul> <li>Encapsulate related functions and subroutines</li> <li>Can \"USE\" these functions in a program or subroutine</li> <li>Can be provided as a library</li> <li>Only routines that contain the use statement can see the routines</li> </ul> </li> <li> <p>Example is a random number package: <pre><code>module ran_mod\n! module contains three functions\n! ran1 returns a uniform random number between 0-1\n! spread returns random number between min - max\n! normal returns a normal distribution\ncontains\n    function ran1()  !returns random number between 0 - 1\n        use numz\n        implicit none\n        real(b8) ran1,x\n        call random_number(x) ! built in fortran 90 random number function\n        ran1=x\n    end function ran1\n    function spread(min,max)  !returns random # between min/max\n        use numz\n        implicit none\n        real(b8) spread\n        real(b8) min,max\n        spread=(max - min) * ran1() + min\n    end function spread\n    function normal(mean,sigma) !returns a normal distribution\n        use numz\n        implicit none\n        real(b8) normal,tmp\n        real(b8) mean,sigma\n        integer flag\n        real(b8) fac,gsave,rsq,r1,r2\n        save flag,gsave\n        data flag /0/\n        if (flag.eq.0) then\n        rsq=2.0_b8\n            do while(rsq.ge.1.0_b8.or.rsq.eq.0.0_b8) ! new from for do\n                r1=2.0_b8*ran1()-1.0_b8\n                r2=2.0_b8*ran1()-1.0_b8\n                rsq=r1*r1+r2*r2\n            enddo\n            fac=sqrt(-2.0_b8*log(rsq)/rsq)\n            gsave=r1*fac\n            tmp=r2*fac\n            flag=1\n        else\n            tmp=gsave\n            flag=0\n        endif\n        normal=tmp*sigma+mean\n        return\n    end function normal end module ran_mod\n</code></pre></p> </li> </ul> <p>Exersize 1:  Write a program that returns 10 uniform random numbers.</p>"},{"location":"Documentation/Development/Languages/Fortran/f90_advanced/#allocatable-arrays-the-basics","title":"Allocatable arrays (the basics)","text":"<ul> <li> <p>Motivation:</p> <ul> <li>At compile time we may not know the size an array needs to be</li> <li>We may want to change problem size without recompiling</li> </ul> </li> <li> <p>Allocatable arrays allow us to set the size at run time</p> </li> <li>We set the size of the array using the allocate statement</li> <li>We may want to change the lower bound for an array</li> <li>A simple example:</li> </ul> <pre><code>module numz\n  integer, parameter:: b8 = selected_real_kind(14)\n  integer gene_size,num_genes\n  integer,allocatable :: a_gene(:),many_genes(:,:)\nend module\nprogram darwin\n    use numz\n    implicit none\n    integer ierr\n    call set_size()\n    allocate(a_gene(gene_size),stat=ierr) !stat= allows for an error code return\n    if(ierr /= 0)write(*,*)\"allocation error\"  ! /= is .ne.\n    allocate(many_genes(gene_size,num_genes),stat=ierr)  !2d array\n    if(ierr /= 0)write(*,*)\"allocation error\"\n    write(*,*)lbound(a_gene),ubound(a_gene) ! get lower and upper bound\n                                            ! for the array\n    write(*,*)size(many_genes),size(many_genes,1) !get total size and size\n                                                  !along 1st dimension\n    deallocate(many_genes) ! free the space for the array and matrix\n    deallocate(a_gene)\n    allocate(a_gene(0:gene_size)) ! now allocate starting at 0 instead of 1\n    write(*,*)allocated(many_genes),allocated(a_gene) ! shows if allocated\n    write(*,*)lbound(a_gene),ubound(a_gene)\nend program\n  subroutine set_size\n    use numz\n    write(*,*)'enter gene size:'\n    read(*,*)gene_size\n    write(*,*)'enter number of genes:'\n    read(*,*)num_genes\nend subroutine set_size\n</code></pre>"},{"location":"Documentation/Development/Languages/Fortran/f90_advanced/#example-run","title":"Example run","text":"<pre><code>    enter gene size:\n10\n enter number of genes:\n20\n           1          10\n         200          10\n F T\n           0          10\n</code></pre>"},{"location":"Documentation/Development/Languages/Fortran/f90_advanced/#passing-arrays-to-subroutines","title":"Passing arrays to subroutines","text":"<ul> <li>There are several ways to specify arrays for subroutines<ul> <li>Explicit shape<ul> <li>integer, dimension(8,8)::an_explicit_shape_array</li> </ul> </li> <li>Assumed size<ul> <li>integer, dimension(i,*)::an_assumed_size_array</li> </ul> </li> <li>Assumed Shape<ul> <li>integer, dimension(:,:)::an_assumed_shape_array</li> </ul> </li> </ul> </li> </ul>"},{"location":"Documentation/Development/Languages/Fortran/f90_advanced/#example","title":"Example","text":"<pre><code>subroutine arrays(an_explicit_shape_array,&amp;\n                  i                      ,&amp; !note we pass all bounds except the last\n                  an_assumed_size_array  ,&amp;\n                  an_assumed_shape_array)\n! Explicit shape\n    integer, dimension(8,8)::an_explicit_shape_array\n! Assumed size\n    integer, dimension(i,*)::an_assumed_size_array\n! Assumed Shape\n    integer, dimension(:,:)::an_assumed_shape_array\n    write(*,*)sum(an_explicit_shape_array)\n    write(*,*)lbound(an_assumed_size_array) ! why does sum not work here?\n    write(*,*)sum(an_assumed_shape_array)\nend subroutine\n</code></pre>"},{"location":"Documentation/Development/Languages/Fortran/f90_advanced/#interface-for-passing-arrays","title":"Interface for passing arrays","text":"<ul> <li>!!!!Warning!!!!  When passing assumed shape arrays as arguments you must provide an interface</li> <li>Similar to C prototypes but much more versatile</li> <li>The interface is a copy of the invocation line and the argument definitions</li> <li>Modules are a good place for interfaces</li> <li>If a procedure is part of a \"contains\" section in a module an interface     is not required</li> <li>!!!!Warning!!!! The compiler may not tell you that you need an interface <pre><code>module numz\n    integer, parameter:: b8 = selected_real_kind(14)\n    integer,allocatable :: a_gene(:),many_genes(:,:)\nend module module face\n    interface fitness\n        function fitness(vector)\n        use numz\n        implicit none\n        real(b8) fitness\n        integer, dimension(:) ::  vector\n        end function fitness\n    end interface\nend module program darwin\n    use numz\n    use face\n    implicit none\n    integer i\n    integer vect(10) ! just a regular array\n    allocate(a_gene(10));allocate(many_genes(3,10))\n    a_gene=1  !sets every element of a_gene to 1\n    write(*,*)fitness(a_gene)\n    vect=8\n    write(*,*)fitness(vect) ! also works with regular arrays\n    many_genes=3  !sets every element to 3\n    many_genes(1,:)=a_gene  !sets column 1 to a_gene\n    many_genes(2,:)=2*many_genes(1,:)\n    do i=1,3\n        write(*,*)fitness(many_genes(i,:))\n    enddo\n    write(*,*)fitness(many_genes(:,1))  !go along other dimension\n!!!!write(*,*)fitness(many_genes)!!!!does not work\nend program\nfunction fitness(vector)\n    use numz\n    implicit none\n    real(b8) fitness\n    integer, dimension(:)::  vector ! must match interface\n    fitness=sum(vector)\nend function\n</code></pre></li> </ul> <p>Exersize 2:  Run this program using the \"does not work line\". Why?  Using intrinsic functions make it work?</p> <p>Exersize 3:  Prove that f90 does not \"pass by address\".</p>"},{"location":"Documentation/Development/Languages/Fortran/f90_advanced/#optional-arguments-and-intent","title":"Optional arguments and intent","text":"<ul> <li>Motivation:<ul> <li>We may have a function or subroutine that we may not want to always pass     all arguments</li> <li>Initialization</li> </ul> </li> <li>Two examples<ul> <li>Seeding the intrinsic random number generator requires keyword arguments</li> <li>To define an optional argument in our own function we use the optional     attribute</li> </ul> </li> </ul> <pre><code>integer :: my_seed\n</code></pre>"},{"location":"Documentation/Development/Languages/Fortran/f90_advanced/#becomes","title":"becomes","text":"<pre><code>integer, optional :: my_seed\n</code></pre> <p>Used like this:</p> <pre><code>! ran1 returns a uniform random number between 0-1\n! the seed is optional and used to reset the generator\ncontains\n   function ran1(my_seed)\n      use numz\n      implicit none\n      real(b8) ran1,r\n      integer, optional ,intent(in) :: my_seed  ! optional argument not changed in the routine\n      integer,allocatable :: seed(:)\n      integer the_size,j\n      if(present(my_seed))then            ! use the seed if present\n          call random_seed(size=the_size) ! how big is the intrisic seed?\n          allocate(seed(the_size))        ! allocate space for seed\n          do j=1,the_size                 ! create the seed\n             seed(j)=abs(my_seed)+(j-1)   ! abs is generic\n          enddo\n          call random_seed(put=seed)      ! assign the seed\n          deallocate(seed)                ! deallocate space\n      endif\n      call random_number(r)\n      ran1=r\n  end function ran1\nend module program darwin\n    use numz\n    use ran_mod          ! interface required if we have\n                         ! optional or intent arguments\n    real(b8) x,y\n    x=ran1(my_seed=12345) ! we can specify the name of the argument\n    y=ran1()\n    write(*,*)x,y\n    x=ran1(12345)         ! with only one optional argument we don't need to\n    y=ran1()\n    write(*,*)x,y\nend program\n</code></pre> <ul> <li>Intent is a hint to the compiler to enable optimization<ul> <li>intent(in)<ul> <li>We will not change this value in our subroutine</li> </ul> </li> <li>intent(out)<ul> <li>We will define this value in our routine</li> </ul> </li> <li>intent(inout)<ul> <li>The normal situation</li> </ul> </li> </ul> </li> </ul>"},{"location":"Documentation/Development/Languages/Fortran/f90_advanced/#derived-data-types","title":"Derived data types","text":"<ul> <li> <p>Motivation:</p> <ul> <li>Derived data types can be used to group different types of data together     (integers, reals, character, complex)</li> <li>Can not be done in F77 although people have \"faked\" it</li> </ul> </li> <li> <p>Example</p> <ul> <li>In our GA we define a collection of genes as a 2d array</li> <li>We call the fitness function for every member of the collection</li> <li>We want to sort the collection of genes based on result of fitness function</li> <li>Define a data type that holds the fitness value and an index into the 2d     array</li> <li>Create an array of this data type, 1 for each member of the collection</li> <li>Call fitness function with the result being placed into the new data type     along with a pointer into the array</li> </ul> </li> <li>Again modules are a good place for data type definitions</li> </ul> <pre><code>module galapagos\n    use numz\n    type thefit !the name of the type\n      sequence  ! sequence forces the data elements\n                ! to be next to each other in memory\n                ! where might this be useful?\n      real(b8) val   ! our result from the fitness function\n      integer index  ! the index into our collection of genes\n    end type thefit\nend module\n</code></pre>"},{"location":"Documentation/Development/Languages/Fortran/f90_advanced/#using-defined-types","title":"Using defined types","text":"<ul> <li>Use the % to reference various components of the derived data type <pre><code>program darwin\n    use numz\n    use galapagos ! the module that contains the type definition\n    use face      ! contains various interfaces\n implicit none\n! define an allocatable array of the data type\n! than contains an index and a real value\n    type (thefit),allocatable ,target  :: results(:)\n! create a single instance of the data type\n    type (thefit) best\n    integer,allocatable :: genes(:,:) ! our genes for the genetic algorithm\n    integer j\n    integer num_genes,gene_size\n    num_genes=10\n    gene_size=10\n    allocate(results(num_genes))         ! allocate the data type\n                                         ! to hold fitness and index\n    allocate(genes(num_genes,gene_size)) ! allocate our collection of genes\n    call init_genes(genes)               ! starting data\n    write(*,'(\"input\")' ) ! we can put format in write statement\n    do j=1,num_genes\n       results(j)%index =j\n       results(j)%val =fitness(genes(j,:)) ! just a dummy routine for now\n       write(*,\"(f10.8,i4)\")results(j)%val,results(j)%index\n    enddo\nend program\n</code></pre></li> </ul>"},{"location":"Documentation/Development/Languages/Fortran/f90_advanced/#user-defined-operators","title":"User defined operators","text":"<ul> <li> <p>Motivation</p> <ul> <li>With derived data types we may want (need) to define operations</li> <li>(Assignment is predefined)</li> </ul> </li> <li> <p>Example:</p> <ul> <li>.lt. .gt. ==  not defined for our data types         -  We want to find the minimum of our fitness values so we need &lt; operator         -  In our sort routine we want to do &lt;, &gt;, ==         -  In C++ terms the operators are overloaded</li> <li>We are free to define new operators</li> </ul> </li> <li> <p>Two step process to define operators</p> <ul> <li>Define a special interface</li> <li>Define the function that performs the operation <pre><code>module sort_mod\n!defining the interfaces\n  interface operator (.lt.)  ! overloads standard .lt.\n    module procedure theless ! the function that does it\n  end interface   interface operator (.gt.)   ! overloads standard .gt.\n    module procedure thegreat ! the function that does it\n  end interface   interface operator (.ge.)  ! overloads standard .ge.\n    module procedure thetest ! the function that does it\n  end interface   interface operator (.converged.)  ! new operator\n    module procedure index_test     ! the function that does it\n  end interface\n  contains      ! our module will contain\n              ! the required functions\n    function theless(a,b) ! overloads .lt. for the type (thefit)\n    use galapagos\n    implicit none\n    type(thefit), intent (in) :: a,b\n    logical theless           ! what we return\n    if(a%val .lt. b%val)then     ! this is where we do the test\n        theless=.true.\n    else\n        theless=.false.\n    endif\n    return\n  end function theless   function thegreat(a,b) ! overloads .gt. for the type (thefit)\n    use galapagos\n    implicit none\n    type(thefit), intent (in) :: a,b\n    logical thegreat\n    if(a%val .gt. b%val)then\n        thegreat=.true.\n    else\n        thegreat=.false.\n    endif\n    return\n  end function thegreat\n  function thetest(a,b)   ! overloads .gt.= for the type (thefit)\n    use galapagos\n    implicit none\n    type(thefit), intent (in) :: a,b\n    logical thetest\n    if(a%val &gt;= b%val)then\n        thetest=.true.\n    else\n        thetest=.false.\n    endif\n    return\nend function thetest\n  function index_test(a,b) ! defines a new operation for the type (thefit)\n    use galapagos\n    implicit none\n    type(thefit), intent (in) :: a,b\n    logical index_test\n    if(a%index .gt. b%index)then   ! check the index value for a difference\n        index_test=.true.\n    else\n        index_test=.false.\n    endif\n    return\nend function index_test\n</code></pre></li> </ul> </li> </ul>"},{"location":"Documentation/Development/Languages/Fortran/f90_advanced/#recursive-functions-introduction","title":"Recursive functions introduction","text":"<ul> <li> <p>Notes</p> <ul> <li>Recursive function is one that calls itself</li> <li>Anything that can be done with a do loop can be done using a recursive     function</li> </ul> </li> <li> <p>Motivation</p> <ul> <li>Sometimes it is easier to think recursively</li> <li>Divide an conquer algorithms are recursive by nature         -  Fast FFTs             -  Searching             -  Sorting</li> </ul> </li> </ul>"},{"location":"Documentation/Development/Languages/Fortran/f90_advanced/#algorithm-of-searching-for-minimum-of-an-array","title":"Algorithm of searching for minimum of an array","text":"<pre><code>    function findmin(array)\n        is size of array 1?\n           min in the array is first element\n        else\n           find minimum in left half of array using findmin function\n           find minimum in right half of array using findmin function\n           global minimum is min of left and right half\n    end function\n</code></pre>"},{"location":"Documentation/Development/Languages/Fortran/f90_advanced/#fortran-90-recursive-functions","title":"Fortran 90 recursive functions","text":"<ul> <li>Recursive functions should have an interface</li> <li>The result and recursive keywords are required as part of the function definition</li> <li>Example is a function finds the minimum value for an array</li> </ul> <pre><code>recursive function realmin(ain) result (themin)\n! recursive and result are required for recursive functions\n    use numz\n    implicit none\n    real(b8) themin,t1,t2\n    integer n,right\n    real(b8) ,dimension(:) :: ain\n    n=size(ain)\n    if(n == 1)then\n       themin=ain(1) ! if the size is 1 return value\n    return\n    else\n      right=n/2\n      t1=realmin(ain(1:right))   ! find min in left half\n      t2=realmin(ain(right+1:n)) ! find min in right half\n      themin=min(t1,t2)          ! find min of the two sides\n     endif\nend function\n</code></pre> <ul> <li>Example 2 is the same except the input data is our derived data type</li> </ul> <pre><code>!this routine works with the data structure thefit not reals\nrecursive function typemin(ain) result (themin)\n    use numz\n use sort_mod\n use galapagos\n implicit none\n real(b8) themin,t1,t2\n integer n,right\n    type (thefit) ,dimension(:) :: ain ! this line is different\n n=size(ain)\n if(n == 1)then\n     themin=ain(1)%val  ! this line is different\n  return\n else\n  right=n/2\n  t1=typemin(ain(1:right))\n  t2=typemin(ain(right+1:n))\n  themin=min(t1,t2)\n endif\nend function\n</code></pre>"},{"location":"Documentation/Development/Languages/Fortran/f90_advanced/#pointers","title":"Pointers","text":"<ul> <li> <p>Motivation</p> <ul> <li>Can increase performance</li> <li>Can improve readability</li> <li>Required for some derived data types (linked lists and trees)</li> <li>Useful for allocating \"arrays\" within subroutines</li> <li>Useful for referencing sections of arrays</li> </ul> </li> <li> <p>Notes</p> <ul> <li>Pointers can be thought of as an alias to another variable</li> <li>In some cases can be used in place of an array</li> <li>To assign a pointer use =&gt; instead of just =</li> <li>Unlike C and C++, pointer arithmetic is not allowed</li> </ul> </li> <li> <p>First pointer example</p> <ul> <li>Similar to the last findmin routine</li> <li>Return a pointer to the minimum</li> </ul> </li> </ul> <pre><code>recursive function pntmin(ain) result (themin) ! return a pointer\n use numz\n use galapagos\n use sort_mod ! contains the .lt. operator for thefit type\n implicit none\n type (thefit),pointer:: themin,t1,t2\n integer n,right\n    type (thefit) ,dimension(:),target :: ain\n n=size(ain)\n if(n == 1)then\n     themin=&gt;ain(1) !this is how we do pointer assignment\n  return\n else\n  right=n/2\n  t1=&gt;pntmin(ain(1:right))\n  t2=&gt;pntmin(ain(right+1:n))\n  if(t1 .lt. t2)then; themin=&gt;t1; else; themin=&gt;t2; endif\n endif\nend function\n</code></pre> <p>Exercise 4:  Carefully write a recursive N! program.</p>"},{"location":"Documentation/Development/Languages/Fortran/f90_advanced/#function-and-subroutine-overloading","title":"Function and subroutine overloading","text":"<ul> <li> <p>Motivation</p> <ul> <li>Allows us to call functions or subroutine with the same name with different     argument types</li> <li>Increases readability</li> </ul> </li> <li> <p>Notes:</p> <ul> <li>Similar in concept to operator overloading</li> <li>Requires an interface</li> <li>Syntax for subroutines is same as for functions</li> <li>Many intrinsic functions have this capability         -  abs (reals,complex,integer)             -  sin,cos,tan,exp(reals, complex)             -  array functions(reals, complex,integer)</li> <li>Example         -  Recall we had two functions that did the same thing but with different argument types</li> </ul> </li> </ul> <p><pre><code>         recursive function realmin(ain) result (themin)\n         real(b8) ,dimension(:) :: ain         recursive function typemin(ain) result (themin)\n         type (thefit) ,dimension(:) :: ain\n</code></pre> - We can define a generic interface for these two functions and call     them using the same name</p> <pre><code>! note we have two functions within the same interface\n! this is how we indicate function overloading\n! both functions are called \"findmin\" in the main program\ninterface findmin\n! the first is called with an array of reals as input\n        recursive function realmin(ain) result (themin)\n          use numz\n       real(b8) themin\n          real(b8) ,dimension(:) :: ain\n        end function ! the second is called with a array of data structures as input\n     recursive function typemin(ain) result (themin)\n          use numz\n    use galapagos\n       real(b8) themin\n          type (thefit) ,dimension(:) :: ain\n     end function\n    end interface\n</code></pre>"},{"location":"Documentation/Development/Languages/Fortran/f90_advanced/#example-usage","title":"Example usage","text":"<pre><code>program darwin\n    use numz\n    use ran_mod\n    use galapagos ! the module that contains the type definition\n    use face      ! contains various interfaces\n    use sort_mod  ! more about this later it\n                  ! contains our sorting routine\n      ! and a few other tricks\n    implicit none\n! create an allocatable array of the data type\n! than contains an index and a real value\n    type (thefit),allocatable ,target :: results(:)\n! create a single instance of the data type\n    type (thefit) best\n! pointers to our type\n    type (thefit) ,pointer :: worst,tmp\n    integer,allocatable :: genes(:,:) ! our genes for the ga\n    integer j\n    integer num_genes,gene_size\n    real(b8) x\n    real(b8),allocatable :: z(:)\n    real(b8),pointer :: xyz(:) ! we'll talk about this next\n    num_genes=10\n    gene_size=10\n    allocate(results(num_genes))         ! allocate the data type to\n    allocate(genes(num_genes,gene_size)) ! hold our collection of genes\n    call init_genes(genes)               ! starting data\n    write(*,'(\"input\")')\n    do j=1,num_genes\n       results(j)%index=j\n       results(j)%val=fitness(genes(j,:)) ! just a dummy routine\n       write(*,\"(f10.8,i4)\")results(j)%val,results(j)%index\n    enddo     allocate(z(size(results)))\n    z=results(:)%val ! copy our results to a real array ! use a recursive subroutine operating on the real array\n    write(*,*)\"the lowest fitness: \",findmin(z)\n! use a recursive subroutine operating on the data structure\n    write(*,*)\"the lowest fitness: \",findmin(results)\nend program\n</code></pre>"},{"location":"Documentation/Development/Languages/Fortran/f90_advanced/#fortran-minval-and-minloc-routines","title":"Fortran Minval and Minloc routines","text":"<ul> <li>Fortran has routines for finding minimum and maximum values in arrays and     the locations<ul> <li>minval</li> <li>maxval</li> <li>minloc (returns an array)</li> <li>maxloc (returns an array)</li> </ul> </li> </ul> <pre><code>! we show two other methods of getting the minimum fitness\n! use the built in f90 routines  on a real array\n    write(*,*)\"the lowest fitness: \",minval(z),minloc(z)\n</code></pre>"},{"location":"Documentation/Development/Languages/Fortran/f90_advanced/#pointer-assignment","title":"Pointer assignment","text":"<ul> <li>This is how we use the pointer function defined above</li> <li>worst is a pointer to our data type</li> <li>note the use of =&gt; <pre><code>! use a recursive subroutine operating on the data\n! structure and returning a pointer to the result\n    worst=&gt;pntmin(results) ! note pointer assignment\n! what will this line write?\n write(*,*)\"the lowest fitness: \",worst\n</code></pre></li> </ul>"},{"location":"Documentation/Development/Languages/Fortran/f90_advanced/#more-pointer-usage-association-and-nullify","title":"More pointer usage, association and nullify","text":"<ul> <li> <p>Motivation</p> <ul> <li>Need to find if pointers point to anything</li> <li>Need to find if two pointers point to the same thing</li> <li>Need to deallocate and nullify when they are no longer used</li> </ul> </li> <li> <p>Usage</p> <ul> <li>We can use associated() to tell if a pointer has been set</li> <li>We can use associated() to compare pointers</li> <li>We use nullify to zero a pointer</li> </ul> </li> </ul> <pre><code>! This code will print \"true\" when we find a match,\n! that is the pointers point to the same object\n    do j=1,num_genes\n     tmp=&gt;results(j)\n        write(*,\"(f10.8,i4,l3)\")results(j)%val,   &amp;\n                                results(j)%index, &amp;\n           associated(tmp,worst)\n    enddo\n    nullify(tmp)\n</code></pre> <ul> <li>Notes:<ul> <li>If a pointer is nullified the object to which it points is not deallocated.</li> <li>In general, pointers as well as allocatable arrays become undefined on leaving a subroutine</li> <li>This can cause a memory leak</li> </ul> </li> </ul>"},{"location":"Documentation/Development/Languages/Fortran/f90_advanced/#pointer-usage-to-reference-an-array-without-copying","title":"Pointer usage to reference an array without copying","text":"<ul> <li>Motivation<ul> <li>Our sort routine calls a recursive sorting routine</li> <li>It is messy and inefficient to pass the array to the recursive routine</li> </ul> </li> <li>Solution<ul> <li>We define a \"global\" pointer in a module</li> <li>We point the pointer to our input array</li> </ul> </li> </ul> <pre><code>module Merge_mod_types\n    use galapagos\n    type(thefit),allocatable :: work(:) ! a \"global\" work array\n    type(thefit), pointer:: a_pntr(:)   ! this will be the pointer to our input array\nend module Merge_mod_types\n  subroutine Sort(ain, n)\n    use Merge_mod_types\n    implicit none\n    integer n\n    type(thefit), target:: ain(n)\n    allocate(work(n))\n    nullify(a_pntr)\n    a_pntr=&gt;ain  ! we assign the pointer to our array\n                 ! in RecMergeSort we reference it just like an array\n    call RecMergeSort(1,n) ! very similar to the findmin functions\n    deallocate(work)\n    return\nend subroutine Sort\n</code></pre> <ul> <li>In our main program sort is called like this: <pre><code>! our sort routine is also recursive but\n! also shows a new usage for pointers\n    call sort(results,num_genes)\n    do j=1,num_genes\n       write(*,\"(f10.8,i4)\")results(j)%val,   &amp;\n                            results(j)%index\n    enddo\n</code></pre></li> </ul>"},{"location":"Documentation/Development/Languages/Fortran/f90_advanced/#data-assignment-with-structures","title":"Data assignment with structures","text":"<pre><code>! we can copy a whole structure\n! with a single assignment\n    best=results(1)\n    write(*,*)\"best result \",best\n</code></pre>"},{"location":"Documentation/Development/Languages/Fortran/f90_advanced/#using-the-user-defined-operator","title":"Using the user defined operator","text":"<pre><code>! using the user defined operator to see if best is worst\n! recall that the operator .converged. checks to see if %index matches\n    worst=&gt;pntmin(results)\n    write(*,*)\"worst result \",worst\n    write(*,*)\"converged=\",(best .converged. worst)\n</code></pre>"},{"location":"Documentation/Development/Languages/Fortran/f90_advanced/#passing-arrays-with-a-given-arbitrary-lower-bounds","title":"Passing arrays with a given arbitrary lower bounds","text":"<ul> <li> <p>Motivation</p> <ul> <li> <p>Default lower bound within a subroutine is 1</p> </li> <li> <p>May want to use a different lower bound</p> </li> </ul> </li> </ul> <pre><code>    if(allocated(z))deallocate(z)\n    allocate(z(-10:10)) ! a 21 element array\n    do j=-10,10\n       z(j)=j\n    enddo ! pass z and its lower bound\n! in this routine we give the array a specific lower\n! bound and show how to use a pointer to reference\n! different parts of an array using different indices\n  call boink1(z,lbound(z,1)) ! why not just lbound(z) instead of lbound(z,1)?\n                             ! lbound(z) returns a rank 1 array\n     subroutine boink1(a,n)\n     use numz\n     implicit none\n     integer,intent(in) :: n\n     real(b8),dimension(n:):: a ! this is how we set lower bounds in a subroutine\n     write(*,*)lbound(a),ubound(a)\n   end subroutine\n</code></pre>"},{"location":"Documentation/Development/Languages/Fortran/f90_advanced/#warning-because-we-are-using-an-assumed-shape-array-we-need-an-interface","title":"Warning:  because we are using an assumed shape array we need an interface","text":""},{"location":"Documentation/Development/Languages/Fortran/f90_advanced/#using-pointers-to-access-sections-of-arrays","title":"Using pointers to access sections of arrays","text":"<ul> <li>Motivation<ul> <li>Can increase efficiency</li> <li>Can increase readability</li> </ul> </li> </ul> <pre><code>call boink2(z,lbound(z,1))\n\nsubroutine boink2(a,n)\nuse numz\nimplicit none\ninteger,intent(in) :: n\nreal(b8),dimension(n:),target:: a\nreal(b8),dimension(:),pointer::b\nb=&gt;a(n:) ! b(1) \"points\" to a(-10)\nwrite(*,*)\"a(-10) =\",a(-10),\"b(1) =\",b(1)\nb=&gt;a(0:) ! b(1) \"points\" to a(0)\nwrite(*,*)\"a(-6) =\",a(-6),\"b(-5) =\",b(-5)\nend subroutine\n</code></pre>"},{"location":"Documentation/Development/Languages/Fortran/f90_advanced/#allocating-an-array-inside-a-subroutine-and-passing-it-back","title":"Allocating an array inside a subroutine and passing it back","text":"<ul> <li>Motivation<ul> <li>Size of arrays are calculated in the subroutine</li> </ul> </li> </ul> <pre><code>module numz\n    integer, parameter:: b8 = selected_real_kind(14)\nend module\nprogram bla\n   use numz\n   real(b8), dimension(:) ,pointer :: xyz\n   interface boink\n     subroutine boink(a)\n     use numz\n     implicit none\n     real(b8), dimension(:), pointer :: a\n     end subroutine\n   end interface\n   nullify(xyz) ! nullify sets a pointer to null\n   write(*,'(l5)')associated(xyz) ! is a pointer null, should be\n   call boink(xyz)\n   write(*,'(l5)',advance=\"no\")associated(xyz)\n   if(associated(xyz))write(*,'(i5)')size(xyz)\nend program\nsubroutine boink(a)\n    use numz\n    implicit none\n    real(b8),dimension(:),pointer:: a\n    if(associated(a))deallocate(a)\n    allocate(a(10))\nend subroutine\n</code></pre>"},{"location":"Documentation/Development/Languages/Fortran/f90_advanced/#an-example-run_1","title":"An example run","text":"<pre><code>     F\n     T\n10\n</code></pre>"},{"location":"Documentation/Development/Languages/Fortran/f90_advanced/#our-fitness-function","title":"Our fitness function","text":"<p>Given a fixed number of colors, M, and a description of a map of a collection of  N states.</p> <p>Find a coloring of the map such that no two states that share a boarder have the same coloring.</p>"},{"location":"Documentation/Development/Languages/Fortran/f90_advanced/#example-input-is-a-sorted-list-of-22-western-states","title":"Example input is a sorted list of 22 western states","text":"<pre><code>22\nar ok tx la mo xx\naz ca nm ut nv xx\nca az nv or xx\nco nm ut wy ne ks xx\nia mo ne sd mn xx\nid wa or nv ut wy mt xx\nks ne co ok mo xx\nla tx ar xx\nmn ia sd nd xx\nmo ar ok ks ne ia xx\nmt wy id nd xx\nnd mt sd wy xx\nne sd wy co ks mo ia xx\nnm az co ok tx mn xx\nnv ca or id ut az xx\nok ks nm tx ar mo xx\nor ca wa id xx\nsd nd wy ne ia mn xx\ntx ok nm la ar xx\nut nv az co wy id xx\nwa id or mt xx\nwy co mt id ut nd sd ne xx\n</code></pre> <p>Our fitness function takes a potential coloring, that is, an integer vector of length N and a returns the number of boarders that have states of the same coloring</p> <ul> <li>How do we represent the map in memory?<ul> <li>One way would be to use an array but it would be very sparse</li> <li>Linked lists are often a better way</li> </ul> </li> </ul>"},{"location":"Documentation/Development/Languages/Fortran/f90_advanced/#linked-lists","title":"Linked lists","text":"<ul> <li> <p>Motivation</p> <ul> <li>We have a collection of states and for each state a list of adjoining states. (Do not count a boarder twice.)</li> <li>Problem is that you do not know the length of the list until runtime.</li> <li> <p>List of adjoining states will be different lengths for different states</p> </li> <li> <p>Solution         -   Linked list are a good way to handle such situations</p> </li> <li>Linked lists use a derived data type with at least two components<ul> <li>Data  </li> <li>Pointer to next element</li> </ul> </li> </ul> </li> </ul> <pre><code>module list_stuff\ntype llist\ninteger index ! data\ntype(llist),pointer::next ! pointer to the\n! next element\nend type llist\nend module\n</code></pre>"},{"location":"Documentation/Development/Languages/Fortran/f90_advanced/#linked-list-usage","title":"Linked list usage","text":"<p>One way to fill a linked list is to use a recursive function <code>`fortran</code> recursive subroutine insert (item, root) use list_stuff implicit none type(llist), pointer :: root integer item if (.not. associated(root)) then allocate(root) nullify(root%next) root%index = item else call insert(item,root%next) endif end subroutine <pre><code>- - -\n- - -\n\n## Our map representation\n- An array of the derived data type states\n            -   State is name of a state\n    -   Linked list containing boarders\n\n```fortran\n    type states\n        character(len=2)name\n        type(llist),pointer:: list\n    end type states\n</code></pre> - Notes:     -  We have an array of linked lists             -  This data structure is often used to represent sparse arrays                 -  We could have a linked list of linked lists     -  State name is not really required</p>"},{"location":"Documentation/Development/Languages/Fortran/f90_advanced/#date-and-time-functions","title":"Date and time functions","text":"<ul> <li> <p>Motivation</p> <ul> <li> <p>May want to know the date and time of your program</p> </li> <li> <p>Two functions</p> </li> </ul> </li> </ul> <pre><code>! all arguments are optional\ncall date_and_time(date=c_date, &amp;  ! character(len=8) ccyymmdd\n                   time=c_time, &amp;  ! character(len=10) hhmmss.sss\n                   zone=c_zone, &amp;  ! character(len=10) +/-hhmm (time zone)\n                   values=ivalues) ! integer ivalues(8) all of the above\n           call system_clock(count=ic,           &amp; ! count of system clock (clicks)\n                  count_rate=icr,     &amp; ! clicks / second\n                  count_max=max_c)      ! max value for count\n</code></pre>"},{"location":"Documentation/Development/Languages/Fortran/f90_advanced/#non-advancing-and-character-io","title":"Non advancing and character IO","text":"<ul> <li> <p>Motivation</p> <ul> <li> <p>We read the states using the two character identification</p> </li> <li> <p>One line per state and do not know how many boarder states per line</p> </li> </ul> </li> <li> <p>Note: Our list of states is presorted <pre><code>character(len=2) a ! we have a character variable of length 2\nread(12,*)nstates ! read the number of states\nallocate(map(nstates)) ! and allocate our map\ndo i=1,nstates\n    read(12,\"(a2)\",advance=\"no\")map(i)%name ! read the name\n    !write(*,*)\"state:\",map(i)%name\n    nullify(map(i)%list) ! \"zero out\" our list\n    do\n        read(12,\"(1x,a2)\",advance=\"no\")a ! read list of states\n        ! without going to the\n        ! next line\n        if(lge(a,\"xx\") .and. lle(a,\"xx\"))then ! if state == xx\n        backspace(12) ! go to the next line\n        read(12,\"(1x,a2)\",end=1)a ! go to the next line\n        exit\n        endif\n        1 continue\n        if(llt(a,map(i)%name))then ! we only add a state to\n        ! our list if its name\n        ! is before ours thus we\n        ! only count boarders 1 time\n        ! what we want put into our linked list is an index\n        ! into our map where we find the bordering state\n        ! thus we do the search here\n        ! any ideas on a better way of doing this search?\n        found=-1\n        do j=1,i-1\n            if(lge(a,map(j)%name) .and. lle(a,map(j)%name))then\n            !write(*,*)a\n            found=j\n            exit\n            endif\n        enddo\n        if(found == -1)then\n            write(*,*)\"error\"\n            stop\n        endif\n        ! found the index of the boarding state insert it into our list\n        ! note we do the insert into the linked list for a particular state\n        call insert(found,map(i)%list)\n        endif\n    enddo\nenddo\n</code></pre></p> </li> </ul>"},{"location":"Documentation/Development/Languages/Fortran/f90_advanced/#internal-io","title":"Internal IO","text":"<ul> <li> <p>Motivation</p> <ul> <li> <p>May need to create strings on the fly</p> </li> <li> <p>May need to convert from strings to reals and integers</p> </li> <li> <p>Similar to sprintf and sscanf</p> </li> </ul> </li> <li> <p>How it works</p> <ul> <li> <p>Create a string</p> </li> <li> <p>Do a normal write except write to the string instead of file number</p> </li> </ul> </li> <li> <p>Example 1: creating a date and time stamped file name</p> </li> </ul> <pre><code>character (len=12)tmpstr\n\nwrite(tmpstr,\"(a12)\")(c_date(5:8)//c_time(1:4)//\".dat\") ! // does string concatination\nwrite(*,*)\"name of file= \",tmpstr\nopen(14,file=tmpstr)\nname of file= 03271114.dat\n</code></pre> <ul> <li>Example 2: Creating a format statement at run time (array of integers and a real)</li> </ul> <p><pre><code>! test_vect is an array that we do not know its length until run time\nnstate=9 ! the size of the array\nwrite(fstr,'(\"(\",i4,\"i1,1x,f10.5)\")')nstates\nwrite(*,*)\"format= \",fstr\nwrite(*,fstr)test_vect,fstr\nformat= ( 9i1,1x,f10.5)\n</code></pre> Any other ideas for writing an array when you do not know its length?</p> <ul> <li>Example 3: Reading from a string <pre><code>integer ht,minut,sec\nread(c_time,\"(3i2)\")hr,minut,sec\n</code></pre></li> </ul>"},{"location":"Documentation/Development/Languages/Fortran/f90_advanced/#inquire-function","title":"Inquire function","text":"<ul> <li>Motivation<ul> <li>Need to get information about I/O</li> </ul> </li> <li> <p>Inquire statement has two forms</p> <ul> <li>Information about files (23 different requests can be done)</li> <li>Information about space required for binary output of a value</li> </ul> </li> <li> <p>Example: find the size of your real relative to the \"standard\" real</p> <ul> <li>Useful for inter language programming</li> <li>Useful for determining data types in MPI (MPI_REAL or MPI_DOUBLE_PRECISION)</li> </ul> </li> </ul> <pre><code>inquire(iolength=len_real)1.0\ninquire(iolength=len_b8)1.0_b8\nwrite(*,*)\"len_b8 \",len_b8\nwrite(*,*)\"len_real\",len_real\niratio=len_b8/len_real\nselect case (iratio)\n    case (1)\n      my_mpi_type=mpi_real\n    case(2)\n      my_mpi_type=mpi_double_precision\n    case default\n      write(*,*)\"type undefined\"\n      my_mpi_type=0\nend select\n</code></pre>"},{"location":"Documentation/Development/Languages/Fortran/f90_advanced/#an-example-run_2","title":"An example run","text":"<pre><code>len_b8 2\nlen_real 1\n</code></pre>"},{"location":"Documentation/Development/Languages/Fortran/f90_advanced/#namelist","title":"Namelist","text":"<ul> <li>Now part of the standard</li> <li>Motivation<ul> <li>A convenient method of doing I/O</li> <li>Good for cases where you have similar runs but change one or two variables</li> <li>Good for formatted output</li> </ul> </li> <li> <p>Notes:</p> <ul> <li>A little flaky</li> <li>No options for overloading format</li> </ul> </li> <li> <p>Example: <pre><code>integer ncolor\nlogical force\nnamelist /the_input/ncolor,force\nncolor=4\nforce=.true.\nread(13,the_input)\nwrite(*,the_input)\n</code></pre> On input: <pre><code>&amp; THE_INPUT NCOLOR=4,FORCE = F /\n</code></pre> Output is <pre><code>&amp;THE_INPUT\nNCOLOR = 4,\nFORCE = F\n/\n</code></pre></p> </li> </ul>"},{"location":"Documentation/Development/Languages/Fortran/f90_advanced/#vector-valued-functions","title":"Vector valued functions","text":"<ul> <li>Motivation<ul> <li>May want a function that returns a vector</li> </ul> </li> <li> <p>Notes</p> <ul> <li>Again requires an interface</li> <li>Use explicit or assumed size array</li> <li>Do not return a pointer to a vector unless you really want a pointer</li> </ul> </li> <li> <p>Example:</p> <ul> <li>Take an integer input vector which represents an integer in some base and     add 1</li> <li>Could be used in our program to find a \"brute force\" solution</li> </ul> </li> </ul> <pre><code>  function add1(vector,max) result (rtn)\n  integer, dimension(:),intent(in) ::  vector\n  integer,dimension(size(vector)) :: rtn\n  integer max\n  integer len\n  logical carry\n  len=size(vector)\n  rtn=vector\n  i=0\n  carry=.true.\n  do while(carry)         ! just continue until we do not do a carry\n      i=i+1\n   rtn(i)=rtn(i)+1\n   if(rtn(i) .gt. max)then\n       if(i == len)then   ! role over set everything back to 0\n        rtn=0\n    else\n        rtn(i)=0\n       endif\n   else\n       carry=.false.\n   endif\n  enddo\nend function\n</code></pre>"},{"location":"Documentation/Development/Languages/Fortran/f90_advanced/#usage","title":"Usage","text":"<pre><code>test_vect=0\n        do\n           test_vect=add1(test_vect,3)\n           result=fitness(test_vect)\n           if(result .lt. 1.0_b8)then\n               write(*,*)test_vect\n               stop\n           endif\n        enddo\n</code></pre>"},{"location":"Documentation/Development/Languages/Fortran/f90_advanced/#complete-source-for-recent-discussions","title":"Complete source for recent discussions","text":"<ul> <li>recent.f90</li> <li>fort.13</li> </ul> <p>Exersize 5  Modify the program to use the random number generator given earlier.</p>"},{"location":"Documentation/Development/Languages/Fortran/f90_advanced/#some-array-specific-intrinsic-functions","title":"Some array specific intrinsic functions","text":"<ul> <li>ALL True if all values are true (LOGICAL)</li> <li>ANY True if any value is true (LOGICAL)</li> <li>COUNT Number of true elements in an array (LOGICAL)</li> <li>DOT_PRODUCT Dot product of two rank one arrays</li> <li>MATMUL Matrix multiplication</li> <li>MAXLOC Location of a maximum value in an array</li> <li>MAXVAL Maximum value in an array</li> <li>MINLOC Location of a minimum value in an array</li> <li>MINVAL Minimum value in an array</li> <li>PACK Pack an array into an array of rank one</li> <li>PRODUCT Product of array elements</li> <li>RESHAPE  Reshape an array</li> <li>SPREAD Replicates array by adding a dimension</li> <li>SUM Sum of array elements</li> <li>TRANSPOSE Transpose an array of rank two</li> <li> <p>UNPACK Unpack an array of rank one into an array under a mask</p> </li> <li> <p>Examples</p> </li> </ul> <pre><code>program matrix\n    real w(10),x(10),mat(10,10)\n    call random_number(w)\n    call random_number(mat)\n    x=matmul(w,mat)   ! regular matrix multiply  USE IT\n    write(*,'(\"dot(x,x)=\",f10.5)'),dot_product(x,x)\nend program\nprogram allit\n     character(len=10):: f1=\"(3l1)\"\n     character(len=10):: f2=\"(3i2)\"\n     integer b(2,3),c(2,3),one_d(6)\n     logical l(2,3)\n     one_d=(/ 1,3,5 , 2,4,6 /)\n     b=transpose(reshape((/ 1,3,5 , 2,4,6 /),shape=(/3,2/)))\n     C=transpose(reshape((/ 0,3,5 , 7,4,8 /),shape=(/3,2/)))\n     l=(b.ne.c)\n     write(*,f2)((b(i,j),j=1,3),i=1,2)\n     write(*,*)\n     write(*,f2)((c(i,j),j=1,3),i=1,2)\n     write(*,*)\n     write(*,f1)((l(i,j),j=1,3),i=1,2)\n     write(*,*)\n     write(*,f1)all ( b .ne. C ) !is .false.\n     write(*,f1)all ( b .ne. C, DIM=1) !is [.true., .false., .false.]\n     write(*,f1)all ( b .ne. C, DIM=2) !is [.false., .false.]\nend\n</code></pre> <ul> <li>The output is:</li> </ul> <pre><code> 1 3 5\n 2 4 6\n 0 3 5\n 7 4 8\n TFF\n TFT\n F\n TFF\n FF\n</code></pre>"},{"location":"Documentation/Development/Languages/Fortran/f90_advanced/#the-rest-of-our-ga","title":"The rest of our GA","text":"<ul> <li>Includes</li> <li>Reproduction</li> <li>Mutation</li> <li>Nothing new in either of these files</li> <li>Source and makefile \"git\"</li> <li>Source and makefile \"*tgz\"</li> </ul>"},{"location":"Documentation/Development/Languages/Fortran/f90_advanced/#compiler-information","title":"Compiler Information","text":""},{"location":"Documentation/Development/Languages/Fortran/f90_advanced/#gfortran","title":"gfortran","text":"<ul> <li>.f, .for, .ftn .f77<ul> <li>fixed-format Fortran source; compile</li> </ul> </li> <li>.f90, .f95<ul> <li>free-format Fortran source; compile</li> </ul> </li> <li>-fbacktrace<ul> <li>Add debug information for runtime traceback</li> </ul> </li> <li>-ffree-form -ffixed-form<ul> <li>source form</li> </ul> </li> <li>-O0, -O1, -O2, -O3<ul> <li>optimization level</li> </ul> </li> <li>.fpp, .FPP,  .F, .FOR, .FTN, .F90, .F95, .F03 or .F08<ul> <li>Fortran source file with preprocessor directives</li> </ul> </li> <li>-fopenmp<ul> <li>turn on OpenMP</li> </ul> </li> </ul>"},{"location":"Documentation/Development/Languages/Fortran/f90_advanced/#intel","title":"Intel","text":"<ul> <li>.f, .for, .ftn<ul> <li>fixed-format Fortran source; compile</li> </ul> </li> <li>.f90, .f95<ul> <li>free-format Fortran source; compile</li> </ul> </li> <li>-O0, -O1, -O2, -O3, -O4<ul> <li>optimization level</li> </ul> </li> <li>.fpp, .F, .FOR, .FTN, .FPP, .F90<ul> <li>Fortran source file with preprocessor directives</li> </ul> </li> <li>-g<ul> <li>compile for debug     * -traceback -notraceback (default)</li> <li>Add debug information for runtime traceback</li> </ul> </li> <li>-nofree, -free<ul> <li>Source is fixed or free format</li> </ul> </li> <li>-fopenmp<ul> <li>turn on OpenMP</li> </ul> </li> </ul>"},{"location":"Documentation/Development/Languages/Fortran/f90_advanced/#portland-group-x86","title":"Portland Group (x86)","text":"<ul> <li>.f, .for, .ftn<ul> <li>fixed-format Fortran source; compile</li> </ul> </li> <li>.f90, .f95, .f03<ul> <li>free-format Fortran source; compile</li> </ul> </li> <li>.cuf<ul> <li>free-format CUDA Fortran source; compile</li> </ul> </li> <li> <p>.CUF</p> <ul> <li>free-format CUDA Fortran source; preprocess, compile</li> </ul> </li> <li> <p>-O0, -O1, -O2, -O3, -O4</p> <ul> <li>optimization level</li> </ul> </li> <li> <p>-g</p> <ul> <li>compile for debug       *    -traceback (default) -notraceback</li> <li>Add debug information for runtime traceback</li> </ul> </li> <li>-Mfixed, -Mfree<ul> <li>Source is fixed or free format</li> </ul> </li> <li>-qmp<ul> <li>turn on OpenMP</li> </ul> </li> </ul>"},{"location":"Documentation/Development/Languages/Fortran/f90_advanced/#ibm-xlf","title":"IBM xlf","text":"<ul> <li>xlf, xlf_r, f77, fort77<ul> <li>Compile FORTRAN 77 source files.  _r = thread safe</li> </ul> </li> <li>xlf90, xlf90_r, f90<ul> <li>Compile Fortran 90 source files.  _r = thread safe</li> </ul> </li> <li>xlf95, xlf95_r, f95<ul> <li>Compile Fortran 95 source files.  _r = thread safe</li> </ul> </li> <li>xlf2003, xlf2003_r,f2003  * Compile Fortran 2003 source files. _r = thread safe</li> <li>xlf2008, xlf2008_r, f2008     * Compile Fortran 2008 source files.</li> <li>.f, .f77, .f90, .f95, .f03, .f08<ul> <li>Fortran source file</li> </ul> </li> <li>.F, .F77, .F90, .F95,  .F03, .F08<ul> <li>Fortran source file with preprocessor directives</li> </ul> </li> <li>-qtbtable=full<ul> <li>Add debug information for runtime traceback</li> </ul> </li> <li>-qsmp=omp<ul> <li>turn on OpenMP</li> </ul> </li> <li>-O0, -O1, -O2, -O3, -O4, O5<ul> <li>optimization level</li> </ul> </li> <li>-g , g0, g1,...g9<ul> <li>compile for debug     </li> </ul> </li> </ul>"},{"location":"Documentation/Development/Languages/Fortran/f90_advanced/#summary_1","title":"Summary","text":"<ul> <li> <p>Fortran 90 has features to:</p> <ul> <li>Enhance performance</li> <li>Enhance portability</li> <li>Enhance reliability</li> <li>Enhance maintainability</li> </ul> </li> <li> <p>Fortran 90 has new language elements</p> <ul> <li>Source form</li> <li>Derived data types</li> <li>Dynamic memory allocation functions</li> <li>Kind facility for portability and easy modification</li> <li>Many new intrinsic function</li> <li>Array assignments</li> </ul> </li> <li> <p>Examples</p> <ul> <li>Help show how things work</li> <li>Reference for future use</li> </ul> </li> </ul>"},{"location":"Documentation/Development/Languages/Fortran/f90_advanced/#introduction-to-fortran-language","title":"Introduction to Fortran Language","text":"<pre><code>  Brought to you by ANSI committee X3J3 and ISO-IEC/JTC1/SC22/WG5 (Fortran)\n  This is neither complete nor precisely accurate, but hopefully, after\n  a small investment of time it is easy to read and very useful.\n\n  This is the free form version of Fortran, no statement numbers,\n  no C in column 1, start in column 1 (not column 7),\n  typically indent 2, 3, or 4 spaces per each structure.\n  The typical extension is  .f90  .\n\n  Continue a statement on the next line by ending the previous line with\n  an ampersand  &amp;amp; .  Start the continuation with  &amp;amp;  for strings.\n\n  The rest of any line is a comment starting with an exclamation mark  ! .\n\n  Put more than one statement per line by separating statements with a\n  semicolon  ; . Null statements are OK, so lines can end with semicolons.\n\n  Separate words with space or any form of \"white space\" or punctuation.\n</code></pre>"},{"location":"Documentation/Development/Languages/Fortran/f90_advanced/#meta-language-used-in-this-compact-summary","title":"Meta language used in this compact summary","text":"<pre><code>  &lt;xxx&gt; means fill in something appropriate for xxx and do not type\n        the  \"&lt;\"  or  \"&gt;\" .\n\n  ...  ellipsis means the usual, fill in something, one or more lines\n\n  [stuff] means supply nothing or at most one copy of \"stuff\"\n          [stuff1 [stuff2]] means if \"stuff1\" is included, supply nothing\n          or at most one copy of stuff2.\n\n  \"old\" means it is in the language, like almost every feature of past\n  Fortran standards, but should not be used to write new programs.\n</code></pre>"},{"location":"Documentation/Development/Languages/Fortran/f90_advanced/#structure-of-files-that-can-be-compiled","title":"Structure of files that can be compiled","text":"<pre><code>  program &lt;name&gt;                  usually file name is  &lt;name&gt;.f90\n    use &lt;module_name&gt;             bring in any needed modules\n    implicit none                 good for error detection\n    &lt;declarations&gt;\n    &lt;executable statements&gt;       order is important, no more declarations\n  end program &lt;name&gt;\n\n\n  block data &lt;name&gt;               old\n    &lt;declarations&gt;                common, dimension, equivalence now obsolete\n  end block data &lt;name&gt;\n\n\n  module &lt;name&gt;                   bring back in with   use &lt;name&gt;\n    implicit none                 good for error detection\n    &lt;declarations&gt;                can have private and public and interface\n  end module &lt;name&gt;\n\n  subroutine &lt;name&gt;               use:  call &lt;name&gt;   to execute\n    implicit none                 good for error detection\n    &lt;declarations&gt;\n    &lt;executable statements&gt;\n  end subroutine &lt;name&gt;\n\n\n  subroutine &lt;name&gt;(par1, par2, ...) \n                                  use:  call &lt;name&gt;(arg1, arg2,... ) to execute\n    implicit none                 optional, good for error detection\n    &lt;declarations&gt;                par1, par2, ... are defined in declarations \n                                  and can be specified in, inout, pointer, etc.\n    &lt;executable statements&gt;\n    return                        optional, end causes automatic return\n    entry &lt;name&gt; (par...)         old, optional other entries\n  end subroutine &lt;name&gt;\n\n\n  function &lt;name&gt;(par1, par2, ...) result(&lt;rslt&gt;)\n                                  use: &lt;name&gt;(arg1, arg2, ... argn) as variable\n    implicit none                 optional, good for error detection\n    &lt;declarations&gt;                rslt, par1, ... are defined in declarations\n    &lt;executable statements&gt;\n    &lt;rslt&gt; = &lt;expression&gt;         required somewhere in execution\n    [return]                      optional, end causes automatic return\n  end function &lt;name&gt;\n\n                                  old\n  &lt;type&gt; function(...) &lt;name&gt;     use: &lt;name&gt;(arg1, arg2, ... argn) as variable\n    &lt;declarations&gt;\n    &lt;executable statements&gt;\n    &lt;name&gt; = &lt;expression&gt;         required somewhere in execution\n    [return]                      optional, end causes automatic return\n  end function &lt;name&gt;\n</code></pre>"},{"location":"Documentation/Development/Languages/Fortran/f90_advanced/#executable-statements-and-constructs","title":"Executable Statements and Constructs","text":"<pre><code>  &lt;statement&gt; will mean exactly one statement in this section\n\n  a construct is multiple lines\n\n  &lt;label&gt; : &lt;statement&gt;      any statement can have a label (a name)\n\n  &lt;variable&gt; = &lt;expression&gt;  assignment statement\n\n  &lt;pointer&gt;  &gt;= &lt;variable&gt;   the pointer is now an alias for the variable\n  &lt;pointer1&gt; &gt;= &lt;pointer2&gt;    pointer1 now points same place as pointer2\n\n  stop                       can be in any executable statement group,\n  stop &lt;integer&gt;             terminates execution of the program,\n  stop &lt;string&gt;              can have optional integer or string\n\n  return                     exit from subroutine or function\n\n  do &lt;variable&gt;=&lt;from&gt;,&lt;to&gt; [,&lt;increment&amp;gt]   optional:  &lt;label&gt; : do ...\n     &lt;statements&gt;\n\n     exit                                   \\_optional   or exit &lt;label&amp;gt\n     if (&lt;boolean expression&gt;) exit         /\n                                            exit the loop\n     cycle                                  \\_optional   or cycle &lt;label&gt;\n     if (&lt;boolean expression&gt;) cycle        /\n                                            continue with next loop iteration\n  end do                                    optional:    end do &lt;name&gt;\n\n\n  do while (&lt;boolean expression&gt;)\n     ...                                   optional exit and cycle allowed\n  end do\n\n\n  do\n     ...                                   exit required to end the loop\n                                           optional  cycle  can be used\n  end do\n\n\n\n  if ( &lt;boolean expression&gt; ) &lt;statement&gt;  execute the statement if the\n                                           boolean expression is true\n\n  if ( &lt;boolean expression1&gt; ) then\n    ...                                    execute if expression1 is true\n  else if ( &lt;boolean expression2&gt; ) then\n    ...                                    execute if expression2 is true\n  else if ( &lt;boolean expression3&gt; ) then\n    ...                                    execute if expression3 is true\n  else\n    ...                                    execute if none above are true\n  end if\n\n\n  select case (&lt;expression&gt;)            optional &lt;name&gt; : select case ...\n     case (&lt;value&gt;)\n        &lt;statements&gt;                    execute if expression == value\n     case (&lt;value1&gt;:&lt;value2&gt;)           \n        &lt;statements&gt;                    execute if value1 &amp;le; expression &amp;le; value2\n     ...\n     case default\n        &lt;statements&gt;                    execute if no values above match\n  end select                            optional  end select &lt;name&gt;\n\n\n  real, dimension(10,12) :: A, R     a sample declaration for use with \"where\"\n    ...\n  where (A /= 0.0)                   conditional assignment, only assignment allowed\n     R = 1.0/A\n  elsewhere\n     R = 1.0                         elements of R set to 1.0 where A == 0.0\n  end where\n\n    go to &lt;statement number&gt;          old\n\n    go to (&lt;statement number list&gt;), &lt;expression&gt;   old\n\n    for I/O statements, see:  section 10.0  Input/Output Statements\n\n    many old forms of statements are not listed\n</code></pre>"},{"location":"Documentation/Development/Languages/Fortran/f90_advanced/#declarations","title":"Declarations","text":"<pre><code>  There are five (5) basic types: integer, real, complex, character and logical.\n  There may be any number of user derived types.  A modern (not old) declaration\n  starts with a type, has attributes, then ::, then variable(s) names\n\n  integer i, pivot, query                             old\n\n  integer, intent (inout) :: arg1\n\n  integer (selected_int_kind (5)) :: i1, i2\n\n  integer, parameter :: m = 7\n\n  integer, dimension(0:4, -5:5, 10:100) :: A3D\n\n  double precision x                                 old\n\n  real  (selected_real_kind(15,300) :: x\n\n  complex :: z\n\n  logical, parameter :: what_if = .true.\n\n  character, parameter :: me = \"Jon Squire\"\n\n  type &lt;name&gt;       a new user type, derived type\n    declarations\n  end type &lt;name&gt;\n\n  type (&lt;name&gt;) :: stuff    declaring stuff to be of derived type &lt;name&gt;\n\n  real, dimension(:,:), allocatable, target :: A\n\n  real, dimension(:,:), pointer :: P\n\n  Attributes may be:\n\n    allocatable  no memory used here, allocate later\n    dimension    vector or multi dimensional array\n    external     will be defined outside this compilation\n    intent       argument may be  in, inout or out\n    intrinsic    declaring function to be an intrinsic\n    optional     argument is optional\n    parameter    declaring a constant, can not be changed later\n    pointer      declaring a pointer\n    private      in a module, a private declaration\n    public       in a module, a public declaration\n    save         keep value from one call to the next, static\n    target       can be pointed to by a pointer\n    Note:        not all combinations of attributes are legal\n</code></pre>"},{"location":"Documentation/Development/Languages/Fortran/f90_advanced/#key-words-other-than-io","title":"Key words (other than I/O)","text":"<pre><code>  note: \"statement\" means key word that starts a statement, one line\n                    unless there is a continuation \"&amp;amp;\"\n        \"construct\" means multiple lines, usually ending with \"end ...\"\n        \"attribute\" means it is used in a statement to further define\n        \"old\"       means it should not be used in new code\n\n  allocatable          attribute, no space allocated here, later allocate\n  allocate             statement, allocate memory space now for variable\n  assign               statement, old, assigned go to\n  assignment           attribute, means subroutine is assignment (=)\n  block data           construct, old, compilation unit, replaced by module\n  call                 statement, call a subroutine\n  case                 statement, used in  select case structure\n  character            statement, basic type, intrinsic data type\n  common               statement, old, allowed overlaying of storage\n  complex              statement, basic type, intrinsic data type\n  contains             statement, internal subroutines and functions follow\n  continue             statement, old, a place to put a statement number\n  cycle                statement, continue the next iteration of a do loop\n  data                 statement, old, initialized variables and arrays\n  deallocate           statement, free up storage used by specified variable\n  default              statement, in a select case structure, all others\n  do                   construct, start a do loop\n  double precision     statement, old, replaced by selected_real_kind(15,300)\n  else                 construct, part of if   else if   else   end if\n  else if              construct, part of if   else if   else   end if\n  elsewhere            construct, part of where  elsewhere  end where\n  end block data       construct, old, ends block data\n  end do               construct, ends do\n  end function         construct, ends function\n  end if               construct, ends if\n  end interface        construct, ends interface\n  end module           construct, ends module\n  end program          construct, ends program\n  end select           construct, ends select case\n  end subroutine       construct, ends subroutine\n  end type             construct, ends type\n  end where            construct, ends where\n  entry                statement, old, another entry point in a procedure\n  equivalence          statement, old, overlaid storage\n  exit                 statement, continue execution outside of a do loop\n  external             attribute, old statement, means defines else where\n  function             construct, starts the definition of a function\n  go to                statement, old, requires fixed form statement number\n  if                   statement and construct, if(...) statement\n  implicit             statement, \"none\" is preferred to help find errors\n  in                   a keyword for intent, the argument is read only\n  inout                a keyword for intent, the argument is read/write\n  integer              statement, basic type, intrinsic data type\n  intent               attribute, intent(in) or intent(out) or intent(inout)\n  interface            construct, begins an interface definition\n  intrinsic            statement, says that following names are intrinsic\n  kind                 attribute, sets the kind of the following variables\n  len                  attribute, sets the length of a character string\n  logical              statement, basic type, intrinsic data type\n  module               construct, beginning of a module definition\n  namelist             statement, defines a namelist of input/output\n  nullify              statement, nullify(some_pointer) now points nowhere\n  only                 attribute, restrict what comes from a module\n  operator             attribute, indicates function is an operator, like +\n  optional             attribute, a parameter or argument is optional\n  out                  a keyword for intent, the argument will be written\n  parameter            attribute, old statement, makes variable real only\n  pause                old, replaced by stop\n  pointer              attribute, defined the variable as a pointer alias\n  private              statement and attribute, in a module, visible inside\n  program              construct, start of a main program\n  public               statement and attribute, in a module, visible outside\n  real                 statement, basic type, intrinsic data type\n  recursive            attribute, allows functions and derived type recursion\n  result               attribute, allows naming of function result  result(Y)\n  return               statement, returns from, exits, subroutine or function\n  save                 attribute, old statement, keep value between calls\n  select case          construct, start of a case construct\n  stop                 statement, terminate execution of the main procedure\n  subroutine           construct, start of a subroutine definition\n  target               attribute, allows a variable to take a pointer alias\n  then                 part of if construct\n  type                 construct, start of user defined type\n  type ( )             statement, declaration of a variable for a users type\n  use                  statement, brings in a module\n  where                construct, conditional assignment\n  while                construct, a while form of a do loop\n</code></pre>"},{"location":"Documentation/Development/Languages/Fortran/f90_advanced/#key-words-related-to-io","title":"Key words related to I/O","text":"<pre><code>  backspace            statement, back up one record\n  close                statement, close a file\n  endfile              statement, mark the end of a file\n  format               statement, old, defines a format\n  inquire              statement, get the status of a unit\n  open                 statement, open or create a file\n  print                statement, performs output to screen\n  read                 statement, performs input\n  rewind               statement, move read or write position to beginning\n  write                statement, performs output\n</code></pre>"},{"location":"Documentation/Development/Languages/Fortran/f90_advanced/#operators","title":"Operators","text":"<pre><code>  **    exponentiation\n  *     multiplication\n  /     division\n  +     addition\n  -     subtraction\n  //    concatenation\n  ==    .eq.  equality\n  /=    .ne.  not equal\n  &lt;     .lt.  less than\n  &gt;     .gt.  greater than\n  &lt;=    .le.  less than or equal\n  &gt;=    .ge.  greater than or equal\n  .not.       complement, negation\n  .and.       logical and\n  .or.        logical or\n  .eqv.       logical equivalence\n  .neqv.      logical not equivalence, exclusive or\n\n  .eq.  ==    equality, old\n  .ne.  /=    not equal. old\n  .lt.  &lt;     less than, old\n  .gt.  &gt;     greater than, old\n  .le.  &lt;=    less than or equal, old\n  .ge.  &gt;=    greater than or equal, old\n\n\n  Other punctuation:\n\n   /  ...  /  used in data, common, namelist and other statements\n   (/ ... /)  array constructor, data is separated by commas\n   6*1.0      in some contexts, 6 copies of 1.0\n   (i:j:k)    in some contexts, a list  i, i+k, i+2k, i+3k, ... i+nk&amp;le;j\n   (:j)       j and all below\n   (i:)       i and all above\n   (:)        undefined or all in range\n</code></pre>"},{"location":"Documentation/Development/Languages/Fortran/f90_advanced/#constants","title":"Constants","text":"<pre><code>  Logical constants:\n\n    .true.      True\n    .false.     False\n\n  Integer constants:\n\n     0    1     -1     123456789\n\n  Real constants:\n\n     0.0   1.0   -1.0    123.456   7.1E+10   -52.715E-30\n\n  Complex constants:\n\n     (0.0, 0.0)    (-123.456E+30, 987.654E-29)\n\n  Character constants:\n\n      \"ABC\"   \"a\"  \"123'abc$%#@!\"    \" a quote \"\" \"\n      'ABC'   'a'  '123\"abc$%#@!'    ' a apostrophe '' '\n\n  Derived type values:\n\n      type name\n        character (len=30) :: last\n        character (len=30) :: first\n        character (len=30) :: middle\n      end type name\n\n      type address\n        character (len=40) :: street\n        character (len=40) :: more\n        character (len=20) :: city\n        character (len=2)  :: state\n        integer (selected_int_kind(5)) :: zip_code\n        integer (selected_int_kind(4)) :: route_code\n      end type address\n\n      type person\n        type (name) lfm\n        type (address) snail_mail\n      end type person\n\n      type (person) :: a_person = person( name(\"Squire\",\"Jon\",\"S.\"), &amp;amp;\n          address(\"106 Regency Circle\", \"\", \"Linthicum\", \"MD\", 21090, 1936))\n\n      a_person%snail_mail%route_code == 1936\n</code></pre>"},{"location":"Documentation/Development/Languages/Fortran/f90_advanced/#inputoutput-statements","title":"Input/Output Statements","text":"<pre><code>    open (&lt;unit number&gt;)\n    open (unit=&lt;unit number&gt;, file=&lt;file name&gt;, iostat=&lt;variable&gt;)\n    open (unit=&lt;unit number&gt;, ... many more, see below )\n\n    close (&lt;unit number&gt;)\n    close (unit=&lt;unit number&gt;, iostat=&lt;variable&gt;,\n           err=&lt;statement number&gt;, status=\"KEEP\")\n\n    read (&lt;unit number&gt;) &lt;input list&gt;\n    read (unit=&lt;unit number&gt;, fmt=&lt;format&gt;, iostat=&lt;variable&gt;,\n          end=&lt;statement number&gt;, err=&lt;statement number&gt;) &lt;input list&gt;\n    read (unit=&lt;unit number&gt;, rec=&lt;record number&gt;) &lt;input list&gt;\n\n    write (&lt;unit number&gt;) &lt;output list&gt;\n    write (unit=&lt;unit number&gt;, fmt=&lt;format&gt;, iostat=&lt;variable&gt;,\n           err=&lt;statement number&gt;) &lt;output list&gt;\n    write (unit=&lt;unit number&gt;, rec=&lt;record number&gt;) &lt;output list&gt;\n\n    print *, &lt;output list&gt;\n\n    print \"(&lt;your format here, use apostrophe, not quote&gt;)\", &lt;output list&gt;\n\n    rewind &lt;unit number&gt;\n    rewind (&lt;unit number&gt;, err=&lt;statement number&gt;)\n\n    backspace &lt;unit number&gt;\n    backspace (&lt;unit number&gt;, iostat=&lt;variable&gt;)\n\n    endfile &lt;unit number&gt;\n    endfile (&lt;unit number&gt;, err=&lt;statement number&gt;, iostat=&lt;variable&gt;)\n\n    inquire ( &lt;unit number&gt;, exists = &lt;variable&gt;)\n    inquire ( file=&lt;\"name\"&gt;, opened = &lt;variable1&gt;, access = &lt;variable2&gt; )\n    inquire ( iolength = &lt;variable&gt; ) x, y, A   ! gives \"recl\" for \"open\"\n\n    namelist /&lt;name&gt;/ &lt;variable list&gt;      defines a name list\n    read(*,nml=&lt;name&gt;)                     reads some/all variables in namelist\n    write(*,nml=&lt;name&gt;)                    writes all variables in namelist\n    &amp;amp;&lt;name&gt; &lt;variable&gt;=&lt;value&gt; ... &lt;variable=value&gt; /  data for namelist read\n\n  Input / Output specifiers\n\n    access   one of  \"sequential\"  \"direct\"  \"undefined\"\n    action   one of  \"read\"  \"write\"  \"readwrite\"\n    advance  one of  \"yes\"  \"no\"  \n    blank    one of  \"null\"  \"zero\"\n    delim    one of  \"apostrophe\"  \"quote\"  \"none\"\n    end      =       &lt;integer statement number&gt;  old\n    eor      =       &lt;integer statement number&gt;  old\n    err      =       &lt;integer statement number&gt;  old\n    exist    =       &lt;logical variable&gt;\n    file     =       &lt;\"file name\"&gt;\n    fmt      =       &lt;\"(format)\"&gt; or &lt;character variable&gt; format\n    form     one of  \"formatted\"  \"unformatted\"  \"undefined\"\n    iolength =       &lt;integer variable, size of unformatted record&gt;\n    iostat   =       &lt;integer variable&gt; 0==good, negative==eof, positive==bad\n    name     =       &lt;character variable for file name&gt;\n    named    =       &lt;logical variable&gt;\n    nml      =       &lt;namelist name&gt;\n    nextrec  =       &lt;integer variable&gt;    one greater than written\n    number   =       &lt;integer variable unit number&gt;\n    opened   =       &lt;logical variable&gt;\n    pad      one of  \"yes\"  \"no\"\n    position one of  \"asis\"  \"rewind\"  \"append\"\n    rec      =       &lt;integer record number&gt;\n    recl     =       &lt;integer unformatted record size&gt;\n    size     =       &lt;integer variable&gt;  number of characters read before eor\n    status   one of  \"old\"  \"new\"  \"unknown\"  \"replace\"  \"scratch\"  \"keep\"\n    unit     =       &lt;integer unit number&gt;\n\n  Individual questions\n    direct      =    &lt;character variable&gt;  \"yes\"  \"no\"  \"unknown\"\n    formatted   =    &lt;character variable&gt;  \"yes\"  \"no\"  \"unknown\"\n    read        =    &lt;character variable&gt;  \"yes\"  \"no\"  \"unknown\"\n    readwrite   =    &lt;character variable&gt;  \"yes\"  \"no\"  \"unknown\"\n    sequential  =    &lt;character variable&gt;  \"yes\"  \"no\"  \"unknown\"\n    unformatted =    &lt;character variable&gt;  \"yes\"  \"no\"  \"unknown\"\n    write       =    &lt;character variable&gt;  \"yes\"  \"no\"  \"unknown\"\n</code></pre>"},{"location":"Documentation/Development/Languages/Fortran/f90_advanced/#formats","title":"Formats","text":"<pre><code>    format                    an explicit format can replace * in any\n                              I/O statement. Include the format in\n                              apostrophes or quotes and keep the parenthesis.\n\n    examples:\n         print \"(3I5,/(2X,3F7.2/))\", &lt;output list&gt;\n         write(6, '(a,E15.6E3/a,G15.2)' ) &lt;output list&gt;\n         read(unit=11, fmt=\"(i4, 4(f3.0,TR1))\" ) &lt;input list&gt;\n\n    A format includes the opening and closing parenthesis.\n    A format consists of format items and format control items separated by comma.\n    A format may contain grouping parenthesis with an optional repeat count.\n\n  Format Items, data edit descriptors:\n\n    key:  w  is the total width of the field   (filled with *** if overflow)\n          m  is the least number of digits in the (sub)field (optional)\n          d  is the number of decimal digits in the field\n          e  is the number of decimal digits in the exponent subfield\n          c  is the repeat count for the format item\n          n  is number of columns\n\n    cAw     data of type character (w is optional)\n    cBw.m   data of type integer with binary base\n    cDw.d   data of type real -- same as E,  old double precision\n    cEw.d   or Ew.dEe  data of type real\n    cENw.d  or ENw.dEe  data of type real  -- exponent a multiple of 3\n    cESw.d  or ESw.dEe  data of type real  -- first digit non zero\n    cFw.d   data of type real  -- no exponent printed\n    cGw.d   or Gw.dEe  data of type real  -- auto format to F or E\n    nH      n characters follow the H,  no list item\n    cIw.m   data of type integer\n    cLw     data of type logical  --  .true.  or  .false.\n    cOw.m   data of type integer with octal base\n    cZw.m   data of type integer with hexadecimal base\n    \"&lt;string&gt;\"  literal characters to output, no list item\n    '&lt;string&gt;'  literal characters to output, no list item\n\n  Format Control Items, control edit descriptors:\n\n    BN      ignore non leading blanks in numeric fields\n    BZ      treat nonleading blanks in numeric fields as zeros\n    nP      apply scale factor to real format items   old\n    S       printing of optional plus signs is processor dependent\n    SP      print optional plus signs\n    SS      do not print optional plus signs\n    Tn      tab to specified column\n    TLn     tab left n columns\n    TRn     tab right n columns\n    nX      tab right n columns\n    /       end of record (implied / at end of all format statements)\n    :       stop format processing if no more list items\n\n  &lt;input list&gt; can be:\n    a variable\n    an array name\n    an implied do   ((A(i,j),j=1,n) ,i=1,m)    parenthesis and commas as shown\n\n    note: when there are more items in the input list than format items, the\n          repeat rules for formats applies.\n\n  &lt;output list&gt; can be:\n    a constant\n    a variable\n    an expression\n    an array name\n    an implied do   ((A(i,j),j=1,n) ,i=1,m)    parenthesis and commas as shown\n\n    note: when there are more items in the output list than format items, the\n          repeat rules for formats applies.\n\n  Repeat Rules for Formats:\n\n    Each format item is used with a list item.  They are used in order.\n    When there are more list items than format items, then the following\n    rule applies:  There is an implied end of record, /, at the closing\n    parenthesis of the format, this is processed.  Scan the format backwards\n    to the first left parenthesis.  Use the repeat count, if any, in front\n    of this parenthesis, continue to process format items and list items.\n\n    Note: an infinite loop is possible\n          print \"(3I5/(1X/))\", I, J, K, L    may never stop\n</code></pre>"},{"location":"Documentation/Development/Languages/Fortran/f90_advanced/#intrinsic-functions","title":"Intrinsic Functions","text":"<pre><code>  Intrinsic Functions are presented in alphabetical order and then grouped\n  by topic.  The function name appears first. The argument(s) and result\n  give an indication of the type(s) of argument(s) and results.\n  [,dim=] indicates an optional argument  \"dim\".\n  \"mask\" must be logical and usually conformable.\n  \"character\" and \"string\" are used interchangeably.\n  A brief description or additional information may appear.\n</code></pre>"},{"location":"Documentation/Development/Languages/Fortran/f90_advanced/#intrinsic-functions-alphabetical","title":"Intrinsic Functions (alphabetical):","text":"<pre><code>    abs(integer_real_complex) result(integer_real_complex)\n    achar(integer) result(character)  integer to character\n    acos(real) result(real)  arccosine  |real| &amp;le; 1.0   0&amp;le;result&amp;le;Pi\n    adjustl(character)  result(character) left adjust, blanks go to back\n    adjustr(character)  result(character) right adjust, blanks to front\n    aimag(complex) result(real)  imaginary part\n    aint(real [,kind=]) result(real)  truncate to integer toward zero\n    all(mask [,dim]) result(logical)  true if all elements of mask are true\n    allocated(array) result(logical)  true if array is allocated in memory\n    anint(real [,kind=]) result(real)  round to nearest integer\n    any(mask [,dim=}) result(logical)  true if any elements of mask are true\n    asin(real) result(real)  arcsine  |real| &amp;le; 1.0   -Pi/2&amp;le;result&amp;le;Pi/2\n    associated(pointer [,target=]) result(logical)  true if pointing\n    atan(real) result(real)  arctangent  -Pi/2&amp;le;result&amp;le;Pi/2 \n    atan2(y=real,x=real) result(real)  arctangent  -Pi&amp;le;result&amp;le;Pi\n    bit_size(integer) result(integer)  size in bits in model of argument\n    btest(i=integer,pos=integer) result(logical)  true if pos has a 1, pos=0..\n    ceiling(real) result(real)  truncate to integer toward infinity\n    char(integer [,kind=]) result(character)  integer to character [of kind]\n    cmplx(x=real [,y=real] [kind=]) result(complex)  x+iy\n    conjg(complex) result(complex)  reverse the sign of the imaginary part\n    cos(real_complex) result(real_complex)  cosine\n    cosh(real) result(real)  hyperbolic cosine\n    count(mask [,dim=]) result(integer)  count of true entries in mask\n    cshift(array,shift [,dim=]) circular shift elements of array, + is right\n    date_and_time([date=] [,time=] [,zone=] [,values=])  y,m,d,utc,h,m,s,milli\n    dble(integer_real_complex) result(real_kind_double)  convert to double\n    digits(integer_real) result(integer)  number of bits to represent model\n    dim(x=integer_real,y=integer_real) result(integer_real) proper subtraction\n    dot_product(vector_a,vector_b) result(integer_real_complex) inner product\n    dprod(x=real,y=real) result(x_times_y_double)  double precision product\n    eoshift(array,shift [,boundary=] [,dim=])  end-off shift using boundary\n    epsilon(real) result(real)  smallest positive number added to 1.0 /= 1.0\n    exp(real_complex) result(real_complex)  e raised to a power\n    exponent(real) result(integer)  the model exponent of the argument\n    floor(real) result(real)  truncate to integer towards negative infinity\n    fraction(real) result(real)  the model fractional part of the argument\n    huge(integer_real) result(integer_real)  the largest model number\n    iachar(character) result(integer)  position of character in ASCII sequence\n    iand(integer,integer) result(integer)  bit by bit logical and\n    ibclr(integer,pos) result(integer)  argument with pos bit cleared to zero\n    ibits(integer,pos,len) result(integer)  extract len bits starting at pos\n    ibset(integer,pos) result(integer)  argument with pos bit set to one\n    ichar(character) result(integer)  pos in collating sequence of character\n    ieor(integer,integer) result(integer)  bit by bit logical exclusive or\n    index(string,substring [,back=])  result(integer)  pos of substring\n    int(integer_real_complex) result(integer)  convert to integer\n    ior(integer,integer) result(integer)  bit by bit logical or\n    ishft(integer,shift) result(integer)  shift bits in argument by shift\n    ishftc(integer, shift) result(integer)  shift circular bits in argument\n    kind(any_intrinsic_type) result(integer)  value of the kind\n    lbound(array,dim) result(integer)  smallest subscript of dim in array\n    len(character) result(integer)  number of characters that can be in argument\n    len_trim(character) result(integer)  length without trailing blanks\n    lge(string_a,string_b) result(logical)  string_a &amp;ge; string_b\n    lgt(string_a,string_b) result(logical)  string_a &gt; string_b\n    lle(string_a,string_b) result(logical)  string_a &amp;le; string_b\n    llt(string_a,string_b) result(logical)  string_a &lt; string_b\n    log(real_complex) result(real_complex)  natural logarithm\n    log10(real) result(real)  logarithm base 10\n    logical(logical [,kind=])  convert to logical\n    matmul(matrix,matrix) result(vector_matrix)  on integer_real_complex_logical\n    max(a1,a2,a3,...) result(integer_real)  maximum of list of values\n    maxexponent(real) result(integer)  maximum exponent of model type\n    maxloc(array [,mask=]) result(integer_vector)  indices in array of maximum\n    maxval(array [,dim=] [,mask=])  result(array_element)  maximum value\n    merge(true_source,false_source,mask) result(source_type)  choose by mask\n    min(a1,a2,a3,...) result(integer-real)  minimum of list of values\n    minexponent(real) result(integer)  minimum(negative) exponent of model type\n    minloc(array [,mask=]) result(integer_vector)  indices in array of minimum\n    minval(array [,dim=] [,mask=])  result(array_element)  minimum value\n    mod(a=integer_real,p) result(integer_real)  a modulo p\n    modulo(a=integer_real,p) result(integer_real)  a modulo p\n    mvbits(from,frompos,len,to,topos) result(integer)  move bits\n    nearest(real,direction) result(real)  nearest value toward direction\n    nint(real [,kind=]) result(real)  round to nearest integer value\n    not(integer) result(integer)  bit by bit logical complement\n    pack(array,mask [,vector=]) result(vector)  vector of elements from array\n    present(argument) result(logical)  true if optional argument is supplied\n    product(array [,dim=] [,mask=]) result(integer_real_complex)  product\n    radix(integer_real) result(integer)  radix of integer or real model, 2\n    random_number(harvest=real_out)  subroutine, uniform random number 0 to 1\n    random_seed([size=] [,put=] [,get=])  subroutine to set random number seed\n    range(integer_real_complex) result(integer_real)  decimal exponent of model\n    real(integer_real_complex [,kind=]) result(real)  convert to real\n    repeat(string,ncopies) result(string)  concatenate n copies of string\n    reshape(source,shape,pad,order) result(array)  reshape source to array\n    rrspacing(real) result(real)  reciprocal of relative spacing of model\n    scale(real,integer) result(real)  multiply by  2**integer\n    scan(string,set [,back]) result(integer)  position of first of set in string\n    selected_int_kind(integer) result(integer)  kind number to represent digits\n    selected_real_kind(integer,integer) result(integer)  kind of digits, exp\n    set_exponent(real,integer) result(real)  put integer as exponent of real\n    shape(array) result(integer_vector)  vector of dimension sizes\n    sign(integer_real,integer_real) result(integer_real) sign of second on first\n    sin(real_complex) result(real_complex)  sine of angle in radians\n    sinh(real) result(real)  hyperbolic sine of argument\n    size(array [,dim=]) result(integer)  number of elements in dimension\n    spacing(real) result(real)  spacing of model numbers near argument\n    spread(source,dim,ncopies) result(array)  expand dimension of source by 1\n    sqrt(real_complex) result(real_complex)  square root of argument\n    sum(array [,dim=] [,mask=]) result(integer_real_complex)  sum of elements\n    system_clock([count=] [,count_rate=] [,count_max=])  subroutine, all out\n    tan(real) result(real)  tangent of angle in radians\n    tanh(real) result(real)  hyperbolic tangent of angle in radians\n    tiny(real) result(real)  smallest positive model representation\n    transfer(source,mold [,size]) result(mold_type)  same bits, new type\n    transpose(matrix) result(matrix)  the transpose of a matrix\n    trim(string) result(string)  trailing blanks are removed\n    ubound(array,dim) result(integer)  largest subscript of dim in array\n    unpack(vector,mask,field) result(v_type,mask_shape)  field when not mask\n    verify(string,set [,back]) result(integer)  pos in string not in set\n</code></pre>"},{"location":"Documentation/Development/Languages/Fortran/f90_advanced/#intrinsic-functions-grouped-by-topic","title":"Intrinsic Functions (grouped by topic):","text":""},{"location":"Documentation/Development/Languages/Fortran/f90_advanced/#intrinsic-functions-numeric","title":"Intrinsic Functions (Numeric)","text":"<pre><code>    abs(integer_real_complex) result(integer_real_complex)\n    acos(real) result(real)  arccosine  |real| &amp;le; 1.0   0&amp;le;result&amp;le;Pi\n    aimag(complex) result(real)  imaginary part\n    aint(real [,kind=]) result(real)  truncate to integer toward zero\n    anint(real [,kind=]) result(real)  round to nearest integer\n    asin(real) result(real)  arcsine  |real| &amp;le; 1.0   -Pi/2&amp;le;result&amp;le;Pi/2\n    atan(real) result(real)  arctangent  -Pi/2&amp;le;result&amp;le;Pi/2 \n    atan2(y=real,x=real) result(real)  arctangent  -Pi&amp;le;result&amp;le;Pi\n    ceiling(real) result(real)  truncate to integer toward infinity\n    cmplx(x=real [,y=real] [kind=]) result(complex)  x+iy\n    conjg(complex) result(complex)  reverse the sign of the imaginary part\n    cos(real_complex) result(real_complex)  cosine\n    cosh(real) result(real)  hyperbolic cosine\n    dble(integer_real_complex) result(real_kind_double)  convert to double\n    digits(integer_real) result(integer)  number of bits to represent model\n    dim(x=integer_real,y=integer_real) result(integer_real) proper subtraction\n    dot_product(vector_a,vector_b) result(integer_real_complex) inner product\n    dprod(x=real,y=real) result(x_times_y_double)  double precision product\n    epsilon(real) result(real)  smallest positive number added to 1.0 /= 1.0\n    exp(real_complex) result(real_complex)  e raised to a power\n    exponent(real) result(integer)  the model exponent of the argument\n    floor(real) result(real)  truncate to integer towards negative infinity\n    fraction(real) result(real)  the model fractional part of the argument\n    huge(integer_real) result(integer_real)  the largest model number\n    int(integer_real_complex) result(integer)  convert to integer\n    log(real_complex) result(real_complex)  natural logarithm\n    log10(real) result(real)  logarithm base 10\n    matmul(matrix,matrix) result(vector_matrix)  on integer_real_complex_logical\n    max(a1,a2,a3,...) result(integer_real)  maximum of list of values\n    maxexponent(real) result(integer)  maximum exponent of model type\n    maxloc(array [,mask=]) result(integer_vector)  indices in array of maximum\n    maxval(array [,dim=] [,mask=])  result(array_element)  maximum value\n    min(a1,a2,a3,...) result(integer-real)  minimum of list of values\n    minexponent(real) result(integer)  minimum(negative) exponent of model type\n    minloc(array [,mask=]) result(integer_vector)  indices in array of minimum\n    minval(array [,dim=] [,mask=])  result(array_element)  minimum value\n    mod(a=integer_real,p) result(integer_real)  a modulo p\n    modulo(a=integer_real,p) result(integer_real)  a modulo p\n    nearest(real,direction) result(real)  nearest value toward direction\n    nint(real [,kind=]) result(real)  round to nearest integer value\n    product(array [,dim=] [,mask=]) result(integer_real_complex)  product\n    radix(integer_real) result(integer)  radix of integer or real model, 2\n    random_number(harvest=real_out)  subroutine, uniform random number 0 to 1\n    random_seed([size=] [,put=] [,get=])  subroutine to set random number seed\n    range(integer_real_complex) result(integer_real)  decimal exponent of model\n    real(integer_real_complex [,kind=]) result(real)  convert to real\n    rrspacing(real) result(real)  reciprocal of relative spacing of model\n    scale(real,integer) result(real)  multiply by  2**integer\n    set_exponent(real,integer) result(real)  put integer as exponent of real\n    sign(integer_real,integer_real) result(integer_real) sign of second on first\n    sin(real_complex) result(real_complex)  sine of angle in radians\n    sinh(real) result(real)  hyperbolic sine of argument\n    spacing(real) result(real)  spacing of model numbers near argument\n    sqrt(real_complex) result(real_complex)  square root of argument\n    sum(array [,dim=] [,mask=]) result(integer_real_complex)  sum of elements\n    tan(real) result(real)  tangent of angle in radians\n    tanh(real) result(real)  hyperbolic tangent of angle in radians\n    tiny(real) result(real)  smallest positive model representation\n    transpose(matrix) result(matrix)  the transpose of a matrix\n</code></pre>"},{"location":"Documentation/Development/Languages/Fortran/f90_advanced/#intrinsic-functions-logical-and-bit","title":"Intrinsic Functions (Logical and bit)","text":"<pre><code>    all(mask [,dim]) result(logical)  true if all elements of mask are true\n    any(mask [,dim=}) result(logical)  true if any elements of mask are true\n    bit_size(integer) result(integer)  size in bits in model of argument\n    btest(i=integer,pos=integer) result(logical)  true if pos has a 1, pos=0..\n    count(mask [,dim=]) result(integer)  count of true entries in mask\n    iand(integer,integer) result(integer)  bit by bit logical and\n    ibclr(integer,pos) result(integer)  argument with pos bit cleared to zero\n    ibits(integer,pos,len) result(integer)  extract len bits starting at pos\n    ibset(integer,pos) result(integer)  argument with pos bit set to one\n    ieor(integer,integer) result(integer)  bit by bit logical exclusive or\n    ior(integer,integer) result(integer)  bit by bit logical or\n    ishft(integer,shift) result(integer)  shift bits in argument by shift\n    ishftc(integer, shift) result(integer)  shift circular bits in argument\n    logical(logical [,kind=])  convert to logical\n    matmul(matrix,matrix) result(vector_matrix)  on integer_real_complex_logical\n    merge(true_source,false_source,mask) result(source_type)  choose by mask\n    mvbits(from,frompos,len,to,topos) result(integer)  move bits\n    not(integer) result(integer)  bit by bit logical complement\n    transfer(source,mold [,size]) result(mold_type)  same bits, new type\n</code></pre>"},{"location":"Documentation/Development/Languages/Fortran/f90_advanced/#intrinsic-functions-character-or-string","title":"Intrinsic Functions (Character or string)","text":"<pre><code>    achar(integer) result(character)  integer to character\n    adjustl(character)  result(character) left adjust, blanks go to back\n    adjustr(character)  result(character) right adjust, blanks to front\n    char(integer [,kind=]) result(character)  integer to character [of kind]\n    iachar(character) result(integer)  position of character in ASCII sequence\n    ichar(character) result(integer)  pos in collating sequence of character\n    index(string,substring [,back=])  result(integer)  pos of substring\n    len(character) result(integer)  number of characters that can be in argument\n    len_trim(character) result(integer)  length without trailing blanks\n    lge(string_a,string_b) result(logical)  string_a &amp;ge; string_b\n    lgt(string_a,string_b) result(logical)  string_a &gt; string_b\n    lle(string_a,string_b) result(logical)  string_a &amp;le; string_b\n    llt(string_a,string_b) result(logical)  string_a &lt; string_b\n    repeat(string,ncopies) result(string)  concatenate n copies of string\n    scan(string,set [,back]) result(integer)  position of first of set in string\n    trim(string) result(string)  trailing blanks are removed\n    verify(string,set [,back]) result(integer)  pos in string not in set\n</code></pre>"},{"location":"Documentation/Development/Languages/Fortran/f90_advanced/#fortran-95","title":"Fortran 95","text":"<ul> <li>New Features<ul> <li>The statement FORALL as an alternative to the DO-statement</li> <li>Partial nesting of FORALL and WHERE statements</li> <li>Masked ELSEWHERE</li> <li>Pure procedures</li> <li>Elemental procedures</li> <li>Pure procedures in specification expressions</li> <li>Revised MINLOC and MAXLOC</li> <li>Extensions to CEILING and FLOOR with the KIND keyword argument</li> <li>Pointer initialization</li> <li>Default initialization of derived type objects</li> <li>Increased compatibility with IEEE arithmetic</li> <li>A CPU_TIME intrinsic subroutine</li> <li>A function NULL to nullify a pointer</li> <li>Automatic deallocation of allocatable arrays at exit of scoping unit</li> <li>Comments in NAMELIST at input</li> <li>Minimal field at input</li> <li>Complete version of END INTERFACE</li> </ul> </li> <li>Deleted Features<ul> <li>real and double precision DO loop index variables</li> <li>branching to END IF from an outer block</li> <li>PAUSE statements</li> <li>ASSIGN statements and assigned GO TO statements and the use of an assigned     integer as a FORMAT specification</li> <li>Hollerith editing in FORMAT</li> <li>See http://www.nsc.liu.se/~boein/f77to90/f95.html#17.5</li> </ul> </li> </ul>"},{"location":"Documentation/Development/Languages/Fortran/f90_advanced/#references","title":"References","text":"<ul> <li>http://www.fortran.com/fortran/ Pointer to everything Fortran</li> <li>http://www.nsc.liu.se/~boein/f77to90/a5.html A good reference for intrinsic functions</li> <li>https://wg5-fortran.org/N1551-N1600/N1579.pdfNew Features of Fortran 2003</li> <li>https://wg5-fortran.org/N1701-N1750/N1729.pdfNew Features of Fortran 2008</li> <li>http://www.nsc.liu.se/~boein/f77to90/ Fortran 90 for the Fortran 77 Programmer</li> <li>Fortran 90 Handbook Complete ANSI/ISO Reference. Jeanne Adams, Walt Brainerd, Jeanne Martin, Brian Smith, Jerrold Wagener</li> <li>Fortran 90 Programming. T. Ellis, Ivor Philips, Thomas Lahey</li> <li>https://github.com/llvm/llvm-project/blob/master/flang/docs/FortranForCProgrammers.md</li> </ul>"},{"location":"Documentation/Development/Languages/Julia/","title":"Julia","text":"<p>Julia is a dynamic programming language that offers high performance while being easy to learn and develop code in.</p> <p>This section contains demos (in the form of scripts and notebooks) and how-to guides for doing various things with Julia on NREL HPC environments.</p>"},{"location":"Documentation/Development/Languages/Julia/#available-modules","title":"Available modules","text":"Swift Vermilion Kestrel (CPU) Kestrel (GPU) julia/1.6.2-ocsfign julia/1.7.2-gdp7a25 julia/1.7.2 julia/1.7.2 julia/1.8.5-generic-linux julia/1.10.0-gcc julia/1.10.4 <p>Julia 1.9.x does not work well on Sapphire Rapids</p> <p>We advise against installing and using Julia 1.9.x on Kestrel as packages can fail to precompile and result in a segmentation fault. This is a known issue with Julia 1.9.x on Sapphire Rapids processors, possibly due to an LLVM issue. Julia 1.10 will be installed as a module once a stable release is available. Until then, please use Julia 1.7 or 1.8.</p>"},{"location":"Documentation/Development/Languages/Julia/#contents","title":"Contents","text":"<ol> <li>Installing Julia</li> <li>Tour of Julia</li> <li>Parallel Computing in Julia</li> <li>Calling Python, C, and FORTRAN from Julia</li> </ol>"},{"location":"Documentation/Development/Languages/Julia/#demo-scripts-and-notebooks","title":"Demo Scripts and Notebooks","text":"<p>The following scripts and notebooks are available on the <code>master</code> branch of NREL/HPC to download and run,</p> <ul> <li>Julia Tour</li> <li>Julia Parallel Computing</li> <li>Calling Python, C, and FORTRAN from Julia</li> <li>PyJulia -- calling Julia from Python (<code>PyJulia_Demo.ipynb</code>)</li> <li>Integrating <code>mpi4py</code> and <code>MPI.jl</code><ul> <li>Hello World</li> <li>Self contained approximation of pi</li> <li>Approximation of pi as \"library\" call</li> <li>Comparing different Control Variates to approximate pi -- uses MPI Split</li> <li>Example batch script for all of the above</li> </ul> </li> </ul>"},{"location":"Documentation/Development/Languages/Julia/#requirements-and-installation","title":"Requirements and Installation","text":"<p>Running the demos requires the python modules <code>mpi4py</code> and <code>julia</code>. For details on installing these modules, see the 'Environment Setup' section of the README found in the demos/scripts directory.</p> <p>For more information on mpi4py, see the mpi4py documentation</p> <p>For more information on PyJulia, see the PyJulia documentation.</p>"},{"location":"Documentation/Development/Languages/Julia/julia_ccall_pycall/","title":"Calling Python, C, and FORTRAN from Julia","text":"<p>The following sections describe Julia packages and native function calls that can be used to call Python, C, and FORTRAN libraries.</p>"},{"location":"Documentation/Development/Languages/Julia/julia_ccall_pycall/#calling-python","title":"Calling Python","text":"<p>We can use the <code>PyCall.jl</code> package to call Python code from Julia.</p> <pre><code>using PyCall\n</code></pre> <pre><code># The following makes it so that print statements in python will appear in this notebook\n# This is not necessary when using PyCall in a terminal based Julia instance\npyimport(\"sys\").\"stdout\" = PyTextIO(stdout)\npyimport(\"sys\").\"stderr\" = PyTextIO(stderr);\n</code></pre> <p>We can execute arbitrary Python code with the special Julia strings <code>py\"...\"</code> and <code>py\"\"\"...\"\"\"</code>.</p> <pre><code>py\"\"\"\nimport math\nclass Point:\n    def __init__(self, x,y):\n        self.x = x\n        self.y = y\n    def distance(self, p):\n        return math.sqrt((self.x - p.x)**2 + (self.y - p.y)**2)\n\"\"\"\n\np = py\"Point(1.0, 2.0)\"\n</code></pre> <pre><code>PyObject &lt;__main__.Point object at 0x7fa3d66bd340&gt;\n</code></pre> <p>We can even use Julia's string interpolation to give values to the Python code:</p> <pre><code>x = rand()\nq = py\"Point($(x), $rand())\"\n</code></pre> <pre><code>PyObject &lt;__main__.Point object at 0x7fa3d66bdb80&gt;\n</code></pre> <p>Attributes are directly accessible through the standard dot syntax:</p> <pre><code>@show p.x\n@show p.distance(q);\n</code></pre> <pre><code>p.x = 1.0\np.distance(q) = 1.7581695820873517\n</code></pre> <p>But say we have a module in Python that we want to call from Julia.  We can do that too (otherwise this wouldn't be much use would it?). The <code>pyimport</code> function returns an object that gives us access to that modules functions:</p> <pre><code>np = pyimport(\"numpy\")\nA = rand(3,3)\nb = rand(3)\nx = np.linalg.solve(A, b)\n@show maximum(abs.(A * x - b));\n</code></pre> <pre><code>maximum(abs.(A * x - b)) = 1.1102230246251565e-16\n</code></pre> <p>In the previous slide <code>A</code> and <code>b</code> are created by Julia while <code>x</code> is created by Python, but we are using them interchangeably. We can do this because PyCall handles most type conversions automatically.</p> <pre><code>for x in [5.0, 2, [\"a\", \"b\"], Dict(\"a\"=&gt;rand(), \"b\"=&gt;rand()), A]\n    @show typeof(x)\n    py\"\"\"print(type($x))\"\"\"\nend\n</code></pre> <pre><code>typeof(x) = Float64\n&lt;class 'float'&gt;\ntypeof(x) = Int64\n&lt;class 'int'&gt;\ntypeof(x) = Vector{String}\n&lt;class 'list'&gt;\ntypeof(x) = Dict{String, Float64}\n&lt;class 'dict'&gt;\ntypeof(x) = Matrix{Float64}\n&lt;class 'numpy.ndarray'&gt;\n</code></pre> <p>Note that the matrix is converted to a <code>numpy</code> array if <code>numpy</code> is installed.</p> <p>The same is true going from Python to Julia.</p> <pre><code>py\"\"\"\nobjs = [{'a':1,'b':2}, [1, 'a', 3.0], 2.0+3j]\nfor k in range(len(objs)):\n    $println($typeof(objs[k]))\n    print(type(objs[k]))\n\"\"\"\n</code></pre> <pre><code>Dict{Any, Any}\n&lt;class 'dict'&gt;\nVector{Any}\n&lt;class 'list'&gt;\nComplexF64\n&lt;class 'complex'&gt;\n</code></pre> <p>We do need to be a little careful with some of Julia's less common types especially if we give it to python and bring it back:</p> <pre><code>a = Int32(5)\n@show typeof(a)\n@show typeof(py\"$a\");\n</code></pre> <pre><code>typeof(a) = Int32\ntypeof(py\"$a\") = Int64\n</code></pre> <p>In these cases, we may want to handle the conversion ourselves. One option is getting the raw <code>PyObject</code> back by using the <code>py\"...\"o</code> syntax and then calling an appropriate <code>convert</code> function:</p> <pre><code>@show typeof(a)\n@show typeof(py\"$a\"o)\n@show typeof(convert(Int32, py\"$a\"o));\n</code></pre> <pre><code>typeof(a) = Int32\ntypeof(py\"$a\"o) = PyObject\ntypeof(convert(Int32, py\"$a\"o)) = Int32\n</code></pre> <p>Another way of handling (or preventing) type conversions is to use the <code>pycall</code> function. </p> <pre><code>pycall(np.random.normal, PyObject, size=3)\n</code></pre> <pre><code>PyObject array([ 1.27173788, -0.55905635, -1.81371862])\n</code></pre> <p>Here we specified to leave the object as a raw PyObject (i.e. no type conversion at all)</p> <p>We can also give it a Julia type to convert to</p> <pre><code>pycall(np.random.normal, Vector{ComplexF32}, size=3)\n</code></pre> <pre><code>3-element Vector{ComplexF32}:\n 0.82824904f0 + 0.0f0im\n -1.8152742f0 + 0.0f0im\n  0.6555549f0 + 0.0f0im\n</code></pre> <p>Here we forced the type conversion to complex numbers with 32-bit precision for the real and imaginary parts.</p> <p>But what if we need to call a Python function that requires a callback? Not a problem. PyCall will automatically convert Julia functions to Python callable objects!</p> <pre><code>si = pyimport(\"scipy.integrate\")\ntk = 0.0:1e-2:10.0\nfunction my_ode(t::Float64, y::Vector{Float64})::Vector{Float64}\n    dy = zeros(length(y))\n    dy[1] = 5.0*y[1] - 5.0*y[1]*y[2]\n    dy[2] = y[1]*y[2] - y[2]\n    return dy\nend\nsoln = si.solve_ivp(my_ode, (0.0, 10.0), [5.0, 1.0], t_eval=tk);\n</code></pre> <pre><code>using Plots\nplot(soln[\"t\"], soln[\"y\"]')\n</code></pre> <p></p> <pre><code>plot(soln[\"y\"][1,:], soln[\"y\"][2,:])\n</code></pre> <p></p> <p>For more details, see the PyCall github repo.</p>"},{"location":"Documentation/Development/Languages/Julia/julia_ccall_pycall/#calling-cfortran-libraries","title":"Calling C/FORTRAN Libraries","text":"<p>Here we will discuss how to call a C library function from within Julia.</p> <p>Calling a FORTRAN library function is the same except that FORTRAN compilers \"mangle\" the function names. This means that they are not precisely the same names as in the source code and you need to know what compiler was used to compile the FORTRAN library so you can determine the mangling scheme.</p> <p>Note that the library we are calling must be compiled as a shared library.</p> <p>As an example we will use the \"silly\" library that was written just for this.</p> <p>Here are the functions available in the silly library: <pre><code>void fill_zeros(double *to_fill, int size);\nvoid fill_value(double *to_fill, int size, double value);\nvoid fill_cb(double *to_fill, int size, double (*func)(int));\n</code></pre></p> <p>To call one of these functions, we will use the builtin Julia function <code>ccall</code>:</p> <pre><code>N = 4\nmy_vector = Vector{Float64}(undef, N)\n@show my_vector\nccall((:fill_zeros,\"fake-lib/libsilly\"), # function and library\n    Cvoid, # return type\n    (Ref{Float64}, Cint), # argument types\n    my_vector, N # arguments\n)\n@show my_vector\nccall((:fill_value,\"fake-lib/libsilly\"),\n    Cvoid,\n    (Ref{Float64}, Cint, Cdouble),\n    my_vector, N, pi\n)\n@show my_vector;\n</code></pre> <pre><code>my_vector = [2.257468188e-314, 0.0, 2.257517705e-314, 2.257468188e-314]\nmy_vector = [0.0, 0.0, 0.0, 0.0]\nmy_vector = [3.141592653589793, 3.141592653589793, 3.141592653589793, 3.141592653589793]\n</code></pre> <p>What if we want to use a function that requires a callback (so one of its arguments is a function pointer)? We can create a pointer to a Julia function with the <code>@cfunction</code> macro.</p> <pre><code>function my_filler(index::Int)::Float64\n    return index / 10.0\nend\ncfunc = @cfunction(my_filler, Float64, (Int,))\n</code></pre> <pre><code>Ptr{Nothing} @0x000000017ee10ec0\n</code></pre> <p>Now we call the C function with <code>ccall</code> as before. The type of the function pointer is <code>Ptr{Cvoid}</code>.</p> <pre><code>ccall((:fill_cb, \"fake-lib/libsilly\"),\n    Cvoid,\n    (Ref{Float64}, Cint, Ptr{Cvoid}),\n    my_vector, N, cfunc)\n@show my_vector;\n</code></pre> <pre><code>my_vector = [0.0, 0.1, 0.2, 0.3]\n</code></pre> <p>For more details, see the Calling C and FORTRAN Code section of the Julia documentation. (If the link does not work, just google \"julia call c library\".)</p> <p>A more complex example is provided by Ipopt.jl. You may also wish to look at the Ipopt library C API. The easiest way to do this is actually to just look at the header file at <code>src/Interfaces/IpStdCInterface.h</code> which is viewable at the Ipopt github repo.</p>"},{"location":"Documentation/Development/Languages/Julia/julia_ccall_pycall/#other-interfaces","title":"Other Interfaces","text":"<p>Interested in calling a function/library written in something other than Python, C or FORTRAN? Checkout the Julia Interop group on GitHub. Interfaces already exist for C++, MATLAB, Mathematica and R to name a few.</p>"},{"location":"Documentation/Development/Languages/Julia/julia_install/","title":"Installation","text":"<p>Julia modules exist on NREL HPC systems. Access simply with</p> <pre><code>module load julia\n</code></pre> <p>To see all available Julia modules on the system, use the command</p> <pre><code>module spider julia\n</code></pre> <p>However, if you need a version of Julia for which a module does not exist or want your own personal Julia build, there are several options described in the rest of this document. Below is a general guide for what approach to use:</p> <ul> <li>fast and easy - Anaconda</li> <li>performance and ease - Spack</li> <li>performance or need to customize Julia build - do it yourself (i.e. build from source)</li> </ul> <p>Warning</p> <p>Julia built on Kestrel CPU nodes will not work on the GPU nodes and vice versa. Always build Julia on the same type of compute node that you intend to run on.</p>"},{"location":"Documentation/Development/Languages/Julia/julia_install/#anaconda","title":"Anaconda","text":"<p>Older versions of Julia are available from <code>conda-forge</code> channel</p> <pre><code>conda create -n julia-env\nsource activate julia-env\nconda install -c conda-forge julia\n</code></pre>"},{"location":"Documentation/Development/Languages/Julia/julia_install/#spack-build","title":"Spack Build","text":""},{"location":"Documentation/Development/Languages/Julia/julia_install/#prerequisites","title":"Prerequisites","text":"<p>A working version of Spack. For detailed instructions on getting Spack setup see the GitHub repository. Briefly, this can be done with the following</p> <pre><code>git clone https://github.com/spack/spack.git\ncd spack\ngit checkout releases/v0.15 # Change to desired release\n. share/spack/setup-env.sh # Activate spack shell support\n</code></pre>"},{"location":"Documentation/Development/Languages/Julia/julia_install/#instructions","title":"Instructions","text":"<p>Info</p> <p>Steps 1 and 2 may be skipped when using the develop branch or any release branch after v0.15.</p> <ol> <li>In the Spack repository, open the file <code>var/spack/repos/builtin/packages/julia/package.py</code> in your favorite editor.</li> <li>There is an if-else statement under the if statement     <pre><code>if spec.target.family == 'x86_64'  or spec.target.family == 'x86':\n</code></pre>     Change the else clause to read     <pre><code>else:\n    target_str = str(spec.target).replace('_','-')\n    options += [\n        'MARCH={0}'.format(target_str),\n        'JULIA_CPU_TARGET={0}'.format(target_str)\n    ]\n</code></pre></li> <li>Now install Julia with Spack     <pre><code>spack install julia\n</code></pre></li> </ol>"},{"location":"Documentation/Development/Languages/Julia/julia_install/#do-it-yourself-build-v-12-or-later","title":"Do It Yourself Build (v 1.2 or later)","text":""},{"location":"Documentation/Development/Languages/Julia/julia_install/#prerequisites_1","title":"Prerequisites","text":"<p>All the required build tools and libraries are available on the clusters either by default or through modules.  The needed modules are covered in the instructions.</p>"},{"location":"Documentation/Development/Languages/Julia/julia_install/#terms","title":"Terms","text":"<ul> <li><code>JULIA_HOME</code> is the base directory of Julia source code (initially called <code>julia</code> after <code>git clone</code>)</li> </ul>"},{"location":"Documentation/Development/Languages/Julia/julia_install/#instructions_1","title":"Instructions","text":"<p>When compiling Julia you can choose to compile against Intel's MKL libraries or OpenBLAS for the Julia linear algebra operations. If you are going to be doing significant matrix-vector operations directly in Julia, then you will want to compile it with MKL. If most of the matrix-vector operations are being done in a subprogram or library (e.g. Ipopt) then it will make no difference what you compile Julia with.  In this latter case, it is recommended that you compile with OpenBLAS since that is significantly easier. Instructions for both choices are given below.</p> <p>Note</p> <p>When compiling Julia with MKL, Julia uses the <code>single dynamic library</code> option for linking.  Any dynamic libraries (e.g. Ipopt or CoinHSL) loaded by Julia also need to be linked to MKL with this approach. Failing to do so will result in unusual behavior, e.g. getting garbage values passed to the MKL function calls.</p> <ol> <li>Load the following modules:<ul> <li>gcc (&gt;= 5.1)</li> <li>cmake (&gt;= 3.4.3)</li> <li>mkl (any version -- optional)</li> </ul> </li> <li>Get the Julia source code  <code>git clone https://github.com/JuliaLang/julia.git</code></li> <li><code>cd julia</code></li> <li>Change to the version of Julia you want to build <code>git checkout &lt;julia_version&gt;</code></li> <li>In <code>Make.user</code> (you will need to create the file if it doesn't exist) in <code>JULIA_HOME</code> put the following:<ul> <li>If you want to compile Julia with MKL also add the following<ul> <li><code>USE_INTEL_MKL=1</code> -- Use Intel versions of BLAS and LAPACK (this is why we loaded mkl module)</li> <li><code>USE_BLAS64=0</code> -- Use the 64-bit library with the 32-bit integer interface. This will necessitate changes in <code>Make.inc</code>. The reasons for this are discussed in step 7.</li> </ul> </li> </ul> <p>Tip</p> <p>I found it useful to create the file <code>Make.user</code> in another location (e.g. home directory) and drop a link into the Julia build directory as I used <code>git clean -x -f -d</code> to make sure everything is completely clean</p> </li> <li>(Skip to step 8 if compiling Julia without MKL.) There are a couple of problems to overcome when compiling Julia with MKL.  The first is that a makefile in the SuiteSparse library package defines a <code>USER</code> variable that leads to problems with <code>xalt/ld</code> (a script that invokes ld).  To fix this do the following:<ul> <li>In JULIA_HOME fetch and unpack the SuiteSparse libraries <code>make -C deps/ extract-suitesparse</code></li> <li>With your favorite editor, open the file <code>JULIA_HOME/deps/scratch/SuiteSparse-5.4.0/UMFPACK/Lib/Makefile</code></li> <li>In the <code>Makefile</code>, do a global replace on <code>USER</code> --i.e. change all occurrences of the variable <code>USER</code> to something else like  <code>MUSER</code></li> </ul> </li> <li>The second problem is that when compiling against MKL, Julia either uses the 32-bit MKL libraries or the 64-bit MKL libraries with 64-bit interface.  It is common for other libraries (e.g. Ipopt or HSL) to compile against the 64-bit MKL libraries with 32-bit interface.  This causes unusual behavior.  To make Julia compile against the 64-bit MKL libraries with 32-bit interface, do the following:<ul> <li>Open <code>Make.inc</code> in your favorite editor and make the following change<ul> <li>find where <code>MKLLIB</code> is set (there will be an if-else statement depending on the value of <code>USE_BLAS64</code>)</li> <li>change the else clause to read <code>MKLLIB := $(MKLROOT)/lib/intel64</code></li> </ul> </li> </ul> </li> <li><code>make -j4</code> -- <code>-j4</code> allows <code>make</code> to use 4 processes to build and can speed up compilation (additional speed ups may be possible by increasing the number of processes)</li> </ol>"},{"location":"Documentation/Development/Languages/Julia/julia_parallel/","title":"Parallel Computing in Julia","text":"<p>We will make use of the following basic Monte Carlo integration function throughout this presentation</p> <pre><code>using Statistics\nusing BenchmarkTools # for the `@btime` macro\n\nfunction mc_integrate(f::Function, a::Real=0, b::Real=1, n::Int=100000)\n    ihat = 0.0\n    for k in 1:n\n        x = (b - a)*rand() + a\n        ihat += (f(x) - ihat) / k\n    end\n    return ihat\nend\n\nfunction intense_computation(t::Real)\n    sleep(t)\n    return rand()\nend;\n</code></pre>"},{"location":"Documentation/Development/Languages/Julia/julia_parallel/#asynchronous-tasks","title":"Asynchronous Tasks","text":""},{"location":"Documentation/Development/Languages/Julia/julia_parallel/#what-are-tasks","title":"What are Tasks?","text":"<p>Tasks are execution streams that do not depend on each other and can be done in any order. They can be executed asynchronously but they are not executed in parallel. That is, only one task is running at a given time but the order of execution is not predetermined.</p> <p>Tasks are also known as coroutines.</p>"},{"location":"Documentation/Development/Languages/Julia/julia_parallel/#creating-and-running-tasks","title":"Creating and Running Tasks","text":"<p>Running a task is done in 3 steps:</p> <ol> <li>Creation</li> <li>Scheduling</li> <li>Collect Results</li> </ol> <p>Creating a task can be done directly with the <code>Task</code> object:</p> <pre><code>my_task = Task(()-&gt;mc_integrate(sin, -pi, pi))\n</code></pre> <pre><code>Task (runnable) @0x000000011ecc0ab0\n</code></pre> <p>Note the <code>Task</code> constructor takes a function with no arguments.</p> <p>We can always define an zero argument anonymous function to pass to the <code>Task</code> constructor. The <code>@task</code> macro exists for this purpose:</p> <pre><code>my_task = @task mc_integrate(sin, -pi, pi)\n</code></pre> <pre><code>Task (runnable) @0x0000000136384cd0\n</code></pre> <p>Next we schedule the task to run using the <code>schedule</code> function</p> <pre><code>schedule(my_task)\n</code></pre> <pre><code>Task (done) @0x0000000136384cd0\n</code></pre> <p>Many times we want to create and schedule a task immediately. We can do this with the <code>@async</code> macro:</p> <pre><code>my_task = @async mc_integrate(sin, -pi, pi)\n</code></pre> <pre><code>Task (done) @0x000000011d14edc0\n</code></pre> <p>We can collect the results of the task once it has completed with the <code>fetch</code> function</p> <pre><code>fetch(my_task)\n</code></pre> <pre><code>0.0020294747408654656\n</code></pre> <p>There are a few helpful details to know about <code>fetch</code>:</p> <ol> <li>If the task has not finished when <code>fetch</code> is called, the call to <code>fetch</code> will block until the task has completed.</li> <li>If the task raises an exception, <code>fetch</code> will raise a <code>TaskFailedException</code> which wraps the original exception.</li> </ol> <p>Remember that tasks are not inherently parallel, just asynchronous execution streams. </p> <pre><code>function run_mci()\n    N = 10\n    result = zeros(N)\n    for k in 1:N\n        result[k] = mc_integrate(sin, -pi, pi)\n    end\n    return mean(result)\nend\n\nfunction run_mci_task()\n    N = 10\n    task_res = zeros(N)\n    @sync for k in 1:N\n        @async(task_res[k] = mc_integrate(sin, -pi, pi))\n    end\n    return mean(task_res)\nend;\n</code></pre> <pre><code>@btime run_mci()\n@btime run_mci_task();\n</code></pre> <pre><code>  22.094 ms (1 allocation: 160 bytes)\n  24.318 ms (75 allocations: 4.78 KiB)\n</code></pre> <p>Note</p> <p>The <code>@sync</code> macro will block at the end of the code block until all enclosed <code>@async</code> statements have completed execution.</p>"},{"location":"Documentation/Development/Languages/Julia/julia_parallel/#communicating-between-tasks","title":"Communicating Between Tasks","text":"<p>Sometimes we need to communicate between tasks. An easy way to accomplish this is to use Julia's <code>Channel</code> type. We can think of a <code>Channel</code> like a pipe or a queue: objects are put in at one end and taken off at the other.</p> <p>Let's rewrite <code>run_mci_task</code> to use channels by dividing the <code>run_mci</code> workflow into two functions.</p> <p>The first function will perform small Monte-Carlo integrations and put the results on a channel with the <code>put!</code> function. When it has finished the requested number of computations it will close the channel with <code>close</code> and return.</p> <pre><code>function integrator(output::Channel{Float64}, N::Int)\n    for k in 1:N\n        result = mc_integrate(sin, -pi, pi)\n        put!(output, result)\n    end\n    close(output)\n    return\nend;\n</code></pre> <p>Note</p> <p>If the channel is full, <code>put!</code> will block until space opens up.</p> <p>The second function will take the results off the channel using the <code>take!</code> function and accumulate them into an average. We keep pulling results from the channel as long as there is a result or the channel is open. We can check the former with <code>isready</code> and the latter with <code>isopen</code>.</p> <pre><code>function accumulator(input::Channel{Float64})\n    mean_val = 0.0\n    k = 0\n    while isready(input) || isopen(input)\n        value = take!(input)\n        k += 1\n        mean_val += (value - mean_val) / k\n    end\n    return mean_val\nend;\n</code></pre> <p>Note</p> <p>If the channel is empty, the <code>take!</code> function will block until there is an item available.</p> <p>Now we create channel which can hold 10 results, create and schedule both tasks and finally fetch the result.</p> <pre><code>function run_mci_chan()\n    comm_ch = Channel{Float64}(10)\n    atask = @async accumulator(comm_ch)\n    @async integrator(comm_ch, 10)\n    result = fetch(atask)    \n    return result\nend;\n</code></pre> <pre><code>@btime run_mci_chan();\n</code></pre> <pre><code>  22.097 ms (25 allocations: 1.45 KiB)\n</code></pre>"},{"location":"Documentation/Development/Languages/Julia/julia_parallel/#why-tasks","title":"Why Tasks?","text":"<p>If tasks aren't parallel, why are we talking about them in a parallel computing tutorial?</p> <p>Remeber that tasks are discrete computation units. They naturally define boundaries between computational tasks. Julia's native parallel capabilities are ways of scheduling tasks on other processors.</p>"},{"location":"Documentation/Development/Languages/Julia/julia_parallel/#multi-threading","title":"Multi-Threading","text":""},{"location":"Documentation/Development/Languages/Julia/julia_parallel/#starting-julia-with-multiple-threads","title":"Starting Julia with Multiple Threads","text":"<p>Julia (v1.3 or greater) has multithreading built into the language. By default, Julia starts with a single thread.  To start Julia with multiple threads either * set the environment variable <code>JULIA_NUM_THREADS</code> to some value &gt; 1 * start Julia with <code>--threads</code> or <code>-t</code> option (Julia v1.5 or greater)</p> <p>Once started, we can see how many threads are running with the function <code>Threads.nthreads</code></p> <pre><code>Threads.nthreads()\n</code></pre> <pre><code>2\n</code></pre>"},{"location":"Documentation/Development/Languages/Julia/julia_parallel/#threads-macro","title":"<code>@threads</code> Macro","text":"<p>Many computations take the form of looping over an array where the result of the computation is put into an element in the array and these computations do not interact. In this case, we can make use of the <code>Threads.@threads</code> macro.</p> <p>Lets apply this to our Monte-Carlo integration.</p> <pre><code>function run_mci_mt()\n    N = 10\n    mt_res = zeros(N)\n    Threads.@threads for k in 1:N\n        mt_res[k] = mc_integrate(sin, -pi, pi)\n    end\n    return mean(mt_res)\nend;\n</code></pre> <pre><code>@btime run_mci_mt();\n</code></pre> <pre><code>  11.118 ms (12 allocations: 1.00 KiB)\n</code></pre>"},{"location":"Documentation/Development/Languages/Julia/julia_parallel/#spawn-macro","title":"<code>@spawn</code> Macro","text":"<p>Some applications require dispatching individual tasks on different threads. We can do this using the <code>Threads.@spawn</code> macro. This is like the <code>@async</code> macro but will schedule the task on an available thread. That is, it creates a <code>Task</code> and schedules it but on an available thread.</p> <pre><code>function run_mci_mt2()\n    N = 10\n    mt_res = Vector{Float64}(undef, N)\n    @sync for k in 1:N\n        @async(mt_res[k] = fetch(Threads.@spawn mc_integrate(sin, -pi, pi)))\n    end\n    return mean(mt_res)\nend;\n</code></pre> <pre><code>@btime run_mci_mt2();\n</code></pre> <pre><code>  11.385 ms (126 allocations: 8.80 KiB)\n</code></pre> <p>There are a couple of oddities about Julia's multi-threading capability to remember:</p> <ol> <li>An available thread is any thread that has completed all assigned tasks or any remaining tasks are blocked.</li> <li>As of Julia 1.6, once a task has been assigned to a thread, it remains on that thread even after blocking operations. This will likely change in future releases of Julia.</li> </ol> <p>The combination of these two behaviors can lead to load imbalances amongst threads when there are blocking operations within a thread's tasks.</p>"},{"location":"Documentation/Development/Languages/Julia/julia_parallel/#using-channels","title":"Using Channels","text":"<p>Just as before, we can use a <code>Channel</code> to communicate between tasks in a multi-threaded environment. The only difference is that we replace <code>@async</code> with <code>Threads.@spawn</code>.</p> <pre><code>function run_mci_mt3()\n    comm_ch = Channel{Float64}(10)\n    itask = Threads.@spawn integrator(comm_ch, 10)\n    atask = Threads.@spawn accumulator(comm_ch)\n    result = fetch(atask)\n    return result\nend;\n</code></pre> <pre><code>@btime run_mci_mt3();\n</code></pre> <pre><code>  22.183 ms (35 allocations: 1.61 KiB)\n</code></pre> <p>Note</p> <p>We can see from the timing results this is not the best way to distribute the work since the <code>integrator</code> function has much more computational work than the <code>accumulator</code> function.</p>"},{"location":"Documentation/Development/Languages/Julia/julia_parallel/#distributed-computing-with-distributedjl","title":"Distributed Computing with Distributed.jl","text":""},{"location":"Documentation/Development/Languages/Julia/julia_parallel/#architecture","title":"Architecture","text":"<p>Communication patterns are one-sided, so users only manage one process. Communication itself takes the form of function or macro calls rather than explicit send and receive calls.</p> <p>Distributed.jl is built on two basic types: remote calls and remote references. A remote call is a directive to execute a particular function on a particular process. A remote reference is a reference to a variable stored on a particular process.</p> <p>There is a strong resemblance to the way Julia handles tasks: Function calls (wrapped in appropriate types) are scheduled on worker processes through remote calls which return remote references. The results of these calls are then retrieved by fetching the values using the remote references.</p>"},{"location":"Documentation/Development/Languages/Julia/julia_parallel/#setting-up","title":"Setting Up","text":"<p>We can launch more Julia processes on the same or other machines with the <code>addprocs</code> function. Here we launch 2 worker processes on the local machine:</p> <pre><code>using Distributed\naddprocs(2);\n</code></pre> <p>Each Julia process is identified by a (64-bit) integer. We can get a list of all active processes with <code>procs</code>:</p> <pre><code>@show procs();\n</code></pre> <pre><code>procs() = [1, 2, 3]\n</code></pre> <p>There is a distinction between the original Julia process and those we launched. The original Julia process is often called the master process and always has id equal to 1. The launched processes are called workers. We can obtain a list of workers with the <code>workers</code> function:</p> <pre><code>@show workers();\n</code></pre> <pre><code>workers() = [2, 3]\n</code></pre> <p>By default, distributed processing operations use the workers only.</p> <p>We can also start up worker processes from the command lines using the <code>-p</code> or <code>--procs</code> option.</p> <p>In order to launch Julia processes on other machines, we give <code>addprocs</code> a vector of tuples where each tuple is the hostname as a string paired with the number of processes to start on that host.</p> <p>The Julia global state is not copied in the new processes. We need to manually load any modules and define any functions we need. This is done with the <code>Distributed.@everywhere</code> macro:</p> <pre><code>@everywhere using Statistics\n@everywhere function mc_integrate(f::Function, a::Real=0, b::Real=1, n::Int=100000)\n    ihat = 0.0\n    for k in 1:n\n        x = (b - a)*rand() + a\n        ihat += (f(x) - ihat) / k\n    end\n    return ihat\nend;\n</code></pre>"},{"location":"Documentation/Development/Languages/Julia/julia_parallel/#distributed-macro","title":"<code>@distributed</code> Macro","text":"<p>The <code>@distributed</code> macro is the distributed memory equivalent of the <code>Threads.@threads</code> macro. This macro partitions the range of the for loop and executes the computation on all worker processes. </p> <pre><code>function run_mci_dist()\n    N = 10\n    total = @distributed (+) for k in 1:N\n        mc_integrate(sin, -pi, pi)\n    end\n    return total/N\nend;\n</code></pre> <pre><code>@btime run_mci_dist();\n</code></pre> <pre><code>  11.224 ms (157 allocations: 7.16 KiB)\n</code></pre> <p>Between the macro and the for loop is an optional reduction. Here we have used <code>+</code> but this can be any valid reduction operator including a user defined function. The values given to the reduction are the values of the last expression in the loop.</p> <p>Note</p> <p>If we do not provide a reduction, <code>@distributed</code> creates a task for each element of the loop and schedules them on worker processes and returns without waiting for the tasks to complete. To wait for completion of the tasks, the whole block can be wrapped with <code>@sync</code> macro.</p>"},{"location":"Documentation/Development/Languages/Julia/julia_parallel/#spawnat-macro","title":"<code>@spawnat</code> Macro","text":"<p>Julia also provides more fine grained control for launching tasks on workers with the <code>@spawnat</code> Macro:</p> <pre><code>function run_mci_dist2()\n    N = 10\n    futures = Vector{Future}(undef, N)\n    for k in 1:N\n        futures[k] = @spawnat(:any, mc_integrate(sin, -pi, pi))\n    end\n    return mean(fetch.(futures))\nend;\n</code></pre> <p>The first argument to <code>@spawnat</code> is the worker to run the computation on. Here we have used <code>:any</code> indicating that Julia should pick a process for us. If we wanted to execute the computation on a particular worker, we could specify which one with the worker id value. The second argument is the expression to compute.</p> <p><code>@spawnat</code> returns a <code>Future</code> which is a remote reference. We call <code>fetch</code> on it to retrieve the value of the computation. Note that <code>fetch</code> will block until the computation is complete.</p> <pre><code>@btime run_mci_dist2();\n</code></pre> <pre><code>  13.020 ms (1119 allocations: 44.34 KiB)\n</code></pre> <p>Warning</p> <p>The entire expression is sent to the worker process before anything in the expression is executed. This can cause performance issues if we need a small part of a big object or array.</p> <pre><code>@everywhere struct MyData\n    Data::Vector{Float64}\n    N::Int\nend\nfunction slow(my_data::MyData)\n    return fetch(@spawnat(2, mean(rand(my_data.N))))\nend;\n</code></pre> <pre><code>large_data = MyData(rand(1000000), 5)\n@btime slow(large_data);\n</code></pre> <pre><code>  1.731 ms (108 allocations: 4.08 KiB)\n</code></pre> <p>This is easily fixed using a local variable:</p> <pre><code>function fast(my_data::MyData)\n    n = my_data.N\n    return fetch(@spawnat(2, mean(rand(n))))\nend;\n</code></pre> <pre><code>@btime fast(large_data);\n</code></pre> <pre><code>  192.843 \u03bcs (100 allocations: 3.80 KiB)\n</code></pre>"},{"location":"Documentation/Development/Languages/Julia/julia_parallel/#remote-channels","title":"Remote Channels","text":"<p>As suggested by the name, these are the remote versions of the <code>Channel</code> type we've already seen. If you look at the source code, they are actually wrap an <code>AbstractChannel</code> to provide the needed remote functionality. We can effectively treat them just like a <code>Channel</code>.</p> <p>Let's redo our <code>integrator</code> - <code>accumulator</code> workflow, but this time let's do a better job of distributing the work:</p> <pre><code>@everywhere function integrator(output::RemoteChannel{Channel{Float64}}, N::Int)\n    for k in 1:N\n        result = mc_integrate(sin, -pi, pi)\n        put!(output, result)\n    end\n    put!(output, NaN)\n    return\nend;\n@everywhere function accumulator(input::RemoteChannel{Channel{Float64}}, nworkers::Int)\n    mean_val = 0.0\n    k = 0\n    finished = 0\n    while finished &lt; nworkers\n        value = take!(input)\n        if value === NaN\n            finished += 1\n        else\n            k += 1\n            mean_val += (value - mean_val) / k\n        end\n    end\n    return mean_val\nend;\n</code></pre> <pre><code>function run_mci_rc()\n    comm_ch = RemoteChannel(()-&gt;Channel{Float64}(10), 1)\n    @spawnat(2, integrator(comm_ch, 5))\n    @spawnat(3, integrator(comm_ch, 5))\n    atask = @async accumulator(comm_ch, nworkers())\n    return fetch(atask)\nend;\n</code></pre> <p>Here we create a <code>RemoteChannel</code> on the master process, divide the computationally intensive <code>integrator</code> function into two calls and remotely execute them on the worker processes. Then we start a task on the master process to accumulate the values and call fetch to wait for and retrieve the result.</p> <pre><code>@btime run_mci_rc();\n</code></pre> <pre><code>  12.328 ms (1066 allocations: 41.97 KiB)\n</code></pre>"},{"location":"Documentation/Development/Languages/Julia/julia_parallel/#shutting-down","title":"Shutting Down","text":"<p>To shutdown the worker processes we can use <code>rmprocs</code>.</p> <pre><code>rmprocs(workers())\n</code></pre> <pre><code>Task (done) @0x000000011cd3cde0\n</code></pre> <p>Alternatively, we can also just exit Julia and the workers will be shutdown as part of the exit process.</p>"},{"location":"Documentation/Development/Languages/Julia/julia_parallel/#distributed-computing-with-mpijl","title":"Distributed Computing with MPI.jl","text":""},{"location":"Documentation/Development/Languages/Julia/julia_parallel/#overview-of-mpijl","title":"Overview of MPI.jl","text":"<p><code>MPI.jl</code> is a Julia wrapper around an MPI library. By default it will download an MPI library suitable for running on the installing system. However, it is easily configured to use an existing system MPI implementation (e.g. one of the MPI modules on the cluster). See the documentation for instructions on how to do this.</p> <p><code>MPI.jl</code> mostly requires transmitted things to be buffers of basic types (types that are easily converted to C). Some functions can transmit arbitrary data by serializing them, but this functionality is not as fleshed out as in mpi4py.</p>"},{"location":"Documentation/Development/Languages/Julia/julia_parallel/#example","title":"Example","text":"<p>We first need to load and initialize MPI.</p> <pre><code>using MPI\nMPI.Init()\n</code></pre> <p><code>MPI.Init</code> loads the MPI library and calls <code>MPI_Init</code> as well as sets up types for that specific MPI library.</p> <p>Now we can implement our Monte-Carlo integration workflow using MPI</p> <pre><code>function run_mci_mpi()\n\n    comm = MPI.COMM_WORLD\n    rank = MPI.Comm_rank(comm)\n    size = MPI.Comm_size(comm)\n\n    if rank == 0\n        N = 10\n        num = [N]\n    else\n        num = Vector{Int}(undef, 1)\n    end\n    MPI.Bcast!(num, 0, comm)\n\n    rank_sum = 0.0\n    for k in rank+1:size:num[1]\n        rank_sum += mc_integrate(sin, -pi, pi)\n    end\n\n    total = MPI.Reduce([rank_sum], MPI.SUM, 0, comm)\n    if rank == 0\n        result = total / N\n    else\n        result = nothing\n    end\n\n    return result\nend\n</code></pre> <p>To benchmark this we time it many (10000) times and track the minimal value (this is similar to what the <code>@btime</code> macro does).</p> <pre><code>function run_loop(nruns::Int)\n\n    min_time = 1e10\n    result = 0.0\n\n    for _ in 1:nruns\n        MPI.Barrier(MPI.COMM_WORLD)\n        start = time()\n        result = run_mci_mpi()\n        stop = time()\n        elapsed = stop - start\n        if elapsed &lt; min_time\n            min_time = elapsed\n        end\n    end\n\n    if MPI.Comm_rank(MPI.COMM_WORLD) == 0\n        println(\"Elapsed time: \", min_time)\n    end\n\n    return\nend\n\nrun_loop(10000)\n</code></pre> <p>Here are the results:</p> <pre><code>mpirun -n 2 julia mpi_mci.jl\n  Activating environment at `~/HPC_Apps/julia-tutorial/Project.toml`\n  Activating environment at `~/HPC_Apps/julia-tutorial/Project.toml`\nElapsed time: 0.01108694076538086\n</code></pre>"},{"location":"Documentation/Development/Languages/Julia/julia_parallel/#gpu-computing","title":"GPU Computing","text":"<p>We provide a brief survey of available packages that can be used to get started.</p> <p>Packages exist for NVIDIA's CUDA, AMD's ROCm, and Intel's oneAPI. CUDA.jl is the most mature while the other two, as of this writing, are still underdevelopment.</p> <p>The package KernelAbstractions.jl is an abstraction layer for enabling different GPU backends.</p> <p>See the JuliaGPU organization's webpage or github repo for a great place to get started.</p>"},{"location":"Documentation/Development/Languages/Julia/julia_parallel/#additional-resources","title":"Additional Resources","text":"<p>The following are great resource for learning more</p> <ul> <li>Julia Documentation -- the manual discusses the inner workings of Julia including the native parallel computing capabilities</li> <li>Julia community especially the following discourse channels<ul> <li>Julia discourse -- all channels</li> <li>Julia at Scale discourse -- for scalable Julia</li> <li>Julia GPU discourse -- for GPU Julia computing</li> </ul> </li> <li>Julia Youtube Channel -- tutorials for Julia and Julia packages</li> <li>MPI.jl package repo and documentation</li> <li>JuliaGPU webpage and github repo</li> </ul>"},{"location":"Documentation/Development/Languages/Julia/julia_tour/","title":"Tour of Julia","text":"<p>\"Julia aims to create an unprecedented combination of ease-of-use, power, and efficiency in a single language.\" --Julia Documentation</p>"},{"location":"Documentation/Development/Languages/Julia/julia_tour/#why-julia","title":"Why Julia?","text":"<p>Feature Highlights:</p> <ul> <li>Designed for scientific computing</li> <li>Non-vectorized code is just as fast as vectorized code</li> <li>Designed for distributed and parallel computing</li> <li>Call C/FORTRAN functions directly</li> <li>Metaprogramming</li> </ul>"},{"location":"Documentation/Development/Languages/Julia/julia_tour/#basics","title":"Basics","text":""},{"location":"Documentation/Development/Languages/Julia/julia_tour/#repl-read-evaluate-print-loop","title":"REPL (Read-Evaluate-Print-Loop)","text":"<ul> <li>Command line julia interface</li> <li>Type the command <code>julia</code> in a terminal (assuming Julia is in your path)</li> <li>Basic way to interact with objects, packages and environments</li> </ul> <p><pre><code>jmaack-32918s:~ jmaack$ julia\n               _\n   _       _ _(_)_     |  Documentation: https://docs.julialang.org\n  (_)     | (_) (_)    |\n   _ _   _| |_  __ _   |  Type \"?\" for help, \"]?\" for Pkg help.\n  | | | | | | |/ _` |  |\n  | | |_| | | | (_| |  |  Version 1.6.1 (2021-04-23)\n _/ |\\__'_|_|_|\\__'_|  |  \n|__/                   |\n</code></pre> <pre><code>julia&gt; 4 * pi^2 + sqrt(2)im\n39.47841760435743 + 1.4142135623730951im\n\nhelp?&gt; Int\nsearch: Int Int8 Int64 Int32 Int16 Int128 Integer intersect intersect! InteractiveUtils InterruptException\n\n  Int64 &lt;: Signed\n\n  64-bit signed integer type.\n\njulia&gt; exit()\n</code></pre></p> <p>Tip</p> <p>When using the REPL, the result of the (last) expression is always printed. This is sometimes undesirable. We can suppress printing by ending the last expression with a semicolon <code>;</code>. This is used throughout this presentation for appearance purposes. Unless otherwise stated any semicolon in code is not needed.</p>"},{"location":"Documentation/Development/Languages/Julia/julia_tour/#defining-functions","title":"Defining Functions","text":"<p>There are two ways to define functions</p> <ul> <li> <p>Standard way:</p> <pre><code>function my_function(x)\n    return x^2\nend;\n</code></pre> </li> <li> <p>Short form way:</p> <pre><code>my_func(x) = x^2;\n</code></pre> </li> </ul> <p>It is also possible to define anonymous functions (and save pointers to them):</p> <pre><code>f = (x)-&gt;x^2;\n</code></pre> <pre><code>@show my_function(pi)\n@show my_func(pi)\n@show f(pi);\n</code></pre> <pre><code>my_function(pi) = 9.869604401089358\nmy_func(pi) = 9.869604401089358\nf(pi) = 9.869604401089358\n</code></pre> <p>Info</p> <p>Julia uses the standard control flow keywords such as <code>for</code>, <code>while</code>, <code>if</code>, <code>elseif</code>, <code>else</code>. See the Control Flow section of the Julia documentation for more details. Obviously, these are helpful in writing functions.</p>"},{"location":"Documentation/Development/Languages/Julia/julia_tour/#using-installed-packages","title":"Using Installed Packages","text":"<p>Packages can be accessed in two ways:</p> <ul> <li> <p><code>import</code> statement -- makes all module attributes (i.e. functions and types) available by prefixing the module name followed by a dot</p> <pre><code>x = rand(5)\nimport Statistics\nStatistics.mean(x)\n</code></pre> <pre><code>0.3339056277968421\n</code></pre> </li> <li> <p><code>using</code> statement -- everything exported by the module is directly accessible</p> <pre><code>using Statistics\nmean(x)\n</code></pre> <pre><code>0.3339056277968421\n</code></pre> </li> </ul> <p>Any attribute that is not exported by the module can still be accessed by prefixing the module name followed by a dot.</p> <pre><code>Statistics._conj(x)\n</code></pre> <pre><code>5-element Vector{Float64}:\n 0.17922586649673145\n 0.7155842248637634\n 0.29280412953665125\n 0.10325841440419592\n 0.3786555036828685\n</code></pre> <p>Note</p> <p>Like in python, there are no private attributes. Users may access anything created by a module. Package authors can suggest attributes that users should not use by not exporting them or with naming conventions (e.g. prefixing <code>_</code> to any name that is internal only).</p> <p>Julia 1.6 introduced the \"pythonic\" import syntax</p> <pre><code>import Statistics as Stats\nStats.mean(x)\n</code></pre> <pre><code>0.3339056277968421\n</code></pre> <p>In older Julia versions, we can declare a constant for our packages</p> <pre><code>import Statistics\nconst St = Statistics\nSt.mean(x)\n</code></pre> <pre><code>0.3339056277968421\n</code></pre> <p>Tip</p> <p>When writing Julia code, use <code>import</code> rather than <code>using</code>. This makes code easier to follow as well as giving hints on where to look for documentation.</p>"},{"location":"Documentation/Development/Languages/Julia/julia_tour/#vectorizing","title":"Vectorizing","text":"<p>Julia uses the MATLAB dot syntax to operate component-wise on arrays (i.e. vectors and matrices)</p> <pre><code>x = rand(3)\ny = rand(3)\n(x.*y).^2\n</code></pre> <pre><code>3-element Vector{Float64}:\n 0.5367929263482071\n 0.008092183589557244\n 0.36146876615689527\n</code></pre> <p>Julia also extends this syntax to ANY function that operates on vector elements</p> <pre><code>number_op(x) = x + 5\nnumber_op.(x)\n</code></pre> <pre><code>3-element Vector{Float64}:\n 5.754141942494573\n 5.8412967567631\n 5.637813968303307\n</code></pre> <p>In Julia, vectorizing is done for convenience rather than performance:</p> <pre><code>function my_mult_for(x,y)\n    z = zeros(length(x))\n    for k in length(x)\n        z[k] = x[k] * y[k]\n    end\n    return z\nend\n\nfunction my_mult_vect(x,y)\n    return x .* y\nend;\n</code></pre> <pre><code># This forces Julia to compile the function definitions\n# so that the timing results in the next cell are correct\nx = rand(2)\ny = rand(2)\n@time my_mult_vect(x,y)\n@time my_mult_for(x,y);\n</code></pre> <pre><code>  0.055219 seconds (145.07 k allocations: 8.243 MiB, 99.96% compilation time)\n  0.009099 seconds (15.42 k allocations: 873.090 KiB, 99.82% compilation time)\n</code></pre> <pre><code>x = rand(10000)\ny = rand(10000)\n@time my_mult_vect(x,y)\n@time my_mult_for(x,y);\n</code></pre> <pre><code>  0.000015 seconds (2 allocations: 78.203 KiB)\n  0.000032 seconds (2 allocations: 78.203 KiB)\n</code></pre>"},{"location":"Documentation/Development/Languages/Julia/julia_tour/#package-manager","title":"Package Manager","text":""},{"location":"Documentation/Development/Languages/Julia/julia_tour/#managing-packages-repl","title":"Managing Packages (REPL)","text":"<p>Open the REPL and hit the <code>[</code> key to enter package management mode. From here we can add or remove packages:</p> <pre><code>(@v1.6) pkg&gt; add Compat\n   Resolving package versions...\n    Updating `~/.julia/environments/v1.6/Project.toml`\n  [34da2185] + Compat v3.31.0\n    Updating `~/.julia/environments/v1.6/Manifest.toml`\n  [34da2185] + Compat v3.31.0\n  [8bb1440f] + DelimitedFiles\n  [8ba89e20] + Distributed\n  [1a1011a3] + SharedArrays\n  [2f01184e] + SparseArrays\n  [10745b16] + Statistics\n\n(@v1.6) pkg&gt; rm Compat\n    Updating `~/.julia/environments/v1.6/Project.toml`\n  [34da2185] - Compat v3.31.0\n    Updating `~/.julia/environments/v1.6/Manifest.toml`\n  [34da2185] - Compat v3.31.0\n  [8bb1440f] - DelimitedFiles\n  [8ba89e20] - Distributed\n  [1a1011a3] - SharedArrays\n  [2f01184e] - SparseArrays\n  [10745b16] - Statistics\n</code></pre> <p>We can also print out what packages are available <pre><code>(@v1.6) pkg&gt; st\n      Status `~/.julia/environments/v1.6/Project.toml`\n  [7073ff75] IJulia v1.23.2\n  [438e738f] PyCall v1.92.3\n</code></pre> or update the packages <pre><code>(@v1.6) pkg&gt; up\n    Updating registry at `~/.julia/registries/General`\n    Updating git-repo `https://github.com/JuliaRegistries/General.git`\n  No Changes to `~/.julia/environments/v1.6/Project.toml`\n  No Changes to `~/.julia/environments/v1.6/Manifest.toml`\n</code></pre></p>"},{"location":"Documentation/Development/Languages/Julia/julia_tour/#managing-packages-scripts","title":"Managing Packages (Scripts)","text":"<p>Package management mode in the REPL is actually just a convenient interface to the Julia package Pkg.jl which is part of the Julia standard library.</p> <p>All package mode commands are functions in Pkg.jl:</p> <pre><code>import Pkg; Pkg.add(\"Compat\"); Pkg.rm(\"Compat\")\n\n    Updating registry at `~/.julia/registries/General`\n    Updating git-repo `https://github.com/JuliaRegistries/General.git`\n   Resolving package versions...\n    Updating `~/.julia/environments/v1.6/Project.toml`\n  [34da2185] + Compat v3.31.0\n    Updating `~/.julia/environments/v1.6/Manifest.toml`\n  [34da2185] + Compat v3.31.0\n  [8bb1440f] + DelimitedFiles\n  [8ba89e20] + Distributed\n  [1a1011a3] + SharedArrays\n  [2f01184e] + SparseArrays\n  [10745b16] + Statistics\n    Updating `~/.julia/environments/v1.6/Project.toml`\n  [34da2185] - Compat v3.31.0\n    Updating `~/.julia/environments/v1.6/Manifest.toml`\n  [34da2185] - Compat v3.31.0\n  [8bb1440f] - DelimitedFiles\n  [8ba89e20] - Distributed\n  [1a1011a3] - SharedArrays\n  [2f01184e] - SparseArrays\n  [10745b16] - Statistics\n</code></pre> <pre><code>Pkg.status(); Pkg.update()\n\n      Status `~/.julia/environments/v1.6/Project.toml`\n  [7073ff75] IJulia v1.23.2\n  [438e738f] PyCall v1.92.3\n    Updating registry at `~/.julia/registries/General`\n    Updating git-repo `https://github.com/JuliaRegistries/General.git`\n  No Changes to `~/.julia/environments/v1.6/Project.toml`\n  No Changes to `~/.julia/environments/v1.6/Manifest.toml`\n</code></pre> <p>Warning</p> <p>If you want to use Julia within Jupyter notebook, some package management features (like adding new packages) do not work well. It is best to add/remove/update either with a script or using the REPL.</p>"},{"location":"Documentation/Development/Languages/Julia/julia_tour/#environments","title":"Environments","text":"<p>Environments allow us to install different versions of packages for use with different projects. Very similar to python virtual environments or conda environments.</p> <pre><code>Pkg.activate(\"env-one\"); Pkg.status()\n\n  Activating environment at `~/HPC_Apps/julia-tutorial/env-one/Project.toml`\n      Status `~/HPC_Apps/julia-tutorial/env-one/Project.toml`\n  [91a5bcdd] Plots v1.13.1\n</code></pre> <pre><code>Pkg.activate(\"env-two\"); Pkg.status()\n\n  Activating environment at `~/HPC_Apps/julia-tutorial/env-two/Project.toml`\n      Status `~/HPC_Apps/julia-tutorial/env-two/Project.toml`\n  [91a5bcdd] Plots v1.16.6\n</code></pre> <p>The environment names are given by the directory in which they reside. The explicitly added packages are given in the <code>Project.toml</code> file. The entire environment with all the required dependencies (down to specific commits) are in the <code>Manifest.toml</code> file.</p>"},{"location":"Documentation/Development/Languages/Julia/julia_tour/#activating-environments","title":"Activating Environments","text":"<p>There are 3 ways to activate an environment:</p> <ul> <li>Using the <code>Pkg.activate</code> function:     <pre><code>Pkg.activate(\"path/to/environment/\")\n</code></pre></li> <li>Within package management mode with the <code>activate</code> command:     <pre><code>activate path/to/environment\n</code></pre></li> <li>From the command line with the <code>--project</code> option:     <pre><code>julia --project=&lt;path/to/environment&gt;\n</code></pre></li> </ul> <p>The first 2 ways can also be used to create new environments.</p>"},{"location":"Documentation/Development/Languages/Julia/julia_tour/#copying-environments","title":"Copying Environments","text":"<p>To copy an environment, all you need is the <code>Project.toml</code> file. Put it in the desired directory and activate that environment. Finally, in package management mode, use the <code>instantiate</code> command:</p> <pre><code>(fake-env) pkg&gt; st\n      Status `~/fake-env/Project.toml`\n\u2192 [da04e1cc] MPI v0.18.1\n        Info packages marked with \u2192 not downloaded, use `instantiate` to download\n\n(fake-env) pkg&gt; instantiate\n   Installed MPI \u2500 v0.18.1\n    Building MPI \u2192 `~/.julia/scratchspaces/44cfe95a-1eb2-52ea-b672-e2afdf69b78f/494d99052881a83f36f5ef08b23de07cc7c03a96/build.log`\nPrecompiling project...\n  1 dependency successfully precompiled in 2 seconds (11 already precompiled)\n</code></pre> <p>Note</p> <p>Alternatively, you can use the <code>Pkg.instantiate</code> function.</p> <p>Info</p> <p>If you need to exactly copy an environment exactly copy both the <code>Project.toml</code> and <code>Manifest.toml</code> files into the desired directory and use the <code>instantiate</code> command.</p>"},{"location":"Documentation/Development/Languages/Julia/julia_tour/#environment-layering","title":"Environment Layering","text":"<p>Julia environments can be layered such that packages from more than just the top layer environment can be imported. This allows us to have access to debugging and development tools without putting them in whatever environment were working on. This is a major difference from conda environments.</p> <pre><code>Pkg.status()\n      Status `~/HPC_Apps/julia-tutorial/env-one/Project.toml`\n  [91a5bcdd] Plots v1.13.1\n</code></pre> <pre><code>import BenchmarkTools as BT # THIS IS NOT IN OUR TOP ENVIRONMENT!!!\n</code></pre> <p>When loading a package, Julia has a hierarchy of environments that it checks for the package. Julia loads the first version of the package it encounters in this hierarchy.  The environment hierarchy can be altered by the <code>JULIA_LOAD_PATH</code> environment variable.</p> <p>These environment stacks are discussed more in the Environments subsection of the Code Loading part of the Julia Manual.</p>"},{"location":"Documentation/Development/Languages/Julia/julia_tour/#types","title":"Types","text":""},{"location":"Documentation/Development/Languages/Julia/julia_tour/#type-hierarchy","title":"Type Hierarchy","text":"<p>In Julia everything has a type. We can access an object's type with the <code>typeof</code> function:</p> <pre><code>typeof(7.5)\n</code></pre> <pre><code>Float64\n</code></pre> <p>Even types have a type:</p> <pre><code>typeof(Float64)\n</code></pre> <pre><code>DataType\n</code></pre> <p>Julia also has a type hierarchy. There are subtypes and supertypes. We can access explore these with the functions <code>subtypes</code> and <code>supertype</code>:</p> <pre><code>subtypes(Float64)\n</code></pre> <pre><code>Type[]\n</code></pre> <pre><code>supertype(Float64)\n</code></pre> <pre><code>AbstractFloat\n</code></pre> <p><code>Float64</code> has no subtypes because it is a Concrete Type. All the supertypes are an Abstract Type.  Only Concrete Types can actually exist.</p> <p>Every type has only one immediate supertype. However, each supertype has a supertype. We can get the whole chain with the <code>supertypes</code> (plural) function:</p> <pre><code>supertypes(Float64)\n</code></pre> <pre><code>(Float64, AbstractFloat, Real, Number, Any)\n</code></pre> <p>Let us see all the floating point types available in Julia:</p> <pre><code>subtypes(AbstractFloat)\n</code></pre> <pre><code>4-element Vector{Any}:\n BigFloat\n Float16\n Float32\n Float64\n</code></pre> <p>We can test whether or not a type is a subtype of something with the <code>&lt;:</code> operator:</p> <pre><code>Float64 &lt;: AbstractFloat\n</code></pre> <pre><code>true\n</code></pre> <pre><code>Float64 &lt;: Float64\n</code></pre> <pre><code>true\n</code></pre> <pre><code>Int &lt;: AbstractFloat\n</code></pre> <pre><code>false\n</code></pre> <p>Warning</p> <p>Subtypes and supertypes get complicated when dealing with containers:</p> <pre><code>Float64 &lt;: Real\n</code></pre> <pre><code>true\n</code></pre> <pre><code>Vector{Float64} &lt;: Vector{Real}\n</code></pre> <pre><code>false\n</code></pre> <pre><code>Vector{Float64} &lt;: Vector\n</code></pre> <pre><code>true\n</code></pre> <p>We can use this to write functions:</p> <pre><code>function my_abs_sub(x)\n    if typeof(x) &lt;: Complex\n        println(\"Complex!\")\n        return sqrt(x.re^2 + x.im^2)\n    elseif typeof(x) &lt;: Real\n        println(\"Real!\")\n        return x &lt; 0 ? -x : x\n    else\n        error(\"Not a number!\")\n    end\nend\n@show my_abs_sub(-5)\n@show my_abs_sub(-5.0)\n@show my_abs_sub(-1 + 2im);\n</code></pre> <pre><code>Real!\nmy_abs_sub(-5) = 5\nReal!\nmy_abs_sub(-5.0) = 5.0\nComplex!\nmy_abs_sub(-1 + 2im) = 2.23606797749979\n</code></pre>"},{"location":"Documentation/Development/Languages/Julia/julia_tour/#multiple-dispatch","title":"Multiple Dispatch","text":"<p>A more Julia way of doing this is to write the typing information directly into the function definition:</p> <pre><code>function my_abs_md(x::Real)\n    println(\"Multiple Dispatch Real!\")\n    return x &lt; 0 ? -x : x\nend\nfunction my_abs_md(x::Complex)\n    println(\"Multiple Dispatch Complex!\")\n    return sqrt(x.re^2 + x.im^2)\nend\n@show my_abs_md(-5)\n@show my_abs_md(-1 + 2im);\n</code></pre> <pre><code>Multiple Dispatch Real!\nmy_abs_md(-5) = 5\nMultiple Dispatch Complex!\nmy_abs_md(-1 + 2im) = 2.23606797749979\n</code></pre> <p>Notice that the functions have the same name, but the correct one is executed based on the type of the argument. This is called Multiple Dispatch.</p> <p>Tip</p> <p>Add typing information for any function you are likely to use a lot. There are two reasons:</p> <ol> <li>Type information is used by the Julia compiler to make code more efficient</li> <li>Type information is a fast and easy way to document your code and catch bugs.</li> </ol>"},{"location":"Documentation/Development/Languages/Julia/julia_tour/#structs","title":"Structs","text":""},{"location":"Documentation/Development/Languages/Julia/julia_tour/#defining-structs","title":"Defining Structs","text":"<p>Julia allows us to define our own (composite) types:</p> <pre><code>struct Point\n    x::Float64\n    y::Float64\nend\np0 = Point(0, 0)\np1 = Point(1.0, 2.0)\n</code></pre> <pre><code>Point(1.0, 2.0)\n</code></pre> <p>We can define functions with this type as the argument now</p> <pre><code>function distance(p::Point, q::Point)\n    return sqrt((p.x - q.x)^2 + (p.y - q.y)^2)\nend\ndistance(p0, p1)\n</code></pre> <pre><code>2.23606797749979\n</code></pre> <p>We can build structs with other structs as components:</p> <pre><code>struct Circle\n    center::Point\n    radius::Float64\nend\n\nmy_circle = Circle(p1, 5)\n</code></pre> <pre><code>Circle(Point(1.0, 2.0), 5.0)\n</code></pre> <pre><code>function is_in(p::Point, c::Circle)\n    return distance(p, c.center) &lt; c.radius\nend\n@show is_in(p0, my_circle)\n@show is_in(Point(100,0), my_circle);\n</code></pre> <pre><code>is_in(p0, my_circle) = true\nis_in(Point(100, 0), my_circle) = false\n</code></pre>"},{"location":"Documentation/Development/Languages/Julia/julia_tour/#mutable-structs","title":"Mutable Structs","text":"<p>What if we want to change the radius of the circle?</p> <pre><code>my_circle.radius = 10.0 # Causes an error!!\n</code></pre> <pre><code>setfield! immutable struct of type Circle cannot be changed\n\n\n\nStacktrace:\n\n [1] setproperty!(x::Circle, f::Symbol, v::Float64)\n\n   @ Base ./Base.jl:34\n\n [2] top-level scope\n\n   @ In[34]:1\n\n [3] eval\n\n   @ ./boot.jl:360 [inlined]\n\n [4] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)\n\n   @ Base ./loading.jl:1116\n</code></pre> <p>Structs are immutable (cannot be changed) by default in Julia. This allows for some optimizations behind the scenes and most of the time we do not need to change the values in a Struct.</p> <p>If we need to change fields in a struct, we add the <code>mutable</code> keyword:</p> <pre><code>mutable struct MutableCircle\n    center::Point\n    radius::Float64\nend\nmy_mutable_circle = MutableCircle(p1, 5.0)\n@show my_mutable_circle\nmy_mutable_circle.radius = 10.0\n@show my_mutable_circle;\n</code></pre> <pre><code>my_mutable_circle = MutableCircle(Point(1.0, 2.0), 5.0)\nmy_mutable_circle = MutableCircle(Point(1.0, 2.0), 10.0)\n</code></pre>"},{"location":"Documentation/Development/Languages/Julia/julia_tour/#parametric-types","title":"Parametric Types","text":"<p>Let us go back to our Point type:</p> <pre><code>struct Point\n    x::Float64\n    y::Float64\nend\n</code></pre> <p>We locked in the types in the fields of this struct. What if we want to use a <code>Point</code> struct with a different type? Such as an <code>Int</code>. We use a Parametric Type.</p> <p>We define a Parametric Type in the following way:</p> <pre><code>struct ParametricPoint{R &lt;: Real}\n    x::R\n    y::R\nend\n\nfunction distance(p::ParametricPoint{&lt;:Real},\n        q::ParametricPoint{&lt;:Real})\n    return sqrt((p.x - q.x)^2 + (p.y - q.y)^2)\nend;\n</code></pre> <pre><code>p0 = ParametricPoint(1, -1)\n@show typeof(p0)\np1 = ParametricPoint(2.0, 0.0)\n@show typeof(p1)\n@show distance(p0,p1);\n</code></pre> <pre><code>typeof(p0) = ParametricPoint{Int64}\ntypeof(p1) = ParametricPoint{Float64}\ndistance(p0, p1) = 1.4142135623730951\n</code></pre>"},{"location":"Documentation/Development/Languages/Julia/julia_tour/#metaprogramming","title":"Metaprogramming","text":""},{"location":"Documentation/Development/Languages/Julia/julia_tour/#how-julia-code-is-executed","title":"How Julia Code is Executed","text":"<p>At a very high level, Julia code is executed in two phases:</p> <ol> <li>Parsing a string and turning it into an expression</li> <li>Evaluating that expression</li> </ol>"},{"location":"Documentation/Development/Languages/Julia/julia_tour/#expressions","title":"Expressions","text":"<p>Julia code is parsed and turned into expressions. These expressions are themselves Julia data structures.</p> <pre><code>expr = Meta.parse(\"z^2 + 1\")\nexpr\n</code></pre> <pre><code>:(z ^ 2 + 1)\n</code></pre> <p>While the expression prints as a human-readable mathematical expression, it is actually a tree:</p> <pre><code>dump(expr)\n</code></pre> <pre><code>Expr\n  head: Symbol call\n  args: Array{Any}((3,))\n    1: Symbol +\n    2: Expr\n      head: Symbol call\n      args: Array{Any}((3,))\n        1: Symbol ^\n        2: Symbol z\n        3: Int64 2\n    3: Int64 1\n</code></pre> <p>Since this is a data structure, we can change the expression</p> <pre><code>expr.args[1] = :-\nexpr.args[2].args[1] = :*\nexpr\n</code></pre> <pre><code>:(z * 2 - 1)\n</code></pre> <p>Then evaluate it</p> <pre><code>z = 3\n@show eval(expr)\nz = 2.5\n@show eval(expr);\n</code></pre> <pre><code>eval(expr) = 5\neval(expr) = 4.0\n</code></pre> <p>Note we gave <code>z</code> a value after we wrote the expression.</p>"},{"location":"Documentation/Development/Languages/Julia/julia_tour/#macros","title":"Macros","text":"<p>A macro is a special function that takes expressions, symbols and literal values as arguments and returns an expression. The biggest difference between a macro and a normal function is that a macro is executed during the parse phase. This means that in a macro we have access to the expression!</p> <p>Let's take a look at the <code>@assert</code> macro:</p> <pre><code>x = 5; y = 4;\n@assert x == y\n</code></pre> <pre><code>AssertionError: x == y\n\n\n\nStacktrace:\n\n [1] top-level scope\n\n   @ In[42]:2\n\n [2] eval\n\n   @ ./boot.jl:360 [inlined]\n\n [3] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)\n\n   @ Base ./loading.jl:1116\n</code></pre> <p>The error contains the expression that caused the error! This is not possible to do with a function because that expression is not available at runtime.</p> <p>How do we write macros? More or less like we write functions but using the <code>macro</code> keyword instead of the <code>function</code> keyword:</p> <pre><code>macro fadd(name::Symbol, f::Symbol, g::Symbol, nargs::Int)\n    x = [gensym() for _ in 1:nargs]\n    quote\n        $(esc(name))($(x...)) = $(esc(f))($(x...)) + $(esc(g))($(x...))\n    end\nend\n</code></pre> <pre><code>@fadd (macro with 1 method)\n</code></pre> <p>This macro takes two functions and creates an expression that for a function that computes the sum of the two. It is actually generating code!</p> <pre><code>p(x) = x^2\nq(x) = (2x + 5) / x^2\n@fadd(h, p, q, 1)\n@show p(pi) + q(pi)\n@show h(pi);\n</code></pre> <pre><code>p(pi) + q(pi) = 11.012830091668627\nh(pi) = 11.012830091668627\n</code></pre> <p>We can look at the expression that the macro generates with the macro <code>@macroexpand</code>:</p> <pre><code>@macroexpand(@fadd(h, p, q, 1))\n</code></pre> <pre><code>quote\n    #= In[43]:4 =#\n    h(var\"#73###258\") = begin\n            #= In[43]:4 =#\n            p(var\"#73###258\") + q(var\"#73###258\")\n        end\nend\n</code></pre> <p>Ignoring all the stuff with <code>#</code> symbols we can see that the expression returned by the macro looks more or less like a function definition.</p> <p>Having seen how this works let's unpack the macro definition a bit more. For context, here's the whole definition again:</p> <pre><code>macro fadd(name::Symbol, f::Symbol, g::Symbol, nargs::Int)\n    x = [gensym() for _ in 1:nargs]\n    quote\n        $(esc(name))($(x...)) = $(esc(f))($(x...)) + $(esc(g))($(x...))\n    end\nend\n</code></pre> <p>We'll unpack it one line at a time.</p> <p>Having seen how this works let's unpack the macro definition a bit more. For context, here's the whole definition again:</p> <pre><code>macro fadd(name::Symbol, f::Symbol, g::Symbol, nargs::Int)\n    x = [gensym() for _ in 1:nargs]\n    quote\n        $(esc(name))($(x...)) = $(esc(f))($(x...)) + $(esc(g))($(x...))\n    end\nend\n</code></pre> <p>First Line:</p> <pre><code>macro fadd(name::Symbol, f::Symbol, g::Symbol, nargs::Int)\n    ...\nend\n</code></pre> <p>The macro definition looks a lot like a function definition but with <code>macro</code> instead of <code>function</code>.</p> <p>Second Line:</p> <pre><code>    x = [gensym() for _ in 1:nargs]\n</code></pre> <p>Here we create a vector of symbols of size <code>nargs</code>. The <code>gensym</code> function generates a symbol for a variable that is guaranteed not to clash with existing variables.  These symbols will be the arguments of our new function.</p> <p>Third Line:</p> <pre><code>    quote\n        # expression here\n    end\n</code></pre> <p>This is an easy way to generate an expression. The contents of this block is the expression returned by the macro.</p> <p>Fourth Line:</p> <pre><code>        $(esc(name))($(x...)) = $(esc(f))($(x...)) + $(esc(g))($(x...))\n</code></pre> <p>This is the meat of the macro and it may seem a bit much at first.  However, each term is essentially the same. So let's just focus on the left hand side of the equality.</p> <pre><code>        $(esc(name))($(x...))\n</code></pre> <ul> <li>The <code>name</code> variable is local to the macro. It's value is what we want to put into the expression. So we interpolate it into the expression using <code>$</code>. </li> <li>However, we want that symbol to be evaluated in the context in which the macro was called. So we tell Julia to leave the value as is with the <code>esc</code> function.</li> <li>Without the call to <code>esc</code>, Julia will assume that the variable is local and needs to be renamed with <code>gensym</code> transformed so that it will not clash with other variables.</li> <li>Finally, we want to interpolate the contents of the vector <code>x</code> into the expression. This is done with the splat operator <code>...</code> in conjunction with <code>$</code>.</li> </ul> <p>Why can't we just write a function to do this? Let's try:</p> <pre><code>function fadd(name, f::Function, g::Function, nargs::Int)\n    x = [gensym() for _ in 1:nargs]\n    [WHAT HERE?](x...) = f(x...) + g(x...)\n    return [WHAT TO RETURN?]\nend\n</code></pre> <p>There are a couple problems here:</p> <ol> <li>What do we put for the function name? We want the value of the argument name. If we just put <code>name</code> we would end up with a function called name.</li> <li>What do we return? Even if we knew what to name the function, that name is only bound to the function in our current scope--the function <code>fadd</code>. Once we return from <code>fadd</code>, the name is no longer bound to this function.</li> </ol> <p>If we do not care about creating function names, we could construct and return an anonymous function:</p> <pre><code>function fadd(f::Function, g::Function, nargs::Int)\n    x = [gensym() for _ in 1:nargs]\n    return (x...)-&gt;(f(x...) + g(x...))\nend\nh1 = fadd(p,q,1)\nh1(pi)\n</code></pre> <pre><code>11.012830091668627\n</code></pre> <p>This gets us pretty close to the same functionality since we could assign the function pointer to any valid variable name.</p> <p>However, we did not maximize the value of the macro. We can actually generate documentation for our function as well:</p> <pre><code>macro fadd(name::Symbol, f::Symbol, g::Symbol, nargs::Int)\n    x = [gensym() for _ in 1:nargs]\n    local help = \"Functions $f and $g added together. Created with the `@fadd` macro!\"\n    quote\n        @doc string($help)\n        $(esc(name))($(x...)) = $(esc(f))($(x...)) + $(esc(g))($(x...))\n    end\nend\n@fadd(h,p,q,1);\n</code></pre> <pre><code>?h\n</code></pre> <pre><code>Functions p and q added together. Created with the `@fadd` macro!\n</code></pre>"},{"location":"Documentation/Development/Languages/Julia/julia_tour/#other-resources","title":"Other Resources","text":"<p>The Julia Documentation is a great place to read about Julia features. Numerous examples are normally given along with detailed explanation.</p> <p>The official Julia website is a great place to find Julia tutorials, learn about the Julia community or discover research using Julia.</p>"},{"location":"Documentation/Development/Languages/Python/","title":"Python","text":""},{"location":"Documentation/Development/Languages/Python/#nrel-hpc-documentation","title":"NREL HPC Documentation","text":"<ul> <li>Anaconda virtual environments: Utilize a specific version of Python and install packages within a conda environment.</li> <li>Dask: Parallelize your Python code using the Python-native package Dask.</li> <li>Intro to Jupyter notebooks: Run interactive Jupyter notebooks on NREL HPC systems.</li> <li>Interactive Parallel Python with Jupyter: Examples of launching parallel Python code from Jupyter notebooks on NREL HPC systems.</li> </ul>"},{"location":"Documentation/Development/Languages/Python/#interactive-tutorials","title":"Interactive Tutorials","text":"<p>The Interactive Parallel Python with Jupyter page demonstrates various examples of using popular parallel Python packages from a Jupyter notebook.</p> <ul> <li>Example notebooks to download and test:<ul> <li>cupy</li> <li>numbaCUDA</li> <li>cupy ipyparallel</li> <li>dask</li> </ul> </li> </ul>"},{"location":"Documentation/Development/Languages/Python/#hpc-python","title":"HPC Python","text":"<p>Links to External resources:</p> <ul> <li>MPI4PY: Python bindings to use MPI to distribute computations across cluster nodes</li> <li>Dask: Easily launch Dask workers on one node or across nodes</li> <li>Numba: Optimize your Python code to run faster</li> <li>PyCUDA: Utilize GPUs to accelerate computations</li> </ul>"},{"location":"Documentation/Development/Languages/Python/dask/","title":"Dask","text":"<p>Dask is a framework for parallelizing Python code. The most common use case is to enable Python programmers to scale scientific and machine learning analyses to run on distributed hardware. Dask has similarities to Apache Spark (see FAQ for comparison), but Dask is more Python native and interfaces with common scientific libraries such as NumPy and Pandas.</p>"},{"location":"Documentation/Development/Languages/Python/dask/#installation","title":"Installation","text":"<p>Warning</p> <p>Conda environments should be always be installed outside of your home directory for storage and performance reasons. This is especially important for frameworks like Dask, whose parallel processes can particularly strain the <code>/home</code> filesystem. Please refer to our dedicated conda documentation for more information on how to setup your conda environments to redirect the installation outside of <code>/home</code> by default.</p> <p>Dask can be installed via Conda/Mamba. For example, to install Dask into a new environment from <code>conda-forge</code> into your <code>/projects</code> allocation folder, first load the appropriate conda (or mamba) module (e.g., <code>module load mamba</code> on Kestrel), and then run the following on a compute node.</p> <pre><code># Be sure to replace \"&lt;allocation_handle&gt;\" with your HPC project.\n\n# interactive job\nsalloc -A &lt;allocation_handle&gt; -p debug -t 01:00:00\n\n# load mamba module\nml mamba\n\n# create and activate `dask-env` environment with Python 3.12\nmamba create --prefix=/projects/&lt;allocation_handle&gt;/dask-env conda-forge::python=3.12 conda-forge::dask\nconda activate /projects/&lt;allocation_handle&gt;/dask-env\n</code></pre> <p>This installs Dask along with common dependencies such as NumPy. Additionally, the <code>dask-jobqueue</code> package (discussed below), can be installed via:</p> <pre><code>mamba install conda-forge::dask-jobqueue\n</code></pre> <p>Further, there is the <code>dask-mpi</code> package (also discussed below). To ensure compatibility with the system MPI libraries, it is recommended to install <code>dask-mpi</code> using pip. As such, we recommending installing any conda packages first. <code>dask-mpi</code> depends on <code>mpi4py</code>, although we have found that the pip install command does not automatically install <code>mpi4py</code>, so we install it explicitly. Also, installation of <code>mpi4py</code> will link against the system libraries, so the desired MPI library should be loaded first. In addition, it may be necessary to explicitly specify the MPI compiler driver. For example, to install mpi4py on Kestrel using the Intel programming environment and its associated MPI (<code>PrgEnv-intel</code>), you would do the following:</p> <pre><code>module load PrgEnv-intel\nMPICC=`which mpicc` pip install dask-mpi mpi4py\n</code></pre>"},{"location":"Documentation/Development/Languages/Python/dask/#dask-single-node","title":"Dask single node","text":"<p>Dask can be used locally on your laptop or an individual node. Additionally, it provides wrappers for multiprocessing and threadpools. One advantage of using <code>LocalCluster</code> is that you can easily drop in another cluster configuration to further parallelize, with minimal modification of the code.</p> <p>The following is a simple example that uses a local cluster with the <code>dask.delayed</code> interface, which can be used when the problem doesn't fit into one of the built-in collection types such as <code>dask.array</code> or <code>dask.dataframe</code>:</p> Dask local cluster <pre><code>from distributed import Client, LocalCluster\nimport dask\nimport time\nimport random \n\n@dask.delayed\ndef inc(x):\n    time.sleep(random.random())\n    return x + 1\n\n@dask.delayed\ndef dec(x):\n    time.sleep(random.random())\n    return x - 1\n\n@dask.delayed\ndef add(x, y):\n    time.sleep(random.random())\n    return x + y\n\ndef main ():\n   cluster = LocalCluster(n_workers=2)\n   client = Client(cluster)\n   zs = []\n   for i in range(256):\n      x = inc(i)\n      y = dec(x)\n      z = add(x, y)\n      zs.append(z)\n\n   result = dask.compute(*zs)\n   print (result)\n\n\nif __name__ == \"__main__\":\n   main()\n</code></pre>"},{"location":"Documentation/Development/Languages/Python/dask/#dask-jobqueue","title":"Dask Jobqueue","text":"<p>The <code>dask-jobqueue</code> library makes it easy to deploy Dask to a distributed cluster using Slurm (via SLURMCluster).  This is particularly useful when running an interactive notebook, where the workers can be scaled dynamically. </p> <p>For the following example, first make sure that both <code>dask</code> and <code>dask-jobqueue</code> have been installed.  Create a file named <code>dask_slurm_example.py</code> with the following contents, and replace <code>&lt;project&gt;</code> with your project allocation.</p> <p>Assuming you are on Kestrel, this example will request two jobs from the <code>shared</code> partition.</p> <code>dask_slurm_example.py</code> <pre><code>from dask_jobqueue import SLURMCluster\nimport socket\nfrom dask.distributed import Client\nfrom collections import Counter\n\ncluster = SLURMCluster(\n   cores=18,\n   memory='24GB',\n   account='&lt;allocation_handle&gt;',\n   walltime='00:30:00',\n   processes=17,\n   queue='shared'\n)\n\nclient = Client(cluster)\n\ndef test():\n   return socket.gethostname()\n\nresult = []\ncluster.scale(jobs=2)\n\nfor i in range(2000):\n   result.append(client.submit(test).result())\n\nprint(Counter(result))\nprint(cluster.job_script())\n</code></pre> <p>Then the script can simply be executed directly from a login node:</p> <pre><code>python dask_slurm_example.py\n</code></pre> <p>Note that although 2 jobs are requested, Dask launches the jobs dynamically, so depending on the status of the job queue, your results may indicate that only a single node was used.</p>"},{"location":"Documentation/Development/Languages/Python/dask/#batch-runners","title":"Batch Runners","text":"<p>Alternatively, the <code>dask-jobqueue</code> library provides <code>batch runners</code> that are desgined to make it simple to kick off Python scripts as multi-node HPC jobs. In contrast to the dynamic cluster, the batch runner starts when all requested nodes are available.</p> <p>The following example was modified from the official <code>dask-jobqueue</code> help to reflect usage on Kestrel.</p> <code>dask_slurm_runner_example.py</code> <pre><code># dask_slurm_runner_example.py\nimport os\nimport getpass\nimport random\nfrom dask.distributed import Client\nfrom dask_jobqueue.slurm import SLURMRunner\n\nuser_name = getpass.getuser()\njob_id = int(os.environ[\"SLURM_JOB_ID\"])\nn_tasks = int(os.environ[\"SLURM_NTASKS\"])\nn_nodes = int(os.environ[\"SLURM_NNODES\"])\ntry:\n    # This is necessary to specify the correct amount of memory for each worker\n    mem_per_node = int(os.environ[\"SLURM_MEM_PER_NODE\"])\n    mem_worker = (1e6 * mem_per_node) / (n_tasks / n_nodes)\nexcept:\n    print(\"Couldn't determine SLURM worker memory.\")\n    raise\n\n# When entering the SlurmRunner context manager processes will decide if they should be\n# the client, schdeduler or a worker.\n# Only process ID 1 executes the contents of the context manager.\n# All other processes start the Dask components and then block here forever.\nwith SLURMRunner(\n    scheduler_file=f\"/scratch/{user_name}/scheduler-{job_id}.json\",\n    scheduler_options={\n        \"dashboard_address\": f\":{random.randint(30000, 40000)}\",\n        \"interface\": \"hsn0\",\n    },\n    worker_options={\n        \"memory_limit\": mem_worker,\n        \"local_directory\": f\"/scratch/{user_name}\",\n        \"interface\": \"hsn0\",\n    },\n) as runner:\n    # The runner object contains the scheduler address info and can be used to construct a client.\n    with Client(runner) as client:\n\n        # Wait for all the workers to be ready before continuing.\n        client.wait_for_workers(runner.n_workers)\n\n        print(f\"Dask cluster dashboard at: {client.dashboard_link}\")\n        print(f\"Dask cluster scheduler address: {client.scheduler.address}\")\n\n        # Then we can submit some work to the Dask scheduler.\n        assert client.submit(lambda x: x + 1, 10).result() == 11\n        assert client.submit(lambda x: x + 1, 20, workers=2).result() == 21\n\n        print(\"Dask SLURMRunner is working!\")\n\n# When process ID 1 exits the SlurmRunner context manager it sends a graceful shutdown to the Dask processes.\n</code></pre> <p>The python script is submitted to SLURM via a sbatch script. Note that because this example job requests two partial nodes, it is submitted to the <code>shared</code> partition. Be sure to replace <code>your-HPC-account</code> accordingly.</p> <code>dask_slurm_runner_launcher.sh</code> <pre><code>#!/bin/bash\n#SBATCH --nodes=2\n#SBATCH --ntasks=4\n#SBATCH --mem=8G\n#SBATCH --partition=shared\n#SBATCH --time=10\n#SBATCH --account=your-HPC-account # replace with your HPC account\n#SBATCH --output=slurm-%j.log\n\nexport MALLOC_TRIM_THRESHOLD_=65536\n\nml conda\nconda activate /path/to/dask-env\nsrun -n 4 python -u dask_slurm_runner_example.py\n</code></pre> <p>Note that SlurmRunner does not start a distributed nanny process, which would normally set the limits for resources consumed by Dask workers. Thus, you must manually export the <code>MALLOC_TRIM_THRESHOLD_</code> variable, which sets the minimum amount of contiguous free memory required to trigger a release of memory back to the system from each worker. You can find further details on worker memory managament <code>here</code>.</p> <p>The job is then launched as:</p> <pre><code>sbatch dask_slurm_runner_launcher.sh\n</code></pre>"},{"location":"Documentation/Development/Languages/Python/dask/#dask-mpi","title":"Dask MPI","text":"<p>Dask also provides a package called <code>dask-mpi</code> that uses MPI to create the cluster.  Note that <code>dask-mpi</code> only uses MPI to start the cluster, not for inter-node communication.</p> <p>Dask-MPI provides two interfaces to launch Dask, either from a batch script using the Python API, or from the command line.</p> <p>Here we show a simple example that uses Dask-MPI with a batch script.  Make sure that you have installed <code>dask-mpi</code> following the Installation Instructions.  Create <code>dask_mpi_example.py</code> and <code>dask_mpi_launcher.sh</code> with the contents below.  In <code>dask_mpi_launcher.sh</code>, replace <code>&lt;project&gt;</code> with your allocation, and <code>/path/to/dask-env</code> with the full conda prefix path into which you installed dask.</p> <code>dask_mpi_example.py</code> <pre><code>from dask_mpi import initialize\nfrom dask.distributed import Client\nimport socket\nimport time\nfrom collections import Counter\n\ndef test():\n   return socket.gethostname()\n\ndef main():\n   initialize(nthreads=5)\n   client = Client()\n   time.sleep(15)\n\n   result = []\n\n   for i in range (0,100):\n      result.append(client.submit(test).result())\n      time.sleep(1)\n\n   out = str(Counter(result))\n   print(f'nodes: {out}')\n\nmain()\n</code></pre> <code>dask_mpi_launcher.sh</code> <pre><code>#!/bin/bash \n#SBATCH --nodes=2\n#SBATCH --ntasks=4\n#SBATCH --time=10\n#SBATCH --account=&lt;project&gt;\n\nml mamba\nconda activate /path/to/dask-env\nsrun -n 4 python dask_mpi_example.py\n</code></pre> <p>The job is then launched as:</p> <pre><code>sbatch dask_mpi_launcher.sh\n</code></pre> <p>Warning</p> <p>We have observed errors such as <code>distributed.comm.core.CommClosedError</code> when using <code>dask-mpi</code>.  These errors may be related to known issues such as GitHub Issue #94.  Users that experience issues with <code>dask-mpi</code> are encouraged to use <code>dask-jobqueue</code> instead.</p>"},{"location":"Documentation/Development/Languages/Python/dask/#references","title":"References","text":"<p>Dask documentation</p> <p>Dask Jobqueue</p> <p>Dask MPI</p>"},{"location":"Documentation/Development/Languages/Python/KestrelParallelPythonJupyter/pyEnvsAndLaunchingJobs/","title":"Interactive Parallel Python with Jupyter","text":"<p>For a general introduction to using Jupyter notebooks on Kestrel, please refer to the official documentation. This page covers how to leverage parallel computing with python through Jupyter notebooks on compute nodes. Accompanying notebooks to test if the environment and the job have been configured correctly are under the topic 'Parallel Interactive Tutorials' on the main Python page.</p>"},{"location":"Documentation/Development/Languages/Python/KestrelParallelPythonJupyter/pyEnvsAndLaunchingJobs/#setting-up-your-account","title":"Setting up your account","text":"<p>Login to Kestrel <pre><code>$ ssh -X &lt;username&gt;@kestrel-gpu.hpc.nrel.gov\n</code></pre></p> <p>Navigate to your /projects directory <pre><code>$ cd /projects/&lt;projectname&gt;/&lt;username&gt;/\n</code></pre></p> <p>Load the Nvidia HPC programming environment <pre><code>$ module load PrgEnv-nvhpc/8.5.0\n</code></pre></p> <p>Check available conda modules and load one <pre><code>$ module avail conda\n$ module load anaconda3/2024.06.1\n</code></pre></p> <p>Create a new environment named \u2018myEnv\u2019 in the current directory</p> <p>Warning</p> <p>Conda environments should be always be installed outside of your home directory for storage and performance reasons. This is especially important for frameworks like Dask, whose parallel processes can particularly strain the <code>/home</code> filesystem. Please refer to our dedicated conda documentation for more information on how to setup your conda environments to redirect the installation outside of <code>/home</code> by default.</p> <pre><code>$ conda create --prefix ./myEnv\n</code></pre> <p>Activate your new environment <pre><code>$ conda activate /kfs2/projects/&lt;projectname&gt;/&lt;username&gt;/myEnv\n# (or) From the same directory\n$ conda activate ./myEnv\n</code></pre></p> <p>Create a jupyter kernel named \u2018myEnvJupyter\u2019 from the myEnv environment <pre><code>$ conda install ipykernel\n$ python -m ipykernel install --user --name=myEnvJupyter\n</code></pre></p> <p>(Optional) To access your <code>/scratch</code> directory from your <code>/projects</code> directory, execute the following in your project directory <pre><code>$ ln -s /scratch/&lt;username&gt;/ scratch\n</code></pre> The above command will create a symbolic link to the <code>scratch</code> folder, which can be navigated to from JupyterHub to access files in your scratch directory.</p>"},{"location":"Documentation/Development/Languages/Python/KestrelParallelPythonJupyter/pyEnvsAndLaunchingJobs/#install-packages","title":"Install packages","text":"<p>CuPy : \u201cAn open-source array library for GPU-accelerated computing with Python\u201d <pre><code>$ nvcc \u2013version\n$ conda install -c conda-forge cupy\n$ conda install -c conda-forge cupy cudnn cutensor nccl\n</code></pre></p> <p>numba-cuda : \u201cCUDA GPU programming by directly compiling a restricted subset of Python code into CUDA kernels\u201d <pre><code>$ conda install -c conda-forge numba-cuda\n$ conda install -c conda-forge cuda-nvcc cuda-nvrtc \"cuda-version&gt;=12.0\"\n</code></pre></p> <p>mpi4py : \u201cPython bindings for the\u00a0Message Passing Interface\u00a0(MPI) standard to exploit multiple processors\u201d <pre><code>$ conda install -c conda-forge mpi4py openmpi\n$ conda install cuda-cudart cuda-version=12\n</code></pre></p> <p>ipyparallel : \u201cInteractive Parallel Computing with IPython\u201d <pre><code>$ conda install ipyparallel\n</code></pre></p> <p>Dask : \u201cA flexible open-source Python library for parallel and distributed computing\u201d <pre><code>$ conda install dask\n$ conda install dask-jobqueue\n$ conda install graphviz\n$ conda install ipycytoscape\n$ conda install matplotlib\n</code></pre></p> Quickstart: Install all of the above packages from our pre-built environment file. <pre><code>$ wget https://raw.githubusercontent.com/NREL/HPC/gh-pages/docs/Documentation/Development/Languages/Python/KestrelParallelPythonJupyter/metadata/myEnv.yml\n$ conda env create --prefix=/projects/&lt;projectname&gt;/&lt;username&gt;/myEnv  --file=myEnv.yml\n$ conda activate /projects/&lt;projectname&gt;/&lt;username&gt;/myEnv\n$ python -m ipykernel install --user --name=myEnvJupyter \n</code></pre>"},{"location":"Documentation/Development/Languages/Python/KestrelParallelPythonJupyter/pyEnvsAndLaunchingJobs/#launching-jobs","title":"Launching jobs","text":"<p>A general guide to running jobs on Kestrel can be found in the official documentation. Below are example procedures suitable for running jobs involving specific python modules, depending on their parallelization capability.</p> <p>The text in the red box shows an example of the output parameter <code>&lt;nodename&gt;</code> and the text in the yellow box shows an example of the output parameter <code>&lt;alphabet soup&gt;</code>, relevant to the following tutorial.</p> <p> </p>"},{"location":"Documentation/Development/Languages/Python/KestrelParallelPythonJupyter/pyEnvsAndLaunchingJobs/#gpu-compatible-modules-eg-cupy-numba-cuda-etc","title":"GPU compatible modules: E.g. CuPy, numba-cuda etc.","text":"<ol> <li> <p>Kestrel: Launch an interactive job     <pre><code>$ salloc -A &lt;projectname&gt; -t 00:15:00 --partition=debug --gres=gpu:1\n$ module load anaconda3/2024.06.1\n$ conda activate ./myEnv\n$ jupyter-lab --no-browser --ip=$(hostname -s)\n</code></pre></p> </li> <li> <p>Local terminal: Establish a SSH tunnel     <pre><code>$ ssh -N -L 8888:&lt;nodename&gt;:8888 &lt;username&gt;@kestrel-gpu.hpc.nrel.gov\n</code></pre></p> </li> <li> <p>Web browser     <pre><code>http://127.0.0.1:8888/?token=&lt;alphabet soup&gt;\n</code></pre></p> <p>File &gt; New &gt; Notebook &gt; myEnvJupyter</p> </li> </ol> <p>Jupyter test notebook for CuPy</p> <p>Jupyter test notebook for numba-cuda</p>"},{"location":"Documentation/Development/Languages/Python/KestrelParallelPythonJupyter/pyEnvsAndLaunchingJobs/#multithread-capable-modules-eg-dask","title":"Multithread capable modules: E.g. Dask","text":"<ol> <li> <p>Kestrel: Launch a multithread interactive job     <pre><code>$ salloc -A &lt;projectname&gt; -t 00:15:00 --nodes=1 --ntasks-per-node=104 --partition=debug\n$ module load anaconda3/2024.06.1\n$ conda activate ./myEnv\n$ jupyter-lab --no-browser --ip=$(hostname -s)\n</code></pre></p> </li> <li> <p>Local terminal: Establish a SSH tunnel     <pre><code>$ ssh -N -L 8888:&lt;nodename&gt;:8888 &lt;username&gt;@kestrel.hpc.nrel.gov\n</code></pre></p> </li> <li> <p>Web browser     <pre><code>http://127.0.0.1:8888/?token=&lt;alphabet soup&gt;\n</code></pre></p> <p>File &gt; New &gt; Notebook &gt; myEnvJupyter</p> </li> </ol> <p>Jupyter test notebook for dask</p>"},{"location":"Documentation/Development/Languages/Python/KestrelParallelPythonJupyter/pyEnvsAndLaunchingJobs/#multinode-capable-job-eg-mpi4py-through-ipyparallel","title":"Multinode capable job. E.g. mpi4py through ipyparallel","text":"<ol> <li> <p>Kestrel: Launch a multinode interactive job     <pre><code>$ salloc -A &lt;projectname&gt; -t 00:15:00 --nodes=2 --ntasks-per-node=1 --partition=short\n$ module load anaconda3/2024.06.1\n$ conda activate ./myEnv\n$ jupyter-lab --no-browser --ip=$(hostname -s)\n</code></pre></p> </li> <li> <p>Local terminal: Establish a SSH tunnel     <pre><code>$ ssh -N -L 8888:&lt;nodename&gt;:8888 &lt;username&gt;@kestrel.hpc.nrel.gov\n</code></pre></p> </li> <li> <p>Web browser     <pre><code>http://127.0.0.1:8888/?token=&lt;alphabet soup&gt;\n</code></pre></p> <p>File &gt; New &gt; Notebook &gt; myEnvJupyter</p> </li> </ol> <p>Jupyter test notebook for mpi4py</p>"},{"location":"Documentation/Development/Languages/Python/KestrelParallelPythonJupyter/pyEnvsAndLaunchingJobs/#gpu-multinode-jobs-eg-cupy-mpi4py-through-ipyparallel","title":"GPU + multinode jobs. E.g. CuPy + mpi4py through ipyparallel","text":"<ol> <li> <p>Kestrel: Launch an interactive job     <pre><code>$ salloc -A projectname -t 00:15:00 --nodes=2 --ntasks-per-node=1 --gres=gpu:1 \n$ module load anaconda3/2024.06.1\n$ conda activate ./myEnv\n$ jupyter-lab --no-browser --ip=$(hostname -s)\n</code></pre></p> </li> <li> <p>Local terminal: Establish a SSH tunnel     <pre><code>$ ssh -N -L 8888:&lt;nodename&gt;:8888 &lt;username&gt;@kestrel-gpu.hpc.nrel.gov\n</code></pre></p> </li> <li> <p>Web browser     <pre><code>http://127.0.0.1:8888/?token=&lt;alphabet soup&gt;\n</code></pre></p> <p>File &gt; New &gt; Notebook &gt; myEnvJupyter</p> </li> </ol> <p>Jupyter test notebook for CuPy + mpi4py</p>"},{"location":"Documentation/Development/Languages/Python/KestrelParallelPythonJupyter/pyEnvsAndLaunchingJobs/#_1","title":"Parallel Python with Jupyter","text":""},{"location":"Documentation/Development/Libraries/","title":"Scientific Libraries Overview","text":"<p>Scientific math libraries are a collection of highly optimized software tools that provide functions and algorithms for performing mathematical operations commonly used in scientific applications. They provide developers with a variety of tools for solving complex problems. These libraries are highly optimized for performance and generally designed to be portable across different platforms and operating systems. </p> <p>We support some of the most widely used scientific math libraries including:</p> <ul> <li>MKL </li> <li>LibSci (Kestrel only)</li> <li>FFTW </li> <li>LAPACK</li> <li>scaLAPACK</li> <li>HDF5 </li> <li>PETSc</li> </ul> <p>For details on how to build an application with scientific libraries, see our how-to guide</p> <p>For more information on a given scientific library, see our individual library pages under our \"Libraries\" drop-down menu.</p>"},{"location":"Documentation/Development/Libraries/fftw/","title":"FFTW","text":"<p>Documentation: FFTW</p> <p>FFTW is a C library for computing discrete Fourier transforms of arbitrary input sizes and dimensions. It is optimized for speed and can perform discrete Fourier transforms up to several orders of magnitude faster than other commonly available Fourier transform libraries. FFTW supports both single-precision and double-precision transforms, as well as multithreading for parallel execution on shared-memory systems.</p>"},{"location":"Documentation/Development/Libraries/hdf5/","title":"HDF5","text":"<p>Documentation: HDF5</p> <p>HDF5 is a versatile data storage and management library designed for storing and exchanging large and complex data collections. It provides a powerful and flexible data model for representing and organizing data, as well as a variety of high-level programming interfaces for accessing and manipulating data. HDF5 supports a wide range of data types and can handle data sets of virtually unlimited size.</p> <p>HDF5 supports both parallel and serial file I/O, achieving high performance with both.</p>"},{"location":"Documentation/Development/Libraries/howto/","title":"Libraries How-To: Linking Scientific Libraries","text":"<p>This page is a tutorial explaining how to include scientific libraries when compiling software. </p> <p>There are a few common scientific libraries: LAPACK, BLAS, BLACS, scaLAPACK, FFTW, HDF5, and others. These libraries are generally highly optimized, and many scientific programs favor use of these libraries over in-house implementations of similar functionality. See our libraries overview page for more information.</p> <p>Scientific libraries can be packaged together, like in the Intel Math Kernel Library (MKL), or Cray\u2019s LibSci. They can also be built completely separately and act as standalone libraries. These libraries can be built with different MPI implementations and compiler choices. </p> <p>If you\u2019re building a code that relies on one or more of these libraries, you can choose how to include these libraries. By the end of this tutorial, how to include these libraries should be clearer. If you need help building a particular package on an NREL machine, please contact HPC help. </p>"},{"location":"Documentation/Development/Libraries/howto/#makefiles-autoconf-and-cmake","title":"Makefiles, autoconf, and cmake","text":"<p>Build tools like make, autoconf, and cmake are convenient ways to automate the compilation of a code. If you\u2019re building a package, you may need to modify/customize how the code compiles, e.g., so it finds and includes the libraries you want. This may involve directly modifying the makefile, modifying the make.include (or make.inc, makefile.include, etc.) file, or using tools like autoconf or CMake to configure the makefile. </p> <p>Modifying a makefile (or make.include, etc.) so it compiles using the scientific libraries you want can be a daunting process. We\u2019ll go through a prototypical example and show how different libraries can be included in the build of a program. To do this, we\u2019ll use a makefile.include file for the electronic structure program VASP.</p> <p>Note</p> <p>We provide a walkthrough of linking scientific libraries using the VASP code as an example. This walkthrough tries to demonstrate key features of the general process of including scientific libraries in a build. We note that the exact build and modification process will vary between codes. Consulting the documentation of the code you\u2019re trying to build is always the best place to start. </p>"},{"location":"Documentation/Development/Libraries/howto/#walkthrough","title":"Walkthrough","text":""},{"location":"Documentation/Development/Libraries/howto/#overview","title":"Overview","text":"<p>We\u2019ll use the VASP makefile.include file as our walkthrough example. We can find a number of VASP makefile.include files here. We\u2019ll be looking specifically at this file.</p> <p>We\u2019ll take a look at building with Intel MKL and the HDF5 package. </p>"},{"location":"Documentation/Development/Libraries/howto/#building-with-mkl-and-hdf5","title":"Building with MKL and HDF5","text":"<p>We want to build with MKL and HDF5. If we look at the VASP documentation, we see that LAPACK, scaLAPACK, BLAS, and FFTW are required. MKL covers all of these needs. Thus, we need to tell the makefile where to look for MKL.</p>"},{"location":"Documentation/Development/Libraries/howto/#environment-preparation","title":"Environment Preparation","text":"<p>We need our MKL to be built with the same compilers and MPI implementation as we\u2019re building VASP with. Let\u2019s see what sorts of MKL builds are available to us. Using the following command to show what builds of mkl are available as a module: </p> <p><code>module avail 2&gt;&amp;1 | grep mkl</code> </p> <p>Yields the output: </p> <p><code>intel-oneapi-mkl/2023.0.0-intel      ucx/1.13.0</code> </p> <p>Thus, if we want to use the toolchains managed by NREL, we must use the Intel oneapi toolchain in our VASP build, since <code>intel-oneapi-mkl/2023.0.0-intel</code> is the only available mkl module. If you want to use a different toolchain, you could build MKL yourself, but that\u2019s outside the scope of this article. </p> <p>To \u201cuse the Intel oneapi toolchain\u201d means to use Intel compilers and Intel\u2019s implementation of MPI to compile VASP. We\u2019re doing this because mkl was built with this toolchain, and we want our toolchains to match as best as possible to minimize build errors and bugs. </p> <p>Let\u2019s prepare our environment to use this toolchain. First, </p> <p><code>module purge</code> </p> <p>To clear your environment. Now, we want the Intel oneapi mkl module, the Intel fortran compiler (ifort), and the Intel MPI fortran compiler (mpiifort). Type: </p> <p><code>module avail 2&gt;&amp;1 | grep oneapi</code> </p> <p>to see which modules are related to the intel-oneapi toolchain. We can locate the three we want: </p> <pre><code>module load intel-oneapi-mkl/2023.0.0-intel \nmodule load intel-oneapi-mpi/2021.8.0-intel \nmodule load intel-oneapi/2022.1.0 \n</code></pre> <p>How do we know these are the ones we want? The first line loads the mkl module. The second line gives us mpiifort, the Intel MPI fortran compiler, and the third line gives us ifort, the Intel Fortran compiler. (test the latter two with <code>which mpiifort</code> and <code>which ifort</code> -- you\u2019ll see that they\u2019re now in your path. If you <code>module purge</code> and try <code>which mpiifort</code> again, you\u2019ll see you\u2019re not able to find mpiifort anymore.) </p>"},{"location":"Documentation/Development/Libraries/howto/#modifying-the-makefile-for-mkl","title":"Modifying the Makefile for MKL","text":"<p>Now that we have the toolchain loaded into our environment, let\u2019s take a look at the actual makefile.include file (link to file here). There are two important sections for the purpose of getting the code to build. The first: </p> <pre><code>CPP         = fpp -f_com=no -free -w0  $*$(FUFFIX) $*$(SUFFIX) $(CPP_OPTIONS) \nFC          = mpiifort -qopenmp \nFCL         = mpiifort \n</code></pre> <p>The first line says that the compiler pre-processor will be fpp (try <code>which fpp</code> and you should get an output <code>/sfs/nopt/nrel/apps/compilers/01-23/spack/opt/spack/linux-rhel8-icelake/gcc-8.4.0/intel-oneapi-compilers-2022.1.0-wosfexnwo5ag3gyfoco2w6upcew5yj6f/compiler/2022.1.0/linux/bin/intel64/fpp</code>, confirming that we\u2019re pulling fpp from intel-oneapi).  </p> <p>The second and third lines say that we\u2019ll be using Intel\u2019s MPI (Try <code>which mpiifort</code> to confirm that it is in your path). FC is the \u201cFortran Compiler\u201d and FCL is the corresponding linker. Line 14 additionally says we\u2019ll be compiling with openmp. Different compilers have different executable names (e.g. mpiifort for Intel MPI fortran compiler, mpifort for GNU). See the Fortran documentation page for a complete list. </p> <p>The next important section is given below: </p> <pre><code># Intel MKL (FFTW, BLAS, LAPACK, and scaLAPACK) \n# (Note: for Intel Parallel Studio's MKL use -mkl instead of -qmkl) \nFCL        += -qmkl \nMKLROOT    ?= /path/to/your/mkl/installation \nLLIBS      += -L$(MKLROOT)/lib/intel64 -lmkl_scalapack_lp64 -lmkl_blacs_intelmpi_lp64\nINCS        =-I$(MKLROOT)/include/fftw \n</code></pre> <p>This makefile.include file has been provided to us by VASP. Our job here is two-fold:</p> <ol> <li>To ensure that we tell make (via the makefile.include file) the correct place to find MKL, I.e., to ensure that <code>MKLROOT</code> in the makefile.include file is set correctly.</li> <li>To ensure that we tell make the correct libraries to reference within <code>MKLROOT</code>.</li> </ol> <p>To do step 1, first type:</p> <p><code>module list</code> </p> <p>To see the modules you\u2019ve loaded into your environment. You should have <code>intel-oneapi-mkl/2023.0.0-intel</code> in the list.  If not, review the environment preparation section. Now, we use the <code>module show</code> command to find the root directory of mkl: </p> <p><code>module show intel-oneapi-mkl/2023.0.0-intel</code> </p> <p>We see in the output of this command the following line: </p> <p><code>setenv      MKLROOT /sfs/nopt/nrel/apps/libraries/01-23/spack/opt/spack/linux-rhel8-icelake/intel-2021.6.0/intel-oneapi-mkl-2023.0.0-gnkrgwyxskxitvptyoubqaxlhh2v2re2/mkl/2023.0.0</code> </p> <p>If we type <code>echo $MKLROOT</code>, we can confirm that this environment variable is properly set from when we ran the command <code>module load intel-oneapi-mkl/2023.0.0-intel</code>. In the VASP makefile, we have <code>MKLROOT    ?= /path/to/your/mkl/installation</code>. The ?= means that this variable will not be set if <code>MKLROOT</code> has already been set. So, we can ignore this line if we\u2019d like. However, to be safe, we should simply copy the path of the MKL root directory to this line in makefile.include, so that this line now reads: </p> <p><code>MKLROOT    ?= /sfs/nopt/nrel/apps/libraries/01-23/spack/opt/spack/linux-rhel8-icelake/intel-2021.6.0/intel-oneapi-mkl-2023.0.0-gnkrgwyxskxitvptyoubqaxlhh2v2re2/mkl/2023.0.0</code> </p> <p>Tip</p> <p>The name of the environment variable for mkl\u2019s root directory set by its module (<code>MKLROOT</code>, set when we <code>module load intel-oneapi-mkl/2023.0.0-intel</code>) is not necessarily going to match the corresponding root directory variable in a given makefile. It did in this instance, but that\u2019s not guaranteed. The VASP makefile.include could have just as easily used <code>MKL_ROOT</code>, instead of <code>MKLROOT</code>. This is one reason why it\u2019s safer to use <code>module show</code> to find the path of the root directory, then copy this path into the makefile, rather than rely on environment variables.  </p> <p>To do step 2, we should first look at the contents of <code>$MKLROOT</code>. To show the contents of the MKL directory, type</p> <p><code>ls /sfs/nopt/nrel/apps/libraries/01-23/spack/opt/spack/linux-rhel8-icelake/intel-2021.6.0/intel-oneapi-mkl-2023.0.0-gnkrgwyxskxitvptyoubqaxlhh2v2re2/mkl/2023.0.0</code></p> <p>We should obtain the following output:</p> <p><code>benchmarks  bin  env  examples  include  interfaces  lib  licensing  modulefiles  tools</code></p> <p>If we look closely at the makefile, we see beneath the <code>MKLROOT</code> line the following: <pre><code>MKLROOT    ?= /sfs/nopt/nrel/apps/libraries/01-23/spack/opt/spack/linux-rhel8-icelake/intel-2021.6.0/intel-oneapi-mkl-2023.0.0-gnkrgwyxskxitvptyoubqaxlhh2v2re2/mkl/2023.0.0\nLLIBS      += -L$(MKLROOT)/lib/intel64 -lmkl_scalapack_lp64 -lmkl_blacs_intelmpi_lp64\n</code></pre></p> <p>the <code>LLIBS</code> line is telling make which libraries in particular to pick out. </p> <p>So, we want to go into the lib directory, and then the intel64 directory (since LLIBS is pointing to <code>$MKLROOT/lib/intel64</code>). Let's see what's inside with the <code>ls</code> command:</p> <p><code>ls  /sfs/nopt/nrel/apps/libraries/01-23/spack/opt/spack/linux-rhel8-icelake/intel-2021.6.0/intel-oneapi-mkl-2023.0.0-gnkrgwyxskxitvptyoubqaxlhh2v2re2/mkl/2023.0.0/lib/intel64</code></p> <p>There's a lot of stuff in this directory! VASP helps us by telling us we need the <code>mkl_scalapack_lp64</code> and <code>mkl_blacs_openmpi_lp64</code> builds specifically. You won't always be told exactly which libraries, and figuring this out, if the information is not provided to you in the package documentation, can require some tinkering.</p> <p>In general, the <code>.a</code> extension is for static linking, and the <code>.so</code> extension is for dynamic linking. For MKL in particular, the part <code>ilp64</code> vs <code>lp64</code> refer to two different interfaces to the MKL library. </p> <p>Tip</p> <p>Notice that, inside <code>$MKLROOT/lib/intel64</code>, the  filenames all start with <code>libmkl</code>, but in our makefile, we reference <code>lmkl_scalapack_lp64</code>. That's not a file in <code>$MKLROOT/lib/intel64</code>, but <code>libmkl_scalapack_lp64.so</code> is. The notation is that \"big L\" references the directories that the libraries are in, and the \"little l\" references the particular libraries. For example: <pre> LLIBS += -L$(MKLROOT)/lib/intel64 </pre> <pre> -lmkl_scalapack_lp64</pre> This is just a convention, but is important to get right because your compile will fail otherwise.</p> <p>Now that we have the correct <code>MKLROOT</code> set in the makefile.include, and we have an idea about how it's referencing the libraries within, we can move on to linking the HDF5 library.</p>"},{"location":"Documentation/Development/Libraries/howto/#modifying-the-makefile-for-hdf5","title":"Modifying the Makefile for HDF5","text":"<p>Because HDF5 is an optional library, we could compile the code now if we wanted to. However, for the sake of practice, let\u2019s uncomment the block in the makefile.include file related to HDF5 and repeat the exercise of linking a library: </p> <pre><code># HDF5-support (optional but strongly recommended) \nCPP_OPTIONS+= -DVASP_HDF5 \nHDF5_ROOT  ?= /path/to/your/hdf5/installation \nLLIBS      += -L$(HDF5_ROOT)/lib -lhdf5_fortran \nINCS       += -I$(HDF5_ROOT)/include \n</code></pre> <p>Our job, again, is to give the makefile the correct directions to our library. In this case, it\u2019s HDF5. Let\u2019s see which HDF5 modules are available: </p> <p><code>module avail hdf5</code> </p> <p>Returns </p> <p><code>hdf5/1.12.2-intel-oneapi-mpi-intel hdf5/1.12.2-openmpi-gcc</code> </p> <p>So, we see that HDF5 has been built with the intel-oneapi-mpi toolchain, and also with the GCC/openmpi toolchain. Since we\u2019re building vasp using the intel-oneapi toolchain, we need to load the corresponding module: </p> <p><code>module load hdf5/1.12.2-intel-oneapi-mpi-intel</code> </p> <p>Again, we must locate the root directory: </p> <p><code>module show hdf5/1.12.2-intel-oneapi-mpi-intel</code> </p> <p>We see the line for setting the HDF5 root directory environment variable: </p> <p><code>setenv      HDF5_ROOT_DIR /sfs/nopt/nrel/apps/libraries/01-23/spack/opt/spack/linux-rhel8-icelake/intel-2021.6.0/hdf5-1.12.2-dzgeixsm2cd3mupx4ti77ozeh7rh6zdo</code> </p> <p>Like before, we copy this path into our makefile.include: </p> <pre><code># HDF5-support (optional but strongly recommended) \nCPP_OPTIONS+= -DVASP_HDF5 \nHDF5_ROOT  ?= /sfs/nopt/nrel/apps/libraries/01-23/spack/opt/spack/linux-rhel8-icelake/intel-2021.6.0/hdf5-1.12.2-dzgeixsm2cd3mupx4ti77ozeh7rh6zdo \nLLIBS      += -L$(HDF5_ROOT)/lib -lhdf5_fortran \nINCS       += -I$(HDF5_ROOT)/include \n</code></pre> <p>We\u2019re ready to compile! In the case of VASP, the compile command is <code>make DEPS=1 std</code> but in general, the command may be <code>make all</code> or similar (consult the documentation of the code you\u2019re trying to build). </p> <p>If you\u2019re working with a code that has a testsuite, now is a good time to run the testsuite to make sure that your compile was successful. </p>"},{"location":"Documentation/Development/Libraries/howto/#summary-of-steps","title":"Summary of Steps","text":"<ol> <li>Download the source code of the package you\u2019re trying to build. This will generally be found on the website of the package. </li> <li>Consult the documentation of the package to find out what scientific libraries are needed, and if the package developers provide guidance on what toolchains/libraries are best </li> <li>Determine the availability of the needed scientific libraries.  <ol> <li>Can a \u201clibrary-of-libraries\u201d like MKL or LibSci be used? </li> <li>Does NREL support the library as a module?  <ol> <li>If so, determine the toolchain it was built with (usually given in the name of the module). If the toolchain is not clear from the name of the module, try the <code>ldd</code> command (e.g., <code>ldd path/to/executable/executable</code>), which will show you the dynamically linked libraries of the executable.</li> </ol> </li> </ol> </li> <li>Prepare your environment <ol> <li><code>module load</code> the necessary modules to prepare your environment. (See  environment preparation step of VASP example) </li> </ol> </li> <li>Prepare your makefile <ol> <li>Make sure that the compilers and (optional) MPI used in the makefile match what is used to build your scientific libraries as best as possible </li> <li>Make sure that the paths to the scientific libraries in the makefile match the path given by the <code>module show</code> command </li> <li>Make sure the proper \u201clittle L\u201d libraries are referenced in the makefile </li> </ol> </li> <li>Compile!</li> </ol>"},{"location":"Documentation/Development/Libraries/howto/#questions","title":"Questions?","text":"<p>If you\u2019re still stuck and unable to successfully link the scientific libraries you need, get in contact with HPC help.</p>"},{"location":"Documentation/Development/Libraries/hsl/","title":"HSL for IPOPT","text":"<p>HSL (Harwell Subroutine Library) for IPOPT are a set of linear solvers that can greatly accelerate the speed of the optimization over the default MUMPS solver.</p>"},{"location":"Documentation/Development/Libraries/hsl/#installation","title":"Installation","text":"<p>Go to the HSL site and follow the instructions to request the source code for all the available solvers. Note that the solver MA27 is free to obtain, but MA27 is a serial solver. Other solvers will require a license. Please request a license that applies to your use case.</p> <p>Info</p> <p>If you are building IPOPT along with HSL, please follow the instructions here.</p> <p>We need to be careful regarding the selection of linear algebra libraries when installing HSL. The default version of IPOPT distributed with <code>Ipopt.jl</code> on Linux links to the OpenBLAS library. This causes issues when linking the HSL library to the Intel oneAPI MKL libraries.  For this reason, to use HSL linear solvers with IPOPT on Kestrel, either we must compile IPOPT from scratch or compile HSL with OpenBLAS and NetLib LAPACK instead of Intel oneAPI MKL. We demonstrated IPOPT + HSL installation with Intel oneAPI MKL here.</p> <p>The following provides detailed instructions for building HSL using OpenBLAS and Netlib LAPACK ON HPC.</p>"},{"location":"Documentation/Development/Libraries/hsl/#pre-requisites","title":"Pre-requisites","text":""},{"location":"Documentation/Development/Libraries/hsl/#metis","title":"Metis","text":"<p>Metis is a serial graph partitioning and fill-reducing matrix ordering software that helps the HSL solvers perform better. Therefore, it is recommended that you also install or build the Metis library.  If you do want to install Metis, it must be done before compiling the HSL library.</p> <p>The easiest way to install Metis is to use anaconda:</p> <p>Warning</p> <p>Using HSL linear solvers requires installing Metis. Metis is optional for MUMPS.</p> <p>We will install Metis using Anaconda, however, it can also be installed from source. To install using Anaconda, we will create a clean environment with nothing but Metis. The conda environment is being constructed within a directory in <code>hpcapps</code> project on  Kestrel. </p> <pre><code>module load conda\nconda create -p /projects/hpcapps/kpanda/conda-envs/metis python\nconda activate /projects/hpcapps/kpanda/conda-envs/metis\nconda install conda-forge::metis\n</code></pre> <p>Info</p> <p><code>module load conda</code> loads the default anaconda module. You may use a different conda module based on your needs.</p> <p>Note</p> <p>Anaconda packages sometimes have issues when they come from different channels.  We tend to pull everything from <code>conda-forge</code> hence the channel choice above.</p> <p>The Metis library and header files are placed in <code>/projects/hpcapps/kpanda/conda-envs/metis/lib/</code> and <code>/projects/hpcapps/kpanda/conda-envs/metis/include/</code>, respectively.</p>"},{"location":"Documentation/Development/Libraries/hsl/#compilers","title":"Compilers","text":"<p>We will be using the GNU compiler suite (<code>gcc</code> and <code>gfortran</code>).  These can be accessed on the cluster by loading the appropriate module.  This should work with any version of the GNU compilers. We use the default <code>gcc</code> and <code>gfortran</code> that are available on the CPU compute nodes.</p>"},{"location":"Documentation/Development/Libraries/hsl/#setting-up-the-environment","title":"Setting up the Environment","text":"<p>We will install HSL in <code>/kfs2/projects/msoc/kpanda/apps/Ipopt/install</code> for this demonstration. This can be set to whatever location you wish to install. Let's create the requisite installation directories</p> <pre><code>mkdir -p /kfs2/projects/msoc/kpanda/apps/Ipopt/install\ncd /kfs2/projects/msoc/kpanda/apps/Ipopt/install\nmkdir lib include\ncd ..\n</code></pre> <p>We will make use of the following environment variables.</p> <pre><code># Location of metis.h\nexport METIS_HEADER=/projects/hpcapps/kpanda/conda-envs/metis/include\n# Location of metis library\nexport METIS_LIBRARY=/projects/hpcapps/kpanda/conda-envs/metis/lib\n\n# Directory for keeping source code and build products\nexport MYAPPS=/kfs2/projects/msoc/kpanda/apps/Ipopt/install\n# Location of header files\nexport MYINC=${MYAPPS}/include\n# Location of static and dynamic libraries\nexport MYLIB=${MYAPPS}/lib\n</code></pre> <p>These can be added to the <code>.bash_profile</code> file (or equivalent for other shells).  Remember after adding these to source <code>.bash_profile</code> (or equivalent) or to open a new terminal and do all building there. Alternatively, to make the Metis header and dynamic library easily accessible to the HSL, MUMPS and IPOPT libraries, we will put symbolic links in the <code>${MYINC}</code> and <code>${MYLIB}</code> directories.  Do this by doing the following:</p> <pre><code>cd ${MYINC}\nln -s ${METIS_HEADER}/metis.h metis.h\ncd ${MYLIB}\nln -s ${METIS_LIBRARY}/libmetis.so libmetis.so\n</code></pre> <p>This has two advantages. First, we don't need to add <code>/projects/hpcapps/kpanda/conda-envs/metis/lib/</code> to the <code>LD_LIBRARY_PATH</code>.  The second advantage is that anaconda puts all the  environments libraries and include files in the same directories with <code>libmetis.so</code> and <code>metis.h</code>.  Many of these libraries overlap with those used by HSL, Mumps and IPOPT but are not necessarily the same versions.  Loading a different version of a library than those compiled against can cause unexpected behavior.</p>"},{"location":"Documentation/Development/Libraries/hsl/#configure-and-install","title":"Configure and Install","text":"<p>We will clone <code>ThirdParty-HSL</code> and configure and install HSL in a working directory</p> <pre><code>git clone git@github.com:coin-or-tools/ThirdParty-HSL.git\n</code></pre> <p>Copy the HSL source code tarball into <code>/projects/msoc/kpanda/apps/ThirdParty/HSL/</code>,  unpack it, and rename or (create a symbolic link to the unpacked directory) as <code>coinhsl</code>.</p> <p>Run the following commands to configure</p> <pre><code>cd ThirdParty-HSL\nmodule load netlib-lapack\n./configure --prefix=${MYAPPS} \\\n--with-metis \\\n--with-metis-cflags=-I${METIS_HEADER} \\\n--with-metis-lflags=\"-L${METIS_LIBRARY} -lmetis\"\nmake &amp;&amp; make install\n</code></pre> <p>This should install the HSL libraries in <code>${MYAPPS}</code>. Finally, add <code>MYLIB</code> to your <code>LD_LIBRARY_PATH</code>. You can append the following line to your <code>.bash_profile</code> to make it permanent or call it every time you need to run IPOPT with HSL solvers.</p> <pre><code>export export LD_LIBRARY_PATH=${LD_LIBRARY_PATH}:${MYAPPS}/lib\n</code></pre>"},{"location":"Documentation/Development/Libraries/hsl/#usage","title":"Usage","text":"<p>IPOPT has a feature called the linear solver loader (read about it here). This allows for loading linear solvers from a dynamic library at run time.  We will use this feature to use the HSL solvers.</p> <p>The only thing you have to do is to make the HSL dynamic library findable.  This is done by adding the directory containing the HSL library to the environment variable <code>LD_LIBRARY_PATH</code>. To use the new linear solvers just use the <code>linear_solver=\"&lt;solver&gt;\"</code> argument to <code>IPOPT.Optimizer</code>.</p> <p>Info</p> <p>The IPOPT build that comes with <code>Ipopt.jl</code> seems to expect the HSL library to have the name <code>libhsl.so</code>. The repo ThirdParty-HSL builds the library <code>libcoinhsl.so</code>.  The simplest fix is to do the following:</p> <pre><code>cd ${MYLIB}\n# Create a symbolic link called libhsl.dylib\nln -s libcoinhsl.dylib libhsl.dylib\n</code></pre> <p>Alternatively, users can follow the instructions mentioned here for Julia JuMP</p>"},{"location":"Documentation/Development/Libraries/lapack/","title":"LAPACK and scaLAPACK","text":"<p>Documentation: LAPACK, scaLAPACK</p> <p>LAPACK is a highly optimized library of linear algebra routines written in Fortran 90. These routines include matrix multiplication, factorization (LU, Cholesky, QR, etc.) least squares solutions of linear systems, eigenvalue problems, and many others. LAPACK routines are available in both single and double precision, and for complex and real numbers.</p> <p>LAPACK depends on BLAS (Basic Linear Algebra Subprograms).</p> <p>ScaLAPACK is a parallel-distributed version of LAPACK (i.e., scalaPACK is MPI-parallel)</p> <p>Both LAPACK and ScaLAPACK are available as either standalone libraries (<code>netlib-lapack</code>), or as part of the \"package-of-packages\" libraries MKL and LibSci.</p>"},{"location":"Documentation/Development/Libraries/libsci/","title":"Cray LibSci","text":"<p>Documentation: LibSci</p> <p>LibSci is a collection of numerical libraries developed by Cray for scientific and engineering computing. LibSci is optimized for performance on Cray architectures, including multi-core processors, and supports both single-precision and double-precision arithmetic. It also includes multithreading support for parallel execution on shared-memory systems. Like MKL, LibSci includes the following math functions: </p> <ul> <li>BLAS (Basic Linear Algebra Subroutines) </li> <li>CBLAS (C interface to the legacy BLAS) Note: not sure if this is also in MKL? </li> <li>BLACS (Basic Linear Algebra Communication Subprograms) </li> <li>LAPACK (Linear Algebra routines) </li> <li>ScaLAPACK (parallel Linear Algebra routines) </li> </ul> <p>And additionally, libraries that are unique to Cray systems including: </p> <ul> <li>IRT (Iterative Refinement Toolkit) - a library of solvers and tools that provides solutions to linear systems using single-precision factorizations while preserving accuracy through mixed-precision iterative refinement. </li> <li>CrayBLAS - a library of BLAS routines autotuned for Cray XC series systems through extensive optimization and runtime adaptation.  </li> </ul>"},{"location":"Documentation/Development/Libraries/mkl/","title":"Intel Math Kernel Library (MKL)","text":"<p>Documentation: MKL</p>"},{"location":"Documentation/Development/Libraries/mkl/#overview","title":"Overview","text":"<p>MKL includes a wealth of routines to accelerate technical application performance on modern multicore architectures.  The library is designed to take full advantage of the latest Intel processors, including multi-core processors, and can significantly improve the performance of numerical applications. Core math functions include: </p> <ul> <li>BLAS (Basic Linear Algebra Subroutines) </li> <li>LAPACK (Linear Algebra routines) </li> <li>ScaLAPACK (parallel Linear Algebra routines) </li> <li>Sparse solvers </li> <li>Fast Fourier Transforms </li> <li>Vector math </li> <li>Data fitting</li> </ul> <p>Note</p> <p>If you are mixing an Anaconda environment with modules to build, always activate the conda environment before loading any library modules like MKL. cmake discovery, for example, is very sensitive to the order in which these actions are taken. </p>"},{"location":"Documentation/Development/Libraries/mkl/#linking","title":"Linking","text":"<p>With the Intel toolchain, linking against MKL is as simple as adding <code>-mkl</code> to the link command. This by default links in the threaded MKL routines. To limit to strictly sequential (i.e., not threaded) routines, use <code>-mkl=sequential</code>; to enable multi-process Scalapack routines, use <code>-mkl=cluster</code>.  To link MKL with GCC, the <code>mkl</code> module includes some convenience environment variables defined as the appropriate <code>LDFLAGS</code> setting. See the <code>module show mkl</code> output; the variable naming is intended to be self-explanatory. </p> <p>If you have needs not covered by these, use Intel's interactive MKL Link Line Advisor website to discover the appropriate linking options. Don't use mkl_link_tool in your build automation, as Intel only provides a 32-bit version of this tool which will cause builds to fail. </p>"},{"location":"Documentation/Development/Libraries/mkl/#user-tips","title":"User Tips","text":"<p>MKL will provide optimized library code based on the most advanced instruction set able to run on discovered hardware. So for floating point math, although GNU and Intel compilers will generate application code with SSE 4.2 instructions by default, MKL libraries will use AVX-512 float point instructions available on Skylake processors.  </p> <p>As the code executes, rapid transition between different such floating point instruction sets may cause a significant performance penalty. Consider compiling the base code optimized for AVX instructions, i.e., adding <code>-xcore-AVX512</code> for Intel and <code>-march=skylake-avx512</code> for GNU.</p> <p>Using <code>-mkl</code> by default generates the code to use multithreaded MKL routines. There is an extra initialization overhead associated with using multithreaded MKL. With the smaller problem size or with sparse vectors it may be more beneficial from the performance standpoint to use sequential MKL routines ( <code>-mkl=sequential</code>). </p>"},{"location":"Documentation/Development/Libraries/petsc/","title":"PETSc","text":"<p>Documentation: PETSc</p> <p>PETSc is a suite of data structures and routines for the scalable (parallel) solution of scientific applications modeled by partial differential equations.</p> <p>On Kestrel, PETSc is provided under multiple toolchains</p> <pre><code>---------------------------------------------------------------------------------------------------------------------------------------------------------------------\n  petsc:\n---------------------------------------------------------------------------------------------------------------------------------------------------------------------\n     Versions:\n        petsc/3.14.6-cray-mpich-intel\n        petsc/3.19.3-intel-oneapi-mpi-intel\n        petsc/3.19.3-openmpi-gcc\n</code></pre> <p><code>petsc/3.14.6-cray-mpich-intel</code> is a PETSc installation that uses HPE provided <code>PrgEnv-intel</code>.  Therefore, the MPI used here is cray-mpich and the compiler is intel/2023.</p> <p><code>petsc/3.19.3-intel-oneapi-mpi-intel</code> is a PETSc installation that uses intel-oneapi-compilers and intel-oneapi-mpi for the compilers and MPI, respectively.</p> <p><code>petsc/3.19.3-openmpi-gcc</code> is a PETSc installation that uses gcc/10.1.0 and openmpi/4.1.5-gcc for the compilers and MPI, respectively.</p>"},{"location":"Documentation/Development/Performance_Tools/Intel/","title":"oneAPI","text":"<p>oneAPI tools are a set of software that enable developing and optimizing software for the latest processor architectures.</p> Intel VTune Amplifier XE <p>Intel VTune Amplifier XE is a performance profiler for C, C++, C#, Fortran, Assembly and Java code. Hot spots analysis provides a sorted list of functions that use a lot of CPU time. Other features enable the user to quickly find common causes of slow performance in parallel programs, including waiting too long at locks and load imbalance among threads and processes. VTune Amplifier XE uses the Performance Monitoring Unit (PMU) on Intel processors to collect data with very low overhead.</p> <p>The recommended way to use this tool is to run the profiler from the command line and view the data using the GUI or generate a text report from the command line. </p> <p>You can list all the available profiling options for the machine you're profiling on, from the GUI or from the command line using <code>amplxe-cl -collect-list</code>.</p> <p>Include the following in you batch script to get a HPC-characterization profile of you application:</p> <pre><code>#!/bin/bash --login\n#SBATCH -J &lt;job name&gt;\n#SBATCH -N &lt;nodes&gt;\n#SBATCH -t 00:30:00\n#SBATCH -A &lt;Allocation handle&gt;\n\n# set your tmpdir, and don't forget to clean it after your job\n# completes. \nexport TMPDIR=/scratch/$USER/tmp\n# load application specific modules\nmodule load intel-oneapi-vtune\n# profile the executable\namplxe-cl --collect hpc-performance ./executable.exe\n</code></pre> <p>GUI:</p> <p><code>amplxe-gui</code></p> Intel Trace Analyzer XE <p>Intel Trace Analyzer and Collector is a tool for understanding the behavior of MPI applications. Use this tool to visualize and understand MPI parallel application behavior, evaluate load balancing, learn more about communication patterns, and identify communication hot spots.</p> <p>The recommended way to use this tool is to collect data from the command line and view the data using the GUI.</p> <p>Example batch script to collect MPI communication data:</p> <pre><code>#!/bin/bash --login\n#SBATCH -J &lt;job name&gt;\n#SBATCH -q &lt;queue&gt;\n#SBATCH -N &lt;nodes&gt;\n#SBATCH -t 00:30:00\n#SBATCH -A &lt;Allocation handle&gt;\n\n# set your tmpdir, and don't forget to clean it after your job\n# completes.\nexport TMPDIR=/scratch/$USER/tmp\n\n# load application specific modules\nmodule load intel-oneapi-trace\n\n# to profile the executable, just append '-trace' to mpirun\nmpirun -trace -n 4 ./executable.exe\n# this generates a .stf file that can viewed using the GUI\n</code></pre> <p>GUI:</p> <p><code>traceanalyzer</code></p> Intel Advisor XE <p>Intel Advisor helps with vectorization and threading in your C++ and Fortran Applications. This tool helps identify areas that would benefit the most from vectorization. It also helps with identifying what is blocking vectorization and gives insights to overcome it:</p> <pre><code># load application specific modules\nmodule load intel-oneapi-advisor\n\n# set your tmpdir, and don't forget to clean it after your job\n# completes.\nexport TMPDIR=/scratch/$USER/tmp\n</code></pre> <p>You can list all the available profiling options for the machine you're profiling on, from the GUI or from the command line using:</p> <p><code>advixe-cl -collect-list</code></p> <p>This tool has a lot of features that can be accessed from the GUI:</p> <p><code>advixe-gui</code></p> Intel Inspector XE <p>Intel Inspector XE is an easy to use memory checker and thread checker for serial and parallel applications written in C, C++, C#, F#, and Fortran. It takes you to the source locations of threading and memory errors and provides a call stack to help you determine how you got there. This tool has a GUI and a command line interface.</p> <pre><code># load application specific modules\nmodule load intel-oneapi-inspector\n\n# set your tmpdir, and don't forget to clean it after your job\n# completes.\nexport TMPDIR=/scratch/$USER/tmp\n</code></pre> <p>You can list all the available profiling options for the machine you're running this tool on, from the GUI or from the command line using:</p> <p><code>inspxe-cl -collect-list</code></p> <p>This tool has a lot of features that can be accessed from the GUI:</p> <p><code>inspxe-gui</code></p> Intel Application Performance Snapshot <p>The new Application Performance Snapshot merges the earlier MPI Performance Snapshot and Application Performance Snapshot Tech Preview. MPI Performance Snapshot is no longer available separately, but all of its capabilities and more are available in the new combined snapshot. This tool lets you take a quick look at your application's performance to see if it is well optimized for modern hardware. It also includes recommendations for further analysis if you need more in-depth information.</p> <p>Using This Tool:</p> <pre><code># load application specific modules\nmodule load intel-oneapi-vtune\n\n# serial/SMP executable\n$ aps &lt;executable&gt; # this generates an aps result directory\n# DMP executable\n$ mpirun -n 4 aps &lt;executable&gt;\n# this generates an aps result directory # to gerate text and /hmtl result files:\n$ aps --report=&lt;the generated results directory from the previous step&gt; \n# the result file can be viewed in a browser or text editor\n</code></pre> <p>Before you begin, please make sure that your application is compiled with the debug flag (-g), to enable profiling and debugging.</p> <p>When using the suite of tools from oneAPI on Kestrel, we recommend that you set your <code>TMPDIR</code> to point to a location in your <code>SCRATCH</code> directory: <code>export TMPDIR=/scratch/$USER/tmp</code></p> <p>Important:</p> <p>Please make sure that you clean up this directory after your job completes.</p>"},{"location":"Documentation/Development/Performance_Tools/Linaro-Forge/","title":"Linaro MAP","text":"<p>Documentation: Linaro Forge Documentation Page</p> <p>Linaro MAP (Memory Access Profiler) is a tool that provides insight into how memory is being accessed by an application. It can help developers understand the memory access patterns of an application and identify performance issues caused by memory bottlenecks. Linaro MAP can profile code running on multiple cores as well as code running on a system with hardware accelerators, such as GPUs, allowing developers to identify memory access patterns specific to these accelerators. The profiling data generated by Linaro MAP can be visualized in a variety of ways, including call graphs, heat maps, and histograms, making it easy to identify patterns and potential bottlenecks. The tool can also generate reports that provide a summary of memory usage and access patterns, as well as recommendations for optimizing memory usage. Here we will go through some of the information you can obtain with Linaro MAP using VASP as an example and in the next section MAP we show how to start up such a MAP profile. If you need help with profiling your programs, reach out to HPC help and we can work with you.</p> <p>Here is some profiling information obtained for VASP.</p> <p>Across the top we see our metrics data for the default metrics: main thread activity, percent time each rank spends on floating-point instructions, and memory usage. The horizontal axis is wall clock time. The colors represent the following:  </p> <ul> <li>\ud83d\udfe2 Green: Single-threaded computation time. </li> <li>\ud83d\udd35 Blue: MPI communication and waiting time. </li> <li>\ud83d\udfe0 Orange: I/O time</li> <li>\ud83d\udfe3 Dark purple: Accelerator time. </li> </ul> <p></p> <p>Across the bottom we have different view tabs. The I/O view displays your program I/O. The Project Files view allows you to navigate through your code base. The Functions view shows a flat profile of the functions in your program. The Stacks view allows you to follow down from the main function to see which code paths took the most time. Each line of the Stacks view shows the performance of one line of your source code, including all the functions called by that line.</p> <p>You can select different metrics to view from the metrics menu: </p> <p></p> <p>As well as zoom in on specific times in your program run.</p> <p></p> <p>By clicking on the functions in the \u201cMain Thread Stacks,\u201d the profiler will take you to those calls in your code. Here we see that the call to the Davidson algorithm takes 68.6% of the program time.</p> <p></p> <p>Digging in further, we can find that most of the time is spent in the CALLMPI function, and the activity shows as blue indicating this MPI communication and wait time.</p> <p></p> <p>See the next section MAP for how to obtain these.</p>"},{"location":"Documentation/Development/Performance_Tools/Linaro-Forge/map/","title":"How to run MAP","text":""},{"location":"Documentation/Development/Performance_Tools/Linaro-Forge/map/#program-setup","title":"Program Setup","text":"<p>Linaro-MAP can show you how much time was spent on each line of code. To see the source code in MAP, you must use a version of your code that is compiled with the debug flag. For most compilers, this is <code>-g</code>. Note: You should not just use a debug build but should keep optimization flags <code>-O0</code> turned on when profiling. </p> <p>For more information, see the Linaro Forge Documentation on getting started with MAP. In particular, if your program uses statically linked libraries, the MAP profiler libraries will not be automatically linked and you will need to do so yourself. </p> <p>Note</p> <p>Ensure that your program is working before trying to run it in MAP</p>"},{"location":"Documentation/Development/Performance_Tools/Linaro-Forge/map/#map-setup","title":"MAP Setup","text":"<p>There are two options for how to run MAP. The first method is to use the remote client (recommended to reduce latencies from X forwarding the display.). The second method is to use FastX. Both are described here.</p>"},{"location":"Documentation/Development/Performance_Tools/Linaro-Forge/map/#option-1-remote-client-setup","title":"Option 1: Remote Client Setup","text":"<p>Download the remote client from the Linaroforge Website Select the client for your platform (Mac/Windows/Linux) and ensure the client version number matches the version number of the Linaro suite you are using. You can see all the versions of linaro-forge available using:</p> <p><code>$ module avail forge</code></p> <p>Once you have the client installed, you will need to configure it to connect to the host:</p> <ol> <li>Open the Linaro Forge Client application</li> <li>Select the configure option in the \"Remote Launch\" dropdown menu, click \"Add\" and set the hostname to \"USER@HOST.hpc.nrel.gov\" where USER is your username and HOST is the host you are trying to connect to. We recommend using DAV nodes if available on your system.</li> <li> <p>In the Remote Installation Directory field, set the path to the Linaro installation on your host. This can be found by running the command: </p> <pre><code>dirname $(dirname $(which map))\n</code></pre> <p>For example:</p> <pre><code>module load forge/24.0.4\ndirname $(dirname $(which map))\n/nopt/nrel/apps/cpu_stack/software/forge/24.0.4\n</code></pre> </li> <li> <p>Hit \"Test Remote Launch\" to test the configuration. </p> </li> </ol> <p>Once the remote client is correctly set up, start a terminal and connect to the desired HPC system. <code>$ ssh USER@$HOST.hpc.nrel.gov</code> </p> <p>Continue to the profiling section</p>"},{"location":"Documentation/Development/Performance_Tools/Linaro-Forge/map/#option-2-fastx-setup","title":"Option 2: FastX Setup","text":"<p>To run MAP with FastX, follow instructions to download and install the desktop client and connect to a host on the FastX page.</p> <p>Once you have FastX installed and an appropriate build of your program to profile, start an xterm window from within FastX connected to an HPC host (We recommend using DAV nodes if available on your system). Then continue to the profiling section</p>"},{"location":"Documentation/Development/Performance_Tools/Linaro-Forge/map/#profiling-a-program","title":"Profiling a program","text":"<p>Once you have an appropriate build of your program to profile and either the Linaro Forge Client or FastX installed, you can obtain profiling data through map with the following steps. We will profile VASP as an example.</p> <ol> <li>Start an xterm window from within FastX connected to a DAV node</li> <li>Start an interactive job session.     Use the debug or other partitions as appropriate. <code>$ salloc --nodes=&lt;N&gt;  --time=&lt;time&gt; --account=&lt;handle&gt;</code></li> <li>Load the linaro-forge module (formerly arm)     Additionally load any other modules needed to run your program <code>$ module load linaro-forge</code> <code>$ module load mkl intel-mpi #for VASP</code> </li> <li> <p>Start a map session using the command <code>map --connect</code> if you are using the desktop client or simply <code>map</code> if you are using FastX.      Optionally, navigate to your working directory and give map the path to your exe <code>$ cd PATH/TO/YOUR/WORKING/DIRECTORY</code> <code>$ map --connect PATH/TO/YOUR/PROGRAM/exe</code> (remove --connect if using FastX)     If using the remote client, it will send a Reverse Connection request. Click 'Accept'. </p> <p>You should now see the linaro forge GUI appear and a submission box with some information filled out if you followed the optional directions. Otherwise use the GUI to input them now. Make sure the path to the application includes your program exe. Make sure your working directory includes your input files, or specify your stdin file and its path. Adjust other parameters as needed for profiling.</p> <p></p> </li> <li> <p>Start your profile by clicking \u201cRun\u201d</p> </li> </ol> <p>You should now see the profiling data we described in the previous section MAP. Please refer to that page as well as the Linaro Forge Documentation for more details on what you can learn from such profiles.</p> <p></p>"},{"location":"Documentation/Development/Performance_Tools/Linaro-Forge/map/#debugging-a-program","title":"Debugging a program","text":"<p>The Forge debugger is ddt.  It uses the same local client at map and perf-report.  To get started, set up your local client version of Forge as described above in the section MAP Setup - Option 1: Remote Client Setup.</p> <p>There are many ways to launch a debug session.  Probably the simplest is to launch from an interactive session on a compute node.  </p> <p>Get an interactive session replacing MYACCOUNT with your account:</p> <pre><code>salloc --exclusive --mem=0 --tasks-per-node=104 --nodes=1 --time=01:00:00 --account=MYACCOUNT --partition=debug\n</code></pre> <p>As with map your application needs to be compiled with the -g option.  Here is a simple build with make.  (Here we also have a OpenMP program so we add the flag -fopenmp.)</p> <pre><code>make\ncc  -g -fopenmp -c triad.c\ncc  -g -fopenmp ex1.c triad.o -o exc\n</code></pre> <p>Our executable is exc.</p> <p>We are going to need our remote directory so we run pwd.</p> <pre><code>pwd\n/kfs3/scratch/user/debug\n</code></pre> <p>We load the module:</p> <pre><code>module load forge/24.0.4\n</code></pre> <p>Then run the command:</p> <pre><code>ddt --connect\n</code></pre> <p>Ddt is running on the compute node, waiting for you to connect with the local client.  Launch your local client.  Then under Remote Launch: select the machine to which you want to connect.  After a few seconds you will see a window announcing that the ddt wants to connect you to your client.  Hit Accept. </p> <p></p> <p>After acceptance completes click Run and debug a program.</p> <p>Here is where you need the directory for your program.  Put the full path to your application in the Application box and the directory in Working Directory.  We assume the Working Directory, the directory which would normally contain your data is the same as your program directory.</p> <p>This is an MPI program so select MPI.  After that you will see more options.  For most programs the Implementation should be SLURM (generic). If this is not what is shown or you know you need something else, select Change... to set it.  For OpenMP programs select that box also.  </p> <p></p> <p>Finally hit Run.  After a few seconds you will see the debug window with the \"main\" source in the center window.  You can set Break Points by clicking in the leftmost column of the source window.  To start your program click the right facing triangle in the top left corner of the window.  </p> <p></p> <p>See the full documentation for complete instructions.  There is a copy of userguide-forge.pdf in the doc directory of the Forge directory.  </p> <pre><code>module load forge\n\n$echo `dirname $(dirname $(which ddt))`/doc\n/nopt/nrel/apps/cpu_stack/software/forge/24.0.4/doc\n\nls /nopt/nrel/apps/cpu_stack/software/forge/24.0.4/doc\nRELEASE-NOTES  stacks.dtd  userguide-forge.pdf\n</code></pre>"},{"location":"Documentation/Development/Performance_Tools/Linaro-Forge/performance_rep/","title":"Linaro-Performance Reports","text":"<p>Documentation: Linaro Performance Reports</p> <p>Linaro Performance Reports is a low-overhead tool that produces one-page text and HTML reports summarizing and characterizing both scalar and MPI application performance. (Only ~5% application slowdown even with thousands of MPI processes.) These high-level reports can help answer:  </p> <ul> <li>Is this application optimized for the system it is running on? </li> <li>Does it benefit from running at this scale? </li> <li>Are there I/O or networking bottlenecks affecting performance? </li> <li>Which configuration changes can be made to improve performance further? </li> </ul>"},{"location":"Documentation/Development/Performance_Tools/Linaro-Forge/performance_rep/#walk-through","title":"Walk through","text":"<p>Here we show the information you can obtain with Linaro Performance reports using VASP as an example. In the next section, we will detail how to obtain these reports. If you need help with profiling your programs, reach out to HPC help and we can work with you.</p> <p>Here is the header of performance report obtained for a VASP run on 1 node with 36 processes:</p> <p></p> <p>This shows time spent running application code, sending MPI calls, and time on I/O. In this case, we see that we are MPI-bound, which makes sense given that we are running a small, simple test case on more MPI tasks than necessary, which creates unnecssary MPI communication overhead.</p> <p>The rest of the report shows a further breakdown of each of these categories:</p> <p></p>"},{"location":"Documentation/Development/Performance_Tools/Linaro-Forge/performance_rep/#running-a-performance-report","title":"Running a performance report","text":"<p>All you need to do is load the module and prefix your execution command with <code>perf-report</code>:</p> <ol> <li>Start an interactive job session. Use the debug or other partitions as appropriate:  <code>$ salloc --nodes=&lt;N&gt;  --time=&lt;time&gt; --account=&lt;handle&gt;</code></li> <li>Load the linaro-forge module (formerly arm), and additionally load any other modules needed to run your program: <code>$ module load linaro-forge</code> <code>$ module load mkl intel-mpi #for VASP</code></li> <li>Set MPI parameters and run your exe using <code>perf-report</code>: <code>$ perf-report srun -n 36 PATH/TO/YOUR/PROGRAM/exe</code></li> </ol> <p>This will generate an .html file and a .txt file that you can view in a browser or text editor. You should now see the overview we described in the previous section.</p>"},{"location":"Documentation/Development/Programming_Models/","title":"Programming Models","text":""},{"location":"Documentation/Development/Programming_Models/gpu_hpc/","title":"Using GPUs for HPC","text":"<p>This page documents how NREL HPC users can utilize GPUs, from submitting the right kind of job to Slurm to examples of creating custom CUDA kernels from Python.</p>"},{"location":"Documentation/Development/Programming_Models/gpu_hpc/#submitting-gpu-jobs-to-slurm","title":"Submitting GPU jobs to Slurm","text":""},{"location":"Documentation/Development/Programming_Models/gpu_hpc/#example-scripts","title":"Example scripts","text":"<p>The following examples are generic templates that NREL HPC users can adapt for their own GPU job scripts for a given system. Be sure to replace <code>&lt;allocation&gt;</code> with the name of your HPC allocation. Note that Kestrel and Swift's GPU partitions have sharable nodes, allowing for multiple jobs to run on one node simultaneously. Since there are four GPU cards on each node on these systems, each node can theoretically accommodate four GPU-driven jobs at once. As such, example scripts for those systems are tailored for requesting one-quarter of a node by default. Although Vermilion's GPUs are technically \"shared\" in the sense that multiple (CPU) jobs can run on one node, there is only one GPU per node. As such the Vermilion example requests the entire node. Please refer to the system-specific pages for more information on the GPUs available on each cluster and how AUs are charged accordingly.</p> <p>Note</p> <p>When launching a GPU job on Kestrel, be sure to do so from one of its dedicated GPU login nodes.</p> <p>Note</p> <p>Be aware that <code>--mem</code> in Slurm ALWAYS refers to CPU, not GPU, memory. You are automatically given all of the per-device GPU memory in a Slurm job.</p> Kestrel <pre><code>#!/bin/bash \n#SBATCH --account=&lt;allocation&gt;\n#SBATCH --time=01:00:00\n#SBATCH --mem=80G\n#SBATCH --gpus=1\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=32\n#SBATCH --output=%j-%x.log\n\n# don't forget to submit this from a GPU login node!\n# note that you do not have to specify a partition on Kestrel;\n# your job will be sent to the appropriate gpu-h100 queue based\n# on your requested --time\n&lt;GPU-enabled code to run&gt;\n</code></pre> Swift <pre><code>#!/bin/bash\n#SBATCH --account=&lt;allocation&gt;\n#SBATCH --partition=gpu\n#SBATCH --time=01:00:00\n#SBATCH --mem=250G\n#SBATCH --gpus=1\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=24\n#SBATCH --output=%j-%x.log\n\n&lt;GPU-enabled code to run&gt;\n</code></pre> Vermilion <pre><code>#!/bin/bash\n#SBATCH --account=&lt;allocation&gt;\n#SBATCH --partition=gpu\n#SBATCH --time=01:00:00\n#SBATCH --mem=0\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=30\n#SBATCH --output=%j-%x.log\n#SBATCH --exclusive\n\n# Note that you do not have to explicitly request a GPU on Vermilion \n# with `#SBATCH --gpus=1` or `#SBATCH --gres=gpu:1`.\n&lt;GPU-enabled code to run&gt;\n</code></pre>"},{"location":"Documentation/Development/Programming_Models/gpu_hpc/#checking-the-gpu-devices","title":"Checking the GPU device(s)","text":"<p>On Kestrel, Swift, and Vermilion, you can use the <code>nvidia-smi</code> utility to query various aspects of the GPU device(s) as your job runs. It is a helpful tool to quickly determine whether your code is exercising the GPU itself. Note that although <code>nvidia-smi</code> is a single-node application, it will return information on every GPU device available on the node from which it is launched (i.e., up to 4 devices on Kestrel or Swift).</p> <p>To illustrate how to use this utility, suppose you (<code>HPC-USER</code>) want to interactively run a Python script named <code>gpu_code.py</code> you wrote out of your /scratch directory on Kestrel that does some calculations on a single GPU device from a custom conda/mamba environment (<code>my-cuda-env</code>). As the example code chunks below demonstrate, you can submit an interactive debug job with a GPU device attached, launch the script, and then query the GPU hardware utilization in a separate terminal session connected to that compute node. </p> <p>First, request an interactive job, activate your environment, and launch the Python script:</p> <pre><code>[HPC-USER@kl5 ~] GPU $ salloc -A &lt;allocation&gt; -p debug -t 01:00:00 --gpus=1 --mem=80G -n 1 -N 1\n[HPC-USER@x3100c0s21b0n0 ~] GPU $ ml mamba &amp;&amp; conda activate my-cuda-env\n(my-cuda-env) [HPC-USER@x3100c0s21b0n0 HPC-USER] GPU $ cd /scratch/HPC-USER\n(my-cuda-env) [HPC-USER@x3100c0s21b0n0 HPC-USER] GPU $ python gpu_code.py\n</code></pre> <p>From there, launch a new terminal session, connect to Kestrel, and <code>ssh</code> into the node on which your interactive job is running. Launching the <code>nvidia-smi</code> command reveals useful information such as the GPU device core and memory utilization at that point in time. In this example, the process <code>python</code> (which launched <code>gpu_code.py</code>) is at 100% GPU core utilization (under <code>Volatile GPU-Util</code>) and is using ~23GB of GPU memory (reported in the bottom right corner):</p> <pre><code>[HPC-USER@kl5 ~] GPU $ ssh x3100c0s21b0n0\n[HPC-USER@x3100c0s21b0n0 ~] GPU $ nvidia-smi\nMon Mar 24 16:42:36 2025\n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  NVIDIA H100 80GB HBM3          On  |   00000000:04:00.0 Off |                    0 |\n| N/A   43C    P0            158W /  699W |   34866MiB /  81559MiB |    100%      Default |\n|                                         |                        |             Disabled |\n+-----------------------------------------+------------------------+----------------------+\n\n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|    0   N/A  N/A    181231      C   python                                      23410MiB |\n+-----------------------------------------------------------------------------------------+\n</code></pre> <p>Please reach out to HPC-Help@nrel.gov if you would like assistance with profiling your multi-node GPU jobs in a similar manner.</p> <p>Note</p> <p>Importantly, if your target process is running but does not show up in the <code>nvidia-smi</code> output, that indicates your code cannot recognize the GPU device(s), likely due to software problems or user error. Always feel free to submit a ticket with HPC-Help@nrel.gov if something does not look right!</p>"},{"location":"Documentation/Development/Programming_Models/gpu_hpc/#gpu-relevant-environment-variables","title":"GPU-relevant environment variables","text":"<p>The following are some GPU-relevant environment variables you can set in your submission scripts to Slurm.</p> Variable Description <code>SLURM_GPUS_ON_NODE</code> Number of GPUs allocated to the batch step. <code>SLURM_JOB_GPUS</code> The global GPU IDs of the GPUs allocated to this job. The GPU IDs are not relative to any device cgroup, even if devices are constrained with task/cgroup. Only set in batch and interactive jobs. <p>Note</p> <p>You can also run <code>nvidia-smi -L</code> while connected to any GPU node to return the available GPU device(s).</p>"},{"location":"Documentation/Development/Programming_Models/gpu_hpc/#software-containers","title":"Software containers","text":"<p>Please refer to our dedicated documentation on using GPUs from software containers for more information.</p>"},{"location":"Documentation/Development/Programming_Models/gpu_hpc/#migrating-workflows-from-cpu-to-gpu","title":"Migrating workflows from CPU to GPU","text":"<p>GPUs contain hundreds or thousands of cores and can considerably speed up certain operations when compared to CPUs. However, unless you are already using a GPU-accelerated application with built-in CUDA kernels (such as some versions of PyTorch), your custom code will likely require significant changes to be able to effectively use a GPU device. This is even more true if your intent is to parallelize your code over multiple GPU devices. Further, some algorithms or routines are much better suited for GPU computation than others. As such, the first question you should always ask yourself is whether it makes sense to invest the time and effort needed to refactor your CPU-driven code for GPU computation. The following subsections describe key points to consider when you want to take the plunge into GPU computing, ending with an example using the <code>numba</code> package to refactor Python functions for Kestrel's H100 GPUs.</p>"},{"location":"Documentation/Development/Programming_Models/gpu_hpc/#ensure-your-algorithm-is-suited-for-gpu-computation","title":"Ensure your algorithm is suited for GPU computation","text":"<p>Not all algorithms are created equal when it comes to being able to effectively utilize a GPU. In general, GPUs best accommodate large numbers of relatively small, simulataneous operations (\"massive parallelism\"); canonical algorithmic examples of this include graphics processing (reflecting the \"G\" in \"GPU\") and many linear algebra computations (e.g., \"matrix-matrix math\" like BLAS3 routines). Algorithms that would likely perform poorly on a GPU without significant modification are those that launch serial tasks (think for-loops or <code>apply</code> statements in Python) that may each require a significant amount of RAM and/or write to the filesystem directly. </p>"},{"location":"Documentation/Development/Programming_Models/gpu_hpc/#minimize-data-transfer-between-cpu-and-gpu-devices","title":"Minimize data transfer between CPU and GPU devices","text":"<p>Without even considering the characteristics of the algorithm itself, one of the largest bottlenecks in GPU computing is copying data from the CPU to the GPU device(s). In many cases, copying data between devices can easily take longer than the execution of the algorithm. As such, to maximize an algorithm's performance on a GPU, it is imperative to consider employing application-specific routines to minimize the total amount of data transferred during runtime. In other words, the goal with effective GPU computing often comes down to designing the code to transfer as little data as possible as infrequently as possible.</p>"},{"location":"Documentation/Development/Programming_Models/gpu_hpc/#ways-to-compile-cuda-code","title":"Ways to compile CUDA code","text":"<p>CUDA is a low-level API distributed by NVIDIA that allows applications to parallelize on NVIDIA GPUs, such as the H100s available on Kestrel or the A100s on Swift. Because of this, any GPU-driven code gets compiled into a CUDA kernel, which is essentially a function translated to machine code for the GPU. There are two CUDA-aware compilers available from NVIDIA: <code>nvcc</code>, a CUDA analog to the more generic <code>cc</code>, and <code>nvrtc</code>, which is NVIDIA's runtime compiler for \"just-in-time\" (JIT) compilation.</p> <p>See this page for specific GPU code compilation examples on Kestrel, which include both CUDA and OpenAcc (an open-source alternative) implementations.</p>"},{"location":"Documentation/Development/Programming_Models/gpu_hpc/#example-create-a-custom-cuda-kernel-in-python-with-numba","title":"Example: Create a custom CUDA kernel in Python with <code>numba</code>","text":"<p>To demonstrate some of the concepts described here, we will use <code>numba</code> to refactor an algorithm that initially performs poorly on a GPU due to how its input/output data are copied between devices. <code>numba</code> is a Python package for creating custom CUDA kernels from Python functions working with numeric data. It has a simple interface to CUDA that feels comfortable for most Python users, though advanced GPU programmers may consider building GPU-accelerated applications with \"pure\" CUDA. For such examples of creating custom CUDA kernels outside of Python, please see here. </p> <p>This example is written assuming you have access to Kestrel, but it should be able to run on any system with at least one GPU node.</p>"},{"location":"Documentation/Development/Programming_Models/gpu_hpc/#install-numba-from-anaconda","title":"Install <code>numba</code> from Anaconda","text":"<p>The <code>numba</code> package is easily installable through Anaconda/mamba. For any GPU-enabled application, the biggest concern during installation is whether the application version matches the GPU drivers. At the time this page was written, the GPU drivers on Kestrel reflect <code>CUDA 12.4</code>, and so we must ensure that our version of numba can work with that. In conda, we can control this by explicitly passing the corresponding <code>cuda-version=CUDA_VERSION</code> from conda-forge and asking for a <code>cuda-toolkit</code> from the <code>nvidia/label/cuda-CUDA_VERSION</code> channel. When we do this, we will force a compatible version of <code>numba</code> to install into the <code>$CONDA_ENVIRONMENT</code> we define (which is in /scratch to save space). We will also install <code>numpy</code> to work with numeric data, as well as <code>pandas</code> for data manipulation tasks:</p> <p>Note</p> <p>It is best to create this environment on a node with at least one available NVIDIA GPU. On any such node, you can run the command <code>nvidia-smi</code> to display the current GPU driver version (as well as any running GPU processes).</p> <pre><code>ml mamba\nCONDA_ENVIRONMENT=/scratch/$USER/.conda-envs/numba-cuda124\nmamba create --prefix=$CONDA_ENVIRONMENT \\\n  conda-forge::numba \\\n  conda-forge::numpy \\\n  conda-forge::pandas \\\n  conda-forge::cuda-version=12.4 \\\n  nvidia/label/cuda-12.4.0::cuda-toolkit \\\n  --yes\nconda activate $CONDA_ENVIRONMENT\n</code></pre>"},{"location":"Documentation/Development/Programming_Models/gpu_hpc/#example-numba-code","title":"Example numba code","text":"<p>Consider the script <code>numba-mat.py</code> below. This script demonstrates the importance of deciding when and how often one should copy data to and from the GPU device to optimize runtime performance.</p> <p>Note</p> <p>This example requires approximately 40GB of CPU RAM to complete successfully. Be sure to run this on a GPU compute node from a Slurm job accordingly, with the defined <code>$CONDA_ENVIRONMENT</code> activated.</p> <code>numba-mat.py</code>: Matrix multiplication with numba <pre><code># Define and JIT-compile a CUDA function (kernel) with numba for simple\n# matrix multiplication. This script demonstrates the importance of \n# balancing the cost of copying data from the host CPU to GPU device in \n# terms of runtime performance.\n\n# Please contact Matt.Selensky@nrel.gov with any questions.\n\nimport numba\nfrom numba import vectorize\nfrom numba import cuda\nimport pandas as pd\nimport numpy as np\nfrom time import time\n\n# Note that you must define the dtype (float32 is preferred over \n# float64) and target device type ('cuda' for GPU)\n@vectorize(['float32(float32, float32)'], target='cuda')\ndef gpu_mult(x, y):\n    z = x * y\n    return z\n\n\n# create random arrays as input data\nasize = pow(10, 9)\narray_a = np.float32(np.random.rand(asize))\narray_b = np.float32(np.random.rand(asize))\narray_c = np.float32(np.random.rand(asize))\nmatrix_a = ([array_a], [array_b], [array_c])\nmatrix_b = ([array_c], [array_b], [array_a])\n\n# define number of function loops to run for each test case\nnloops = 10\n\n### numpy - CPU\n# Test Case 1: Here, we just use pure numpy to perform matrix multiplication on the CPU.\nt0 = time()\nfor i in np.arange(nloops):\n    np.multiply(matrix_a, matrix_b)\ncpu_time = time()-t0\nprint(\"numpy on CPU required\", cpu_time, \"seconds for\", nloops, \"function loops\")\n\n### numba - GPU\n# Test Case 2: Here, we copy arrays to GPU device __during__ the execution of gpu_mult()\nt0 = time()\nfor i in np.arange(nloops):\n    gpu_mult(matrix_a, matrix_b)\ngpu_time0 = time()-t0\nprint(\"numba on GPU required\", gpu_time0, \"seconds for\", nloops, \"function loops (data are actively copied to GPU device)\")\n\n# Test Case 3: Here, we copy arrays to GPU device __before__ the execution of gpu_mult()\n# output is then copied back to GPU\nmatrix_a_on_gpu = cuda.to_device(matrix_a)\nmatrix_b_on_gpu = cuda.to_device(matrix_b)\nt0 = time()\nfor i in np.arange(nloops):\n    gpu_mult(matrix_a_on_gpu, matrix_b_on_gpu)\ngpu_time1 = time()-t0\nprint(\"numba on GPU required\", gpu_time1, \"seconds for\", nloops, \"function loops (data were pre-copied to GPU device; output is copied back to CPU)\")\n\n# Test Case 4: Here, we copy arrays to GPU device __before__ the execution of gpu_mult()\n# output remains on GPU unless we copy it back with out_device.copy_to_host()\nmatrix_a_on_gpu = cuda.to_device(matrix_a)\nmatrix_b_on_gpu = cuda.to_device(matrix_b)\nout_device = cuda.device_array(shape=(asize,len(matrix_a)), dtype=np.float32)  # does not initialize the contents, like np.empty()\nt0 = time()\nfor i in np.arange(nloops):\n    gpu_mult(matrix_a_on_gpu, matrix_b_on_gpu, out=out_device)\ngpu_time2 = time()-t0\nprint(\"numba on GPU required\", gpu_time2, \"seconds for\", nloops, \"function loops (data were pre-copied to GPU device; output remains on GPU)\")\n# out_device.copy_to_host() # what you would run if you needed to bring this back to the CPU non-GPU work\n\n# format runtime data as output table\nd = {'device_used': ['CPU', 'GPU', 'GPU', 'GPU'],\n    'input_precopied_to_gpu': [np.nan, False, True, True],\n    'output_copied_from_gpu': [np.nan, True, True, False],\n    'seconds_required': [cpu_time, gpu_time0, gpu_time1, gpu_time2]}\ndf = pd.DataFrame(d)\nprint(\"\")\nprint(df)\nprint(\"\")\ndf.to_csv('numba-runtimes.csv', index=False)\n</code></pre> <p>This script runs through four cases of multiplying two large random matrices, each with dimensions (10<sup>9</sup>, 3). For each test case, 10 loops of the function are executed, and the time required reflects the time it takes for all 10 loops. <code>Test Case 1</code> is the CPU speed baseline to which we will compare our various GPU runtimes. Matrix multiplication using pure <code>numpy.multiply()</code>, which does not invoke the GPU and runs entirely on the CPU, requires approximately 39.86 seconds. The remaining Test Cases will all run on the GPU, but have dramatically different runtime performances depending on how frequently data are copied between the CPU and GPU devices. </p> <p>Note that to use the GPU in this script, we define the function <code>gpu_mult()</code>, which is vectorized with a <code>numba</code> decorator that also tells the device to operate on <code>float32</code> values, and defines <code>cuda</code> as the runtime target device. Following these instructions, <code>numba</code> JIT-compiles <code>gpu_mult()</code> into a CUDA kernel that can execute on a GPU.</p> <p>Note</p> <p>In general, computing on numeric <code>float32</code> data performs substantially better compared to <code>float64</code> on GPUs.</p> <p>In <code>Test Case 2</code>, we simply call the vectorized <code>gpu_mult()</code>, which actually has much slower performance (55.67 seconds) than the CPU test case! On the surface, this is counterintuitive (aren't GPUs supposed to be faster?!), however a deeper examination of the code explains why we observe this. Becuase we initialized <code>matrix_a</code> and <code>matrix_b</code> on the CPU (a normal use case), we have to copy each object to the GPU before they can be multiplied together. After <code>gpu_mult()</code> is executed, the output matrix is then copied back to the CPU. Without some extra effort on our part, <code>numba</code> will default to copying these data before and after the execution of <code>gpu_mult()</code>. By contrast, since everything is already on the CPU, <code>numpy</code> simply does not have to deal with this, so it runs faster.</p> <p><code>Test Case 3</code> reflects a situation in which we pre-copy <code>matrix_a</code> and <code>matrix_b</code> to GPU memory before executing <code>gpu_mult()</code>. We do this with the <code>numba</code> command <code>cuda.to_device()</code>, which allows the input data to only be copied between devices once, even though we perform 10 executions on them. With this simple change, we observe a dramatic decrease in runtime to only ~0.8 seconds. However, because we do not specify an 'output device' in our vectorized <code>gpu_mult()</code>, the output matrix is actually copied back to CPU memory after each execution. However, with a bit of extra code, we can keep the output on the GPU, which would make sense if we wanted to do more work on it there later in the script. </p> <p>To that end, <code>Test Case 4</code> squeezes all possible performance out of <code>gpu_mult()</code> by both pre-copying the input data to the GPU and leaving the output matrix on the same device. The blazing-fast runtime of this test case (only about a millisecond) measures the GPU computation itself, without the clutter of copying data between devices. When compared to the runtime of <code>Test Case 1</code>, which also does not include any kind of data copying step, <code>Test Case 4</code> shows a roughly 24,000X speedup in multiplying two matrices of this size, allowing us to appreciate the true power of the GPU.</p> <p>This table summarizes the results and reflect runtimes of ten function loops on a node from Kestrel's <code>gpu-h100</code> partition.</p> Test Case Input pre-copied to GPU Output copied from GPU Time required (seconds) 1 (CPU) NaN NaN 39.860077 2 (GPU) False True 55.670377 3 (GPU) True True 0.797287 4 (GPU) True False 0.001643 <p>To be sure, there are many more considerations to have when developing a highly performant custom CUDA kernel, and there are many other packages that can do similar things. However, minimizing the amount of data copied between the CPU and GPU devices is a relatively easy approach that introductory GPU programmers can implement in their kernels to see immediate paybacks in performance regardless of computing platform. </p>"},{"location":"Documentation/Development/Programming_Models/gpu_hpc/#extra-resources","title":"Extra resources","text":"<ul> <li>\"Preparing your Python code for Perlmutter's GPUs\" (NERSC)</li> <li>Another <code>numba</code> example (NREL)</li> <li>\"Just-in-time (JIT) compilation\" (NVIDIA)</li> <li>Numba documentation (Numba)</li> </ul>"},{"location":"Documentation/Development/Programming_Models/mpi/","title":"MPI","text":""},{"location":"Documentation/Development/Programming_Models/mpi/#cray-mpich","title":"Cray-MPICH","text":"<p>Documentation: Cray-MPICH</p> <p>Cray's MPICH is a high performance and widely portable implementation of the Message Passing Interface (MPI) standard.</p> <p>Note Cray-MPICH is only available on Kestrel. In order to use Cray-MPICH, it is recommended to use the HPE Cray complier wrappers <code>cc</code>, <code>CC</code> and <code>ftn</code>. The wrappers will find the necessary MPI headers and libraries as well as scientific libraries provided by LibSci. </p> <p>Depending on the compiler of choice, we can load a different instance of Cray-MPICH. For example, if we decide to use <code>PrgEnv-intel</code>, we can load the module <code>PrgEnv-intel</code> which will invoke an Intel instance of <code>cray-mpich</code> that can be used through <code>cc</code>, <code>CC</code> and <code>ftn</code>. We can also use the usual MPI compilers <code>mpicc</code>, <code>mpicxx</code> and <code>mpif90</code>/<code>mpifort</code> but it is recommended to use the wrappers. </p> <p>Cray-MPICH takes into consideration the processor architecture through <code>craype-x86-spr</code> and the network type through <code>craype-network-ofi</code>.</p>"},{"location":"Documentation/Development/Programming_Models/mpi/#cray-mpich-abi","title":"cray-mpich-abi","text":"<p>For codes compiled using <code>intel-mpi</code> or <code>mpich</code>, we can load the module <code>cray-mpich-abi</code>, an HPE provided MPI that allows pre-compiled software to leverage MPICH benefits on Kestrel's network topology. </p>"},{"location":"Documentation/Development/Programming_Models/mpi/#openmpi","title":"OpenMPI","text":"<p>Documentation: OpenMPI</p> <p>The Open MPI Project is an open source Message Passing Interface implementation that is developed and maintained by a consortium of academic, research, and industry partners. Open MPI is therefore able to combine the expertise, technologies, and resources from all across the High Performance Computing community in order to build the best MPI library available. Open MPI offers advantages for system and software vendors, application developers and computer science researchers.</p> <p>The Open MPI framework is a free and open-source communications library that is commonly developed against by many programmers. As an open-source package with strong academic support, the latest ideas may appear as implementations here prior to commercial MPI libraries.</p> <p>Note that the Slurm-integrated builds of OpenMPI do not create the <code>mpirun</code> or <code>mpiexec</code> wrapper scripts that you may be used to. Ideally you should use <code>srun</code> (to take advantage of Slurm integration), but you can also use OpenMPI's native job launcher <code>orterun</code>. Some have also had success simply symlinking <code>mpirun</code> to <code>orterun</code>.</p> <p>OpenMPI implements two Byte Transfer Layers for data transport between ranks in the same physical memory space: <code>sm</code> and <code>vader</code>.  Both use a memory-mapped file, which by default is placed in <code>/tmp</code>.  The node-local <code>/tmp</code> filesystem is quite small, and it is easy to fill this and crash or hang your job.  Non-default locations of this file may be set through the <code>OMPI_TMPDIR</code> environment variable.</p> <ul> <li> <p>If you are running only a few ranks per node with modest buffer space requirements, consider setting <code>OMPI_TMPDIR</code> to <code>/dev/shm</code> in your job script.</p> </li> <li> <p>If you are running many nodes per rank, you should set i<code>OMPI_TMPDIR</code> to <code>/tmp/scratch</code>, which holds at least 1 TB depending on Eagle node type.</p> </li> </ul>"},{"location":"Documentation/Development/Programming_Models/mpi/#supported-versions","title":"Supported Versions","text":"Kestrel Swift Vermilion openmpi/4.1.6-gcc  (CPU) openmpi/4.1.1-6vr2flz openmpi/4.1.4-gcc openmpi/4.1.6-intel(CPU) openmpi/5.0.1-gcc  (CPU) openmpi/5.0.3-gcc  (CPU) openmpi/4.1.6-gcc (GPU)"},{"location":"Documentation/Development/Programming_Models/mpi/#intelmpi","title":"IntelMPI","text":"<p>Documentation: IntelMPI</p> <p>Intel\u00ae MPI Library is a multifabric message-passing library that implements the open source MPICH specification. Use the library to create, maintain, and test advanced, complex applications that perform better on HPC clusters based on Intel\u00ae and compatible processors.</p> <p>Intel's MPI library enables tight interoperability with its processors and software development framework, and is a solid choice for most HPC applications.</p>"},{"location":"Documentation/Development/Programming_Models/mpi/#supported-versions_1","title":"Supported Versions","text":"Kestrel Swift Vermilion intel-oneapi-mpi/2021.10.0-intel (CPU) intel-oneapi-mpi/2021.3.0-hcp2lkf intel-oneapi-mpi/2021.7.1-intel intel-oneapi-mpi/2021.11.0-intel (CPU) intel-oneapi-mpi/2021.12.1-intel (CPU) intel-oneapi-mpi/2021.13.0-intel (GPU)"},{"location":"Documentation/Development/Programming_Models/mpi/#mpich","title":"MPICH","text":"<p>Documentation: MPICH</p> <p>MPICH is a high performance and widely portable implementation of the Message Passing Interface (MPI) standard.  MPICH and its derivatives form the most widely used implementations of MPI in the world. They are used exclusively on nine of the top 10 supercomputers (June 2016 ranking), including the world\u2019s fastest supercomputer: Taihu Light.</p>"},{"location":"Documentation/Development/Programming_Models/mpi/#supported-versions_2","title":"Supported Versions","text":"Kestrel Swift Vermilion mpich/4.1-gcc   (CPU) mpich/3.4.2-h2s5tru mpich/4.0.2-gcc mpich/4.1-intel (CPU) mpich/4.1-gcc (GPU)"},{"location":"Documentation/Development/Programming_Models/mpi/#running-mpi-jobs-on-kestrel-gpus","title":"Running MPI Jobs on Kestrel GPUs","text":"<p>To run MPI (message-passing interface) jobs on Kestrel system's NVidia GPUs, the MPI library must be \"CUDA-aware.\" All modules with <code>(GPU)</code> are gpu aware and built with <code>CUDA</code>. </p>"},{"location":"Documentation/Development/Programming_Models/mpi/#interactive-use","title":"Interactive Use","text":"<p><code>srun</code> does not work with this OpenMPI build when running interactively, so please use <code>orterun</code> instead.  However, OpenMPI is cognizant of the Slurm environment, so one should request the resources needed via <code>salloc</code> (for example, the number of available \"slots\" is determined by the number of tasks requested via <code>salloc</code>).  Ranks are mapped round-robin to the GPUs on a node.  <code>nvidia-smi</code> shows, for example,</p> <p>Processes:                                                              </p> GPU PID Type Process name GPU Memory Usage 0 24625 C ./jacobi 803MiB 0 24627 C ./jacobi 803MiB 1 24626 C ./jacobi 803MiB <p>when oversubscribing 3 ranks onto the 2 GPUs via the commands</p> <pre><code>srun --nodes=1 --ntasks-per-node=3 --account=&lt;allocation_id&gt; --time=10:00 --gres=gpu:2 --pty $SHELL\n...&lt;getting node&gt;...\norterun -np 3 ./jacobi\n</code></pre> <p>If more ranks are desired than were originally requested via srun, the OpenMPI flag --oversubscribe could be added to the orterun command.</p>"},{"location":"Documentation/Development/Programming_Models/mpi/#batch-use","title":"Batch Use","text":"<p>An example batch script to run 4 MPI ranks across two nodes is as follows.</p> batch script <pre><code>#!/bin/bash --login\n#SBATCH --nodes=2\n#SBATCH --ntasks-per-node=2\n#SBATCH --time=2:00\n#SBATCH --gres=gpu:2\n#SBATCH --job-name=GPU_MPItest\n#SBATCH --account=&lt;allocation_id&gt;\n#SBATCH --error=%x-%j.err\n#SBATCH --output=%x-%j.out\n\nml gcc cuda openmpi\n\ncd $SLURM_SUBMIT_DIR\nsrun ./jacobi\n</code></pre>"},{"location":"Documentation/Development/Programming_Models/mpi/#multi-process-service","title":"Multi-Process Service","text":"<p>To run multiple ranks per GPU, you may find it beneficial to run NVidia's Multi-Process Service. This process management service can increase GPU utilization, reduce on-GPU storage requirements, and reduce context switching. To do so, include the following functionality in your Slurm script or interactive session:</p>"},{"location":"Documentation/Development/Programming_Models/mpi/#mps-setup","title":"MPS setup","text":"MPS setup <pre><code>export CUDA_MPS_PIPE_DIRECTORY=/tmp/scratch/nvidia-mps\nif [ -d $CUDA_MPS_PIPE_DIRECTORY ]\nthen\n   rm -rf $CUDA_MPS_PIPE_DIRECTORY\nfi\nmkdir $CUDA_MPS_PIPE_DIRECTORY\n\nexport CUDA_MPS_LOG_DIRECTORY=/tmp/scratch/nvidia-log\nif [ -d $CUDA_MPS_LOG_DIRECTORY ]\nthen\n   rm -rf $CUDA_MPS_LOG_DIRECTORY\nfi\nmkdir $CUDA_MPS_LOG_DIRECTORY\n\n# Start user-space daemon\nnvidia-cuda-mps-control -d\n\n# Run OpenMPI job.\norterun ...\n\n# To clean up afterward, shut down daemon, remove directories, and unset variables\necho quit | nvidia-cuda-mps-control\nfor i in `env | grep CUDA_MPS | sed 's/=.*//'`; do rm -rf ${!i}; unset $i; done\n</code></pre> <p>For more information on MPS, see the NVidia guide.</p>"},{"location":"Documentation/Development/VSCode/","title":"Visual Studio Code","text":"<p>Microsoft Visual Studio Code (VS Code) is a popular tool for development in many programming languages, and may be used on HPC systems. However, there are some caveats to be aware of when running it remotely.</p>"},{"location":"Documentation/Development/VSCode/#connecting-with-vs-code","title":"Connecting with VS Code","text":"<p>To connect to an HPC system with VS Code, install the \"Remote - SSH\" extension from the Extensions menu. </p> <p>Press \"F1\" to open the command bar, and type or search for <code>Remote-SSH: Connect to Host...</code></p> <p>You may then enter your HPC username and the address of an HPC system to connect to. </p> <ul> <li> <p>To connect to Kestrel from the NREL VPN, enter <code>username@kestrel.hpc.nrel.gov</code>, replacing \"username\" with your HPC user name.</p> </li> <li> <p>To connect to Kestrel as an external collaborator, enter <code>username@kestrel.nrel.gov</code>, replacing \"username\" with your HPC user name.</p> </li> </ul> <p>Enter your HPC password (or password and OTP code if external) and you will be connected to a login node. You may open a folder on the remote host to browse your home directory and select files to edit, and so on.</p> <p>VS Code Remote-SSH Bug</p> <p>If you are no longer able to connect to Kestrel with VS Code, in your settings for the Remote-SSH extension set \"Use Exec Server\" to False by unchecking the box. This issue is due to a VS Code bug in an update to the Remote-SSH plugin or VS code itself. </p> <p>Windows SSH \"Corrupted MAC on input\" Error</p> <p>Some people who use Windows 10/11 computers to ssh to Kestrel via Visual Studio Code's SSH extension might receive an error message about a \"Corrupted MAC on input\" or \"message authentication code incorrect.\" To workaround this issue, you will need to create an ssh config file on your local computer, <code>~/.ssh/config</code>, with a host entry for Kestrel that specifies a new message authentication code: <pre><code>Host kestrel\n    HostName kestrel.hpc.nrel.gov\n    MACs hmac-sha2-512\n</code></pre> This Visual Studio Blog post has further instructions on how to create the ssh configuration file for Windows and VS Code.</p>"},{"location":"Documentation/Development/VSCode/#caution-about-vs-code-processes","title":"Caution About VS Code Processes","text":"<p>Please be aware that the Remote SSH extension runs processes on the remote host. This includes any extensions or helpers, include language parsers, code analyzers, AI code assistants, and so on. These extensions can take up a considerable amount of CPU and RAM on any remote host that VS Code connects to. Jupyter notebooks loaded through VS Code will also be executed on the remote host and can use excessive CPU and RAM, as well. When the remote host is a shared login node on an HPC system, this can be a considerable drain on the resources of the login node, and cause system slowdowns for all users of that login node. </p> <p>This problem can be circumvented by using a compute node to run VS Code. This will cost AU, but will allow for full resource usage of CPU and/or RAM. </p>"},{"location":"Documentation/Development/VSCode/#connecting-to-kestrel","title":"Connecting to Kestrel","text":"<p>Using VS Code on a compute node will require adding an ssh key.</p>"},{"location":"Documentation/Development/VSCode/#ssh-key-setup","title":"SSH Key Setup","text":"<p>You may use an existing key pair on your local computer/laptop, or create one with <code>ssh-keygen</code> (adding <code>-t ed25519</code> is optional, but recommended.) </p> <p>We recommend choosing a strong passphrase and storing it in a password manager. The passphrase on your key will allow you to log in via ssh, but it is not the same as your HPC account password.</p> <p>SSH Key Pair Caution</p> <p>Do not replace the key pair in your Kestrel home directory. These keys are generated when you log into the cluster, and are used by Slurm jobs to communicate between nodes. There is a corresponding public key entry in your cluster home directory ~/.ssh/authorized_keys that must also be left in place.</p> <p>Reminder About Passwords</p> <p>Using an SSH key with an SSH agent can remove the need to use a password to SSH to Kestrel. However, not all HPC services (including Lex) use SSH keys. An SSH key does NOT replace your HPC account password. You must maintain a regular HPC account password in accordance with our Appropriate Use Policy and User Account Password Guidelines. Ignoring password expiration date notices will lead to automatic account lockouts, and you will need to contact HPC Support to restore your account.</p> <p>Once you have a key pair on your local computer, use the <code>ssh-copy-id &lt;username&gt;@kestrel.hpc.nrel.gov</code> command to copy the public portion to Kestrel. This will add your public key to the ~/.ssh/authorized_keys file in your Kestrel home directory. Alternatively, you may manually add the contents of your PUBLIC key file (for example, the contents of ~/.ssh/id_ed25519.pub or ~/.ssh/id_rsa.pub) onto the end of this file. Do not delete the existing entries in these files on Kestrel.</p>"},{"location":"Documentation/Development/VSCode/#editing-the-vs-code-ssh-config-file","title":"Editing the VS Code SSH Config File","text":"<p>We will now create a host entry in your local ssh config file to make connecting to Kestrel compute nodes easier. </p> <p>Use the remote-ssh command to edit your VS Code ssh config file (~/.ssh/config). Add the following:</p> <pre><code>Host x????c*\n    ProxyJump &lt;username&gt;@kestrel.hpc.nrel.gov\n</code></pre> <p>This create a \"wildcard\" entry that should match Kestrel compute node names. Any time an ssh command is issued on your computer that matches the wildcard, the ssh connection will \"jump\" through a Kestrel login node and directly to the compute node.</p> <p>If your allocation is finished on Kestrel (e.g. at the end of the FY and your allocation will not be continuing to the next) or you otherwise anticipate no further need to use VS Code with Kestrel in this fashion, you may delete this entry from your SSH config file.</p>"},{"location":"Documentation/Development/VSCode/#start-a-job-and-connect-vs-code","title":"Start a Job and Connect VS Code","text":"<p>SSH to Kestrel as usual (outside of VS Code) and use sbatch or salloc to start a job. (An interactive job with <code>salloc</code> is suggested, using a <code>--time</code> limited to only the expected duration of your working session with VS Code.)</p> <p>Wait until the job has started running, and take note of the node assigned to the job. Put the terminal aside, but leave the job running.</p> <p>Now use the Remote-SSH extension in VS Code to <code>Connect to Host...</code> and use the hostname of the node that your job was assigned. For example, <code>&lt;username&gt;@x1000c0s0b0n1</code>. </p> <p>This should open a new VS Code window that will connect to the compute node automatically. You may begin browsing your home directory and editing files in the VS Code window.</p>"},{"location":"Documentation/Development/VSCode/#jupyter-in-vs-code-on-kestrel","title":"Jupyter in VS Code on Kestrel","text":""},{"location":"Documentation/Development/VSCode/#setting-up-vs-code","title":"Setting Up VS Code","text":"<p>To begin, proceed to VSCode and install these additional extensions, if you do not already have them: Python and Jupyter.</p>"},{"location":"Documentation/Development/VSCode/#setting-up-conda-environment","title":"Setting Up Conda Environment","text":"<p>In addition, you will also need to set up a Python Environment on Kestrel. At a minimum, you must perform the following commands:</p> <pre><code>module load anaconda3\nconda create -n myJupEnv\nconda activate myJupEnv\nconda install Jupyter\n</code></pre>"},{"location":"Documentation/Development/VSCode/#running-the-code","title":"Running the Code","text":"<p>To begin, refer to the above section Connecting to Kestrel to connect to a compute node.</p> <p>Then, open or create an .ipynb file in VS Code. Go to the top right corner and select the Python Kernel created earlier. VS Code may prompt for installing some Python packages; please allow these or the process will not work.</p> <p>Once an interpreter has been selected (and it has finished installing anything needed), you will have a working Jupyter Notebook open in VS Code running on a Kestrel compute node.</p>"},{"location":"Documentation/Environment/lmod/","title":"Environment Modules","text":"<p>The Lmod environment modules system is used to easily manage software environments. Modules facilitate the use of different versions of applications, libraries, and toolchains, which enables support of multiple package versions concurrently.</p> <p>Modules typically just set environment variables that one might traditionally do manually by, for example, adding export or setenv commands to their login script. Modules add the ability to back out changes in an orderly manner as well, so users can change their environment in a reversible way. To learn how to build your own modules see Building an Application.</p> <p>For system specific information on environments and modules, please visit the Systems section.</p>"},{"location":"Documentation/Environment/lmod/#common-module-commands","title":"Common Module Commands","text":"<p>The <code>module</code> command accepts parameters that enable users to inquire about and change the module environment. Most of the basic functionality can be accessed through the following commands.</p> Option Description spider Prints available modules in a path-agnostic format. avail Prints available modules grouped by path. list Prints all currently loaded modules. display'name' Prints settings and paths specified for a particular module. help 'name' Prints help message for a particular module. load 'name' Loads particular module. For modules listed as the '(default)', the short package name is sufficient. To load another version of the package the long package name is required (e.g., <code>module load fftw/3.3.8/gcc-7.3.0</code>). unload 'name' Unloads particular module. swap  'name 1''name 2' First unload modName1 and then load modName2. use {-a}  A_PATH Prefix {suffix} the path $A_PATH to your $MODULEPATH variable, in order to find modules in that location. unuse {-a}  A_PATH Remove the path $A_PATH from your $MODULEPATH variable."},{"location":"Documentation/Environment/lmod/#examples","title":"Examples","text":"Determining loaded modules <p>To determine which modules are already loaded, run the command: <pre><code>$ module list\n</code></pre></p> Seeing available modules <p>To get a list of available modules, type:</p> <pre><code>$ module avail\n</code></pre> <p>This should outut a full list of all modules and their versions in the system available for you to load. The modules denoted with (L) are already loaded in your environment. The module versions denoted with (D) are the default versions that will load if you do not specify the version when running <code>module load</code>.</p> <p>To get a list of the available module defaults, type: <pre><code>$ module --default avail\n</code></pre></p> Loading and unloading a module <p><pre><code>$ module load &lt;module_name&gt;/&lt;version&gt;\n...\n$ module unload &lt;module_name&gt;/&lt;version&gt;\n...\n</code></pre> Here <code>&lt;module_name&gt;</code> is to be replaced by the name of the module to load. It is advised to ALWAYS include the full versioning in your load statements, and not rely on explicit or implicit default behaviors.</p> Seeing module specifics <p>It's a good idea to look at two other commands to see what a module does, and what software dependencies there are, as illustrated below:</p> <pre><code>$ module show &lt;module_name&gt;/&lt;version&gt;\n...\n$ module help &lt;module_name&gt;/&lt;version&gt;\n...\n</code></pre> <p>The environment variables set by the module can then be used in build scripts.</p> <p>It is not necessary to load a module in order to use the <code>module show</code> command, this may be done at any time to see what a module does.</p> Swap a module environment <p>Module files for different versions can easily be swapped: <pre><code>$ module load openmpi/3.1.3/gcc-7.3.0\n$ module list\nCurrently Loaded Modulefiles:\n1) openmpi/3.1.3/gcc-7.3.0\n$ module swap openmpi/3.1.3/gcc-7.3.0 openmpi/2.1.5/gcc-7.3.0\n$ module list\nCurrently Loaded Modulefiles:\n1) openmpi/2.1.5/gcc-7.3.0\n</code></pre></p>"},{"location":"Documentation/Environment/shell/","title":"Shell Startup","text":"<p>When you login to a linux-based machine you interact with the operating system via a program called a shell.  There are various types of shell programs.  One of the more common is bash.  Bash is the default shell on NREL's HPC platforms.  This document describes ways you can customize your shell's \u2014 in particular, bash's \u2014 behavior.</p>"},{"location":"Documentation/Environment/shell/#getting-started","title":"Getting Started","text":"<p>When you have a window open attached to a platform, you are actually running a program on the remote computer called a shell.  There are various types of shell programs.  One of the more common is bash.  </p> <p>The shell program provides your link to the machine's operating system (OS).  It is the interface between a user and the computer. It controls the computer and provides output to the user.  There are various types of interfaces but here we discuss the command-line interface. That is, you type commands and the computer responds.</p>"},{"location":"Documentation/Environment/shell/#what-happens-on-login","title":"What happens on login","text":"<p>When you login to a machine, you are put in your home directory.  You can see this by running the command pwd.  Run the command ls -a to get a listing of the files.  The -a option for the ls commands enables it to show files that are normally hidden.  You'll see two important files that are used for setting up your environment.</p> <ul> <li>.bash_profile</li> <li>.bashrc</li> </ul> <p>These files are added to your home directory when your account is created.  </p> <p>When you login, the file .bash_profile is sourced (run) to set up your environment.  The environment includes settings for important variables, command aliases, and functions.  </p> <p>Here is the default version of .bash_profile:  </p> <pre><code>[nreluser@kl2 ~] CPU $ cat ~/.bash_profile\n# .bash_profile\n\n# Get the aliases and functions\nif [ -f ~/.bashrc ]; then\n    . ~/.bashrc\nfi\n\n# User specific environment and startup programs\n</code></pre> <p>Notice the lines:</p> <pre><code>if [ -f ~/.bashrc ]; then\n    . ~/.bashrc\nfi\n</code></pre> <p>The \"if\" statement says that if you have a file .bashrc in your home directory, then run it.  The . by itself is shorthand for \"source\" and ~/  is shorthand for your home directory.</p> <p>Now, let's look at the default ~/.bashrc file:</p> <pre><code>[nreluser@kl2 ~] CPU $ cat ~/.bashrc\n# .bashrc\n\n# Source global definitions\nif [ -f /etc/bashrc ]; then\n    . /etc/bashrc\nfi\n\n# User specific environment\nif ! [[ \"$PATH\" =~ \"$HOME/.local/bin:$HOME/bin:\" ]]\nthen\n    PATH=\"$HOME/.local/bin:$HOME/bin:$PATH\"\nfi\nexport PATH\n\n# Uncomment the following line if you don't like systemctl's auto-paging feature:\n# export SYSTEMD_PAGER=\n\n# User specific aliases and functions\n</code></pre> <p>First, you will see that this runs the system's version of the bashrc from /etc/; the purpose is to initialize system-wide functions and aliases. </p> <p>Second, note that it sets up the user specific environment by updating the PATH environment variable. The PATH environment variable contains the file directory paths a computer will look for commands to run. You can append or, like in this case, prepend directories to it; the PATH is set to <code>$HOME/.local/bin:$HOME/bin:$PATH</code>, <code>$HOME</code> being another environment variable that points to your home directory. This adds the user's commands to the PATH if those commands are in either <code>$HOME/.local/bin</code> or <code>$HOME/bin</code>.</p> <p>Note in both of these files we have a place where you are encouraged to add user defined aliases and functions.  You can also set environmental variables, such as PATH and a related variable LD_LIBRARY_PATH.  You may want to load modules which also set environmental variables. </p>"},{"location":"Documentation/Environment/shell/#suggestions-philosophy","title":"Suggestions (Philosophy)","text":"<p>We're going to discuss customizing your environment. This is done by editing these two files. Before we do that here are three suggestions:  </p> <ol> <li>If you are new to linux use the nano editor</li> <li>Make a backup of your current versions of the two files</li> <li>Make additions in external files</li> </ol> <p>Nano is an easy to learn and use text editor.  The official web page for nano is https://www.nano-editor.org.  There are many online tutorials.  There are other editors available, but nano is a good starting point.</p> <p>It is very easy to make mistakes when doing edits or you just might want to go back to a previous version.  So back it up.  Here are commands to do so:</p> <p><pre><code>[hpcuser2@eyas1 ~]$ NOW=`date +\"%y%m%d%H%M\"`\n[hpcuser2@eyas1 ~]$ echo $NOW\n2303221513\n[hpcuser2@eyas1 ~]$ cp .bashrc bashrc.$NOW\n[hpcuser2@eyas1 ~]$ cp .bash_profile bash_profile.$NOW\n</code></pre> The first command creates a date/time stamp.  The last commands copy files using the date/time stamp as part of the filename.</p> <pre><code>[hpcuser2@eyas1 ~]$ ls *2303221513\nbash_profile.2303221513  bashrc.2303221513\n[hpcuser2@eyas1 ~]$ \n</code></pre> <p>In most cases, you won't need to edit both .bashrc and .bash_profile.  Since running .bash_profile runs .bashrc you can usually just edit .bashrc.  (See the section Difference between login and interactive shells which describes cases where .bashrc is run even if .bash_profile is not.)</p> <p>Instead of adding a bunch of text to .bashrc, make your additions in an external file(s) and just source the file(s) inside of .bashrc.  Then, you can \"turn off\" additions by just commenting out the source lines.  Also, you can test additions by sourcing the file from the command lines.</p>"},{"location":"Documentation/Environment/shell/#additions","title":"Additions","text":"<p>The most common additions to your environment fall into these categories:</p> <ol> <li>Setting variables </li> <li>Creating Aliases</li> <li>Loading modules</li> <li>Adding Functions</li> </ol> <p>We'll discuss each. We're going to assume that you created a directory ~/MYENV and in that directory you have the files:</p> <ul> <li>myvars</li> <li>myaliases</li> <li>mymods</li> <li>myfuncs</li> </ul> <p>Then, to enable all of your additions you can add the following lines to your .bashrc file:</p> <pre><code>if [ -f ~/MYENV/myvars ];    then . ~/MYENV/myvars ;    fi\nif [ -f ~/MYENV/myaliases ]; then . ~/MYENV/myaliases ; fi\nif [ -f ~/MYENV/mymods ];    then . ~/MYENV/mymods ;    fi\nif [ -f ~/MYENV/myfuncs ];   then . ~/MYENV/myfuncs ;   fi\n</code></pre> <p>Note the additions will not take effect until you logout/login or until you run the command source ~/.bashrc   Before going through the logout/login process, you should test your additions by manually running these commands in the terminal window.</p>"},{"location":"Documentation/Environment/shell/#setting-variables","title":"Setting variables","text":"<p>We have discussed the PATH variable.  It points to directories which contain programs.  If you have an application that you built, such as myapp in /projects/mystuff/apps, you can add the line</p> <pre><code>export PATH=/projects/mystuff/apps:$PATH\n</code></pre> <p>to your ~/MYENV/myvars file.  Then when you login, the system will be able to find your application.  The directories in path variables are seperated by a \":\". If you forget to add $PATH to the export line the new PATH variable will be truncated and you will not see many \"system\" commands.  </p> <p>Another important variable is LD_LIBRARY_PATH.  This points to directories containing libraries your applications need that are not \"bundled\" with your code.  Assuming the libraries are in projects/mystuff/lib, you would add the following line:</p> <pre><code>export LD_LIBRARY_PATH=/projects/mystuff/lib:$LD_LIBRARY_PATH\n</code></pre> <p>If you have a commercial application that requires a license server you may need to set a variable to point to it.  For example:</p> <pre><code>export LSERVER=license-1.hpc.nrel.gov:4691\n</code></pre>"},{"location":"Documentation/Environment/shell/#creating-aliases","title":"Creating aliases","text":"<p>Aliases are command shortcuts.  If there is a complicated command that you often use, you might want to crate an alias for it.  You can get a list of aliases defined for you by just running the command alias.  The syntax for an alias is: <pre><code>alias NAME=\"what you want to do\"\n</code></pre> Here are a few examples that you could add to your ~/MYENV/myalias file:</p> <pre><code>#Show my running and queued jobs in useful format\nalias sq='squeue -u $USER --format='\\''%10A%15l%15L%6D%20S%15P%15r%20V%N'\\'''\n\n#Kill all my running and queued jobs\nalias killjobs=\"scancel -u $USER\"\n\n#Get a list of available modules\nalias ma='module avail'\n\n#Get the \"source\" for a git repository\nalias git-home='git remote show origin'\n\n#Get a compact list of loaded modules\nalias mlist='module list 2&gt;&amp;1 |  egrep -v \"Current|No modules loaded\" | sed \"s/..)//g\"'\n</code></pre>"},{"location":"Documentation/Environment/shell/#loading-modules","title":"Loading modules","text":"<p>Most HPC platforms run module systems.  When you load a module, it will change some environmental variable settings.  Often, PATH and LD_LIBARAY_PATH are changed.  In general, loading a module will allow you to use a particular application or library.  </p> <p>If you always want gcc version 12 and python 3.10 in you path, you could add the following to your ~/MYENV/mymods file:</p> <pre><code>module load gcc/12.1.0  \nmodule load python/3.10.2\n</code></pre> <p>Running the command module avail will show the modules installed on the system.</p> <p>If you have modules that you created, you can make them available to the load command by adding a command like the following in your ~/MYENV/mymods file:</p> <pre><code>module use /projects/mystuff/mods\n</code></pre> <p>The \"module use\" command needs to be before any module load command that loads your coustom modules.</p>"},{"location":"Documentation/Environment/shell/#adding-functions","title":"Adding functions","text":"<p>Functions are like aliases but generally multiline and more complex. You can run the command **compgen -A function ** to see a list of defined functions.  Here are a few functions you might want to add to your environment:</p> <pre><code># given a name of a function or alias show its definition\nfunc () \n{ \n    typeset -f $1 || alias $1\n}\n\n# find files in a directory that changed today\ntoday () \n{ \n    local now=`date +\"%Y-%m-%d\"`;\n    if (( $# &gt; 0 )); then\n        if [[ $1 == \"-f\" ]]; then\n            find . -type f -newermt $now;\n        fi;\n        if [[ $1 == \"-d\" ]]; then\n            find . -type d -newermt $now;\n        fi;\n    else\n        find . -newermt $now;\n    fi\n}\n</code></pre> <p>Most people who have worked in HPC for some time have collected many functions and alias they would be willing to share with you.</p> <p>If you have a number of files in your ~/MYENV directory you want sourced at startup, you can replace the set of 4 \"if\" lines shown above with a \"for list\" statement.  The following will source every file in the directory.  It will not source files in subdirectories within ~/MYENV.  If you want to temporarly turn off additions, you can put them in a subdirectory ~/MYENV/OFF.  The find command shown here will return a list of files in the directory but not subdirectories.  Again, recall that the changes will not be in effect until you logout/login.</p> <pre><code>for x in `find ~/MYENV  -type f` ; do\n   source $x \ndone\n</code></pre>"},{"location":"Documentation/Environment/shell/#difference-between-login-and-interactive-shells","title":"Difference between login and interactive shells","text":"<p>This section is based in part on Stack Overflow question What are the differences between a login shell and interactive shell?</p> <p>The shell that gets started when you open a window on a HPC is called a login shell.  It is also an interactive shell in that you are using it to interact with the computer.  Bash can also be run as a command.  That is, if you enter bash as a command you will start a new instance of the bash shell.  This new shell is an interactive shell but not a login shell because it was not used to do the login to the platform.   </p> <p>When you start a new interactive shell the file .bashrc is sourced.  When you start a login shell the file .bash_profile is sourced. However, most versions of .bash_profile have a line that will also source .bashrc.</p> <p>When you submit a slurm batch job with the command sbatch neither of the two files .bashrc or .bash_profile are sourced.  Note that, by default, the environment you have set up at the time you run sbatch is passed to the job. </p> <p>When you start a slurm interactive session, for example, using the command</p> <pre><code>salloc --nodes=1 --time=01:00:00 --account=$MYACCOUNT --partition=debug\n</code></pre> <p>the file .bashrc is sourced.</p>"},{"location":"Documentation/Environment/shell/#troubleshooting","title":"Troubleshooting","text":"<p>The most common issue when modifying your environment is forgetting to add the previous version of PATH when you set a new one.  For example,</p> <p>Do this:</p> <pre><code>export PATH=/projects/myapps:$PATH\n</code></pre> <p>Don't do this:</p> <pre><code>export PATH=/projects/myapps\n</code></pre> <p>If you do the second command you will lose access to most commands and you'll need to logout/login to restore access.</p> <p>Always test additions before actually implementing them.  If you use the files in ~/MYENV to modify your environment manually run the commands</p> <pre><code>if [ -f ~/MYENV/myvars ];    then . ~/MYENV/myvars ;    fi\nif [ -f ~/MYENV/myaliases ]; then . ~/MYENV/myaliases ; fi\nif [ -f ~/MYENV/mymods ];    then . ~/MYENV/mymods ;    fi\nif [ -f ~/MYENV/myfuncs ];   then . ~/MYENV/myfuncs ;   fi\n</code></pre> <p>to test things.  After they are working as desired then add these lines to your .bashrc file.  You can add a # to the lines in your .bashrc file to disable them.  </p> <p>There are copies of the default .bashrc and .bash_profile files in</p> <ul> <li>/etc/skel/.bash_profile</li> <li>/etc/skel/.bashrc</li> </ul>"},{"location":"Documentation/Environment/shell/#some-commands","title":"Some commands","text":"Command Explanation <code>man</code> Print manual or get help for a command. Examples: <ul> <li> <code>man ls</code>: Shows an explanation of the ls command </li> <li> <code>man bash</code>: Shows many built-in commands </li> </ul> <code>ls</code> List directory contents. Note: <ul> <li> <code>ls -a</code>: Show all files, including hidden files </li> <li> <code>ls -l</code>: Do a detailed listing </li> <li> <code>ls -R</code>: Recursive listing, current directories subdirectories </li> <li> <code>ls  *.c</code>: List files that end in \"c\" </li> </ul> <code>echo</code> Prints text to the terminal window <code>mkdir</code> Creates a directory <code>pwd</code> Prints working directory; that is, gives the name of the directory you are currently in. <code>cd</code> Changes directory. Note: <ul> <li> <code>cd ~</code>:  Go to your home directory </li> <li> <code>cd ..</code>: Go up one level in the directory tree </li> </ul> <code>mv</code> Moves or renames a file or directory <code>nano</code> Opens a simple text editor. See above. <code>rm</code> Removes a file. Note: <ul> <li> <code>rm -r [DIRECTORY]</code>: Recursively removes a directory. </li> </ul> DO NOT <code>rm -rf ~</code> as this will completely delete your home directory. <code>rmdir</code> Removes an empty directory. It's safer than <code>rm -rf</code> <code>less</code> Views the contents of a text file <code>&gt;</code> Redirects output from a command to a file. Example: <ul> <li> <code>ls &gt; myfiles.txt</code>: Redirects the output of the ls command into a text file called \"myfiles\" </li> </ul> <code>&gt; /dev/null</code> A special case of <code>&gt;</code> suppress normal output by sending it the the \"null file\" <code>2&gt; err 1&gt; out</code> Sends errors from a command to the file err and normal output to out. Note: <ul> <li> <code>1&gt; both 2&gt;&amp;1</code>: Sends output and errors to the file \"both\" </li> </ul> <code>&gt;&gt;</code> similar to <code>&gt;</code> but appends to the file <code>cat</code> Read a file and send output to the terminal. Note: <ul> <li> <code>cat file1 file2 &gt; combined</code>: Concatenates the two files together </li> </ul> <code>sort</code> Outputs a sorted version of a file <code>|</code> A pipe takes the standard output of one command and passes it as the input to another. Example: <ul> <li> <code>cat myData | sort</code>: Outputs the contents of \"myData\" to the sort command </li> </ul> <code>head</code> Shows the start of a file <code>tail</code> Shows the end of a file <code>which</code> Shows the location of a command. Of note, you cannot use <code>which</code> with builtin shell commands. <code>exit</code> Exits out of a shell. Can be used to logout. <code>logout</code> Logs out of a shell. <code>grep</code> Searches for a string in output or file(s) <code>history</code> Displays the command history <code>source</code> Read  and  execute  commands  from a script and executes it in the current shell. Note: <ul> <li> <code>.</code> by itself is a synonym of <code>source</code> </li> </ul> <code>find</code> Locates files/directories with particular characteristics; a very useful command. Examples: <ul> <li> <code>find . -name \"\\*xyz\\*\"</code>: Finds all files in current the directory and below that have names that contain \"xyz\" </li> <li> <code>find . -type f</code>: Finds all files in current the directory and below </li> <li> <code>find . -type d</code>: Finds all directories in current the directory and below </li> <li> <code>find . -newermt `date +\"%Y-%m-%d\"`</code>: Finds files that have changed today </li> </ul> <code>compgen</code> Shows various sets of commands. Examples: <ul> <li> <code>compgen -a</code>: Lists all bash aliases </li> <li> <code>compgen -b</code>: Lists all built-in commands </li> <li> <code>compgen -A function</code>: Lists all bash functions </li> <li> <code>compgen -k</code>: Lists all bash keywords </li> <li> <code>compgen -c</code>: Lists all commands available to you </li> </ul>"},{"location":"Documentation/Environment/Building_Packages/","title":"Building packages on NREL HPC for individual or project use.","text":"<p>This training module will walk through how to build a reasonably complex package, OpenMPI, and deploy it for use by yourself or members of a project.</p> <ol> <li> <p>Acquire the package and set up for build</p> </li> <li> <p>Configure, build, and install the package</p> </li> <li> <p>Setting up your own environment module</p> </li> </ol>"},{"location":"Documentation/Environment/Building_Packages/#why-build-your-own-application","title":"Why build your own application?","text":"<ul> <li> <p>Sometimes, the package version that you need, or the capabilities you want, are only available as source code.</p> </li> <li> <p>Other times, a package has dependencies on other ones with application programming interfaces that change rapidly.  A source code build might have code to adapt to the (older, newer) libraries you have available, whereas a binary distribution will likely not.  In other cases, a binary distribution may be associated with a particular Linux distribution and version different from Kestrel's or Eagle's.  One example is a package for Linux version X+1 (with a shiny new libc).  If you try to run this on Linux version X, you will almost certainly get errors associated with the GLIBC version required.  If you build the application against your own, older libc version, those dependencies are not created.</p> </li> <li> <p>Performance; for example, if a more performant numerical library is available, you may be able to link against it.  A pre-built binary may have been built against a more universally available but lower performance library.  The same holds for optimizing compilers.</p> </li> <li> <p>Curiosity to know more about the tools you use.</p> </li> <li> <p>Pride of building one's tools oneself.</p> </li> <li> <p>For the sheer thrill of building packages.</p> </li> </ul>"},{"location":"Documentation/Environment/Building_Packages/acquire/","title":"Getting the package","text":"<ol> <li> <p>Change working directory to the location where you'll build the package. A convenient location is <code>/scratch/$USER</code>, which we'll use for this example. <code>cd /scratch/$USER</code></p> </li> <li> <p>OpenMPI can be found at https://www.open-mpi.org/software/ompi/. This will automatically redirect you to the latest version, but older releases can be seen in the left menu bar. For this, choose version 4.1.</p> </li> <li> <p>There are several packaging options.  Here, we'll get the bzipped tarball <code>openmpi-4.1.0.tar.bz2</code>.  You can either download it to a local machine (laptop) and then <code>scp</code> the file over to the HPC cluster, or get it directly on the supercomputer using <code>wget</code>. <pre><code>wget https://download.open-mpi.org/release/open-mpi/v4.1/openmpi-4.1.0.tar.bz2\n</code></pre> You should now have a compressed tarball in your scratch directory.</p> </li> <li>List the contents of the tarball before unpacking.  This is very useful to avoid inadvertently filling a directory with gobs of files and directories when the tarball has them at the top of the file structure), <pre><code>tar -tf openmpi-4.1.0.tar.bz2\n</code></pre></li> <li>Unpack it via <pre><code>tar -xjf openmpi-4.1.0.tar.bz2\n</code></pre> If you're curious to see what's in the file as it unpacks, add the <code>-v</code> option. </li> <li>You should now have an <code>openmpi-4.1.0</code> directory.  <code>cd openmpi-4.1.0</code>, at which point you are in the top level of the package distribution. You can now proceed to configuring, making, and installing.</li> </ol>"},{"location":"Documentation/Environment/Building_Packages/config_make_install/","title":"Configuring your build","text":"<ol> <li> <p>We will illustrate a package build that relies on the popular autotools system.  Colloquially, this is the <code>configure; make; make install</code> process that is often encountered first by those new to package builds on Linux.  Other build systems like CMake (which differ primarily in the configuration steps) won't be covered.  If you need to build a package that relies on CMake, please contact hpc-help@nrel.gov for assistance.</p> </li> <li> <p>We'll use GCC version 8.4.0 for this illustration, so load the associated module first (i.e., <code>gcc/8.4.0</code>).</p> </li> </ol> Building on Kestrel  <p>You can use any version of GCC available to you on Kestrel.   The paths in step 3 are for Eagle, please make the necessary changes for Kestrel.</p> <ol> <li> <p>Now that you've acquired and unpacked the package tarball and changed into the top-level directory of the package, you should see a script named \"configure\".  In order to see all available options to an autotools configure script, use <code>./configure -h</code> (don't forget to include the <code>./</code> explicit path, otherwise the script will not be found in the default Linux search paths, or worse, a different script will be found).</p> <p>We will build with the following command:  <pre><code>./configure --prefix=/scratch/$USER/openmpi/4.1.0-gcc-8.4.0 --with-slurm --with-pmi=/nopt/slurm/current --with-gnu-ld --with-lustre --with-zlib --without-psm --without-psm2 --with-ucx --without-verbs --with-hwloc=external --with-hwloc-libdir=/nopt/nrel/apps/base/2020-05-12/spack/opt/spack/linux-centos7-x86_64/gcc-8.4.0/hwloc-1.11.11-mb5lwdajmllvrdtwltwe3r732aca76ny/lib --enable-cxx-exceptions --enable-mpi-cxx --enable-mpi-fortran --enable-static LDFLAGS=\"-L/nopt/nrel/apps/base/2020-05-12/spack/opt/spack/linux-centos7-x86_64/gcc-8.4.0/11.0.2-4x2ws7fkooqbrerbsnfbzs6wyr5xutdk/lib64 -L/nopt/nrel/apps/base/2020-05-12/spack/opt/spack/linux-centos7-x86_64/gcc-8.4.0/cuda-11.0.2-4x2ws7fkooqbrerbsnfbzs6wyr5xutdk/lib64 -Wl,-rpath=/nopt/nrel/apps/base/2020-05-12/spack/opt/spack/linux-centos7-x86_64/gcc-8.4.0/hwloc-1.11.11-mb5lwdajmllvrdtwltwe3r732aca76ny/lib -Wl,-rpath=/nopt/nrel/apps/base/2020-05-12/spack/opt/spack/linux-centos7-x86_64/gcc-8.4.0/cuda-11.0.2-4x2ws7fkooqbrerbsnfbzs6wyr5xutdk/lib64\" CPPFLAGS=-I/nopt/nrel/apps/base/2020-05-12/spack/opt/spack/linux-centos7-x86_64/gcc-8.4.0/hwloc-1.11.11-mb5lwdajmllvrdtwltwe3r732aca76ny/include\n</code></pre> These options are given for the following reasons.</p> <ul> <li><code>--prefix=</code> : This sets the location that \"make install\" will ultimately populate. If this isn't given, generally the default is to install into /usr or /usr/local, both of which require privileged access. We'll set up the environment using environment modules to point to this custom location.</li> <li><code>--with-slurm</code> : Enables the interface with the Slurm resource manager</li> <li><code>--with-pmi=</code> : Point to the Process Management Interface, the abstraction layer for MPI options</li> <li><code>--with-gnu-ld</code> : Letting the build system know that linking will be done with GNU's linker, rather than a commercial or alternative open one.</li> <li><code>--with-lustre</code> : Enable Lustre features</li> <li><code>--with-zlib</code> : Enable compression library</li> <li><code>--without-psm[2]</code> : Explicitly turn off interfaces to Intel's Performance Scaled Messaging for the now-defunct Omni-Path network</li> <li><code>--with-ucx=</code> : Point to UCX, an intermediate layer between the network drivers and MPI</li> <li><code>--without-verbs=</code> : For newer MPIs, communications go through UCX and/or libfabric, not directly to the Verbs layer</li> <li><code>--with-hwloc[-libdir]=</code> : Point to a separately built hardware localization library for process pinning</li> <li><code>--enable-cxx-exceptions</code>, <code>--enable-mpi-cxx</code> : Build the C++ interface for the libraries</li> <li><code>--enable-mpi-fortran</code> : Build the Fortran interface for the libraries</li> <li><code>--enable-static</code> : Build the .a archive files for static linking of applications</li> <li><code>LDFLAGS</code> : -L options point to non-standard library locations. -Wl,-rpath options embed paths into the binaries, so that having LD_LIBRARY_PATH set correctly is not necessary (i.e., no separate module for these components).</li> <li><code>CPPFLAGS</code> : Point to header files in non-standard locations.</li> </ul> <p>NOTE: The CUDA paths are not needed for CUDA function per se, but the resulting MPI errors out without setting them.  There appears to be a lack of modularity that sets up a seemingly unneeded dependency.</p> <p>After lots of messages scroll by, you should be returned to a prompt following a summary of options.  It's not a bad idea to glance through these, and make sure everything makes sense and is what you intended.</p> </li> <li> <p>Now that the build is configured, you can \"make\" it.  For packages that are well integrated with automake, you can speed the build up by parallelizing it over multiple processes with the <code>-j #</code> option.  If you're building this on a compute node, feel free to set this option to the total number of cores available.  On the other hand, if you're using a login node, be a good citizen and leave cores available for other users (i.e., don't use more than 4; Arbiter should limit access at any rate regardless of this setting).</p> <pre><code>make -j 4\n</code></pre> </li> <li> <p>Try a <code>make check</code> and/or a <code>make test</code>.  Not every package enables these tests, but if they do, it's a great idea to run these sanity checks to find if your build is perfect, maybe-good-enough, or totally wrong before building lots of other software on top of it.</p> </li> <li> <p>Assuming checks passed if present, it's now time for <code>make install</code>.  Assuming that completes without errors, you can move onto creating an environment module to use your new MPI library.</p> </li> </ol>"},{"location":"Documentation/Environment/Building_Packages/modules/","title":"Setting up your module","text":"<ol> <li> <p>Now that the package has been installed to your preferred location, we can set up an environment module.</p> <p>a. If this is your first package, then you probably need to create a place to collect modulefiles.  For example, <code>mkdir -p /scratch/$USER/modules/default</code>.</p> <p>b. You can look at the systems module collection(s), e.g., <code>/nopt/nrel/apps/modules/default/modulefiles</code> on Eagle or <code>/nopt/nrel/apps/modules/default</code> on Kestrel, to see how modules are organized from a filesystem perspective.  In short, each library, application, or framework has its own directory in the <code>modulefiles</code> directory, and the modulefile itself sits either in this directory, or one level lower to accomodate additional versioning.  In this example, there is the MPI version (4.1.0), as well as the compiler type and version (GCC 8.4.0) to keep track of.  So, we'll make a <code>/scratch/$USER/modules/default/openmpi/4.1.0</code> directory, and name the file by the compiler version used to build (gcc-8.4.0).  You're free to modify this scheme to suit your own intentions.</p> <p>c. In the <code>openmpi/4.1.0/gcc840</code> directory you just made, or whatever directory name you chose, goes the actual modulefile.  It's much easier to copy an example from the system collection than to write one de novo, so you can do</p> On Eagle <pre><code>cp /nopt/nrel/apps/modules/default/modulefiles/openmpi/4.0.4/gcc-8.4.0.lua /scratch/$USER/modules/default/openmpi/4.1.0/.\n</code></pre> On Eagle <pre><code>cp /nopt/nrel/apps/modules/default/compilers_mpi/openmpi/4.1.5-gcc /scratch/$USER/modules/default/openmpi/4.1.0/.\n</code></pre> OpenMpi modulefile on Kestrel <p>Please note that the OpenMpi modulefile on Kestrel is of TCL type  It is not necessary for you to know the language to modify our examples. </p> <p>The Lmod modules system uses the Lua language natively for module code.  Tcl modules will also work under Lmod, but don't offer quite as much flexibility.</p> <p>d. For this example, (a) the OpenMPI version we're building is 4.1.0 instead of 4.0.4 on Eagle or 4.1.5 on Kestrel, and (b) the location is in <code>/scratch/$USER</code>, rather than <code>/nopt/nrel/apps</code>.  So, edit <code>/scratch/$USER/modules/default/openmpi/4.1.0/gcc-8.4.0.lua</code> to make the required changes.  Most of these changes only need to be made at the top of the file; variable definitions take care of the rest.</p> <p>e. Now you need to make a one-time change in order to see modules that you put in this collection (<code>/scratch/$USER/modules/default</code>).  In your <code>$HOME/.bash_profile</code>, add the following line near the top:</p> <pre><code>module use /scratch/$USER/modules/default\n</code></pre> <p>Obviously, if you've built packages before and enabled them this way, you don't have to do this again!</p> </li> <li> <p>Now logout, log back in, and you should see your personal modules collection with a brand new module.</p> <pre><code>[$USER@el1 ~]$ module avail\n\n---------------------------------- /scratch/$USER/modules/default -----------------------------------\nopenmpi/4.1.0/gcc-8.4.0\n</code></pre> <p>Notice that the \".lua\" extension does not appear--the converse is also true, if the extension is missing it will not appear via module commands! As a sanity check, it's a good idea to load the module, and check that an executable file you know exists there is in fact on your PATH:</p> <pre><code>[$USER@el1 ~]$ module load openmpi/4.1.0/gcc-8.4.0\n[$USER@el1 ~]$ which mpirun\n/scratch/$USER/openmpi/4.1.0-gcc-8.4.0/bin/mpirun\n</code></pre> </li> </ol>"},{"location":"Documentation/Environment/Customization/conda/","title":"Conda","text":""},{"location":"Documentation/Environment/Customization/conda/#why-conda","title":"Why Conda?","text":"<p>Conda is a package manager which allows you to easily create and switch between different software environments in different languages for different purposes.  With Conda, it's easy to:</p> <ul> <li> <p>Manage different (potentially conflicting) versions of the same software without complication</p> </li> <li> <p>Quickly stand up even complicated dependencies for stacks of software</p> </li> <li> <p>Share your specific programming environment with others for reproducible results </p> </li> </ul>"},{"location":"Documentation/Environment/Customization/conda/#conda-module","title":"Conda Module","text":"<p>To use Conda on an NREL HPC cluster, you will need to load the appropriate module. To find the Conda module name on the system you are using, run <code>module spider conda</code>. Then, <code>module load &lt;module name&gt;</code>. </p>"},{"location":"Documentation/Environment/Customization/conda/#creating-environments-by-name","title":"Creating Environments by Name","text":"<p>To create a basic Conda environment, we'll start by running</p> <pre><code>conda create --name mypy python\n</code></pre> <p>where the <code>--name</code> option (or the shortened <code>-n</code>) means the environment will be specified by name and <code>myenv</code> will be the name of the created environment.  Any arguments following the environment name are the packages to be installed.</p> <p>To specify a specific version of a package, simply add the version number after the \"=\" sign</p> <pre><code>conda create --name mypy37 python=3.7\n</code></pre> <p>You can specify multiple packages for installation during environment creation</p> <pre><code>conda create --name mynumpy python=3.7 numpy\n</code></pre> <p>Conda ensures dependencies are satisfied when installing packages, so the version of the numpy package installed will be consistent with Python 3.7 (and any other packages specified).</p> <p>Tip</p> <p>It\u2019s recommended to install all the packages you want to include in an environment at the same time to help avoid dependency conflicts.</p>"},{"location":"Documentation/Environment/Customization/conda/#environment-navigation","title":"Environment Navigation","text":"<p>To see a list of all existing environments (useful to confirm the successful creation of a new environment):</p> <pre><code>conda env list\n</code></pre> <p>To activate your new environment:</p> <pre><code>conda activate mypy\n</code></pre> <p>Your usual command prompt should now be prefixed with <code>(mypy)</code>, which helps keep track of which environment is currently activated.</p> <p>To see which packages are installed from within a currently active environment:</p> <pre><code>conda list\n</code></pre> <p>When finished with this programming session, deactivate your environment with:</p> <pre><code>conda deactivate\n</code></pre>"},{"location":"Documentation/Environment/Customization/conda/#creating-environments-by-location","title":"Creating Environments by Location","text":"<p>Creating environments by location is especially helpful when working on the HPC systems, as the default location is your <code>/home/&lt;username&gt;/</code> directory, which is limited to 50 GB.  To create a Conda environment somewhere besides the default location, use the <code>--prefix</code> flag (or the shortened <code>-p</code>) instead of <code>--name</code> when creating:</p> <pre><code>conda create --prefix /path/to/mypy python=3.7 numpy\n</code></pre> <p>This re-creates the python+numpy environment from earlier, but with all downloaded packages stored in the specified location.</p> <p>Warning</p> <p>Keep in mind that <code>/scratch/&lt;username&gt;</code> is temporary, and files are purged after 28 days of inactivity.</p> <p>Unfortunately, placing an environment outside of the default folder means that it needs to be activated with the full path (<code>conda activate /path/to/mypy</code>) and will show the full path rather than the environment name at the command prompt. </p> <p>To fix the cumbersome command prompt, simply modify the <code>env_prompt</code> setting in your <code>.condarc</code> file:</p> <pre><code>conda config --set env_prompt '({name}) '\n</code></pre> <p>Note that <code>'({name})'</code> is not a placeholder for your desired environment name but text to be copied literally.  This will edit your <code>.condarc</code> file if you already have one or create a <code>.condarc</code> file if you do not. For more on modifying your <code>.condarc</code> file, check out the User Guide.  Once you've completed this step, the command prompt will show the shortened name (mypy, in the previous example).</p>"},{"location":"Documentation/Environment/Customization/conda/#managing-conda-environments","title":"Managing Conda Environments","text":"<p>Over time, it may become necessary to add additional packages to your environments.  New packages can be installed in the currently active environment with:</p> <pre><code>conda install pandas\n</code></pre> <p>Conda will ensure that all dependencies are satisfied which may include upgrades to existing packages in this repository.  To install packages from other sources, specify the <code>channel</code> option:</p> <pre><code>conda install --channel conda-forge fenics\n</code></pre> <p>To add a pip-installable package to your environment:</p> <pre><code>conda install pip\npip &lt;pip_subcommand&gt;\n</code></pre> <p>Warning: Mixing Conda and Pip</p> <p>Issues may arise when using pip and conda together. When combining conda and pip, it is best to use an isolated conda environment. Only after conda has been used to install as many packages as possible should pip be used to install any remaining software. If modifications are needed to the environment, it is best to create a new environment rather than running conda after pip. When appropriate, conda and pip requirements should be stored in text files.</p> <p>For more information, see the User Guide.</p> <p>We can use <code>conda list</code> to see which packages are currently installed, but for a more version-control-flavored approach:</p> <pre><code>conda list --revisions\n</code></pre> <p>which shows changes to the environment over time.  To revert back to a previous environment</p> <pre><code>conda install --revision 1\n</code></pre> <p>To remove packages from the currently activated environment:</p> <pre><code>conda remove pkg1\n</code></pre> <p>To completely remove an environment and all installed packages:</p> <pre><code>conda remove --name mypy --all\n</code></pre> <p>Conda environments can become large quickly due to the liberal creation of cached files.  To remove these files and free up space you can use</p> <pre><code>conda clean --all\n</code></pre> <p>or to simply preview the potential changes before doing any actual deletion</p> <pre><code>conda clean --all --dry-run\n</code></pre>"},{"location":"Documentation/Environment/Customization/conda/#sharing-conda-environments","title":"Sharing Conda Environments","text":"<p>To create a file with the the exact \"recipe\" used to create the current environment:</p> <pre><code>conda env export &gt; environment.yaml\n</code></pre> <p>In practice, this recipe may be overly-specific to the point of creating problems on different hardware.  To save an abbreviated version of the recipe with only the packages you explicitly requested: </p> <pre><code>conda env export --from-history &gt; environment.yaml\n</code></pre> <p>To create a new environment with the recipe specified in the .yaml file:</p> <pre><code>conda env create --name mypyhpc --file environment.yaml\n</code></pre> <p>If a name or prefix isn't specified, the environment will be given the same name as the original environment the recipe was exported from (which may be desirable if you're moving to a different computer).</p>"},{"location":"Documentation/Environment/Customization/conda/#speed-up-dependency-solving","title":"Speed up dependency solving","text":"<p>To speed up dependency solving, substitute the mamba command for conda.  Mamba is a dependency solver written in C++ designed to speed up the conda environment solve.</p> <pre><code>mamba create --prefix /path/to/mypy python=3.7 numpy\n</code></pre>"},{"location":"Documentation/Environment/Customization/conda/#hpc-considerations","title":"HPC Considerations","text":""},{"location":"Documentation/Environment/Customization/conda/#migrating-from-local-to-hpc-system","title":"Migrating from local to HPC system","text":"<p>Interacting with your Conda environments on the HPC systems should feel exactly the same as working on your desktop.  An example desktop-to-HPC workflow might go:</p> <ol> <li>Create the environment locally</li> <li>Verify that environment works on a minimal working example</li> <li>Export local environment file and copy to HPC system (<code>conda env export &gt; environment.yaml</code>)</li> <li>Duplicate local environment on HPC system (<code>conda env create -f environment.yaml</code>)</li> <li>Execute production-level runs on HPC system:</li> </ol> <pre><code>#!/bin/bash \n#SBATCH --nodes=1\n#SBATCH --time=60\n#SBATCH --account=&lt;project_handle&gt;\n\nmodule purge\nmodule load conda\nconda activate mypy\n\npython my_main.py\n</code></pre>"},{"location":"Documentation/Environment/Customization/conda/#where-to-store-conda-environments","title":"Where to store Conda environments","text":"<p>By default, the conda module uses the home directory for named environments. This can cause problems on the HPC systems because conda environments can require a lot of storage space, and home directories have a quota of 50GB. Additionally, the home filesystem is not designed to handle heavy I/O loads, so if you're running a lot of jobs or large multi-node jobs calling conda environments that are stored in home, it can strain the filesystem. </p> <p>The conda module uses <code>/scratch/$USER/.conda-pkgs</code> for package caches by default to avoid filling up the home directory with cached conda data. If you would like to change this location, you can call <code>export CONDA_PKGS_DIRS=PATH_NAME</code> to specify somewhere to store downloads and cached files such as <code>/projects/&lt;allocation handle&gt;/$USER/.conda-pkgs</code>. We don't recommend using your home directory since this data can use a lot of space. </p> <p>Some ways to change the default storage location for conda environments and packages:</p> <ul> <li> <p>Use the <code>-p PATH_NAME</code> switch when creating or updating your environment.  Make sure <code>PATH_NAME</code> isn't in the home directory. Keep in mind files in /scratch are deleted after about a month of inactivity.</p> </li> <li> <p>Similarly, you can specify the directory in which environments are stored by default. To do this, either set the <code>CONDA_ENVS_PATH</code> environment variable, or use the <code>--prefix</code> option as described above. </p> </li> </ul> <p>Warning</p> <p>Overriding the default location for the environment and package cache directories in your <code>.condarc</code> file by setting <code>envs_dirs</code> and <code>pkgs_dirs</code> won't work as expected on Kestrel. When the conda module is loaded, it overrides these settings. Instead, set the environment variables after you load the conda module as described above. </p> <p>Following are some guidelines and suggestions regarding where to store environments:</p> Path When to use Caveats <code>/home</code> <code>$HOME/.conda</code> is the default location for environments. For one-off environments, or if you don't create environments often, this is a reasonable location for your environments and doesn't require any extra flags or parameters. On systems such as Kestrel, <code>$HOME</code> is limited to 50 GB.  Not suited for multi-node jobs. <code>/scratch</code> <code>/scratch</code> or <code>/projects</code> are well-suited for multiple-node jobs because these locations provide enhanced filesystem performance for parallel access. The contents of <code>/scratch</code> are purged after 28 days of inactivity. <code>/projects</code> Ideal location for storing environments that will be shared with colleagues that are working on the same project. Storage under <code>/projects</code> is contingent on having an HPC project allocation, and the project allocation has its own storage quota."},{"location":"Documentation/Environment/Customization/conda/#cheat-sheet-of-common-commands","title":"Cheat Sheet of Common Commands","text":"Task ... outside environment ... inside environment Create by name <code>conda create -n mypy pkg1 pkg2</code> N/A Create by path <code>conda create -p path/to/mypy pkg1 pkg2</code> N/A Create by file <code>conda env create -f environment.yaml</code> N/A Show environments <code>conda env list</code> N/A Activate <code>conda activate mypy</code> N/A Deactivate N/A <code>conda deactivate</code> Install New Package <code>conda install -n mypy pkg1 pkg2</code> <code>conda install pkg1 pkg2</code> List All Packages <code>conda list -n mypy</code> <code>conda list</code> Revision Listing <code>conda list --revisions -n mypy</code> <code>conda list --revisions</code> Export Environment <code>conda env export -n mypy &gt; environment.yaml</code> <code>conda env export &gt; environment.yaml</code> Remove Package <code>conda remove -n mypy pkg1 pkg2</code> <code>conda remove pkg1 pkg2</code>"},{"location":"Documentation/Machine_Learning/","title":"Machine Learning","text":"<p>Machine learning refers to a set of techniques and algorithms that enable computers to automatically learn from data and improve their performance on a specific task over time. Types of machine learning methods include, but are not limited to, supervised learning (algorithms trained on labeled datasets), unsupervised learning (algorithms trained on unlabeled datasets), and reinforcement learning (learning by trial and error). The Computational Science Center at NREL conducts research in these types of machine learning, and also supports the use of machine learning software on Kestrel.</p>"},{"location":"Documentation/Machine_Learning/#getting-started","title":"Getting Started","text":"<p>This section provides basic examples for getting started with two popular machine learning libraries: PyTorch and TensorFlow. Both examples use Anaconda environments, so if you are not familiar with their use please refer to the NREL HPC page on using Conda environments and also the Conda guide to managing environments. </p>"},{"location":"Documentation/Machine_Learning/#getting-started-with-pytorch","title":"Getting started with PyTorch","text":"<p>To begin, we will outline basic steps for building a simple CPU-based conda environment for PyTorch. First, load the anaconda module and create a new conda environment: <pre><code>module load anaconda3\n\nconda create -p /projects/YOUR_PROJECT/YOUR_USER_NAME_HERE/FOLDER_FOR_CONDA_ENVIRONMENTS/pt python=3.9\n</code></pre> Answer yes to proceed, and you should end up with directions for starting your conda environment pt. Note that these instructions place your environment in the specified /projects folder. This is advisable, as opposed to installing conda environments in their default location in your home directory. See our Conda documentation for more information.</p> <p>Activate the pt conda environment and install PyTorch into the active conda environment: <pre><code>conda activate /projects/YOUR_PROJECT/YOUR_USER_NAME_HERE/FOLDER_FOR_CONDA_ENVIRONMENTS/pt\n\nconda install pytorch torchvision torchaudio cpuonly -c pytorch\n</code></pre> Answer yes to proceed, and you should be up and running with PyTorch! The PyTorch webpage has great resources for getting started, including resources on learning the basics and PyTorch recipes.</p>"},{"location":"Documentation/Machine_Learning/#getting-started-with-tensorflow","title":"Getting started with TensorFlow","text":"<p>Getting started with TensorFlow is similar to the process for PyTorch. The first step is to construct an empty conda environment to work in: <pre><code>module load anaconda3\n\nconda create -p /projects/YOUR_PROJECT/YOUR_USER_NAME_HERE/FOLDER_FOR_CONDA_ENVIRONMENTS/tf python=3.9\n</code></pre> Subsequently, activate the tf conda environment, ensure you are running the latest version of pip in your environment, and install the CPU only version of TensorFlow using pip: <pre><code>conda activate /projects/YOUR_PROJECT/YOUR_USER_NAME_HERE/FOLDER_FOR_CONDA_ENVIRONMENTS/tf\npip install --upgrade pip\npip install tensorflow-cpu\n</code></pre> You should now be up and running with a TensorFlow! Similar to PyTorch, the TensorFlow webpage has lots of great resources for getting started, including tutorials, basic examples, and more! </p>"},{"location":"Documentation/Machine_Learning/#example-job-script","title":"Example Job Script","text":"PyTorch or TensorFlow shared partition CPU example <pre><code>#!/bin/bash\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=1\n#SBATCH --mem=64G\n#SBATCH --cpus-per-task=26\n#SBATCH --partition=shared\n#SBATCH --account=&lt;your account&gt;\n#SBATCH --time=00:30:00\n\nmodule load conda\nconda activate /projects/&lt;your_project&gt;/&lt;conda_envs_dir&gt;/&lt;pt_or_tf&gt;\n\nsrun python &lt;your_code&gt;.py\n</code></pre> <p>Note</p> <p>This Getting Started section is only scratching the surface of ML libraries and resources that can be used on Kestrel. Tools such as LightGBM, XGBoost, and scikit-learn work well with conda environments, and other tools such as Flux for the Julia Language can be used on Kestrel as well.</p> <p>Once you have completed your batch file, submit using <pre><code>sbatch &lt;your_batch_file_name&gt;.sb\n</code></pre></p>"},{"location":"Documentation/Machine_Learning/#advanced-gpu","title":"Advanced (GPU)","text":"<p>The above examples show how to build CPU-based conda environments. The following section covers how to build and run PyTorch and TensorFlow for use with GPUs on Kestrel. </p> <p>To install either PyTorch or TensorFlow for use with GPUs on Kestrel, the first step is to load the anaconda module on the GPU node using <code>module load conda</code>. Once the anaconda module has been loaded, create a new environment in which to install PyTorch or TensorFlow, e.g.,</p> Creating and activating a new conda environment <pre><code>conda create --prefix /projects/&lt;your-project-name&gt;/&lt;your-user-name&gt;/&lt;conda-env-dir&gt;/pt python=3.9\nconda activate /projects/&lt;your-project-name&gt;/&lt;your-user-name&gt;/&lt;conda-env-dir&gt;/&lt;pt or tf&gt;\n</code></pre> <p>Note</p> <p>If you are not familiar with using Anaconda environments please refer to the NREL HPC page on using Conda environments and also the Conda guide to managing environments.</p>"},{"location":"Documentation/Machine_Learning/#installing-tensorflow-on-kestrel","title":"Installing TensorFlow on Kestrel","text":"<p>Presented below are instructions for installing TensorFlow following in the <code>pip</code> install instructions found here: TensorFlow. For optimized TensorFlow performance, we recommend using a containerized version of TensorFlow.</p> <p>Once the conda environment created above has been activated, you can install TensorFlow using the <code>pip</code> based approach described in TensorFlow, but with a couple modifications. Instead of using the <code>cudatoolkit</code>, we recommend using the nvhpc programming environment accessed using the module <code>Prg-Env-nvhpc</code>. Also, there is a module for <code>cudnn</code>. Using these two modules, we install TensorFlow with the following commands: </p> Installing TensorFlow using pip <pre><code>module load PrgEnv-nvhpc\nmodule load cudnn\npython3 -m pip install tensorflow[and-cuda]\n</code></pre>"},{"location":"Documentation/Machine_Learning/#installing-pytorch-on-kestrel","title":"Installing PyTorch on Kestrel","text":"<p>Once the environment has been activated, you can install PyTorch using the standard approach found under the Get Started tab of the PyTorch website, e.g., using <code>pip</code>,</p> Installing PyTorch using pip <p><code>pip3 install torch torchvision torchaudio</code></p> <p>or using <code>conda,</code></p> Installing PyTorch using conda specifying CUDA 12.4 <p><code>conda install pytorch torchvision torchaudio pytorch-cuda=12.4 -c pytorch -c nvidia</code></p> <p>Note</p> <p>We recommend installing software for GPU jobs using the GPU nodes. There are two GPU login nodes available on Kestrel. </p>"},{"location":"Documentation/Machine_Learning/#running-a-pytorch-or-tensorflow-batch-job-on-kestrel-gpu","title":"Running a PyTorch or TensorFlow Batch Job on Kestrel - GPU","text":"Sample job script: Kestrel - Shared (partial) GPU node <pre><code>#!/bin/bash\n#SBATCH --account=&lt;your-account-name&gt; \n#SBATCH --nodes=1\n#SBATCH --gpus=1 \n#SBATCH --ntasks-per-node=1\n#SBATCH --mem=96G\n#SBATCH --cpus-per-task=32\n#SBATCH --time=00:30:00\n#SBATCH --job-name=&lt;your-job-name&gt;\n\nmodule load conda\nconda activate /projects/&lt;your-project-name&gt;/&lt;your-user-name&gt;/&lt;conda-env-dir&gt;/&lt;pt or tf&gt;\n\nsrun python &lt;your-pytorch or tensorflow-code&gt;.py\n</code></pre>"},{"location":"Documentation/Machine_Learning/#tensorflow-example","title":"TensorFlow Example","text":"<p>Find below a simple neural network example using the MNIST data set for getting started using TensorFlow with Kestrel GPUs. This example was based on TensorFlow's quick start documentation found here.</p> MNIST example <pre><code>import tensorflow as tf\n\n# Select a standard data set and normalize\nmnist = tf.keras.datasets.mnist    \n(x_train, y_train),(x_test, y_test) = mnist.load_data()\nx_train, x_test = x_train / 255.0, x_test / 255.0\n\n# Set up and compile a model \nmodel = tf.keras.models.Sequential([\n    tf.keras.layers.Flatten(input_shape=(28, 28)),  tf.keras.layers.Dense(128, activation='relu\u2019), \n    tf.keras.layers.Dropout(0.2), tf.keras.layers.Dense(10, activation='softmax')]) \n\nmodel.compile(optimizer='adam\u2019, \nloss='sparse_categorical_crossentropy\u2019, metrics=['accuracy'])\n\n# Fit model to training data and evaluate on test data\nmodel.fit(x_train, y_train, epochs=5)\n\nmodel.evaluate(x_test, y_test)\n</code></pre>"},{"location":"Documentation/Machine_Learning/#pytorch-example","title":"PyTorch Example","text":"<p>Below we present a simple convolutional neural network example for getting started using PyTorch with Kestrel GPUs. The original, more detailed version of this example can be found in the pytorch tutorials repo here.</p> CIFAR10 example <pre><code>import torch\nimport torchvision\nimport torchvision.transforms as transforms\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\n# Check if there are GPUs. If so, use the first one in the list\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\nprint(device)\n\n# Load data and normalize\ntransform = transforms.Compose(\n    [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n\nbatch_size = 4\ntrainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n                                    download=True, transform=transform)\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n                                      shuffle=True, num_workers=2)\ntestset = torchvision.datasets.CIFAR10(root='./data', train=False,\n                                   download=True, transform=transform)\ntestloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n                                     shuffle=False, num_workers=2)\n\n# Define the CNN\nclass Net(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(3, 6, 5)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n        self.fc2 = nn.Linear(120, 84)\n        self.fc3 = nn.Linear(84, 10)\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = torch.flatten(x, 1) # flatten all dimensions except batch\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\nnet = Net()\n\n# send the network to the device\n# If you want to use data parallelism across multiple GPUs, uncomment if statement below\n#if torch.cuda.device_count() &gt; 1:\n#    net = nn.DataParallel(net)\n\nnet.to(device)\n\n# Define loss function and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n\n# Train the network\nfor epoch in range(2):  # loop over the dataset multiple times\n\n    running_loss = 0.0\n    for i, data in enumerate(trainloader, 0):\n        # get the inputs; data is a list of [inputs, labels]\n        # inputs, labels = data # setup without device\n        inputs, labels = data[0].to(device), data[1].to(device)\n\n        # zero the parameter gradients\n        optimizer.zero_grad()\n\n        # forward + backward + optimize\n        outputs = net(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        # print statistics\n        running_loss += loss.item()\n        if i % 2000 == 1999:    # print every 2000 mini-batches\n            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n            running_loss = 0.0\n\nprint('Finished Training')\n</code></pre> <p>Note</p> <p>Currently, this code will run on a single GPU, specifically the GPU denoted <code>cuda:0</code>. To use multiple GPUs via data parallelism, uncomment the two lines above the <code>net.to(device)</code> command. Furthermore, use of multiple GPUs require requesting multiple GPUs for the batch or interactive job.</p> <p>Note</p> <p>To better observe the multi-GPU peformance of the above example, you can change the size of the CNN. For example, by increasing the size of the second argument in the definition of <code>self.conv1</code> and the first argument in <code>self.conv2</code>, you can increase the size of the network and use more resources for training.</p>"},{"location":"Documentation/Machine_Learning/Containerized_TensorFlow/","title":"Containerized TensorFlow","text":""},{"location":"Documentation/Machine_Learning/Containerized_TensorFlow/#tensorflow-with-gpu-support-apptainer","title":"TensorFlow with GPU support - Apptainer","text":"<p>This Apptainer image supplies TensorFlow 2.15.0 optimized for use with GPU nodes running CUDA &gt; 12.3 (which works with Kestrel's H100s). It also includes opencv, numpy, pandas, seaborn, scikit-learn, and a number of other Python libraries. More information about Tensorflow's containerized images can be found on DockerHub.</p> <p>For more information on Apptainer in general, on please see: Containers.</p>"},{"location":"Documentation/Machine_Learning/Containerized_TensorFlow/#quickstart","title":"Quickstart","text":"<p>After allocating a job, note that you will have to bind mount <code>/nopt</code> (where the image lives) as well as the parent directory of where you are working from (e.g., <code>/scratch</code> or <code>/projects</code>)</p> <pre><code># Get allocation\nsalloc --gres=gpu:2 -N 1 --mem=80G -n 32 -A &lt;allocation handle&gt; -t 01:00:00 -p debug\n# Run Apptainer in srun environment\nmodule load apptainer\n# Note that you will have to bind mount /nopt (where the image lives) as well as the parent directory of where you are working from (e.g., /scratch or /projects)\ncd /projects/&lt;MY_HPC_PROJECT&gt;\nsrun --gpus=2 --pty apptainer shell -B /nopt:/nopt -B /projects:/projects --nv /nopt/nrel/apps/gpu_stack/ai_substack/tensorflow-2.17.0-gpu-jupyter.sif\n</code></pre>"},{"location":"Documentation/Machine_Learning/Containerized_TensorFlow/#building-a-custom-image-based-on-tensorflow","title":"Building a custom image based on TensorFlow","text":"<p>In order to build a custom Apptainer image based on this one, Docker must be installed on your local computer. Please refer to our example Docker build workflow for HPC users for more information on how to get started.</p> <p>This workflow is useful if you need to modify the prebuilt Tensorflow image for your own purposes (such as if you need extra Python libraries to be available). You can copy a <code>requirements.txt</code> into the container during buildtime and upload the resulting image to Kestrel, where it can be converted to Apptainer format for runtime.</p> <ol> <li>Update Dockerfile shown below to represent the changes desired and save to working directory. <pre><code>FROM tensorflow/tensorflow:2.17.0-gpu-jupyter\nENV DEBIAN_FRONTEND=\"noninteractive\" \nRUN apt-get -y update\nRUN apt-get -y install python3-opencv\nRUN mkdir /custom_env\nCOPY requirements.txt /custom_env\nRUN pip install -r /custom_env/requirements.txt\n</code></pre></li> <li>Update <code>requirements.txt</code> shown below for changing the python library list and save to working directory. <pre><code>seaborn\npandas\nnumpy\nscikit-learn\ngit+https://github.com/tensorflow/docs\n</code></pre></li> <li>Build new Docker image for x86_64. <pre><code>docker build -t tensorflow-custom-tag-name . --platform=linux/amd64\n</code></pre></li> <li>Follow the instructions here for exporting the Docker image to a <code>.tar</code> archive, uploading it to Kestrel, and using Apptainer to convert it to Apptainer format to run on HPC.</li> </ol>"},{"location":"Documentation/Machine_Learning/Reinforcement_Learning/","title":"Reinforcement Learning on Eagle","text":"<p>Welcome to the first NREL HPC tutorial for Reinforcement Learning (RL)! </p> <p>This tutorial covers an extended, albeit simplified, introduction of OpenAI Gym and Ray/RLlib which you can use to effortlessly design, create, and run your own RL experiments on Eagle. </p> <p>You can find the full material of this tutorial in the NREL/HPC GitHub repo.</p> <p>The tutorial covers the following:</p> <ul> <li>Brief introduction to RL and Ray</li> <li>Agent training with Ray/RLlib:<ul> <li>Experimenting with Ray Tune</li> <li>Single node/Single core.</li> <li>Single node/Multiple cores. </li> <li>Multiple nodes.</li> </ul> </li> <li>Run experiments using GPUs for policy learning (helpful for large-scale observation and/or action spaces)</li> </ul>"},{"location":"Documentation/Machine_Learning/Reinforcement_Learning/#run-openai-gym-on-a-single-nodesingle-core","title":"Run OpenAI Gym on a single node/single core","text":"<p>Login on your Eagle account, create a new Anaconda environment as described in the tutorial repo, and test your installation by running a small example using one of the standard Gym environments (e.g. <code>CartPole-v0</code>).</p> <p>Activate the Anaconda enironment and start a Python session <pre><code>module purge\nconda activate /scratch/$USER/conda-envs/myenv\npython\n</code></pre> Then, run the following: <pre><code>import gym\n\nenv = gym.ens.make(\"CartPole-v0\")\nenv.reset()\n\ndone = False\n\nwhile not done:\n    action = env.action_space.sample()\n    obs, rew, done, _ = env.step(action)\n    print(action, obs, rew, done)\n</code></pre> If everything works correctly, you will see an output similar to: <pre><code>0 [-0.04506794 -0.22440939 -0.00831435  0.26149667] 1.0 False\n1 [-0.04955613 -0.02916975 -0.00308441 -0.03379707] 1.0 False\n0 [-0.05013952 -0.22424733 -0.00376036  0.2579111 ] 1.0 False\n0 [-0.05462447 -0.4193154   0.00139787  0.54940559] 1.0 False\n0 [-0.06301078 -0.61445696  0.01238598  0.84252861] 1.0 False\n1 [-0.07529992 -0.41950623  0.02923655  0.55376634] 1.0 False\n0 [-0.08369004 -0.61502627  0.04031188  0.85551538] 1.0 False\n0 [-0.09599057 -0.8106737   0.05742218  1.16059658] 1.0 False\n0 [-0.11220404 -1.00649474  0.08063412  1.47071687] 1.0 False\n1 [-0.13233393 -0.81244634  0.11004845  1.20427076] 1.0 False\n1 [-0.14858286 -0.61890536  0.13413387  0.94800442] 1.0 False\n0 [-0.16096097 -0.8155534   0.15309396  1.27964413] 1.0 False\n1 [-0.17727204 -0.62267747  0.17868684  1.03854806] 1.0 False\n0 [-0.18972559 -0.81966549  0.1994578   1.38158021] 1.0 False\n0 [-0.2061189  -1.0166379   0.22708941  1.72943365] 1.0 True\n</code></pre> Note that the above process does not involve any training.</p>"},{"location":"Documentation/Machine_Learning/Reinforcement_Learning/#agent-training-with-rayrllib","title":"Agent training with Ray/RLlib","text":"<p>RL algorithms are notorious for the amount of data they need to collect in order to learn policies. The more data collected, the better the training will (usually) be. The best way to do it is to run many Gym instances in parallel and collecting experience, and this is where RLlib assists.</p> <p>RLlib is an open-source library for reinforcement learning that offers both high scalability and a unified API for a variety of applications. It supports all known deep learning frameworks such as Tensorflow, Pytorch, although most parts are framework-agnostic and can be used by either one.</p> <p>The RL policy learning examples provided in this tutorial demonstrate the RLlib abilities. For convenience, the <code>CartPole-v0</code> OpenAI Gym environment will be used.</p> <p>The most straightforward way is to create a Python \"trainer\" script. It will call the necessary packages, setup flags, and run the experiments, all nicely put in a few lines of Python code.</p>"},{"location":"Documentation/Machine_Learning/Reinforcement_Learning/#import-packages","title":"Import packages","text":"<p>Begin trainer by importing the <code>ray</code> package: <pre><code>import ray\nfrom ray import tune\n</code></pre> <code>Ray</code> consists of an API readily available for building distributed applications. On top of it, there are several problem-solving libraries, one of which is RLlib.</p> <p><code>Tune</code> is also one of <code>Ray</code>'s libraries for scalable hyperparameter tuning. All RLlib trainers (scripts for RL agent training) are compatible with Tune API, making experimenting easy and streamlined.</p> <p>Import also the <code>argparse</code> package and setup some flags. Although that step is not mandatory, these flags will allow controlling of certain hyperparameters, such as:</p> <ul> <li>RL algorithm utilized (e.g. PPO, DQN)</li> <li>Number of CPUs/GPUs</li> <li>...and others</li> </ul> <pre><code>import argparse\n</code></pre>"},{"location":"Documentation/Machine_Learning/Reinforcement_Learning/#create-flags","title":"Create flags","text":"<p>Begin by defining the following flags: <pre><code>parser.add_argument(\"--num-cpus\", type=int, default=0)\nparser.add_argument(\"--num-gpus\", type=int, default=0)\nparser.add_argument(\"--name-env\", type=str, default=\"CartPole-v0\")\nparser.add_argument(\"--run\", type=str, default=\"DQN\")\nparser.add_argument(\"--local-mode\", action=\"store_true\")\n</code></pre> All of them are self-explanatory, however let's see each one separately.</p> <ol> <li><code>--num-cpus</code>: Defines the number of CPU cores used for experience collection (Default value 0 means allocation of a single CPU core).</li> <li><code>--num-gpus</code>: Allocates a GPU node for policy learning (works only for Tensorflow-GPU). Except whole values (1,2,etc.), it also accepts partial values, in case 100% of the GPU is not necessary.</li> <li><code>--name-env</code>: The name of the OpenAI Gym environment.</li> <li><code>--run</code>: Specifies the RL algorithm for agent training.</li> <li><code>--local-mode</code>: Helps defining whether experiments running on a single core or multiple cores.</li> </ol>"},{"location":"Documentation/Machine_Learning/Reinforcement_Learning/#initialize-ray","title":"Initialize Ray","text":"<p>Ray is able to run either on a local mode (e.g. laptop, personal computer), or on a cluster.</p> <p>For the first experiment, only a single core is needed, therefore, setup ray to run on a local mode. Then, set the number of CPU cores to be used.</p>"},{"location":"Documentation/Machine_Learning/Reinforcement_Learning/#run-experiments-with-tune","title":"Run experiments with Tune","text":"<p>This is the final step in this basic trainer. Tune's <code>tune.run</code> function initiates the agent training process. There are three main arguments in this function:</p> <ul> <li>RL algorithm (string): It is defined in the <code>--run</code> flag (PPO, DQN, etc.).</li> <li><code>stop</code> (dictionary): Provides a criterion to stop training (in this example is the number of training iterations; stop training when iterations reach 10,000).</li> <li><code>config</code> (dictionary): Basic information for training, contains the OpenAI Gym environment name, number of CPUs/GPUs, and others.</li> </ul> <p><pre><code>tune.run(\n    args.run,\n    name=args.name_env,\n    stop={\"training_iteration\": 10000},\n    config={\n        \"env\": args.name_env,\n        \"num_workers\": args.num_cpus, \n        \"num_gpus\": args.num_gpus,\n        \"ignore_worker_failures\": True\n        }\n    )\n</code></pre> The RLlib trainer is ready!</p> <p>Except the aforementioned default hyperparameters, every RL algorithm provided by RLlib has its own hyperparameters and their default values that can be tuned in advance.</p> <p>The code of the trainer in this example can be found in the tutorial repo.</p>"},{"location":"Documentation/Machine_Learning/Reinforcement_Learning/#run-experiments-on-eagle","title":"Run experiments on Eagle","text":"<p>Follow the steps in the tutorial repo carefully.</p>"},{"location":"Documentation/Machine_Learning/Reinforcement_Learning/#run-multi-core-experiments","title":"Run multi-core experiments","text":"<p>The previous example is designed to run on a single CPU core. However, as explained above, RL training is highly benefited from running multiple concurrent OpenAI Gym rollouts. A single node on Eagle has 36 CPU cores, therefore use any number of those in order to speed up your agent training. </p> <p>For all 36 cores, adjust the <code>--num-cpus</code> hyperparameter to reflect to all CPUs on the node: <pre><code>python simple_trainer.py --num-cpus 35\n</code></pre> Again, RLlib by default utilizes a single CPU core, therefore by putting <code>--num-cpus</code> equal to 35 means that all 36 cores are requested.</p> <p>Such is not the case with the <code>num_gpus</code> key, where zero means no GPU allocation is permitted. This is because GPUs are used for policy training and not running the OpenAI Gym environment instances, thus they are not mandatory (although having a GPU node can assist the agent training by reducing training time).</p>"},{"location":"Documentation/Machine_Learning/Reinforcement_Learning/#run-experiments-on-multiple-nodes","title":"Run experiments on multiple nodes","text":"<p>Let's focus now on cases where the problem under consideration is highly complex and requires vast amounts of training data for training the policy network in a reasonable amount of time. It could be then, that you will require more than one nodes to run your experiments. In this case, it is better to use a slurm script file that will include all the necessary commands for agent train using multiple CPUs and multiple nodes.</p>"},{"location":"Documentation/Machine_Learning/Reinforcement_Learning/#example-cartpole-v0","title":"Example: CartPole-v0","text":"<p>As explained above, CartPole is a rather simple environment and solving it using multiple cores on a single node feels like an overkill, let alone multiple nodes! However, it is a good example for giving you an experience on running RL experiments using RLlib.</p> <p>For multiple nodes it is more convenient to use a slurm script instead of an interactive node. Slurm files are submitted as <code>sbatch &lt;name_of_your_batch_script&gt;</code>, and the results are exported in an <code>slurm-&lt;job_id&gt;.out</code> file. The <code>.out</code> file can be interactively accessed during training using the <code>tail -f slurm-&lt;job_id&gt;.out</code> command. Otherwise, after training, open it using a standard text editor (e.g. <code>nano</code>). Next, the basic parts of the slurm script file are given. The repo also provides the complete script.</p> <p>The slurm file begins with defining some basic <code>SBATCH</code> options, including the desired training time, number of nodes, tasks per node, etc.</p> <p><pre><code>#!/bin/bash --login\n\n#SBATCH --job-name=cartpole-multiple-nodes\n#SBATCH --time=00:10:00\n#SBATCH --nodes=3\n#SBATCH --tasks-per-node=1\n#SBATCH --cpus-per-task=36\n#SBATCH --account=A&lt;account&gt;\nenv\n</code></pre> Allocating multiple nodes means creating a Ray cluster. A Ray cluster consists of a head node and a set of worker nodes. The head node needs to be started first, and the worker nodes are given the address of the head node to form the cluster. </p> <p>The agent training will run for 20 minutes (<code>SBATCH --time=00:20:00</code>), and on three Eagle CPU nodes (<code>SBATCH --nodes=3</code>). Every node will execute a single task (<code>SBATCH --tasks-per-node=1</code>), which will be executed on all 36 cores (<code>SBATCH --cpus-per-task=36</code>). Then, define the project account. Other options are also available, such as whether to prioritize the experiment (<code>--qos=high</code>).</p> <p>Use the commands to activate the Anaconda environment. Do not forget to <code>unset LD_PRELOAD</code>. <pre><code>module purge\nconda activate /scratch/$USER/conda-envs/env_example\nunset LD_PRELOAD\n</code></pre> Set up the Redis server that will allow all the nodes you requested to communicate with each other. For that, set a Redis password: <pre><code>ip_prefix=$(srun --nodes=1 --ntasks=1 -w $node1 hostname --ip-address)\nport=6379\nip_head=$ip_prefix:$port\nredis_password=$(uuidgen)\n</code></pre> Submit the jobs one at a time at the workers, starting with the head node and moving on to the rest of them. <pre><code>srun --nodes=1 --ntasks=1 -w $node1 ray start --block --head \\\n--node-ip-address=\"$ip_prefix\" --port=$port --redis-password=$redis_password &amp;\nsleep 10\n\necho \"starting workers\"\nfor ((  i=1; i&lt;=$worker_num; i++ ))\ndo\n  node2=${nodes_array[$i]}\n  echo \"i=${i}, node2=${node2}\"\n  srun --nodes=1 --ntasks=1 -w $node2 ray start --block --address \"$ip_head\" --redis-password=$redis_password &amp;\n  sleep 5\ndone\n</code></pre> Set the Python script to run. Since this experiment will run on a cluster, Ray will be initialized as: <pre><code>ray.init(_redis_password=args.redis_password, address=os.environ[\"ip_head\"])\nnum_cpus = args.num_cpus - 1\n</code></pre> The <code>--redis-password</code> option must be active, along with the total number of CPUs: <pre><code>python -u simple_trainer.py --redis-password $redis_password --num-cpus $total_cpus\n</code></pre> The experiment is ready to begin, simply run: <pre><code>sbatch &lt;your_slurm_file&gt;\n</code></pre> If the trainer script is on a different directory, make sure to <code>cd</code> to this directory in the slurm script before executing it. <pre><code>### Example where the trainer is on scratch:\ncd /scratch/$USER/path_to_specific_directory\npython -u simple_trainer.py --redis-password $redis_password --num-cpus $total_cpus\n</code></pre></p>"},{"location":"Documentation/Machine_Learning/Reinforcement_Learning/#experimenting-using-gpus","title":"Experimenting using GPUs","text":"<p>It is now time to learn running experiments using GPU nodes on Eagle that can boost training times considerably. GPU nodes however is better to be utilized only in cases of environments with very large observation and/or action spaces. CartPole will be used again for establishing a template.</p>"},{"location":"Documentation/Machine_Learning/Reinforcement_Learning/#allocate-gpu-node","title":"Allocate GPU node","text":"<p>The following instructions are the same for both regular and Optimized TF versions of the Anaconda environments</p> <p>Running experiments with combined CPU and GPU nodes is not so straightforward as running them using only CPU nodes (either single or multiple nodes). Particularly, heterogenous jobs using slurm have to be submitted.</p> <p>Begin at first by specifying some basic options, similarly to previous section: <pre><code>#!/bin/bash  --login\n\n#SBATCH --account=A&lt;account&gt;\n#SBATCH --job-name=cartpole-gpus\n#SBATCH --time=00:10:00\n</code></pre> The slurm script will clearly define the various jobs. These jobs include the CPU nodes that will carry the environment rollouts, and the GPU node for policy learning. Eagle has 44 GPU nodes and each node has 2 GPUs. Either request one GPU per node (<code>--gres=gpu:1</code>), or both of them (<code>--gres=gpu:2</code>). For the purposes of this tutorial, one GPU core on a single node is utilized.</p> <p>In total, slurm nodes can be categorized as: </p> <ul> <li>A head node, and multiple rollout nodes (as before)</li> <li>A policy training node (GPU)</li> </ul> <p>Include the <code>hetjob</code> header for both the rollout nodes and the policy training node. Three CPU nodes are requested to be used for rollouts and a single GPU node is requested for policy learning: <pre><code># Ray head node\n#SBATCH --nodes=1\n#SBATCH --tasks-per-node=1\n\n# Rollout nodes - Nodes with multiple runs of OpenAI Gym \n#SBATCH hetjob\n#SBATCH --nodes=3\n#SBATCH --tasks-per-node=1\n#SBATCH --cpus-per-task=36\n\n# Policy training node - This is the GPU node\n#SBATCH hetjob\n#SBATCH --nodes=1\n#SBATCH --tasks-per-node=1\n#SBATCH --partition=debug\n#SBATCH --gres=gpu:1\n</code></pre> Of course, any number of CPU/GPU nodes can be requested, depending on problem complexity. </p> <p>As an example, a single node and perhaps just a single CPU core may be requested. Now, it is more reasonable to request GPUs for an OpenAI Gym environment that utilizes high-dimensional observation and/or action spaces. Hence, the first priority would be to start with multiple CPU nodes, and request GPUs only if they are needed.</p> <p>For the three types of nodes (head, rollouts, training), define three separate groups: <pre><code>head_node=$(scontrol show hostnames $SLURM_JOB_NODELIST_HET_GROUP_0)\nrollout_nodes=$(scontrol show hostnames $SLURM_JOB_NODELIST_HET_GROUP_1)\nrollout_nodes_array=( $rollout_nodes )\nlearner_node=$(scontrol show hostnames $SLURM_JOB_NODELIST_HET_GROUP_2)\necho \"head node    : \"$head_node\necho \"rollout nodes: \"$rollout_nodes\necho \"learner node : \"$learner_node\n</code></pre> Each group of nodes requires its separate <code>srun</code> command so that they will run independently of each other. <pre><code>echo \"starting head node at $head_node\"\nsrun --pack-group=0 --nodes=1 --ntasks=1 -w $head_node ray start --block --head \\\n--node-ip-address=\"$ip_prefix\" --port=$port --redis-password=$redis_password &amp; # Starting the head\nsleep 10\n\necho \"starting rollout workers\"\nfor ((  i=0; i&lt;$rollout_node_num; i++ ))\ndo\n  rollout_node=${rollout_nodes_array[$i]}\n  echo \"i=${i}, rollout_node=${rollout_node}\"\n  srun --pack-group=1 --nodes=1 --ntasks=1 -w $rollout_node \\\n   ray start --block --address \"$ip_head\" --redis-password=$redis_password &amp; # Starting the workers\n  sleep 5\ndone\n\necho \"starting learning on GPU\"\nsrun --pack-group=2 --nodes=1 --gres=gpu:1 -w $learner_node ray start --block --address \"$ip_head\" --redis-password=$redis_password &amp;\n</code></pre> The slurm commands for the head and rollout nodes are identical to those from the previous section. A third command is also added for engaging the GPU node.</p> <p>Finally, call <pre><code>python -u simple_trainer.py --redis-password $redis_password --num-cpus $rollout_num_cpus --num-gpus 1\n</code></pre> to begin training. Add the <code>---num-gpus</code> argument to include the requested GPU node (or nodes in case of <code>--gres=gpu:2</code>) for policy training. There is no need to manually declare the GPU for policy training in the <code>simple_trainer.py</code>, RLlib will automatically recognize the available GPU and use it accordingly.</p> <p>The repo contains the complete slurm file versions for both <code>env_example_gpu</code> and <code>env_gpu_optimized_tf</code>, and they can be used as templates for future projects.</p>"},{"location":"Documentation/Machine_Learning/Reinforcement_Learning/#create-gym-environments-from-scratch","title":"Create Gym environments from scratch","text":"<p>So far, only benchmark Gym environments were used in order to demonstrate the processes for running experiments. It is time now to see how one can create their own Gym environment, carefully tailor-made to one's needs. OpenAI Gym functionality allows the creation of custom-made environments using the same structure as the benchmark ones. </p> <p>Custom-made environments can become extremely complex due to the mechanics involved and may require many subscripts that perform parts of the simulation. Nevertheless, the basis of all environments is simply a Python class that inherits the <code>gym.Env</code> class, where the user can implement the three main Gym functions and define any hyperpameters necessary:</p> <ul> <li><code>def __init__(self)</code>: Initializes the environment. It defines initial values for variables/hyperparameters and may contain other necessary information. It also defines the dimensionality of the problem. Dimensionality is expressed at the sizes of the observation and action spaces, which are given using the parameters <code>self.observation_space</code> and <code>self.action_space</code>, respectively. Depending on their nature, they can take discrete, continuous, or a combination of values. OpenAI provides detailed examples of each one of these types of spaces.</li> <li><code>def reset(self)</code>: When called, it resets the environment on a previous state (hence the name). This state can either be a user-defined initial state or it may be a random initial position. The latter can be found on environments that describe locomotion like <code>CartPole</code>, where the initial state can be any possible position of the pole on the cart.</li> <li><code>def step(self, action)</code>: The heart of the class. It defines the inner mechanics of the environment, hence it can be seen as some kind of simulator. Its main input is the sampled action, which when acted upon moves the environment into a new state and calculates the new reward. The new state and reward are two of the function's output and they are necessary for policy training since they are also inputs to the policy network. Other outputs include a boolean variable <code>done</code> that is True when the environment reaches its final state (if it exists), and False otherwise<sup>*</sup>, as well as a dictionary (<code>info</code>) with user-defined key-value objects that contain further information from the inner workings of the environment.</li> </ul> <p><sup>*</sup> Many environments do not consider a final state, since it might not make sense (e.g. a traffic simulator for fleets of autonomous ridesharing vehicles that reposition themselves based on a certain criterion. In this case the reward will get better every time, but there is no notion of a final vehicle position).</p> <p>Directions of how to create and register a custom-made OpenAI Gym environment are given below.</p>"},{"location":"Documentation/Machine_Learning/Reinforcement_Learning/#create-an-environment-class","title":"Create an environment class","text":"<p>As stated above, the basis of any Gym environment is a Python class that inherits the <code>gym.Env</code> class. After importing the gym package, define the class as: <pre><code>import gym\n\nclass BasicEnv(gym.Env):(...)\n</code></pre> The example environment is very simple and is represented by two possible states (0, 1) and 5 possible actions (0-4). For the purposes of this tutorial, consider state 0 as the initial state, and state 1 as the final state.</p> <p>Define the dimensions of observation and action spaces in the <code>def __init__(self)</code> function: <pre><code>def __init__(self):\n    self.action_space = gym.spaces.Discrete(5) # --&gt; Actions take values in the 0-4 interval\n    self.observation_space = gym.spaces.Discrete(2) # --&gt; Two possible states [0,1]\n</code></pre> Both spaces take discrete values, therefore they are defined using Gym's <code>Discrete</code> function. Other possible functions are <code>Box</code> for continuous single- or multi-dimensional observations and states, <code>MultiDiscrete</code> for vectors of discrete values, etc. OpenAi provides detailed explanation for all different space forms.</p> <p>Next, define the <code>def reset(self)</code> function: <pre><code>def reset(self):\n    state = 0\n    return state\n</code></pre> In this example, the reset function simply returns the environment to the initial state.</p> <p>Finally, define the <code>def step(self, action)</code> function, which takes as input the sampled action. Here the step function takes the environment at state 1 and based on the action, returns a reward of 1 or -1: <pre><code>def step(self, action):\n    state = 1\n\n    if action == 2:\n        reward = 1\n    else:\n        reward = -1\n\n    done = True\n    info = {}\n\n    return state, reward, done, info\n</code></pre> That's it, the new Gym environment is ready! Make note that there is one more function usually found on Gym environments. This is the <code>def render(self)</code> function, and is called in random intervals throughout training returning a \"snapshot\" of the environment at that time. While this is helpful for evaluating the agent training process, it is not necessary for the actual training process. OpenAI documentation provides details for every one of these functions.</p> <p>You can find the full script of this environment in the repo.</p>"},{"location":"Documentation/Machine_Learning/Reinforcement_Learning/#run-experiments-on-rllib","title":"Run experiments on RLlib","text":"<p>Let's now train the agent with RLlib. The full trainer script is given at the repo.</p> <p>The trainer is almost identical to the one used before, with few additions that are necessary to register the new environment.</p> <p>At first, along with <code>ray</code> and <code>tune</code>, import: <pre><code>from ray.tune.registry import register_env\nfrom custom_env import BasicEnv\n</code></pre> The <code>register_env</code> function is used to register the new environment, which is imported from the <code>custom_env.py</code>.</p> <p>Function <code>register_env</code> takes two arguments:</p> <ul> <li>Training name of the environment, chosen by the developer.</li> <li>Actual name of the environment (<code>BasicEnv</code>) in a <code>lambda config:</code> function.</li> </ul> <p><pre><code>env_name = \"custom-env\"\nregister_env(env_name, lambda config: BasicEnv())\n</code></pre> Once again, RLlib provides detailed explanation of how <code>register_env</code> works.</p> <p>The <code>tune.run</code> function, instead of <code>args.name_env</code>, it uses the <code>env_name</code> defined above.</p> <p>That's all! Proceed with agent training using any of the slurm scripts provided by the repo.</p> <p>As a final note, creating custom-made OpenAI Gym environment is more like an art than science. The main issue is to really clarify what the environment represents and how it works, and then define this functionality in Python.</p>"},{"location":"Documentation/Machine_Learning/Reinforcement_Learning/#validating-results-using-tensorboard","title":"Validating results using Tensorboard","text":"<p>Another way of visualizing the performance of agent training is with Tensorboard. </p> <p>Navigate to the <code>ray_results</code> directory: <pre><code>cd ~/ray_results/\n</code></pre> Every RL experiment generates a subdirectory named from the OpenAI Gym environment used in the experiment. </p> <p>E.g., after running all the examples previously shown in this tutorial, <code>ray_results</code> will have a subdirectory named <code>CartPole-v0</code>. Within, every experiment using CartPole generates a new subdirectory.</p> <p>For the purpose of this tutorial, <code>cd</code> to the <code>CartPole-v0</code> subdirectory and activate one of the environments: <pre><code>module purge\nconda activate &lt;your_environment&gt;\n</code></pre> Initialize Tensorboard following the steps in this tutorial. Open the localhost url in a browser, and all plots for rewards, iterations and other metrics will be demonstrated as:</p> <p> </p> <p>The <code>tune/episode_reward_mean</code> plot is essentialy the same as the figure plotted from data in the <code>progress.csv</code> file. The difference in the x-axis scale has a simple explanation. The <code>episode_reward_mean</code> column on the <code>progress.csv</code> file shows the reward progress on every training iteration, while the <code>tune/episode_reward_mean</code> plot on Tensorboard shows reward progress on every training episode (a single RLlib training iteration consists of thousands of episodes).</p>"},{"location":"Documentation/Machine_Learning/TensorBoard/","title":"Validating ML results using Tensorboard","text":"<p>Tensorboard provides visualization and tooling needed for machine learning, deep learning, and reinforcement learning experimentation:</p> <ul> <li>Tracking and visualizing metrics such as loss and accuracy.</li> <li>Visualizing the model graph (ops and layers).</li> <li>Viewing histograms of weights, biases, or other tensors as they change over time.</li> <li>Projecting embeddings to a lower dimensional space.</li> <li>Displaying images, text, and audio data.</li> <li>Profiling TensorFlow programs.</li> </ul> <p>For RL it is useful to visualize metrics such as:</p> <ul> <li>Mean, min, and max reward values.</li> <li>Episodes/iteration.</li> <li>Estimated Q-values.</li> <li>Algorithm-specific metrics (e.g. entropy for PPO).</li> </ul> <p>To visualize results from Tensorboard, first <code>cd</code> to the directory where your results reside. E.g., if you ran experiments using <code>ray</code>, then do the following: <pre><code>cd ~/ray_results/\n</code></pre></p> <p>There are three main methods for activating Tensorboard:</p> <ul> <li>If you included Tensorboard installation in an Anaconda environment, simply activate it:    <pre><code>module purge\nconda activate &lt;your_environment&gt;\n</code></pre></li> <li>You can also install Tensorboard in userspace using <code>pip install</code>:    <pre><code>pip install tensorboard --user\n</code></pre></li> <li>Or, install using container images:    <pre><code>ml singularity-container\nsingularity pull docker://tensorflow/tensorflow\nsingularity run tensorflow_latest.sif\n</code></pre></li> </ul> <p>Then, initialize Tensorboard using a pre-specified port number of your choosing (e.g. 6006, 8008): <pre><code>tensorboard --logdir=. --port 6006 --bind_all\n</code></pre> If everything works properly, terminal will show: <pre><code>Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\nTensorBoard 2.5.0 at http://localhost:6006/ (Press CTRL+C to quit)\n</code></pre> Open a new Terminal tab and create a tunnel: <pre><code>ssh -NfL 6006:localhost:6006 $USER@el1.hpc.nrel.gov\n</code></pre> Finally, open the above localhost url (<code>http://localhost:6006/</code>) in a browser, where all the aforementioned plots will be shown.</p>"},{"location":"Documentation/Managing_Data/file_permissions/","title":"Linux File Permissions and Ownership","text":"<p>Linux uses standard POSIX-style permissions to control who has the ability to read, write, or execute a file or a directory.</p>"},{"location":"Documentation/Managing_Data/file_permissions/#permission-levels-and-ownership","title":"Permission Levels and Ownership","text":"<p>Under this standard, all files and all directories have three types of permission that can be granted. </p> <p>The three permission types are:</p> <ul> <li>r (Read): permission to read or copy the contents of the file, but not make changes</li> <li>w (Write): permission to make changes, move, or delete a file</li> <li>x (eXecute): permission to run a file that is an executable program, such as a compiled binary, shell script, python code, etc, OR to access a directory.</li> </ul> <p>Files are also \"owned\" by both an individual user, and a user group. This ownership is used to provide varying levels of access to different cohorts of users on the system. </p> <p>The cohorts of users to which file permissions may be assigned include:</p> <ul> <li>u: permissions granted to the (u)ser who owns the file</li> <li>g: permissions granted to the (g)roup of users who own the file</li> <li>o: permissions granted to any (o)thers who are not the user or the group that own the file</li> </ul> <p>On most systems, every user is a member of their own personal group that has the same name as their username, and only that user has access  to their own personal group. Whenever a new file is created, the default is that it will be created with the user and group ownership of the  user that created the file. However, this may not always be the case, and the permissions of the directory in which the file is created can have an effect on the group ownership. This will be discussed in a later section.</p>"},{"location":"Documentation/Managing_Data/file_permissions/#viewing-file-and-directory-permissions","title":"Viewing File and Directory Permissions","text":"<p>The <code>ls -l</code> command will show the ownership and permissions of a file, a list of files, or all files in a directory. Here's an example output with two files, two directories, and a symbolic link to another directory. The user in the example is \"alice\".  <pre><code>[alice@el1 ~]$ ls -l \n-rwxrwx---.  1 alice alice         49 Oct 13  2020 testjob.sh\n-rw-rw----.  1 alice alice         35 Mar  9 16:45 test.txt\ndrwxrwx---.  3 alice alice       1536 Mar 31  2021 myscripts\ndrwxrws---.  3 alice csc000         4096 Dec 14  2020 shared-data\nlrwxrwxrwx.  1 alice alice         16 Jan 30  2023 my_proj -&gt; /projects/csc000\n</code></pre></p> <p>The first field of <code>ls -l</code> output for each file consists of ten characters. These represent the permission bits for the file.</p> <p>The first bit is reserved to describe the type of file. The three most common file types are:</p> <ul> <li>- : a dash indicates a regular file (no special file type)</li> <li>d : a <code>d</code> indicates that this is a directory (a type of \"file\" that stores a list of other files)</li> <li>l : an <code>l</code> indicates a symbolic link to another file/directory</li> </ul> <p>The next nine bits describe the file permissions that are set. These are always in the order of read, write, and execute. </p> <p>A letter indicates that this permission is granted, a <code>-</code> indicates that the permission is not granted. </p> <p>This \"rwx\" order repeats three times: the first triplet is for User permissions, the second triplet is for Group permissions, and the third triplet is for Other permissions.</p> <p>In the example above, <code>testjob.sh</code> has the permissions <code>-rwxrwx---</code>. This means that the User and Group owners have read, write, and execute permission. The last three characters are <code>-</code>, which indicates that \"Other\" users do not have permissions to this file.</p> <p>There also may be a dot (<code>.</code>) or other character at the end of the permissions list, depending on the variety of Linux that is installed. The dot indicates that no further access controls are in place. A <code>+</code> indicates that ACLs (Access Control Lists) are in place that provide additional permissions. ACLs are an extension of the file permission system that is present on some, but not all, NREL HPC systems, and may be used to provide more fine-grained access control on a per-user basis. If the system you are using supports ACLs, you may see <code>man getfacl</code> and <code>man setfacl</code> for more help on ACLs. </p> <p>After the permissions flags is a number indicating the number of hard links to the file. It has no bearing on permissions and can be ignored.</p> <p>The next two fields are the User and Group with access rights to the file. A file may only be owned by one User and one Group at a time.</p>"},{"location":"Documentation/Managing_Data/file_permissions/#special-permissions-flags-setuid-setgid-and-sticky-bits","title":"Special Permissions Flags: Setuid, Setgid, and Sticky Bits","text":"<p>An <code>s</code> in the e(x)ecute bit field has a special meaning, depending on whether it's in the User or Group permissions. A <code>t</code> in the \"Others\"  e(x)ecute also has a special meaning.</p> <p>In the Group permission bits, an <code>s</code> for the eXecute bit indicates that <code>SETGID</code> is enabled. This can be set for an individual file or for a directory, but is most common on a directory. When setgid is enabled on a directory, any files created in the directory will have a group ownership that corresponds to the group ownership of the directory itself, instead of the default group of the user who created the file. This is very useful when an entire directory is intended to be used for collaboration between members of a group, when combined with appropriate group read, write, and/or execute bits.</p> <p>In the User permission bits, an <code>s</code> for the eXecute bit indicates that <code>SETUID</code> is enabled. This is only used for executable files, and means that regardless of the user who runs the program, the owner of the process that starts up will be changed to the owner of the file. This is very rarely used by regular users and can pose a considerable security risk, because a process that belongs to a user also has access to that user's  files as though it had been run by that user. Setuid should almost never be used.</p> <p>In the Other permission bits, a <code>t</code> for the eXecute bit indicates that a \"sticky bit\" has been set. This is only used on directories. With the sticky bit set, files in that directory may only be deleted by the owner of the file or the owner of the directory. This is commonly used for directories that  are globally writeable, such as /tmp or /tmp/scratch and will be set by a system administrator. It is otherwise rarely used by regular users.</p>"},{"location":"Documentation/Managing_Data/file_permissions/#changing-permissions-and-ownership","title":"Changing Permissions and Ownership","text":"<p>Only the User that owns a file may change ownership or permissions.</p> <p>The <code>chgrp</code> command is used to change the Group ownership of a file or directory. </p> <p>The <code>chmod</code> command is used to change the permissions of a file or directory.</p> <p>The <code>chown</code> command is used to change the User owner and/or Group owner of a file, but only system administrators may change the User owner, so this command will not be covered in this document. Please see <code>man chown</code> for more information.</p>"},{"location":"Documentation/Managing_Data/file_permissions/#the-chgrp-command","title":"The chgrp Command","text":"<p>The <code>chgrp</code> command is used to change the group ownership of a file. You must be a member of the group the file currently belongs to, as well as a  member of the destination group. </p> <p><code>chgrp -c group filename</code></p> <p>The -c flag is recommended, as it explicitly shows any changes that are made to ownership. </p> <p>Filename can be a file, a list of files, a wildcard (e.g. <code>*.txt</code>), or a directory.</p> <p>Please see <code>man chgrp</code> for more detailed information on this command.</p>"},{"location":"Documentation/Managing_Data/file_permissions/#the-chmod-command-and-symbolic-permissions","title":"The chmod Command and Symbolic Permissions","text":"<p>The chmod command is used to change the permissions (also called file mode bits) of a file or directory. Using an alphabetic shorthand (\"symbolic mode\"), permissions can be changed for a file or directory, in the general format:</p> <p><code>chmod -c ugoa+-rwxst file</code></p> <p>The cohort to which permissions should be applied is first: (u)ser, (g)roup, (o)ther, or (a)ll.</p> <p>The <code>+</code> or <code>-</code> following the cohort denotes whether the permissions should be added or removed, respectively.</p> <p>After the +/- is the list of permissions to change: (r)ead, (w)rite, e(x)ecute are the primary attributes. (s)etuid or (s)etgid depend on the cohort chosen: u+s is for setuid, g+s is for setgid. The s(t)icky bit may also be set.</p> <p>To add eXecute permission for the User owner of a file:</p> <p><code>chmod u+x myscript.sh</code></p> <p>To add group read, write, and execute, and REMOVE read, write, execute from others:</p> <p><code>chmod g+rwx mydirectory</code></p> <p>To remove write and execute from other users:</p> <p><code>chmod o-wx myscript.sh</code></p> <p>You can also combine arguments, for example:</p> <p><code>chmod g+rwx,o-rwx myscript.sh</code></p> <p><code>chmod ug+rwx,o+r,o-w myscript.sh</code></p> <p>Please avoid setting global read, write, and execute permissions, as it is a security risk:</p> <p><code>chmod a+rwx myscript.sh</code></p>"},{"location":"Documentation/Managing_Data/file_permissions/#using-octal-permissions-with-chmod","title":"Using Octal Permissions With chmod","text":"<p>Chmod can also accept numeric arguments for permissions, instead of the symbolic permissions. This is called  \"octal\" mode, as it uses base 8 (numbers 0 through 7) for binary encoding. Symbolic permissions are now generally preferred for clarity, but octal is sometimes used as a shorthand way of accomplishing the same thing. </p> <p>In octal mode, a three or sometimes four digit number is used to represent the permission bits. The octal equivalent to \"ug+rwx\" is:</p> <p><code>chmod 770 myscript.sh</code></p> <p>The first position is User, the second is Group, and the last is Other.</p> <p>The following table describes the value of the bit and the corresponding permission.</p> bit permission 0 none 1 execute 2 write 4 read <p>The permission is set by the sum of the bits, from 0 to 7, with 0 being \"no permissions\" and 7 being \"read, write, and execute.\"</p> <p>760 and 770 are the most common for data shared by a group of users. 700 is common for protected files that should only be viewed or edited by the User who owns the file.</p> <p>Occasionally there may be a fourth leading digit. This is used for setuid, setgid, or a sticky bit setting. </p>"},{"location":"Documentation/Managing_Data/file_permissions/#caution-with-mode-777","title":"Caution with Mode 777","text":"<p>The command <code>chmod 777</code> is the equivalent of <code>chmod a+rwx</code>, which grants read, write, and execute permission to ALL users on the system for the file(s) specified. Use of this command should be EXTREMELY rare, and any suggestions that it be applied should be examined closely, as it poses a major security risk to your files and data. Use your best judgement.</p>"},{"location":"Documentation/Managing_Data/file_permissions/#further-reading-about-file-permissions","title":"Further Reading About File Permissions","text":"<p>All of the command listed have manual pages available at the command line. See <code>man &lt;command&gt;</code> for more information, or <code>man man</code> for help with the manual page system itself.</p> <p>Further documentation regarding file permissions and other Linux fundamentals is widely available online in text or video format, and many paper books are available. </p> <p>We do not endorse any particular source, site, or vendor. The following links may be helpful:</p> <ul> <li>https://www.redhat.com/sysadmin/linux-file-permissions-explained</li> <li>https://www.linuxfoundation.org/blog/blog/classic-sysadmin-understanding-linux-file-permissions</li> <li>https://docs.nersc.gov/filesystems/unix-file-permissions/</li> <li>https://en.wikipedia.org/wiki/File-system_permissions</li> <li>https://www.linux.com/training-tutorials/file-types-linuxunix-explained-detail/</li> <li>https://en.wikipedia.org/wiki/Unix_file_types</li> </ul>"},{"location":"Documentation/Managing_Data/file_permissions/#default-permissions-on-nrel-systems","title":"Default Permissions on NREL Systems","text":"<p>When first created, all /projects directories will be owned by the allocation's HPC Lead User and the project's shared Group. The default permissions will typically be ug+rwx (chmod 770) or ug+rwx,o+rx (chmod 776), depending on the system. The setgid bit will also be set on the directory, so that all files created in the /projects directory will have a Group ownership of the project's group. </p>"},{"location":"Documentation/Managing_Data/file_permissions/#nrel-technical-help-with-file-permissions","title":"NREL Technical Help with File Permissions","text":"<p>The NREL HPC Support Team relies on allocation owners and users to be responsible for file permissions and ownership as a part of managing the allocation and its data, but the PI or HPC Leads of a project may request assistance in changing permissions or ownership of files that belong to the allocation by opening a support ticket with hpc-help@nrel.gov.</p>"},{"location":"Documentation/Managing_Data/localstorage/","title":"Local and Scratch Storage on NREL HPC Systems","text":"<p>The table below summarizes the local and scratch storage currently on NREL HPC systems. </p> System Name Node Local Storage $TMPDIR Default Default $TMPDIR Storage Type Global Scratch Storage Kestrel 1.7TB on 256 of the standard compute nodes, 5.6TB on bigmem nodes, 3.4TB GPU nodes, 14TB on 24 GPU nodes. Other nodes have none. <code>/tmp/scratch/$SLURM_JOBID</code> Local disk when available, or RAM <code>/scratch/$USER</code> (Lustre) Swift 1.8TB <code>/scratch/$USER/$SLURM_JOBID</code> Local disk None Vermilion 60GB (t), 250GB (sm), 500GB (std), 1.0TB (lg), 2.0TB (gpu) <code>/tmp</code> RAM. Write to <code>/tmp/scratch</code> instead to use local disk. <code>/scratch/$USER</code> <p>Important Notes</p> <ul> <li>Local storage is local to a node and usually faster to access by the processes running on the node. Some scenarios in which using the local disk might make your job run faster are:<ul> <li>Your job may access or create many small (temporary) files</li> <li>Your job may have many parallel tasks accessing the same file</li> <li>Your job may do many random reads/writes or memory mapping.</li> </ul> </li> <li>Local or scratch spaces are for temporary files only and there is no expectation of data longevity in these spaces. HPC users should copy results from those spaces to a <code>/projects</code> or global scratch directory as part of the job script before the job finishes.</li> <li>A node will not have read or write access to any other node's local scratch, only its own</li> <li>On Kestrel, the path <code>/tmp/scratch</code> is not writeable. Use <code>$TMPDIR</code> instead.</li> <li>On Kestrel, only 256 of the standard compute nodes have real local disk, the other standard compute nodes have no local disk space. For the nodes without local storage, writing to <code>$TMPDIR</code> uses RAM. This could cause an out-of-memory error if using a lot of space in $TMPDIR. To solve this problem:<ul> <li>Use <code>/scratch/$USER</code> instead of the default <code>$TMPDIR</code> path if the job benefits little from local storage (e.g. jobs with low I/O communication)</li> <li>Request nodes with local storage by using the <code>--partition=nvme</code> option in your job submission script. Then, <code>$TMPDIR</code> will be using a local disk. </li> <li>In addition, on Kestrel, this bash command can be used to check if there is a local disk on the node: \"<code>if [ -e /dev/nvme0n1 ]</code>\". This will only work on standard compute nodes. For example:</li> </ul> </li> </ul> <p><pre><code>if [ -e /dev/nvme0n1 ]; then\n echo \"This node has a local storage and will use as the scratch path\"\n APP_SCRATCH=$TMPDIR\nelse\n echo \"This node does not have a local storage drive and will use /scratch as the scratch path\"\n APP_SCRATCH=/scratch/$USER/$SLURM_JOB_ID\nfi\n</code></pre> This does not work on bigmem nodes. All bigmem nodes have a real local disk.</p>"},{"location":"Documentation/Managing_Data/mss/","title":"Mass Storage Sytem (MSS)","text":"<p>NREL\u2019s Amazon Web Services (AWS) Mass Storage System (MSS) is an additional data archival resource available to active projects and users on the Kestrel high-performance computing (HPC) system.</p> <p>The AWS MSS keeps and protects important data, primarily as an addition to Kestrel's high-performance Lustre filesystem (/projects and /scratch).</p> <p>NREL implemented the AWS MSS to take advantage of S3 Deep Glacier archiving, replacing the previous on-premises MSS, Gyrfalcon, which reached end-of-life at the end of 2020. </p>"},{"location":"Documentation/Managing_Data/mss/#how-to-copymove-data-from-kestrel","title":"How To Copy/Move Data from Kestrel","text":"<p>AWS charges per inode.  Therefore, to keep costs down it is recommended  users create a compressed tarball of any files and/or directories desired  to be archived to AWS MSS.  The size limit per archived file is 5TB, and therefore individual tarballs need to be under this limit (although multiple tarballs that sum to greater than 5 TB can be archived).  </p> <p>The recommended command is:</p> <p><code>$ tar czvf /destination/descriptor-YYYMMDD.tgz &lt;source-files-directories\\&gt;</code></p> <p>Example, from Kestrel\u2019s /projects/csc000/data-to-be-copied from a Kestrel Login node:</p> <pre><code>$ cd /projects/csc000\n$ tar czvf /kfs2/shared-projects/MSS/projects/csc000/data-to-be-copied-20211215.tgz data-to-be-copied\n</code></pre> <p>Data placed in <code>/kfs2/shared-projects/MSS/projects/&lt;project_handle&gt;</code> and <code>/kfs2/shared-projects/MSS/home/&lt;username&gt;</code> is synced to AWS MSS and then purged from Kestrel.</p>"},{"location":"Documentation/Managing_Data/mss/#how-to-restore-data","title":"How To Restore Data","text":"<ul> <li>Restore requests of AWS MSS data will require a request to the HPC Help Desk and may require 48 hours or more to be able to stage from Deep Archive to recover.  </li> <li> <p>Users can see a list of the archived files they have on AWS MSS by searching the following file: <code>/kfs2/shared-projects/MSS/MSS-archived-files</code></p> <ul> <li>The MSS-archived-files has limited information, but all archives    related to a project can be found using a command such as:   <code>$ grep &lt;project name&gt; /kfs2/shared-projects/MSS/MSS-archived-files</code></li> </ul> </li> <li> <p>Let the HPC Help Desk know specifically what file(s) you would like to recover, and where the recovered files should be placed.  </p> </li> </ul>"},{"location":"Documentation/Managing_Data/mss/#usage-policies","title":"Usage Policies","text":"<p>Follow the AWS MSS policies.</p>"},{"location":"Documentation/Managing_Data/mss/#contact","title":"Contact","text":"<p>Contact the HPC Help Desk if you have any questions or issues.</p>"},{"location":"Documentation/Managing_Data/Transferring_Files/","title":"Transferring Files","text":"<p>Learn how to transfer data within, to and from NREL's high-performance computing (HPC) systems.</p> <p>For a video presentation on this topic, please see Transfering data to and from Kestrel by Matt Selensky.</p> <p>For further information about invidiual systems' filesystem architecture and quotas, please see the Systems section. </p>"},{"location":"Documentation/Managing_Data/Transferring_Files/#best-practices-for-transferring-files","title":"Best Practices for Transferring Files","text":""},{"location":"Documentation/Managing_Data/Transferring_Files/#file-transfers-between-filesystems-on-the-nrel-network","title":"File Transfers Between Filesystems on the NREL network","text":"<p>rsync is the recommended tool for transferring data between NREL systems. It allows you to easily restart transfers if they fail, and also provides more consistency when dealing with symbolic links, hard links, and sparse files than either scp or cp. It is recommended you do not use compression for transfers within NREL systems. An example command is:</p> <pre><code>$ rsync -aP --no-g /scratch/username/dataset1/ /mss/users/username/dataset1/\n</code></pre> <p>Mass Storage has quotas that limit the number of individual files you can store. If you are copying hundreds of thousands of files then it is best to archive these files prior to copying to Mass Storage. See the guide on how to archive files.</p> <p>Mass Storage quotas rely on the group of the file and not the directory path. It is best to use the <code>--no-g</code> option when rsyncing to MSS so you use the destination group rather than the group permissions of your source.  You can also <code>chgrp</code> your files to the appropriate group prior to rsyncing to MSS.</p>"},{"location":"Documentation/Managing_Data/Transferring_Files/#small-transfers-100gb-outside-of-the-nrel-network","title":"Small Transfers (&lt;100GB) outside of the NREL network","text":"<p><code>rsync</code>, <code>scp</code>, and <code>curl</code> will be your best option for small transfers (&lt;100GB) outside of the NREL network. If your rsync/scp/curl transfers are taking hours to complete then you should consider using Globus.</p> <p>If you're transferring many files then you should use rsync:</p> <pre><code>$ rsync -azP --no-g /mss/users/username/dataset1/ user@desthost:/home/username/dataset1/\n</code></pre> <p>If you're transferring an individual file then use scp:</p> <pre><code>$ scp /home/username/example.tar.gz user@desthost:/home/username/\n</code></pre> <p>You can use curl or wget to download individual files: <pre><code>$ curl -O https://URL\n$ wget https://URL\n</code></pre></p> <p>Additional rsync examples are available here.</p>"},{"location":"Documentation/Managing_Data/Transferring_Files/#large-transfers-100gb-outside-of-the-nrel-network","title":"Large Transfers (&gt;100GB) outside of the NREL network","text":"<p>Globus is optimized for file transfers between data centers and anything outside of the NREL network. It will be several times faster than any other tools you will have available. Documentation about requesting a HPC Globus account is available on the Globus Services page on the HPC website.  See Transfering files using Globus for instructions on transfering files with Globus.</p>"},{"location":"Documentation/Managing_Data/Transferring_Files/#transfering-files-using-windows","title":"Transfering files using Windows","text":"<p>For Windows you will need to download WinSCP to transfer files to and from HPC systems over SCP. See Transfering using WinSCP.</p>"},{"location":"Documentation/Managing_Data/Transferring_Files/#archiving-files-and-directories","title":"Archiving files and directories","text":"<p>Learn various techniques to combine and compress multiple files or directories into a single file to reduce storage footprint or simplify sharing.</p>"},{"location":"Documentation/Managing_Data/Transferring_Files/#tar","title":"tar","text":"<p><code>tar</code>, along with <code>zip</code>, is one of the basic commands to combine multiple individual files into a single file (called a \"tarball\"). <code>tar</code> requires at least one command line option. A typical usage would be: <pre><code>$ tar -cf newArchiveName.tar file1 file2 file3\n# or\n$ tar -cf newArchiveName.tar /path/to/folder/\n</code></pre></p> <p>The <code>-c</code> flag denotes creating an archive, and <code>-f</code> denotes that the next argument given will be the archive name\u2014in this case it means the name you would prefer for the resulting archive file. </p> <p>To extract files from a tar, it's recommended to use: <pre><code>$ tar -xvf existingArchiveName.tar\n</code></pre> <code>-x</code> is for extracting, <code>-v</code> uses verbose mode which will print the name of each file as it is extracted from the archive.</p>"},{"location":"Documentation/Managing_Data/Transferring_Files/#compressing","title":"Compressing","text":"<p><code>tar</code> can also generate compressed tarballs which reduce the size of the resulting archive. This can be done with the <code>-z</code> flag (which just calls <code>gzip</code> on the resulting archive automatically, resulting in a <code>.tar.gz</code> extension) or <code>-j</code> (which uses <code>bzip2</code>, creating a <code>.tar.bz2</code>).</p> <p>For example:</p> <pre><code># gzip\n$ tar -czvf newArchive.tar.gz file1 file2 file3\n$ tar -xvzf newArchive.tar.gz\n\n# bzip2\n$ tar -czjf newArchive.tar.bz2 file1 file2 file3\n$ tar -xvjf newArchive.tar.bz2\n</code></pre>"},{"location":"Documentation/Managing_Data/Transferring_Files/FileZilla/","title":"Transferring files using FileZilla","text":"<p>FileZilla can be used to securely transfer files between your local computer running Windows, Linux or MacOS to a remote computer running Linux.</p>"},{"location":"Documentation/Managing_Data/Transferring_Files/FileZilla/#setting-up-filezilla","title":"Setting Up FileZilla","text":"<ul> <li>Download and install FileZilla.</li> </ul>"},{"location":"Documentation/Managing_Data/Transferring_Files/FileZilla/#connecting-to-a-host","title":"Connecting to a Host","text":"<ul> <li>Decide which host you wish to connect to such as, kestrel.hpc.nrel.gov</li> <li>Enter your username in the Username field.</li> <li>Enter your password or Password+OTP Token in the Password field.</li> <li>Use 22 as the Port.</li> <li>Click the 'Quickconnect' button.</li> </ul>"},{"location":"Documentation/Managing_Data/Transferring_Files/FileZilla/#transferring-files","title":"Transferring Files","text":"<p>You may use FileZilla to transfer individual files or directories from the Local Directory to the Remote Directory or vice versa.</p> <p>Transfer files by dragging them from the Local Directory (left pane) to the Remote Directory (right pane) or vice versa.  Once the transfer is complete the selected file will be visible in the pane it was transferred to.</p>"},{"location":"Documentation/Managing_Data/Transferring_Files/globus/","title":"Transferring Files with Globus","text":"<p>For large data transfers between NREL\u2019s high-performance computing (HPC) systems and another data center, or even a laptop off-site, we recommend using Globus.</p> <p>A supporting set of instructions for requesting a Globus account and data transfer using Globus is available on the HPC NREL Website</p>"},{"location":"Documentation/Managing_Data/Transferring_Files/globus/#what-is-globus","title":"What Is Globus?","text":"<p>Globus provides services for research data management, including file transfer. It enables you to quickly, securely and reliably move your data to and from locations you have access to.</p> <p>Globus transfers files using GridFTP. GridFTP is a high-performance data transfer protocol which is optimized for high-bandwidth wide-area networks.  It provides more reliable high performance file transfer and synchronization than scp or rsync. It automatically tunes parameters to maximize bandwidth while providing automatic fault recovery and notification of completion or problems.</p>"},{"location":"Documentation/Managing_Data/Transferring_Files/globus/#get-a-globus-account","title":"Get a Globus Account","text":"<p>To get a Globus account, sign up on the Globus account website.</p>"},{"location":"Documentation/Managing_Data/Transferring_Files/globus/#globus-nrel-endpoints","title":"Globus NREL Endpoints","text":"<p>The current NREL Globus Endpoints are:</p> <ul> <li>nrel#kglobus_projects - This endpoint will give you access to any files you have on the Kestrel Project File System: /datasets, /projects, and /shared-projects.</li> <li>nrel#kglobus_scratch - This endpoint will give you access to any files you have on the Kestrel Scratch File System: /scratch.</li> <li>nrel#vast - This endpoint will give you access to files you have on our VAST file system: /campaign and /bscl. It is available for other shares on VAST upon request.  </li> </ul> <p>Warning</p> <p>Note that if you already have a Globus account with a different institution, such as a university, be sue sure to select an \"NREL OIDC\" identity as the \"Owner Identity\" when connecting to an NREL endpoint. Otherwise, you will encounter permission errors. </p>"},{"location":"Documentation/Managing_Data/Transferring_Files/globus/#globus-personal-endpoints","title":"Globus Personal Endpoints","text":"<p>You can set up a \"Globus Connect Personal EndPoint\", which turns your personal computer into an endpoint, by downloading and installing the Globus Connect Personal application on your system. </p>"},{"location":"Documentation/Managing_Data/Transferring_Files/globus/#set-up-a-personal-endpoint","title":"Set Up a Personal EndPoint","text":"<ul> <li>Download Globus Connect Personal</li> <li>Once installed, you will be able to start the Globus Connect Personal   application locally, and login using your previously created Globus    account credentials.</li> <li>Within the application, you will need to grant consent for Globus to access   and link your identity before creating a collection that will be visible from   the Globus Transfer website.</li> <li>Additional tutorials and information on this process is located at the Globus   Website for both Mac and Windows.</li> </ul>"},{"location":"Documentation/Managing_Data/Transferring_Files/globus/#transferring-files","title":"Transferring Files","text":"<p>You can transfer files with Globus through the Globus Online website or via the CLI  (command line interface).</p> <p>Important</p> <p>It is strongly recommended to compress multiple files into a single archive (tar.gz, zip) before transferring data with Globus.</p> <p>To compress a directory: <pre><code>tar -czvf filename.tar.gz /path/to/dir\n</code></pre> To extract an archive: <pre><code>tar -xzvf filename.tar.gz\n</code></pre></p> Globus Online <p>Globus Online is a hosted service that allows you to use a browser to transfer files between trusted sites called \"endpoints\".  To use it, the Globus software must be installed on the systems at both ends of the data transfer. The NREL endpoints are listed above.</p> <ol> <li>Click Login on the Globus web site. On the login page select \"Globus ID\" as the login method and click continue.  Use the Globus credentials you used to register your Globus.org account.  </li> <li>The ribbon on the left side of the screen acts as a Navigator, select File Manager if not already selected.  In addition, select the 'middle' option for Panels in the upper right, which will display space for two Globus endpoints. </li> <li>The collection tab will be searchable (e.g. nrel), or one of the NREl endpoints (e.g. nrel#kglobus_projects) can be  entered in the left collection tab.  In the box asking for authentication, enter  your NREL HPC username and password.  Do not use your globus.org username  or password when authenticating to the NREL endpoints.</li> <li>Select another Globus endpoint, such as a personal endpoint or  an endpoint at another institution that you have access to. To use your personal endpoint, first start the Globus Connect Personal application.  Then search for either the endpoint name or your username in the collections tab,  and select your endpoint. After the first use, you should see your endpoints in  the recent tab when searching.  You may also setup an endpoint/directory as a bookmark.</li> <li>To transfer files:<ul> <li>select the files you want to transfer from one of the endpoints </li> <li>select the destination location in the other endpoint (a folder or directory) </li> <li>click the 'start' button on the source collection, and it will transfer files   to the target collection</li> </ul> </li> <li>For additional information, the Globus Webpage has  tutorials and documentation under the Resources tab.</li> </ol> <p>When your transfer is complete, you will be notified by email.</p> Globus CLI (command line interface) <p>Globus supports a command line interface (CLI), which can be used for scripting and automating some transfer tasks.  For more information, it is suggested that the user refer to the Globus CLI documentation located on the Globus Webpage.</p> <p>For installing globus-cli, the recommendation is to use a Conda environment.  In this  case, it is advised to follow the instructions about mixing Conda and Pip,  and only use Pip after establishing a base environment using Conda.  For more information about mixing Conda and Pip, refer to our internal documentation at: Conda</p>"},{"location":"Documentation/Managing_Data/Transferring_Files/winscp/","title":"WinSCP for Windows File Transfers","text":"<p>WinSCP can be used to securely transfer files between your local computer running Microsoft Windows and a remote computer running Linux.</p>"},{"location":"Documentation/Managing_Data/Transferring_Files/winscp/#setting-up-winscp","title":"Setting Up WinSCP","text":"<p>Download and install WinSCP.</p> <p>You may follow the prompts to import your PuTTY sites to simplify host management.</p>"},{"location":"Documentation/Managing_Data/Transferring_Files/winscp/#connecting-to-a-host","title":"Connecting to a Host","text":"<ul> <li>Set up a host (if needed) by selecting \"New Site\" and providing a host name (e.g., kestrel.nrel.gov) and your user name.  In most cases, use the SFTP protocol.</li> <li>Connect to the server by selecting a site and clicking [Login].</li> <li>Enter your password or Password+Token when prompted.</li> </ul>"},{"location":"Documentation/Managing_Data/Transferring_Files/winscp/#transferring-files","title":"Transferring Files","text":"<p>You may use WinSCP to transfer individual files or to synchronize the Local Directory to the Remote Directory.</p> <p>Transfer files by dragging them from the Local Directory (left pane) to the Remote Directory (right pane) or vice versa.  Once the transfer is complete the selected file will be visible in the Remote Directory pane.</p> <p>Synchronizing directories allows you to easily replicate changes affecting entire directory structures back and forth.  To synchronize the Remote Directory and the Local Directory select Synchronize from the Commands menu. Select the Synchronize Files mode and click OK.</p>"},{"location":"Documentation/Resource_Management_System/","title":"Lex Allocation Management System","text":"<p>Lex is a system for managing your high-performance computing (HPC) resource assignments, account information, and tracking utilization.</p>"},{"location":"Documentation/Resource_Management_System/#access","title":"Access","text":"<p>An NREL HPC account is required to access Lex. To log in to Lex, open a web browser to https://hpcprojects.nrel.gov. Log in with your NREL HPC username and password. An OTP token is not required to authenticate. </p>"},{"location":"Documentation/Resource_Management_System/#requesting-an-allocation","title":"Requesting an Allocation","text":"<p>The system resource allocation request form is available on Lex. </p> <p>Once logged in, the request form buttons will be on the home page. To request an allocation for an out-of-cycle/pilot allocation for use in the current fiscal year, click the current year's button. When the allocation cycle is open, a button will be available for the next fiscal year. </p> <p></p> <p>If you have an existing allocation that you need to continue for the next fiscal year and you are one of the leads on it, there is an option to copy the information over to a new request form as a starting point. </p> <p></p> <p>Warning</p> <p>An allocation request is required each fiscal year, even if your project is continuing.  </p> <p>Info</p> <p>You should submit one allocation request per funded project. Do not split projects up into multiple allocations or use one large allocation for multiple projects.</p>"},{"location":"Documentation/Resource_Management_System/#request-type","title":"Request Type","text":"<p>If your request is for 150,000 AUs or less, check the Pilot Request option. Fewer fields are required for pilot requests, so be sure to select this option before filling out the rest of the form. If approved, pilots are granted 150,000 AUs on Kestrel and 10TB of /projects storage by default. Pilot projects can be granted at anytime throughout the year. </p> <p></p> <p>Info</p> <p>Once submitted, the pilot request will automatically put the 150,000 AU request in the CPU AUs section. Note that once awarded, these AUs can be used on CPUs and GPUs. </p>"},{"location":"Documentation/Resource_Management_System/#project-information","title":"Project Information","text":"<p>A project handle other than the default is required. It is used for directory names and for the account used to submit jobs under the allocation. Years and names are not allowed in the handle, as it will be carried over from year to year if your allocation continues. </p> <p>Tip</p> <p>Use the info buttons next to the fields for more information on the question's requirements. </p> <p>The HPC Lead is the person with primary responsibility for the computational work of the allocation. They are the lead on managing user access to the allocation and permissions for the /projects storage directory. The HPC Lead and HPC Alternate Lead are required to have NREL HPC accounts and will be contacted for approving changes to the allocation's user list.</p> <p></p>"},{"location":"Documentation/Resource_Management_System/#computational-resources-and-readiness","title":"Computational Resources and Readiness","text":"<p>To calculate the AU request for the allocation, input the software that you will be using and information about the runs that will be using that software. You should add an entry for each major application that you will be running. Fractional node values are allowed and can be used if your runs don't require a full node's resources. You can assign GPU and/or CPU hours to a software record. The total AU calculation for all of the software entries is used to automatically populate the resource request for the allocation. Please see the image below for an example computational request. </p> <p></p> <p>Tip</p> <p>Kestrel's GPU nodes are all shared and have 4 GPUs per node. You can use fractional values for the GPU Nodes field to reflect the number of GPUs per node that your runs will be using.</p> <p>Info</p> <p>Note that all awarded AUs can be used on either CPUs or GPUs, but we have them split in the request for informational and planning purposes. </p> <p>The Data Storage request is the amount of data in terabytes that you need on the /projects filesystem. This space is the primary persistent store of data for the project. If your request is over 10TB, you also need to fill out the Storage Request Explanation field with a high-level, quantitative breakdown of what you are estimating to store and the size. </p> <p>The Use Pattern describes how you will use your AUs throughout the year. The closer the pattern matches your actual use, the better priority you will have and the less likely you are to lose unused AUs. </p> <p>Spread Options</p> <p>Distribute equally across 4 quarters: even AU split across quarters. Designed for ongoing projects.</p> <p>Development in Q1, production in Q2 or later: 10% Q1, 30% in remaining quarters. Designed for projects starting off and need time to develop code and workflow.</p> <p>Start in 2nd Quarter: 33% in Q2-Q4. Designed for projects with late starts.</p> <p>Use in first half of FY: 45% in Q1 and Q2, 5% in Q3 and Q4. Designed for projects with mid-year end dates or early milestones.</p> <p>Use in second half of FY: 5% in Q1 and Q2, and 45% in Q3 and Q4. Designed for projects with mid-year star dates or late milestones.</p> <p>The Computational Approach should be a high-level HPC-centric abstract of the computational method that the project will use, including what software and what types of calculations you will be doing. </p>"},{"location":"Documentation/Resource_Management_System/#submitting-your-request","title":"Submitting your Request","text":"<p>You can save your request as many times as needed, but once it's submitted you will need to contact HPC Operations to change it. Be sure that you selected the Pilot Request option if your request is under 150,000 AUs. </p> <p>After you have submitted your project, it will undergo an initital screening and a Technical Readiness Review. You may be contacted by the NREL HPC team with questions; please resond to these emails as soon as possible to ensure your request can be processed on time. </p> <p>For further information on allocations and how to request one, please visit the Resource Allocations page. </p> <p>Tip</p> <p>Please contact HPC-Requests@nrel.gov for any questions about the allocation request process. </p>"},{"location":"Documentation/Resource_Management_System/#managing-users","title":"Managing Users","text":"<p>The allocation PI, Project Lead, and Alternate Project Lead can use Lex to manage the allocation entitlement list. </p> <p>To add and remove users, navigate to the \"Manage Users\" page for the allocation. </p> <p>To add users, click the \"Add User\" button and enter the user's email. Repeat for any additional users. Click \"Delete\" to remove users from the allocation entitlement list. Be sure to click \"Submit Changes\" to apply your changes.</p> <p></p> <p>Users who have an existing NREL HPC account will be added to the allocation group and permissions will be granted within 24 hours.  Otherwise, they will be sent an email invitation to request an account and will be listed in the \"Pending Invitations\" list until their account has been created. You can cancel an invitation by selecting the \"Cancel\" button and clicking \"Submit Changes\". </p>"},{"location":"Documentation/Resource_Management_System/#tracking-allocation-usage","title":"Tracking Allocation Usage","text":""},{"location":"Documentation/Resource_Management_System/#au-usage-data","title":"AU Usage Data","text":"<p>The sidebar on the Lex homepage shows all of the allocations that you have access to. Select a project to check the resource usage.</p> <p></p> <p>On the \"AU Use Report\" page, the \"Project Use\" section lists all of the systems that the project has an allocation on and corresponding AU usage. The \"AUs Charged by User\" section provides a breakdown of AU usage by user for the project. By default, the data displayed shows each user's total usage across all systems. To filter this to show data for a specific system, use the system button as shown below. </p> <p></p>"},{"location":"Documentation/Resource_Management_System/#aus_report-command-line-utility","title":"aus_report Command Line Utility","text":"<p>There is a CLI utility <code>aus_report</code> available on NREL HPC systems to track your AU usage. This utility uses the data from Lex to output AU usage information on a per-allocation and per-user basis. Please refer to Lex in the case of an discrepancies. Run <code>aus_report --help</code> for more information. </p>"},{"location":"Documentation/Resource_Management_System/#jobs-data","title":"Jobs Data","text":"<p>To see the list of jobs run using the allocation, select the \"List Jobs\" tab. </p> <p></p> <p>To filter the job data, such as by a specific user or partition, use the search bar. Click the column headers to sort the list by that feature. </p> <p></p> <p>To see a summary about a job, click the job ID.</p> <p></p> <p>For any questions or feedback on Lex, please contact HPC-Help@nrel.gov.</p>"},{"location":"Documentation/Slurm/","title":"Slurm Job Scheduler","text":""},{"location":"Documentation/Slurm/#schedule-your-computational-work-with-slurm","title":"Schedule Your Computational Work with Slurm","text":"<p>Slurm is the job scheduler and workload manager used by the HPC clusters hosted at NREL. </p> <p>A job contains a list of required consumable resources (such as nodes), a list of job constraints (when, where and how the job should run), and an execution environment, which includes things like an executable, input and output files. All computational work on an HPC cluster should generally be contained in a job.</p> <p>There are two key types of jobs:</p> <ul> <li> <p>Batch jobs are unattended scripts that launch programs to complete computational work. Batch jobs are placed in a queue and launched at a future time and date, determined by the priority of the job. Batch jobs are submitted to the queue using the <code>sbatch</code> command. </p> </li> <li> <p>Interactive jobs provide a shell prompt on a compute node and allow for software to be run that requires keyboard input from the user. The <code>salloc</code> and <code>srun</code> commands can be used to start an interactive job.</p> </li> </ul> <p>Most computational work is typically submitted as a batch script and queued for later automatic execution. Results from standard output and/or standard error will be stored in a file or files by Slurm (this behavior is customizable in your sbatch script.) Your software may or may not also produce its own output files.</p> <p>Please see the navigation bar on the left under the Slurm Job Scheduling section for more information about how to submit a job.</p>"},{"location":"Documentation/Slurm/batch_jobs/","title":"Running Batch Jobs","text":""},{"location":"Documentation/Slurm/batch_jobs/#job-scheduling-and-management","title":"Job Scheduling and Management","text":"<p>Batch jobs are run by submitting a job script to the scheduler with the <code>sbatch</code> command. The job script contains the commands needed to set up your environment and run your application. (This is an \"unattended\" run, with results written to a file for later access.)</p> <p>Once submitted, the scheduler will insert your job script into the queue to be run at some point in the future, based on priority and how many jobs are in the queue currently.</p> <p>Priority factors vary on a cluster-by-cluster basis, but typically include a \"fairshare\" value based on the resources assigned to the allocation, as well as weighting by the job's age, partition, resources (e.g. node count) and/or Quality of Service (qos) factor. Please see the Monitoring and Control commands page for more information on checking your job's priority. The Systems documentation for each cluster will also have more information about the priority weighting, QOS factors, and any associated AU upcharges. </p> <p>To submit batch jobs on an HPC system at NREL, the Slurm <code>sbatch</code> command should be used:</p> <p><code>$ sbatch --account=&lt;project-handle&gt; &lt;batch_script&gt;</code></p> <p>Sbatch scripts may be stored on or run from any file system (/home or /projects, for example), as they are typically fairly lightweight shell scripts. However, on most HPC systems it's generally a good idea to have your executables, conda environments, other software that your sbatch script executes stored in a /projects directory. Your input and output files should typically be read from and/or written to either /projects or /scratch directories, as well. Please see the appropriate Systems page for more information specific to the filesystems on the NREL-hosted cluster you're working on to maximize I/O performance.</p> <p>Arguments to <code>sbatch</code> may be used to specify resource limits such as job duration (referred to as \"walltime\"), number of nodes, etc., as well as what hardware features you want your job to run with. These can also be supplied within the script itself by placing #SBATCH comment directives within the file. </p>"},{"location":"Documentation/Slurm/batch_jobs/#required-flags","title":"Required Flags","text":"<p>Resources for your job are requested from the scheduler either through command line flags to sbatch, or directly inside your script with an <code>#SBATCH</code> directive. All jobs require the following two flags to specify an allocation (\"account\") to charge the compute time to, and a maximum duration:</p> Parameter Flag Example Explanation Project handle <code>--account</code>, <code>-A</code> <code>--account=&lt;handle&gt;</code> or <code>-A &lt;handle&gt;</code> Project handles are provided by HPC Operations at the beginning of an allocation cycle. Maximum Job Duration (walltime) <code>--time</code>, <code>-t</code> <code>--time=1-12:05:50</code> (1 day, 12 hours, 5 minutes, and 50 seconds)  or  <code>-t5</code> (5 minutes) Recognized Time Formats:  <code>&lt;days&gt;-&lt;hours&gt;</code> <code>&lt;days&gt;-&lt;hours&gt;:&lt;min&gt;</code> <code>&lt;days&gt;-&lt;hours&gt;:&lt;min&gt;:&lt;sec&gt;</code> <code>&lt;hours&gt;:&lt;min&gt;:&lt;sec&gt;</code> <code>&lt;min&gt;:&lt;sec&gt;</code> <code>&lt;min&gt;</code>"},{"location":"Documentation/Slurm/batch_jobs/#resource-request-descriptions","title":"Resource Request Descriptions","text":"<p>Specific resources may be requested from the scheduler to help the scheduler assign appropriate number and type of node or nodes to your job:</p> Parameter Flag Example Explanation Nodes, Tasks, MPI Ranks <code>--nodes</code> or <code>-N</code> <code>--ntasks</code> or <code>-n</code> <code>--ntasks-per-node</code> <code>--nodes=20</code> <code>--ntasks=40</code> <code>--ntasks-per-node=20</code> if <code>ntasks</code> is specified, it is important to indicate the number of nodes request as well. This helps with scheduling jobs on the fewest possible Ecells (racks) required for the job.  The maximum number of tasks that can be assigned per node is equal to the CPU (core) count of the node. Memory <code>--mem</code> <code>--mem-per-cpu</code> <code>--mem=50000</code> Memory per node  memory per task/MPI rank Local disk (/tmp/scratch) <code>--tmp</code> <code>--tmp=10TB</code><code>--tmp=100GB</code><code>--tmp=1000000</code> Request /tmp/scratch space in megabytes (default), GB, or TB. GPUs <code>--gpus</code> <code>--gpus=2</code> Requests 2 GPUs. See system information for total number of GPUs."},{"location":"Documentation/Slurm/batch_jobs/#job-management-and-output","title":"Job Management and Output","text":"<p>Command and control and monitoring customization are also available:</p> Parameter Flag Example Explanation High priority <code>--qos</code> <code>--qos=high</code> High-priority jobs will take precedence in the queue. Note: There is an AU penalty of 2X for high-priority jobs. Standby priority <code>--qos</code> <code>--qos=standby</code> Standby jobs will only run when nodes are idle. Note: Jobs with standby priority do not consume AUs. Dependencies <code>--dependency</code> <code>--dependency=&lt;condition&gt;:&lt;job_id&gt;</code> Conditions:<code>after</code><code>afterany</code><code>afternotok</code><code>afterok</code><code>singleton</code> You can submit jobs that will wait until a condition is met before running. Conditions:After the listed jobs have startedAfter the listed jobs have finishedAfter the listed jobs have failedAfter the listed jobs return exit code 0After all existing jobs with the same name and user have ended Job Name <code>--job-name</code> <code>--job-name=myjob</code> A short, descriptive job name for easier identification in the queue. Email notifications <code>--mail-user</code> <code>--mail-user=my.email@nrel.gov</code><code>--mail=type=ALL</code> Slurm will send updates on job status change. Type can be specified with <code>--mail-type</code> as BEGIN, END, FAIL, or ALL. Output <code>--output</code><code>--error</code> <code>--output=job_stdout</code><code>--output=job_stderr</code> Defaults to <code>slurm-&lt;jobid&gt;.out</code>Defaults to <code>slurm-&lt;jobid&gt;.out</code> (same file as stdout) stdout and stderr will be written to the same file unless specified otherwise"},{"location":"Documentation/Slurm/batch_jobs/#commonly-used-slurm-environment-variables","title":"Commonly Used Slurm Environment Variables","text":"<p>You may use these environment variables in your sbatch scripts to help control or monitor various aspects of your job directly within the script, as well:</p> Parameter Semantic Value Sample Value <code>$LOCAL_SCRATCH</code> Absolute directory path for local-only disk space per node. This should always be /tmp/scratch for compute nodes with local disk. <code>/tmp/scratch</code> <code>$TMPDIR</code> Path for temporary directory for scratch space. Uses local storage on compute nodes with local disk, and RAM on those without. <code>/tmp/scratch/&lt;JOBID&gt;</code> (default value on Kestrel) <code>$SLURM_CLUSTER_NAME</code> The cluster name as per the master configuration in Slurm. Identical to <code>$NREL_CLUSTER</code>. <code>kestrel</code>, <code>swift</code> <code>$SLURM_CPUS_ON_NODE</code> Quantity of CPUs per compute node. <code>104</code> <code>$SLURMD_NODENAME</code> Slurm name of the node on which the variable is evaluated. Matches hostname. <code>r4i2n3</code> <code>$SLURMD_JOB_ACCOUNT</code> The Slurm account used to submit the job. Matches the project handle. <code>csc000</code> <code>$SLURM_JOB_CPUS_PER_NODE</code> Contains value of <code>--cpus-per-node</code>, if specified. Should be equal or less than <code>$SLURM_CPUS_ON_NODE</code>. 104 <code>$SLURM_JOBID</code> or <code>$SLURM_JOB_ID</code> Job ID assigned to the job. 521837 <code>$SLURM_JOB_NAME</code> The assigned name of the job, or the command run if no name was assigned. bash <code>$SLURM_JOB_NODELIST</code> or <code>$SLURM_NODELIST</code> Hostnames of all nodes assigned to the job, in Slurm syntax. <code>r4i2n[1,3-6]</code> <code>$SLURM_JOB_NUM_NODES</code> or <code>$SLURM_NNODES</code> Quantity of nodes assigned to the job. 5 <code>$SLURM_JOB_PARTITION</code> The scheduler partition the job is assigned to. short <code>$SLURM_JOB_QOS</code> The Quality of Service the job is assigned to. high <code>$SLURM_NODEID</code> A unique index value for each node of the job, ranging from 0 to <code>$SLURM_NNODES</code>. 0 <code>$SLURM_STEP_ID</code> or <code>$SLURM_STEPID</code> Within a job, sequential srun commands are called \"steps\". Each srun increments this variable, giving each step a unique index nmber. This may be helpful for debugging, when seeking which step a job fails at. 0 <code>$SLURM_STEP_NODELIST</code> Within a job, <code>srun</code> calls can contain differing specifications of how many nodes should be used for the step. If your job requests 5 total nodes and you used <code>srun --nodes=3</code>, this variable would contain the list of the 3 nodes that participated in this job step. <code>r4i2n[2-4]</code> <code>$SLURM_STEP_NUM_NODES</code> Returns the quantity of nodes requested for the job step (see entry on <code>$SLURM_STEP_NODELIST</code>.) 3 <code>$SLURM_STEP_NUM_TASKS</code> Returns the quantity of tasks requested to be executed in the job step. Defaults to the task quantity of the job request. 1 <code>$SLURM_STEP_TASKS_PER_NODE</code> Contains the value specified by <code>--tasks-per-node</code> in the job step. Defaults to the tasks-per-node of the job request. 1 <code>$SLURM_SUBMIT_DIR</code> Contains the absolute path of the directory the job was submitted from. <code>/projects/csc000</code> <code>$SLURM_SUBMIT_HOST</code> The hostname of the system from which the job was submitted. Should always be a login node. el1 <code>$SLURM_TASKS_PER_NODE</code> Contained the value specified by <code>--tasks-per-node</code> in the job request. 1"},{"location":"Documentation/Slurm/batch_jobs/#example-sbatch-script-walkthrough","title":"Example SBATCH Script Walkthrough","text":"<p>Many examples of sbatch scripts are available in the HPC Repository Slurm Directory on Github.</p> <p>Here's a basic template job script to get started, followed by a breakdown of the individual components of the script. This script may be adapted to any HPC system with minor modifications. Copy it into a file on the cluster, make any necessary changes, and save it as a file, e.g. \"myjob.sh\". </p> <pre><code>#!/bin/bash\n#SBATCH --account=&lt;allocation&gt;\n#SBATCH --time=4:00:00\n#SBATCH --job-name=job\n#SBATCH --mail-user=your.email@nrel.gov\n#SBATCH --mail-type=BEGIN,END,FAIL\n#SBATCH --output=job_output_filename.%j.out  # %j will be replaced with the job ID\n\nmodule load myprogram\nmyprogram.sh\n</code></pre>"},{"location":"Documentation/Slurm/batch_jobs/#script-details","title":"Script Details","text":"<p>Here is a section-by-section breakdown of the sample sbatch script, to help you begin writing your own.</p>"},{"location":"Documentation/Slurm/batch_jobs/#script-begin","title":"Script Begin","text":"<p><code>#!/bin/bash</code></p> <p>This denotes the start of the script, and that it is written in BASH shell language, the most common Linux environment.</p>"},{"location":"Documentation/Slurm/batch_jobs/#sbatch-directives","title":"SBATCH Directives","text":"<p><pre><code>#SBATCH --account=&lt;allocation&gt;\n#SBATCH --time=4:00:00\n#SBATCH --job-name=job\n#SBATCH --mail-user=your.email@nrel.gov\n#SBATCH --mail-type=BEGIN,END,FAIL\n#SBATCH --output=job_output_filename.%j.out  # %j will be replaced with the job ID\n</code></pre> Generalized form:</p> <p><code>#SBATCH --&lt;command&gt;=&lt;value&gt;</code></p> <p>Command flags to the sbatch program are given via <code>#SBATCH</code> directives in the sbatch script. There are many flags available that can affect your job, listed in the previous section. Please see the official Slurm documentation on sbatch for a complete list, or view the man page on a login node with <code>man sbatch</code>.</p> <p>Sbatch directives must be at the beginning of your sbatch script. Once a line with any other non-directive content is detected, Slurm will no longer parse further directives.</p> <p>Note that sbatch flags do not need to be issued via directives inside the script. They can also be issued via the commandline when submitting the job. Flags issued via commandline will supercede directives issued inside the script. For example:</p> <p><code>sbatch --account=csc000 --time=60 --partition=debug mytestjob.sh</code></p>"},{"location":"Documentation/Slurm/batch_jobs/#job-instructions","title":"Job Instructions","text":"<p>After the sbatch directive block, you may then begin executing your job. The syntax is normal BASH shell scripting. You may load system modules for software, load virtual environments, define environment variables, and execute your software to perform work.</p> <p>In the simplest form, your sbatch script should load your software module(s) required, and then execute your program.</p> <p><pre><code>module load myprogram\nsrun myprogram.sh\n</code></pre> or</p> <pre><code>module load myprogram\nmyprogram.sh\n</code></pre> <p>You may also use more advanced bash scripting as a part of your sbatch script, e.g. to set up environments, manage your input and output files, and so on.</p> <p>More system-specific information about Slurm partitions, node counts, memory limits, and other details can be found under the appropriate Systems page.</p> <p>You may also visit the \"master\" main branch of the Github repository for downloadable examples, or to contribute your own.</p>"},{"location":"Documentation/Slurm/interactive_jobs/","title":"Running Interactive Jobs","text":"<p>Interactive jobs provide a shell prompt on a compute node. This allows users to execute commands and scripts \"live\" as they would on the login nodes, with direct user input and output immediately available. </p> <p>Login nodes are primarily intended to be used for logging in, editing scripts, and submitting batch jobs. Interactive work that involves substantial resources\u2014either memory, CPU cycles, or file system I/O\u2014should be performed on the compute nodes rather than on login nodes.</p> <p>Interactive jobs may be submitted to any partition and are subject to the same time and node limits as non-interactive jobs.</p>"},{"location":"Documentation/Slurm/interactive_jobs/#requesting-interactive-access","title":"Requesting Interactive Access","text":"<p>The <code>salloc</code> command is used to start an interactive session on one or more compute nodes. When resources become available, interactive access is provided by a shell prompt. The user may then work interactively on the node for the time specified.</p> <p>The job is held until the scheduler can allocate a node to you. You will see a series of messages such as: </p> <pre><code>$ salloc --time=30 --account=&lt;handle&gt; --nodes=2\nsalloc: Pending job allocation 512998\nsalloc: job 512998 queued and waiting for resources\nsalloc: job 512998 has been allocated resources\nsalloc: Granted job allocation 512998\nsalloc: Waiting for resource configuration\nsalloc: Nodes x1008c7s6b1n0,x1008c7s6b1n1 are ready for job\n[hpc_user@x1008c7s6b1n0 ~]$ \n</code></pre> <p>You can view the nodes that are assigned to your interactive jobs using one of these methods:</p> <pre><code>$ echo $SLURM_NODELIST\nx1008c7s6b1n[0-1]\n$ scontrol show hostname\nx1008c7s6b1n0\nx1008c7s6b1n1\n</code></pre> <p>Once a job is allocated, you will automatically \"ssh\" to the first allocated node so you do not need to manually ssh to the node after it is assigned. If you requested more than one node, you may ssh to any of the additional nodes assigned to your job. </p> <p>You may load modules, run applications, start GUIs, etc., and the commands will execute on that node instead of on the login node.</p> <p>Note</p> <p>When requesting multiple nodes, please use number of nodes <code>--nodes</code> (or <code>-N</code>) instead of number of tasks <code>--ntasks</code> (or <code>-n</code>) to reduce the total number of network \"hops\" between the allocated nodes.  </p> <p>Type <code>exit</code> when finished using the node.</p> <p>Interactive jobs are useful for many tasks. For example, to debug a job script, users may submit a request to get a set of nodes for interactive use. When the job starts, the user \"lands\" on a compute node, with a shell prompt. Users may then run the script to be debugged many times without having to wait in the queue multiple times.</p> <p>A debug job allows up to two nodes to be available with shorter wait times when the system is heavily utilized. This is accomplished by specifying <code>--partition=debug</code>. For example:</p> <pre><code>[hpc_user@kl1 ~]$ salloc --time=60 --account=&lt;handle&gt; --partition=debug\n</code></pre> <p>Add <code>--nodes=2</code> to claim two nodes.</p> <p>Add <code>--gpus=#</code> (substituting the number of GPUs you want to use) to claim a debug GPU node. Note that there are fewer GPU nodes in the debug queue, so there may be more of a wait time.</p> <p>A debug job on any node type will only be available for jobs with a maximum walltime (--time) of 1 hour, and only one debug job at a time is permitted per person.</p>"},{"location":"Documentation/Slurm/interactive_jobs/#sample-interactive-job-commands","title":"Sample Interactive Job Commands","text":"<p>The following command requests interactive access to one node with at least 150 GB RAM for 20 minutes:</p> <pre><code>$ salloc --time=20 --account=&lt;handle&gt; --nodes=1 --mem=150G\n</code></pre> <p>For an interactive job that will require multiple nodes, for example, running interactive software that uses MPI, launch with an salloc first:</p> <pre><code>$ salloc --time=20 --account=&lt;handle&gt; --nodes=2\n</code></pre> <p>The above salloc command will log you into one of the two nodes automatically. You can then launch your software using an srun command with the appropriate flags, such as --ntasks or --ntasks-per-node:</p> <pre><code>[hpc_user@x1008c7s6b1n0 ~]$ module purge; module load paraview\n[hpc_user@x1008c7s6b1n0 ~]$ srun --ntasks=20 --ntasks-per-node=10 pvserver --force-offscreen-rendering\n</code></pre> <p>If your single-node job needs a GUI that uses X-windows:</p> <pre><code>$ ssh -Y kestrel.hpc.nrel.gov\n...\n$ salloc --time=20 --account=&lt;handle&gt; --nodes=1 --x11\n</code></pre> <p>If your multi-node job needs a GUI that uses X-windows, the least fragile mechanism is to acquire nodes as above, then in a separate session set up X11 forwarding:</p> <pre><code>$ salloc --time=20 --account=&lt;handle&gt; --nodes=2\n...\n[hpc_user@x1008c7s6b1n0 ~]$ (your compute node x1008c7s6b1n0)\n</code></pre> <p>Then from your local workstation:</p> <pre><code>$ ssh -Y kestrel.hpc.nrel.gov\n...\n[hpc_user@kl1 ~]$ ssh -Y x1008c7s6b1n0  #(from login node to reserved compute node)\n...\n[hpc_user@x1008c7s6b1n0 ~]$  #(your compute node x1008c7s6b1n0, now X11-capable)\n[hpc_user@x1008c7s6b1n0 ~]$ xterm  #(or another X11 GUI application)\n</code></pre> <p>From a Kestrel-DAV FastX remote desktop session, you can omit the <code>ssh -Y kestrel.hpc.nrel.gov</code> above since your terminal in FastX will already be connected to a DAV (kd#) login node. </p>"},{"location":"Documentation/Slurm/interactive_jobs/#requesting-interactive-gpu-nodes","title":"Requesting Interactive GPU Nodes","text":"<p>The following command requests interactive access to GPU nodes:</p> <p><pre><code>[hpc_user@kl2 ~] $ salloc --account=&lt;handle&gt; --time=5 --gpus=2\n</code></pre> You may run the nvidia-smi command to confirm the GPUs are visible:</p> <pre><code>[hpc_user@x3100c0s29b0n0 ~] $ nvidia-smi\nWed Mar 12 16:20:53 2025\n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  NVIDIA H100 80GB HBM3          On  |   00000000:04:00.0 Off |                    0 |\n| N/A   40C    P0             71W /  699W |       0MiB /  81559MiB |      0%      Default |\n|                                         |                        |             Disabled |\n+-----------------------------------------+------------------------+----------------------+\n|   1  NVIDIA H100 80GB HBM3          On  |   00000000:64:00.0 Off |                    0 |\n| N/A   40C    P0             73W /  699W |       0MiB /  81559MiB |      0%      Default |\n|                                         |                        |             Disabled |\n+-----------------------------------------+------------------------+----------------------+\n\n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|  No running processes found                                                             |\n+-----------------------------------------------------------------------------------------+\n</code></pre>"},{"location":"Documentation/Slurm/job_arrays/","title":"Job Arrays","text":"<p>Job arrays are typically used when a user wants to submit many similar jobs with different inputs. Job arrays are capable of submitting hundreds, and even thousands, of similar jobs together. Here, we will describe how to submit job arrays on Slurm. More details on job arrays can be found in the Slurm documentation.</p> <p>An example of a job array submission script can be found in our NREL HPC Slurm Examples directory. The job array example is titled uselist.sh, and requires doarray.py and invertc.c from the source folder.</p>"},{"location":"Documentation/Slurm/job_arrays/#sbatch-directives-for-job-arrays","title":"SBATCH Directives for Job Arrays","text":"<p>In order to submit a job array to Slurm, the SBATCH directives at the top of your script or sbatch command line submission must contain the flag <code>--array=&lt;ARRAY_VALS&gt;</code>, where <code>ARRAY_VALS</code> is a list or range of numbers that will represent the index values of your job array. For example:</p> <pre><code># SBATCH --array=0-12  # Submits a job array with index values between 0 and 12\n...\n\n# SBATCH --array=2,4,6,10  # Submits a job array with index values 2, 4, 6, and 10\n...\n\n# SBATCH --array=1-43:2  # Submits a job array with index values between 1 and 43 with a step size of 2\n...\n\n# SBATCH --array=1-25%5  # Submits a job array with index values between 1 and 25 and limits the number of simultaneously running tasks to 5\n</code></pre> <p>Submitting Job Arrays on Kestrel</p> <p>To ensure that your job array is running optimally, it is recommended that job arrays are submitted on the shared partition using <code>--partition=shared</code>. See more about shared partitions on Kestrel here.</p>"},{"location":"Documentation/Slurm/job_arrays/#job-control","title":"Job Control","text":"<p>Like standard slurm jobs, job arrays have a JOB_ID, which is stored in the environment variable <code>SLURM_ARRAY_JOB_ID</code>. The environment variable <code>SLURM_ARRAY_TASK_ID</code> will hold information about the index of the job array.</p> <p>For example, if there is a job array in the queue, the output may look like this:</p> <pre><code>$ squeue\n JOBID   PARTITION     NAME     USER  ST  TIME NODES NODELIST\n 45678_1  standard    array     user  R  0:13  1     x1007c0s0b0n1\n 45678_2  standard    array     user  R  0:13  1     x1007c0s0b0n1\n 45678_3  standard    array     user  R  0:13  1     x1007c0s0b0n1\n 45678_4  standard    array     user  R  0:13  1     x1007c0s0b0n1\n</code></pre> <p>Here, <code>SLURM_ARRAY_JOB_ID</code> is 45678. The number followed by the underscore in row is the <code>SLURM_ARRAY_TASK_ID</code>. This job is a job array that was submitted with <code>--array=1-4</code>.</p> <p>Scontrol commands can be executed on entire job arrays or specific indices of a job array. <pre><code>$ scontrol suspend 45678 \n$ squeue\n JOBID   PARTITION     NAME     USER  ST  TIME NODES NODELIST\n 45678_1  standard    array     user  S  0:13  1     x1007c0s0b0n1\n 45678_2  standard    array     user  S  0:13  1     x1007c0s0b0n1\n 45678_3  standard    array     user  S  0:13  1     x1007c0s0b0n1\n 45678_4  standard    array     user  S  0:13  1     x1007c0s0b0n1\n\n$ scontrol resume 45678\n$ squeue\n JOBID   PARTITION     NAME     USER  ST  TIME NODES NODELIST\n 45678_1  standard    array     user  R  0:13  1     x1007c0s0b0n1\n 45678_2  standard    array     user  R  0:13  1     x1007c0s0b0n1\n 45678_3  standard    array     user  R  0:13  1     x1007c0s0b0n1\n 45678_4  standard    array     user  R  0:13  1     x1007c0s0b0n1\n</code></pre> <pre><code>$ scontrol suspend 45678_2 \n$ squeue\n JOBID   PARTITION     NAME     USER  ST  TIME NODES NODELIST\n 45678_1  standard    array     user  R  0:13  1     x1007c0s0b0n1\n 45678_2  standard    array     user  S  0:13  1     x1007c0s0b0n1\n 45678_3  standard    array     user  R  0:13  1     x1007c0s0b0n1\n 45678_4  standard    array     user  R  0:13  1     x1007c0s0b0n1\n\n$ scontrol resume 45678_2\n$ squeue\n JOBID   PARTITION     NAME     USER  ST  TIME NODES NODELIST\n 45678_1  standard    array     user  R  0:13  1     x1007c0s0b0n1\n 45678_2  standard    array     user  R  0:13  1     x1007c0s0b0n1\n 45678_3  standard    array     user  R  0:13  1     x1007c0s0b0n1\n 45678_4  standard    array     user  R  0:13  1     x1007c0s0b0n1\n</code></pre></p>"},{"location":"Documentation/Slurm/monitor_and_control/","title":"Commands to Monitor and Control Jobs","text":"<p>Slurm includes a suite of command-line tools used to submit, monitor, and control jobs and the job queue.</p> Command Description <code>squeue</code> Show the Slurm queue. Users can specify JOBID or USER. <code>scontrol</code> Controls various aspects of jobs such as job suspension, re-queuing or resuming jobs and can display diagnostic info about each job. <code>scancel</code> Cancel specified job(s). <code>sinfo</code> View information about all Slurm nodes and partitions. <code>sacct</code> Detailed information on accounting for all jobs and job steps. <code>sprio</code> View priority and the factors that determine scheduling priority. <p>Please see <code>man</code> pages on the cluster for more information on each command. Also see <code>--help</code> or <code>--usage</code> flags for each.</p> <p>Our Presentation on Advanced Slurm Features is also available as a resource, which has supplementary information on how to manage jobs.</p> <p>Another great resource for Slurm at NREL is this repository on Github.</p>"},{"location":"Documentation/Slurm/monitor_and_control/#usage-examples","title":"Usage Examples","text":""},{"location":"Documentation/Slurm/monitor_and_control/#squeue","title":"squeue","text":"<p>The <code>squeue</code> command is used to view the current state of jobs in the queue. </p> <p>To show your jobs:</p> <pre><code>$ squeue -u hpcuser\n           JOBID    PARTITION       NAME      USER   ST       TIME      NODES   NODELIST(REASON)\n          506955          gpu   wait_tes   hpcuser   PD       0:00          1      (Resources)\n</code></pre> <p>To show all jobs in the queue with extended information:</p> <pre><code>$ squeue -l\nThu Dec 13 12:17:31 2018\n JOBID  PARTITION NAME     USER     STATE   TIME    TIME_LIMIT   NODES  NODELIST(REASON)\n 516890 standard Job007    user1    PENDING 0:00    12:00:00    1050   (Dependency)\n 516891 standard Job008    user1    PENDING 0:00    12:00:00    1050   (Dependency)\n 516897      gpu Job009    user2    PENDING 0:00    04:00:00       1   (Resources)\n 516898 standard Job010    user3    PENDING 0:00    15:00:00      71   (Priority)\n 516899 standard Job011    user3    PENDING 0:00    15:00:00      71   (Priority)\n-----------------------------------------------------------------------------\n 516704 standard Job001    user4    RUNNING 4:09:48 15:00:00      71    r1i0n[0-35],r1i1n[0-34]\n 516702 standard Job002    user4    RUNNING 4:16:50 15:00:00      71    r1i6n35,r1i7n[0-35],r2i0n[0-33]\n 516703 standard Job003    user4    RUNNING 4:16:57 15:00:00      71    r1i5n[0-35],r1i6n[0-34]\n 516893 standard Job004    user4    RUNNING 7:19     3:00:00      71    r1i1n35,r1i2n[0-35],r1i3n[0-33]\n 516894 standard Job005    user4    RUNNING 7:19     3:00:00      71    r4i2n[20-25],r6i6n[7-35],r6i7n[0-35]\n 516895 standard Job006    user4    RUNNING 7:19     3:00:00      71    r4i2n[29-35],r4i3n[0-35],r4i4n[0-20]\n</code></pre> <p>To estimate when your jobs will start to run, use the <code>squeue --start</code> command with the JOBID.</p> <p>Note that the Slurm start times are only an estimate, and are updated frequently based on the current state of the queue and the specified <code>--time</code> of all jobs in the queue. </p> <pre><code>$ squeue --start -j 509851,509852\n JOBID    PARTITION    NAME      USER      ST          START_TIME    NODES   SCHEDNODES   NODELIST(REASON)\n 509851   short      test1.sh   hpcuser    PD                 N/A      100       (null)       (Dependency)\n 509852   short      test2.sh   hpcuser    PD 2018-12-19T16:54:00        1      r1i6n35         (Priority)\n</code></pre>"},{"location":"Documentation/Slurm/monitor_and_control/#output-customization-of-the-squeue-command","title":"Output Customization of the squeue Command","text":"<p>The displayed fields in <code>squeue</code> can be highly customized to display the information that's most relevant for the user by using the <code>-o</code> or <code>-O</code> flags. The full list of customizable fields can be found under the entries for these flags in the <code>man squeue</code> command on the system. </p> <p>By setting the environment variable export $SQUEUE_FORMAT, you can override the system's default squeue fields with your own. For example, if you run the following line (or place it in your <code>~/.bashrc</code> or <code>~/.bash_aliases</code> file to make it persistent across logins):</p> <p><code>export SQUEUE_FORMAT=\"%.18i %.15P %.8q %.12a %.8p %.8j %.8u %.2t %.10M %.6D %R\"</code></p> <p>Using <code>squeue</code> will now provide the formatted output:</p> <pre><code>JOBID    PARTITION   QOS    ACCOUNT   PRIORITY     NAME     USER    ST     TIME    NODES NODELIST(REASON)\n13141110 standard   normal  csc000    0.051768    my_job   hpcuser  R   2-04:01:17   1    r1i3n29\n</code></pre> <p>Or you may wish to add the <code>%V</code> to show the timestamp that a job was submitted, and sort by timestamp, ascending:</p> <p><code>squeue -o \"%.18i %.9P %.8j %.8u %.2t %.10M %.6D %20V %6q %12l %R\" -S \"V\"</code></p> <p>Example output:</p> <pre><code>             JOBID PARTITION     NAME     USER ST       TIME  NODES SUBMIT_TIME          QOS    TIME_LIMIT   NODELIST(REASON)\n          13166762    bigmem    first  hpcuser PD       0:00      1 2023-08-30T14:08:11  high   2-00:00:00   (Priority)\n          13166761    bigmem       P5  hpcuser PD       0:00      1 2023-08-30T14:08:11  high   2-00:00:00   (Priority)\n          13166760    bigmem       P4  hpcuser PD       0:00      1 2023-08-30T14:08:11  high   2-00:00:00   (Priority)\n          13166759    bigmem      Qm3  hpcuser PD       0:00      1 2023-08-30T14:08:11  high   2-00:00:00   (Priority)\n          13166758    bigmem       P2  hpcuser PD       0:00      1 2023-08-30T14:08:11  high   2-00:00:00   (Priority)\n          13166757    bigmem       G1  hpcuser PD       0:00      1 2023-08-30T14:08:11  high   2-00:00:00   (Priority)\n          13167383    bigmem       r8  hpcuser PD       0:00      1 2023-08-30T16:25:52  high   2-00:00:00   (Priority)\n          13167390  standard      P12  hpcuser PD       0:00      1 2023-08-30T16:25:55  high   2-00:00:00   (Priority)\n          13167391    bigmem      P34  hpcuser PD       0:00      1 2023-08-30T16:25:55  high   2-00:00:00   (Priority)\n          13167392    bigmem    qchem  hpcuser PD       0:00      1 2023-08-30T16:25:55  high   2-00:00:00   (Priority)\n          13167393     debug  testrun  hpcuser PD       0:00      1 2023-08-30T16:25:55  high   2-00:00:00   (Priority)\n          13167394    bigmem   latest  hpcuser PD       0:00      1 2023-08-30T16:25:55  high   2-00:00:00   (Priority)\n          13182480     debug  runtest  jwright2 R      31:01      1 2023-09-01T14:49:54  normal 59:00        r3i7n35\n</code></pre> <p>Many other options are available in the <code>man</code> page.</p>"},{"location":"Documentation/Slurm/monitor_and_control/#scontrol","title":"scontrol","text":"<p>To get detailed information about your job before and while it runs, you may use <code>scontrol show job</code> with the JOBID.  For example: <pre><code>$ scontrol show job 522616\nJobId=522616 JobName=myscript.sh\n UserId=hpcuser(123456) GroupId=hpcuser(123456) MCS_label=N/A\n Priority=43295364 Nice=0 Account=csc000 QOS=normal\n JobState=PENDING Reason=Dependency Dependency=afterany:522615\n</code></pre> The <code>scontrol</code> command can also be used to modify pending and running jobs: <pre><code>$ scontrol update jobid=526501 qos=high\n$ sacct -j 526501 --format=jobid,partition,state,qos\n       JobID  Partition      State        QOS\n------------ ---------- ---------- ----------\n526501            short    RUNNING       high\n526501.exte+               RUNNING\n526501.0                 COMPLETED\n</code></pre> To pause a job: <code>scontrol hold &lt;JOBID&gt;</code></p> <p>To resume a job: <code>scontrol resume &lt;JOBID&gt;</code></p> <p>To cancel and rerun: <code>scontrol requeue &lt;JOBID&gt;</code></p>"},{"location":"Documentation/Slurm/monitor_and_control/#scancel","title":"scancel","text":"<p>Use <code>scancel -i &lt;jobID&gt;</code> for an interactive mode to confirm each job_id.step_id before performing the cancel operation. Use <code>scancel --state=PENDING,RUNNING,SUSPENDED -u &lt;userid&gt;</code> to cancel your jobs by STATE or <code>scancel -u &lt;userid&gt;</code> to cancel ALL of your jobs.</p>"},{"location":"Documentation/Slurm/monitor_and_control/#sinfo","title":"sinfo","text":"<p>Use <code>sinfo</code> to view cluster information: <pre><code>$ sinfo -o %A\nNODES(A/I)\n2299/140\n</code></pre> Above, <code>sinfo</code> shows nodes Allocated (A) and nodes idle (I) in the entire cluster.</p> <p>To check the state of nodes in a partition (for example, 'gpu-h100' on Kestrel), you can run:</p> <p><pre><code>$ sinfo -o \"%A %t\" -p gpu-h100\n</code></pre> This will return the number of nodes associated with a given state ('idle', 'mix', 'alloc', etc.) at that moment. 'Idle' indicates nodes that are free, 'alloc' refers to fully allocated nodes, and 'mix' represents nodes that are not fully allocated and could accept jobs requesting less than a full node\u2019s resources. Note: a 'mix' node state is only valid for shareable partitions. </p> <p>To see specific node information use <code>sinfo -n &lt;node id&gt;</code> to show information about a single or comma-separated list of nodes. You will see the partition to which the node can allocate as well as the node STATE. <pre><code>$ sinfo -n x3100c0s17b0n0\nPARTITION       AVAIL  TIMELIMIT  NODES  STATE NODELIST\nbigmem             up 2-00:00:00      0    n/a\nbigmem-stdby       up 2-00:00:00      0    n/a\nbigmeml            up 10-00:00:0      0    n/a\nbigmeml-stdby      up 10-00:00:0      0    n/a\nshort*             up    4:00:00      0    n/a\nshort-stdby        up    4:00:00      0    n/a\nstandard           up 2-00:00:00      0    n/a\nstandard-stdby     up 2-00:00:00      0    n/a\nlong               up 10-00:00:0      0    n/a\nhbw                up 2-00:00:00      0    n/a\nhbwl               up 10-00:00:0      0    n/a\nhbw-stdby          up 2-00:00:00      0    n/a\ndebug              up    1:00:00      0    n/a\ndebug-stdby        up    1:00:00      0    n/a\nshared             up 2-00:00:00      0    n/a\nshared-stdby       up 2-00:00:00      0    n/a\nsharedl            up 10-00:00:0      0    n/a\nsharedl-stdby      up 10-00:00:0      0    n/a\ndebug-gpu          up    1:00:00      0    n/a\ndebug-gpu-stdby    up    1:00:00      0    n/a\ngpu-h100s          up    4:00:00      1    mix x3100c0s17b0n0\ngpu-h100s-stdby    up    4:00:00      1    mix x3100c0s17b0n0\ngpu-h100           up 2-00:00:00      1    mix x3100c0s17b0n0\ngpu-h100-stdby     up 2-00:00:00      1    mix x3100c0s17b0n0\ngpu-h100l          up 10-00:00:0      1    mix x3100c0s17b0n0\nvto                up 2-00:00:00      1    mix x3100c0s17b0n0\n</code></pre></p>"},{"location":"Documentation/Slurm/monitor_and_control/#sacct","title":"sacct","text":"<p>Use <code>sacct</code> to view accounting information about jobs AND job steps: <pre><code>$ sacct -j 7379855 --format=User,JobID,Jobname,partition,state,time,start,elapsed,nnodes,ncpus\n     User JobID           JobName  Partition      State  Timelimit               Start    Elapsed   NNodes      NCPUS\n--------- ------------ ---------- ---------- ---------- ---------- ------------------- ---------- -------- ----------\n hpcuser  7379855      AllReduce_   gpu-h100  COMPLETED   00:01:00 2025-03-05T18:22:43   00:00:43        4         16\n          7379855.bat+      batch             COMPLETED            2025-03-05T18:22:43   00:00:43        1          4\n          7379855.ext+     extern             COMPLETED            2025-03-05T18:22:43   00:00:43        4         16\n          7379855.0    all_reduc+             COMPLETED            2025-03-05T18:22:51   00:00:35        4         16\n</code></pre> Use <code>sacct -e</code> to print a list of fields that can be specified with the <code>--format</code> option.</p>"},{"location":"Documentation/Slurm/monitor_and_control/#sprio","title":"sprio","text":"<p>By default, <code>sprio</code> returns information for all pending jobs. Options exist to display specific jobs by JOBID and USER. <pre><code>$ sprio -u hpcuser\n  JOBID PARTITION     USER  PRIORITY       SITE        AGE  FAIRSHARE    JOBSIZE  PARTITION        QOS\n8571640     short  hpcuser  38940102          0          0   35071134      45319    3823650          0\n\nUse the `-n` flag to provide a normalized priority weighting with a value between 0-1:\n\n$ sprio -u hpcuser -n\n  JOBID PARTITION     USER    PRIORITY       AGE  FAIRSHARE    JOBSIZE  PARTITION        QOS       \n8571680     short  hpcuser  0.00906644 0.0000000  0.0881939  0.0002043  0.1000000  0.0000000\n</code></pre></p> <p>The <code>sprio</code> command also has some options that can be used to view the entire queue by priority order. The following command will show the \"long\" (<code>-l</code>) format sprio with extended information, sorted by priority in descending order (<code>-S -Y</code>), and piped through the <code>less</code> command with line numbers shown on the far left (<code>less -N</code>):</p> <p><code>sprio -S -Y -l | less -N</code></p> <pre><code>1           JOBID PARTITION     USER   PRIORITY       SITE        AGE      ASSOC  FAIRSHARE    JOBSIZE  PARTITION        QOS        NICE                 TRES\n2        13150512 standard-  hpcuser  373290120          0    8909585          0  360472143      84743    3823650          0           0\n3        13150514 standard-  hpcuser  373290070          0    8909534          0  360472143      84743    3823650          0           0\n</code></pre> <p>When <code>sprio</code> is piped through the <code>less</code> command for paginating, press the <code>/</code> key and type in a jobid or a username and press the return key to search for and jump to that jobid or username. Press <code>/</code> and hit return again to search for the next occurrence of your search term, or use the <code>?</code> instead of <code>/</code> to search upwards in the list. Press q to exit.</p> <p>Note that when piped through <code>less -N</code>, line numbers may be equated to position in the priority queue plus 1, because the top column label line of <code>sprio</code> is counted by <code>less</code>. To remove the column labels from <code>sprio</code> output, add the <code>-h</code> or <code>--noheader</code> flag to <code>sprio</code>.</p> <p>The <code>-l</code>(<code>--long</code>) flag precludes using the <code>-n</code> for normalized priority values. </p> <p>Like <code>squeue</code> and other Slurm commands, <code>sprio</code> supports the <code>-o</code> format flag to customize the columns that are displayed. For example:</p> <p><code>sprio S -Y -o \"%i %r %u %y\"</code></p> <p>Will show only the jobid, partition, username, and normalized priority. More details about output formatting are available in <code>man sprio</code>.</p>"},{"location":"Documentation/Slurm/multiple_sub_jobs/","title":"Running Multiple Sub-Jobs with One Job Script","text":"<p>If your workload consists of serial or modestly parallel programs, you can run multiple instances of your program at the same time using different processor cores on a single node. This will allow you to make better use of your allocation because it will use the resources on the node that would otherwise be idle.</p>"},{"location":"Documentation/Slurm/multiple_sub_jobs/#example","title":"Example","text":"<p>For illustration, we use a simple C code to calculate pi. The source code and instructions for building that program are provided below:</p>"},{"location":"Documentation/Slurm/multiple_sub_jobs/#sample-program","title":"Sample Program","text":"<p>Copy and paste the following into a terminal window that's connected to the cluster. This will stream the pasted contents into a file called <code>pi.c</code> using the command <code>cat &lt;&lt; eof &gt; pi.c</code>.</p> <pre><code>cat &lt;&lt; eof &gt; pi.c\n#include &lt;stdio.h&gt;\n\n// pi.c: A sample C code calculating pi\n\nmain() {\n  double x,h,sum = 0;\n  int i,N;\n  printf(\"Input number of iterations: \");\n  scanf(\"%d\",&amp;N);\n  h=1.0/(double) N;\n\n  for (i=0; i&lt;N; i++) {\n   x=h*((double) i + 0.5);\n   sum += 4.0*h/(1.0+x*x);\n  }\n\n  printf(\"\\nN=%d, PI=%.15f\\n\", N,sum);\n}\n\neof\n</code></pre>"},{"location":"Documentation/Slurm/multiple_sub_jobs/#compile-the-code","title":"Compile the Code","text":"<p>This example uses the Intel C compiler. Load the module and compile pi.c with the following commands:</p> <pre><code>$ module purge\n$ module load intel-mpi\n$ icc -O2 pi.c -o pi_test\n$ ./pi_test\n</code></pre> <p>A sample batch job script file to run 8 copies of the pi_test program on a node with 24 processor cores is given below. This script creates 8 directories and starts 8 jobs, each in the background. It waits for all 8 jobs to complete before finishing.</p>"},{"location":"Documentation/Slurm/multiple_sub_jobs/#copy-and-paste-the-following-into-a-text-file","title":"Copy and paste the following into a text file","text":"<p>Place that batch file into one of your directories on the cluster. Make sure to change the allocation to a project-handle you belong to.</p> <pre><code>#!/bin/bash\n## Required Parameters   ##############################################\n#SBATCH --time 10:00               # WALLTIME limit of 10 minutes\n\n## Double ## will cause SLURM to ignore the directive:\n#SBATCH -A &lt;handle&gt;                # Account (replace with appropriate)\n\n#SBATCH -n 8                       # ask for 8 tasks   \n#SBATCH -N 1                       # ask for 1 node\n## Optional Parameters   ##############################################\n#SBATCH --job-name wait_test       # name to display in queue\n#SBATCH --output std.out\n#SBATCH --error std.err\n\nJOBNAME=$SLURM_JOB_NAME            # re-use the job-name specified above\n\n# Run 1 job per task\nN_JOB=$SLURM_NTASKS                # create as many jobs as tasks\n\nfor((i=1;i&lt;=$N_JOB;i++))\ndo\n  mkdir $JOBNAME.run$i             # Make subdirectories for each job\n  cd $JOBNAME.run$i                # Go to job directory\n  echo 10*10^$i | bc &gt; input       # Make input files\n  time ../pi_test &lt; input &gt; log &amp;  # Run your executable, note the \"&amp;\"\n  cd ..\ndone\n\n#Wait for all\nwait\n\necho\necho \"All done. Checking results:\"\ngrep \"PI\" $JOBNAME.*/log\n</code></pre>"},{"location":"Documentation/Slurm/multiple_sub_jobs/#submit-the-batch-script","title":"Submit the Batch Script","text":"<p>Use the following Slurm sbatch command to submit the script. The job will be scheduled, and you can view the output once the job completes to confirm the results.</p> <p><code>$ sbatch -A &lt;project-handle&gt; &lt;batch_file&gt;</code></p>"},{"location":"Documentation/Systems/","title":"NREL Systems","text":"<p>NREL operates three on-premises systems for computational work. </p>"},{"location":"Documentation/Systems/#system-configurations","title":"System Configurations","text":"Name Kestrel Swift Vermilion OS RedHat Enterprise Linux Rocky Linux RedHat Login kestrel.hpc.nrel.gov swift.hpc.nrel.gov vs.hpc.nrel.gov CPU Dual socket Intel Xeon Sapphire Rapids Dual AMD EPYC 7532 Rome CPU Dual AMD EPYC 7532 Rome CPU Cores per CPU Node 104 cores 128 cores Varies by partition Interconnect HPE Slingshot 11 InfiniBand HDR 25GbE HPC scheduler Slurm Slurm Slurm Network Storage 95PB Lustre 3PB NFS 440 TB GPU 156 4x NVIDIA H100 SXM GPUs 10 4x NVIDIA A100 40GB GPUs 5 nodes Single A100 Memory 256GB, 384GB, 700GB, 2TB 256GB(CPU) 1T(GPU) 256GB (base) Number of Nodes 2478 484 133 virtual"},{"location":"Documentation/Systems/Kestrel/","title":"About the Kestrel Cluster","text":"<p>Kestrel is configured to run compute-intensive and parallel computing jobs. It is a heterogeneous system comprised of 2,314 CPU-only nodes, and 156 GPU-accelerated nodes that run the Linux operating system (Red Hat Enterprise Linux), with a peak performance of 44 PetaFLOPS.</p> <p>Please see the System Configurations page for more information about hardware, storage, and networking.</p>"},{"location":"Documentation/Systems/Kestrel/#accessing-kestrel","title":"Accessing Kestrel","text":"<p>Access to Kestrel requires an NREL HPC account and permission to join an existing allocation. Please see the System Access page for more information on accounts and allocations.</p> <p>Kestrel has two types of login nodes, CPU and GPU, which share the same architecture as the corresponding compute nodes. You should use the CPU login nodes to compile software for use on and to submit jobs to the CPU compute nodes, and the GPU login nodes for GPU jobs.  </p>"},{"location":"Documentation/Systems/Kestrel/#for-nrel-employees","title":"For NREL Employees:","text":"<p>Users on an NREL device may connect via ssh to Kestrel from the NREL network using:</p> <ul> <li>kestrel.hpc.nrel.gov (CPU)</li> <li>kestrel-gpu.hpc.nrel.gov (GPU)</li> </ul> <p>This will connect to one of the three login nodes using a round-robin load balancing approach. Users also have the option of connecting directly to an individual login node using one of the following names: </p> <ul> <li>kl1.hpc.nrel.gov (CPU)</li> <li>kl2.hpc.nrel.gov (CPU)</li> <li>kl3.hpc.nrel.gov (CPU)</li> <li>kl5.hpc.nrel.gov (GPU)</li> <li>kl6.hpc.nrel.gov (GPU)</li> </ul>"},{"location":"Documentation/Systems/Kestrel/#for-external-collaborators","title":"For External Collaborators:","text":"<p>If you are an external HPC user, you will need a One-Time Password Multifactor token (OTP) for two-factor authentication.</p> <p>For command line access, you may login directly to kestrel.nrel.gov.  Alternatively, you can connect to the SSH gateway host or the HPC VPN.</p> <p>To access the GPU login nodes, first connect with one of the methods described above, and then ssh to kestrel-gpu.hpc.nrel.gov. </p> <p>Windows SSH \"Corrupted MAC on input\" Error</p> <p>When attempting to SSH, some Windows users experience an error message stating \"Corrupted MAC on input\" or \"message authentication code incorrect.\" To solve the error, run the ssh command with the flag <code>-m hmac-sha2-512</code>. Example below:</p> <pre><code>ssh -m hmac-sha2-512 username@kestrel.hpc.nrel.gov\n</code></pre> <p>See the Workaround blog post for further details and information.</p> <p>Login Node Policies</p> <p>Kestrel login nodes are shared resources, and because of that are subject to process limiting based on usage to ensure that these resources aren't being used inappropriately. Each user is permitted up to 8 cores and 100GB of RAM at a time, after which the Arbiter monitoring software will begin moderating resource consumption, restricting further processes by the user until usage is reduced to acceptable limits.</p>"},{"location":"Documentation/Systems/Kestrel/#data-analytics-and-visualization-dav-nodes","title":"Data Analytics and Visualization (DAV) Nodes","text":"<p>There are eight DAV nodes available on Kestrel, which are nodes intended for HPC applications that require a graphical user interface.  They are not general-purpose remote desktops, and are intended for HPC or visualization software that requires Kestrel.</p> <p>FastX is available for HPC users to use graphical applications on the DAV nodes.</p> <p>To connect to a DAV node using the load balancing algorithim, NREL employees can connect to kestrel-dav.hpc.nrel.gov. To connect from outside the NREL network, use kestrel-dav.nrel.gov. </p>"},{"location":"Documentation/Systems/Kestrel/#get-help-with-kestrel","title":"Get Help With Kestrel","text":"<p>Please see the Help and Support Page for further information on how to seek assistance with Kestrel or your NREL HPC account. </p>"},{"location":"Documentation/Systems/Kestrel/kestrel_release_notes/","title":"Kestrel Release Notes","text":"<p>We will update this page with Kestrel release notes after major Kestrel upgrades.</p>"},{"location":"Documentation/Systems/Kestrel/kestrel_release_notes/#july-2-2025","title":"July 2, 2025","text":"<ol> <li>Slurm was upgraded to 24.11.05. </li> <li>The default version of Ansys was changed to 2025R1. The old version (2024R1) is still available by using <code>module load ansys/2024R1</code>. </li> <li>The default version of STAR-CCM+ was changed to 20.02.007. Version 19.02.009 will no longer be available. </li> <li>The VASP 6.5.1 CPU modules have been updated with hdf5 support.</li> <li>A MACE enabled version of LAMMPS has been added to the GPU modules. This can be loaded using <code>ml mace/lammps-mace</code> and run with <code>srun -n 1 lmp -in lammps.inp</code>. Use <code>module show mace/lammps-mace</code> for more information. </li> <li>32 CPU nodes received a RAM upgrade from 256GB to 1TB. Access them by requesting the amount of memory you need with the <code>--mem</code> flag.</li> <li>32 additional CPU nodes in the <code>hbw</code> partition received a RAM upgrade from 256GB to 1TB. Use the <code>hbw</code> partition and request additional memory to access them.</li> <li>24 GPU nodes received a RAM upgrade from 384GB to 1.5TB. Access them by requesting a GPU node with the amount of memory you need with the <code>--mem</code> flag.</li> <li>The same 24 GPU nodes also received a local disk upgrade to 14TB. Use the GPU partition and the <code>--tmp</code> flag to request the space you need.</li> </ol>"},{"location":"Documentation/Systems/Kestrel/kestrel_release_notes/#december-13-2024","title":"December 13, 2024","text":"<ol> <li>Two new racks of GPU nodes were integrated into the system.</li> <li>Two racks of CPU compute nodes had dual interconnect network cards added to form a new high-bandwidth partition. An announcement will be made when the high-bandwidth nodes are available for general use, and documentation will be added. </li> <li>The following legacy VASP modules were removed:</li> </ol> <pre><code>   vasp/5.4.4         vasp/6.3.2               vasp/6.4.2            (D)\n</code></pre> <p>The available VASP modules are now as follows: <pre><code>   vasp/5.4.4+tpc    vasp/5.4.4    vasp/6.3.2_openMP+tpc    vasp/6.3.2_openMP    vasp/6.4.2_openMP+tpc    vasp/6.4.2_openMP (D)\n</code></pre></p> <p>The <code>vasp/5.4.4_base</code> module was renamed to <code>vasp/5.4.4</code>. Note that the default vasp module is now <code>vasp/6.4.2_openMP</code>.</p>"},{"location":"Documentation/Systems/Kestrel/kestrel_release_notes/#october-8-2024","title":"October 8, 2024","text":"<ol> <li>Slurm was upgraded from 23.11.7 to 23.11.10. </li> <li>The load order of default bash profile data was changed on login nodes such that app-related environment variables load last.</li> <li>PrgEnv-gnu/8.5.0 is now loaded by default when you login to Kestrel instead of PrgEnv-cray. </li> <li>The <code>module restore</code> command shouldn't be used. It will load broken modules. </li> </ol>"},{"location":"Documentation/Systems/Kestrel/kestrel_release_notes/#august-14-2024","title":"August 14, 2024","text":"<p>Jobs running on <code>debug</code> GPU nodes are now limited to a total of half of one GPU node's resources across one or two nodes. This is equivalent to 64 CPUs, 2 GPUs, and 180G of RAM on one node or 32 CPUs, 1 GPU, and 90GB of RAM on two nodes. <code>--exclusive</code> can no longer be used for GPU debug jobs. </p>"},{"location":"Documentation/Systems/Kestrel/kestrel_release_notes/#august-9-2024","title":"August 9, 2024","text":"<p>As of 08/09/2024 we have released new modules for VASP on Kestrel CPUs: </p> <pre><code>------------ /nopt/nrel/apps/cpu_stack/modules/default/application -------------\n   #new modules:\n   vasp/5.4.4+tpc     vasp/6.3.2_openMP+tpc    vasp/6.4.2_openMP+tpc\n   vasp/5.4.4_base    vasp/6.3.2_openMP        vasp/6.4.2_openMP\n\n   #legacy modules will be removed during next system time:\n   vasp/5.4.4         vasp/6.3.2               vasp/6.4.2            (D)\n</code></pre> <p>What\u2019s new: </p> <ul> <li>New modules have been rebuilt with the latest Cray Programming Environment (cpe23), updated compilers, and math libraries.</li> <li>OpenMP capability has been added to VASP 6 builds.</li> <li>Modules that include third-party codes (e.g., libXC, libBEEF, VTST tools, and VASPsol) are now denoted with +tpc. Use <code>module show vasp/&lt;version&gt;</code> to see details of a specific version.</li> </ul>"},{"location":"Documentation/Systems/Kestrel/kestrel_release_notes/#july-29-july-30-2024","title":"July 29 - July 30, 2024","text":"<ol> <li>Two GPU login nodes were added. Use the GPU login nodes for compiling software to run on GPU nodes and for submitting GPU jobs. </li> <li>GPU compute nodes were made available for general use and additional GPU partitions were added. See Running on Kestrel for additional information and recommendations.</li> </ol> <p>Module Updates/Changes </p> <ol> <li> <p>Modules are automatically loaded depending on node type, e.g., the GPU module stack is automatically loaded on GPU nodes. </p> </li> <li> <p>Naming convention for compilers:      example gcc compiler: </p> <ul> <li>Gcc/version is the compiler used by CPE with Prgenv</li> <li>Gcc-native/version: also meant to be used with Prgenv. The difference gcc-native and gcc is that the former is optimized for the specific architecture</li> <li>Gcc-stdalone/version this gcc is meant to be used outside of CPE. </li> <li>The same applies to nvhpc and aocc.</li> </ul> </li> <li> <p>Intel vs oneapi:  Moving forward the naming -intel in modules e.g. adios/1.13.1-intel-oneapi-mpi-intel will be deprecated in favor of -oneapi e.g. adios/1.13.1-intel-oneapi-mpi-oneapi.  This is implemented for the gpu modules and will be implemented for the CPU in the future.  Oneapi is the new naming convention for intel compilers.</p> </li> <li> <p>compilers-mixed:  In the list of compilers, you\u2019ll see compilers with -mixed e.g. nvhpc-mixed (same applies to intel, gcc, aocc, etc).  Those are meant to be used with CPE Prgenv, where you can force a mix and match between compilers.  Example: loading Prgenv-nvhpc and loading gcc-mixed.  This is not recommended and should only be used if you know what you\u2019re doing. </p> </li> <li> <p>Nvhpc:  There 5 types of nvhpc modules:  Nvidia module is equivalent to nvhpc and is meant to be used with CPE (Prgenv-nvidia).  Per HPE\u2019s instruction, only Prgenv-nvhpc should be used and not Prgenv-nvidia</p> <ul> <li>Nvhpc which is meant to be used with CPE (Prgenv-nvhpc)</li> <li>Nvhpc-mixed : meant to be used with CPE</li> <li>Nvhpc-stdalone : can be used outside of CPE for your usual compilation will load the compilers and a precompiled openmpi that ships with nvhpc</li> <li>nvhpc-nompi:  Similar to Nvhpc-stdalone but doesn\u2019t load the precompiled ompi</li> <li>nvhpc-byo-compiler: only load libs and header files contained in the nvidia SDK, no compiler or mpi is loaded </li> </ul> </li> <li> <p>Cuda: </p> <ul> <li>Cuda/11.7 was removed. If you'd like to access cuda as a standalone you can load cuda/12.3, cuda/12.1 was also added (for the gpus)</li> </ul> </li> <li> <p>Intel: </p> <ul> <li>Intel, intel-oneapi and intel-classic are modules to be used with CPE. If you want to use standalone intel compilers outside of CPE please use:  Intel-oneapi-compilers. </li> <li>intel-oneapi-compilers/2024.1.0 was added.</li> </ul> </li> <li> <p>Anaconda: </p> <ul> <li>The 2024 version is now added.</li> </ul> </li> </ol>"},{"location":"Documentation/Systems/Kestrel/kestrel_release_notes/#april-12-april-17-2024","title":"April 12 - April 17, 2024","text":"<ol> <li> <p>The size of the shared node partition was doubled from 32 nodes to 64 nodes. </p> </li> <li> <p>Cray programming environment (CPE) 23.12 is now the default on the system. </p> </li> <li> <p>To use node local storage, you will now need to use the <code>$TMPDIR</code> environment variable. <code>$TMPDIR</code> will now be set to <code>/tmp/scratch/$JOBID</code>. Hard-coding <code>/tmp/scratch</code> won't work. This change was made to prevent conflicts between multiple users/jobs writing to local disk on shared nodes. As a reminder, writing to <code>$TMPDIR</code> will use local disk on the nodes that have one, and RAM (up to 128Gb) on nodes without.</p> </li> <li> <p><code>/kfs2/pdatasets</code> was renamed to <code>/kfs2/datasets</code> and a symlink <code>/datasets</code> was added. </p> </li> </ol>"},{"location":"Documentation/Systems/Kestrel/kestrel_release_notes/#jan-29-feb-14-2024-upgrades","title":"Jan. 29 - Feb. 14, 2024 Upgrades","text":"<ol> <li> <p>We have experienced that most previously built software runs without modification (this includes NREL provided modules) and performs at the same level. </p> </li> <li> <p>Cray programming environment (CPE) 22.10, the default on the system, produces an error with cray-libsci when using PrgEnv-intel and the cc, CC, or ftn compiler wrappers. This error can be overcome either by swapping in a newer revision of cray-libsci, or by loading CPE/22.12. </p> <p>In the first case, you can load PrgEnv-intel then swap to the newer libsci library: </p> <pre><code>module swap PrgEnv-cray PrgEnv-intel \nmodule swap cray-libsci cray-libsci/22.12.1.1 \n</code></pre> <p>In the second case, you can load the newer CPE with PrgEnv-intel by:  </p> <pre><code>module restore system \nmodule purge \nmodule use /opt/cray/pe/modulefiles/ \nmodule load cpe/22.12 \nmodule load craype-x86-spr \nmodule load PrgEnv-cray \nmodule swap PrgEnv-cray PrgEnv-intel  \n</code></pre> </li> <li> <p>CPE 23.12 is now available on the system but is a work-in-progress. We are still building out the CPE 23 NREL modules.  </p> <p>To load CPE 23.12: </p> <pre><code>module restore system \nsource /nopt/nrel/apps/cpu_stack/env_cpe23.sh\nmodule purge\nmodule use /opt/cray/pe/modulefiles/\nmodule load cpe/23.12\nmodule load craype-x86-spr\nmodule load intel-oneapi/2023.0.0\nmodule load PrgEnv-intel\n</code></pre> <p>To load our modules built with CPE 23.12, you need to source the following environment. (Note that we are still building/updating these) </p> <p><code>source /nopt/nrel/apps/cpu_stack/env_cpe23.sh</code> </p> <p>NOTE: In CPE 23.12, some modules, when invoked, silently fail to load. We are still working on fixing this. For now, check that your modules have loaded appropriately with <code>module list</code>.</p> </li> </ol>"},{"location":"Documentation/Systems/Kestrel/Environments/","title":"Kestrel Programming Environments Overview","text":""},{"location":"Documentation/Systems/Kestrel/Environments/#definitions","title":"Definitions","text":"<p>Toolchain: a combination of a compiler and an mpi library. Sometimes associated scientific libraries (scalapack, blas, etc.) or bundles of scientific libraries (MKL, libsci, etc.) are considered part of the toolchain.</p> <p>Environment: a set of modules, including a toolchain. A \"build environment\" refers to the set of modules (including compiler and MPI library) used to compile a code. A \"run-time environment\" is the set of modules used to execute a code. The two typically, but not always, match.</p>"},{"location":"Documentation/Systems/Kestrel/Environments/#environments","title":"Environments","text":"<p>There are three types of module-based Toolchains available on Kestrel:</p> <ol> <li>\"PrgEnv-\" Environments, shipped with Kestrel</li> <li>NREL-built Environments</li> <li>NREL-built Environments with <code>cray-mpich-abi</code></li> </ol> <p>The \"PrgEnv-\" environments are new on Kestrel. PrgEnv stands for \"programming environment,\" and Kestrel ships with several of these. There are advantages to using a PrgEnv environment, as these environments are tailored for some of the Cray-specific features of Kestrel. For example, Cray MPICH utilizes Kestrel's Cray Slingshot network more effectively than OpenMPI or Intel MPI, so it runs noticeably faster than the other two for jobs that require two or more nodes. All <code>PrgEnv-</code> environments utilize Cray MPICH by default.</p> <p>The NREL-built environments function similarly to those on Eagle, and it is up to the user to load all necessary modules to build and run their applications.</p> <p>NREL-built environments can make use of Cray MPICH via the <code>cray-mpich-abi</code>. As long as program is compiled with an MPICH-based MPI (e.g., Intel MPI but not Open MPI), the <code>cray-mpich-abi</code> can be loaded at runtime, which causes the program to use Cray MPICH for dynamically built binaries.</p>"},{"location":"Documentation/Systems/Kestrel/Environments/#a-note-on-openmpi","title":"A note on OpenMPI","text":"<p>Currently, OpenMPI does not run performantly or stably on Kestrel. You should do your best to avoid using OpenMPI. Please reach out to hpc-help@nrel.gov if you need help working around OpenMPI.</p>"},{"location":"Documentation/Systems/Kestrel/Environments/#summary-of-available-compiler-environments","title":"Summary of available compiler environments","text":"<ul> <li>(Cray) denotes that the module belongs to the default Cray module set.</li> <li>(NREL) denotes that the module belongs to the NREL-built module set. If a compiler module is denoted (NREL), then the corresponding MPI module is also (NREL).</li> </ul>"},{"location":"Documentation/Systems/Kestrel/Environments/#gnu","title":"GNU","text":"PrgEnv Compiler Module MPI module Language Wrapper Compiler MPI gnu gcc (Cray) cray-mpich Fortran ftn gfortran Cray MPICH gnu gcc (Cray) cray-mpich C cc gcc Cray MPICH gnu gcc (Cray) cray-mpich C++ CC g++ Cray MPICH n/a gcc (NREL) openmpi/4.1.5-gcc Fortran mpifort gfortran Open MPI n/a gcc (NREL) openmpi/4.1.5-gcc C mpicc gcc Open MPI n/a gcc (NREL) openmpi/4.1.5-gcc C++ mpic++ g++ Open MPI"},{"location":"Documentation/Systems/Kestrel/Environments/#cray","title":"Cray","text":"PrgEnv Compiler Module MPI module Language Wrapper Compiler MPI cray cce (Cray) cray-mpich Fortran ftn crayftn Cray MPICH cray cce (Cray) cray-mpich C cc craycc Cray MPICH cray cce (Cray) cray-mpich C++ CC crayCC Cray MPICH"},{"location":"Documentation/Systems/Kestrel/Environments/#intel","title":"Intel","text":"PrgEnv Compiler Module MPI Module Language Wrapper Compiler MPI intel intel (Cray) cray-mpich Fortran ftn ifort Cray MPICH intel intel (Cray) cray-mpich C cc icc Cray MPICH intel intel (Cray) cray-mpich C++ CC icpc Cray MPICH n/a intel-oneapi (NREL) intel-oneapi-mpi Fortran mpiifort ifort intel MPI n/a intel-oneapi (NREL) intel-oneapi-mpi C mpiicc icc intel MPI n/a intel-oneapi (NREL) intel-oneapi-mpi C++ mpiicpc icpc intel MPI <p>Note: </p> <p>The <code>Cray MPICH</code> used for each different <code>PrgEnv-</code> is pointing to a different instance of MPICH, E.g. for <code>PrgEnv-intel</code> the MPICH used is located under <code>/opt/cray/pe/mpich/8.1.21/ofi/intel/19.0</code> and for <code>PrgEnv-cray</code> the MPICH used is located under <code>/opt/cray/pe/mpich/8.1.20/ofi/crayclang/10.0</code>.</p>"},{"location":"Documentation/Systems/Kestrel/Environments/#prgenv-programming-environments","title":"PrgEnv- Programming Environments","text":"<p>Module Known Issues</p> <p>As of July 30th, 2024, there are some modules that do not work correctly. The following points describe the issues and workarounds. We are working on permanent fixes for these issues.  * If using PrgEnv-intel, cray-libsci/23.12 is loaded by default. Load cray-libsci/22.12 instead.  * If using PrgEnv-gnu with gcc version 10, load cray-libsci/22.12 instead of the default cray-libsci version.  * On the GPU nodes, if using PrgEnv-nvhpc, you need to load nvhpc/23.9. nvhpc/24.1 is loaded by default and will not work.  * The <code>module restore</code> command could cause an \"Unable to find cray-mpich\" libraries error when used with PrgEnv-intel. If this happens, remove <code>module restore</code> from the list of commands.</p>"},{"location":"Documentation/Systems/Kestrel/Environments/#introduction","title":"Introduction","text":"<p>These environments come packaged with:</p> <ol> <li>A compiler, which corresponds to the name of the environment. E.g., <code>PrgEnv-intel</code> uses intel compilers</li> <li>Cray MPICH</li> <li>Cray LibSci, which can be used in place of MKL</li> <li>Additional communication and network libraries</li> </ol> <p>Upon logging into the machine, the <code>PrgEnv-gnu</code> is loaded by default on both the CPU and GPU login nodes. If we <code>module list</code>, we can see the modules associated with <code>PrgEnv-gnu</code>.</p> <p>We can swap between programming environments using the <code>module swap</code> command. For example, if <code>PrgEnv-gnu</code> is loaded but we want to use <code>PrgEnv-cray</code> instead, we can <code>module swap PrgEnv-gnu PrgEnv-cray</code>.</p>"},{"location":"Documentation/Systems/Kestrel/Environments/#what-is-a-prgenv-module-doing","title":"What is a PrgEnv module doing?","text":"<p>PrgEnv modules can seem a bit mysterious. We can check out the inner workings of a PrgEnv module with the <code>module show</code> command. For example, for <code>PrgEnv-gnu</code> we can:</p> <p><code>module show PrgEnv-gnu</code></p> <p>Which outputs:</p> <pre><code>/opt/cray/pe/modulefiles/PrgEnv-gnu/8.3.3:\n\nconflict     PrgEnv-amd \nconflict     PrgEnv-aocc \nconflict     PrgEnv-cray \nconflict     PrgEnv-gnu \nconflict     PrgEnv-intel \nconflict     PrgEnv-nvidia \nsetenv       PE_ENV GNU \nsetenv       gcc_already_loaded 1 \nmodule       swap gcc/12.1.0 \nmodule       switch cray-libsci cray-libsci/22.10.1.2 \nmodule       switch cray-mpich cray-mpich/8.1.20 \nmodule       load craype \nmodule       load cray-dsmml \nmodule       load craype-network-ofi \nmodule       load cray-mpich \nmodule       load cray-libsci \nsetenv       CRAY_PRGENVGNU loaded \n</code></pre> <p>This tells us that PrgEnv-gnu conflicts with all other PrgEnvs. The modulefile sets some environment variables (the <code>setenv</code> lines), and loads the modules associated with the programming environment.</p> <p>We can use the <code>module whatis</code> command to give us a brief summary of a module. For example, the command:</p> <p><code>module whatis craype</code></p> <p>outputs:</p> <p><code>craype               : Setup for Cray PE driver set and targeting modules</code></p> <p>We mentioned previously that the different PrgEnvs use different locations for Cray-MPICH. We can see this by using <code>module show cray-mpich</code> in each different PrgEnv, and examining (for example) the <code>CRAY_LD_LIBRARY_PATH</code> environment variable. </p>"},{"location":"Documentation/Systems/Kestrel/Environments/#compiling-inside-a-prgenv-ftn-cc-and-cc","title":"Compiling inside a PrgEnv: ftn, cc, and CC","text":"<p><code>ftn</code>, <code>cc</code>, and <code>CC</code> are the Cray compiler wrappers for Fortran, C, and C++, respectively, which are part of the <code>craype</code> module. When a particular <code>PrgEnv-</code> programming environment is loaded, these wrappers will make use of the corresponding compiler. For example, if we load PrgEnv-gnu with:</p> <pre><code>module swap PrgEnv-cray PrgEnv-gnu\n</code></pre> <p>we would expect <code>ftn</code> to wrap around gfortran, the GNU fortran compiler. We can test this with:</p> <p><code>ftn --version</code></p> <p>Which outputs:</p> <pre><code>GNU Fortran (GCC) 12.1.0 20220506 (HPE)\nCopyright (C) 2022 Free Software Foundation, Inc.\nThis is free software; see the source for copying conditions.  There is NO\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n</code></pre> <p>As expected. We can also <code>which ftn</code>: <pre><code>/opt/cray/pe/craype/2.7.17/bin/ftn\n</code></pre> Note1: In contrast with mpich, the location of the wrappers <code>cc</code>, <code>CC</code> and <code>ftn</code> is always the same <code>/opt/cray/pe/craype/2.7.17/bin/ftn</code> and does NOT depend on the loaded PrgEnv.</p> <p>Note2: <code>cc</code>, <code>CC</code> and <code>ftn</code> are also wrappers around their mpi couterparts. For mpi codes, the wrappers call the necessary mpi compilers depending on which PrgEnv is loaded. </p> <p>Note3: When changing between PrgEnvs, it is better to use <code>module swap [current prgenv] [new prgenv]</code> instead of <code>module purge; module load [new prgenv]</code> due to the way the environments set some environment variables.</p> <p><code>ftn</code> is part of the <code>craype</code> module. If we <code>module unload craype</code> and then type <code>which ftn</code> we find: <pre><code>/usr/bin/which: no ftn in (/opt/cray/pe/mpich/8.1.20/ofi/gnu/9.1/bin:/opt/cray/pe/mpich/8.1.20/bin:/opt/cray/libfabric/1.15.2.0/bin:/opt/cray/pe/gcc/12.1.0/bin:/home/ohull/.local/bin:/home/ohull/bin:/opt/clmgr/sbin:/opt/clmgr/bin:/opt/sgi/sbin:/opt/sgi/bin:/nopt/xalt/xalt/bin:/nopt/nrel/utils/bin:/nopt/slurm/current/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/opt/c3/bin:/sbin:/bin)\n</code></pre></p> <p>I.e., it can't find the path to <code>ftn</code>, because <code>craype</code> is not loaded into the environment.</p> <p>What happens if we <code>module swap PrgEnv-gnu PrgEnv-cray</code>, so that we're now using PrgEnv-cray, and then check <code>ftn</code>?</p> <pre><code>[ohull@eyas1 ~]$ ftn --version\nCray Fortran : Version 14.0.4\n</code></pre> <p><code>ftn</code> is now using Cray Fortran under the hood.</p> <p>Note: you can still directly access the underlying compiler. For example, if we're using PrgEnv-gnu (so our compilers are the GCC compilers), we can use <code>ftn</code>, or we can use <code>gfortran</code> or <code>mpifort</code> directly. It is considered best practice to use the Cray wrappers (<code>ftn</code>, <code>cc</code>, <code>CC</code>) on a Cray machine like Kestrel.</p> <p>In fact, the use of <code>mpifort</code> can be quite confusing. Inside the PrgEnv-gnu environment, we might assume that <code>mpifort</code> is a wrapper around OpenMPI. This is not correct, as <code>mpifort</code> wraps around Cray MPICH inside PrgEnv-gnu. If we <code>module unload PrgEnv-gnu</code> and then <code>module load openmpi</code>, then <code>mpifort</code> will wrap around OpenMPI. Using the Cray wrappers (<code>ftn</code>, <code>cc</code>, <code>CC</code>) helps avoid this confusion.</p>"},{"location":"Documentation/Systems/Kestrel/Environments/#submitting-a-job-within-a-prgenv","title":"Submitting a job within a PrgEnv","text":"<p>Submitting a Slurm job using a PrgEnv environment is no different than how you would normally submit a job. In your slurm script, below the #SBATCH directives, include:</p> <pre><code>module swap PrgEnv-cray [new PrgEnv]\n</code></pre> <p>We swap from <code>PrgEnv-cray</code> because this is the default PrgEnv that is loaded when logging onto Kestrel.</p> <p><code>[new PrgEnv]</code> can be <code>PrgEnv-gnu</code> or <code>PrgEnv-intel</code>.</p> <p>Depending on the software you're trying to run, you may need to load additional modules like <code>cray-hdf5</code> or <code>cray-fftw</code>.</p>"},{"location":"Documentation/Systems/Kestrel/Environments/#nrel-built-environments","title":"NREL-built environments","text":"<p>The NREL build modules are similar to Eagle, where the module are separate and no dependecy is created between modules. </p> <p>The modules are grouped by type <code>compilers_mpis</code> <code>utilities_libraries</code> and <code>applications</code>, and a module can be loaded using <code>module load $module_name</code>.</p> <p>The modules are optimized for Kestrel architecture and will be updated/upgraded every 6/12months or upon request. If there is a module you need but is not available, email hpc-help@nrel.gov</p>"},{"location":"Documentation/Systems/Kestrel/Environments/#nrel-built-environments-with-cray-mpich-abi","title":"NREL-built environments with cray-mpich-abi","text":"<p>For binaries dyanamically built with an MPICH-based MPI such as intel-mpi, the user can choose to use <code>cray-mpich-abi</code> at runtime to leverage its optimization for Kestrel. To check if your executable was dynamically built with intel MPI, you can <code>ldd [your program name] | grep mpi</code>.</p> <p>the module <code>cray-mpich-abi</code> will cause the program to run with Cray MPICH at runtime instead of Intel MPI. In your slurm submit script, you must include the two lines:</p> <p><code>module load craype</code> <code>module load cray-mpich-abi</code></p> <p>in order for the Cray MPICH abi to work properly.</p> <p>Note: If your code depends on libmpicxx, the Cray MPICH ABI is unlikely to work. You can check this by <code>ldd [your program name] | grep mpicxx</code>.</p>"},{"location":"Documentation/Systems/Kestrel/Environments/gpubuildandrun/","title":"Building and Running on Kestrel's H100 GPU nodes.","text":"<p>This page describes how to build and run on Kestrel's GPU nodes using several programming paradigms.  There are pure Cuda programs, Cuda aware MPI programs, MPI programs without Cuda, MPI programs with Cuda, MPI programs with Openacc, and pure Openacc programs. </p> <p>The examples are contained in a tarball available on Kestrel via the command:</p> <pre><code>tar -xzf /nopt/nrel/apps/examples/gpu/h100.tgz\n</code></pre> <p>Or you can use git to do a download:</p> <pre><code>git clone $USER@kestrel.hpc.nrel.gov:/nopt/nrel/apps/examples/gpu/0824 h100\n</code></pre> <p>After getting the source you can run all of the examples:</p> <p><pre><code>cd h100\nsbatch --account=MYACCOUNT script\n</code></pre> where you need to provide your account name.  This will run in about 22 minutes using 2 GPU nodes.  Some of the examples require 2 nodes but most will run on a single node.</p>"},{"location":"Documentation/Systems/Kestrel/Environments/gpubuildandrun/#note-about-gccgfortran","title":"Note about gcc/gfortran","text":"<p>Almost all compiling/running on a linux system will at some point reference or in some way use some portion of the GNU (gcc/gfortran/linker) system.  Kestrel has many versions of gcc.  These fall into three categories:</p> <ul> <li>Native to the Operating system</li> <li>Built by Cray</li> <li>Built by NREL</li> </ul> <p>You will also see modules for \"mixed\" versions.  These are just duplicates of others and should not be loaded.</p> <p>Here are some of the options:  </p>"},{"location":"Documentation/Systems/Kestrel/Environments/gpubuildandrun/#module-load-gcc-native121","title":"module load  gcc-native/12.1","text":"<ul> <li>which gcc<ul> <li>/opt/rh/gcc-toolset-12/root/usr/bin/gcc</li> </ul> </li> <li>Native to the operating system</li> </ul>"},{"location":"Documentation/Systems/Kestrel/Environments/gpubuildandrun/#module-load-gcc1220","title":"module load gcc/12.2.0","text":"<ul> <li>which gcc<ul> <li>/opt/cray/pe/gcc/12.2.0/bin/gcc</li> </ul> </li> <li>Built by the vendor</li> </ul>"},{"location":"Documentation/Systems/Kestrel/Environments/gpubuildandrun/#module-load-gcc-standalone1310","title":"module load gcc-standalone/13.1.0","text":"<ul> <li>which gcc<ul> <li>/nopt/nrel/apps/gpu_stack/compilers/03-24/.../gcc-13.1.0.../bin/gcc</li> </ul> </li> <li>Built by NREL</li> </ul>"},{"location":"Documentation/Systems/Kestrel/Environments/gpubuildandrun/#module-load-gcc-standalone1230","title":"module load gcc-standalone/12.3.0","text":"<ul> <li>which gcc<ul> <li>/nopt/nrel/apps/cpu_stack/compilers/06-24/.../gcc-12.3.0.../bin/gcc</li> </ul> </li> <li>Built by NREL</li> </ul>"},{"location":"Documentation/Systems/Kestrel/Environments/gpubuildandrun/#helper-files","title":"Helper files","text":"<p>There are a number of \"helper\" files  shipped with the examples.  The script onnodes is run while you have a job running.  You specify the jobid and it will report what is running on each node owned by the job.  This will include the core on which each task/thread is running.  On GPU nodes it will also report what you have running on each GPU.</p> onnodes script <pre><code>[tkaiser2@kl6 h100]$ ./onnodes\nx3102c0s41b0n0\nPID    LWP PSR COMMAND         %CPU\n3658483 3659124   4 jacobi           0.0\n3653038 3653038  14 (sd-pam)         0.0\n3653037 3653037  16 systemd          0.0\n3659075 3659075  27 sshd             0.0\n3658483 3658499  52 cuda00001800007  0.0\n3658480 3658497  64 cuda00001800007  0.0\n3658481 3658481  65 jacobi          23.6\n3658481 3659129  66 jacobi           0.0\n3658482 3658482  72 jacobi          20.8\n3658482 3658498  79 cuda00001800007  0.0\n3658480 3658480  84 jacobi          64.6\n3658483 3658483  88 jacobi          20.2\n3658480 3659127  89 jacobi           0.0\n3658481 3658496  92 cuda00001800007  0.0\n3659076 3659076  95 ps               0.0\n3658482 3659125 101 jacobi           0.0\n/usr/bin/nvidia-smi\nWed Aug 21 11:55:50 2024       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  NVIDIA H100 80GB HBM3          On  |   00000000:04:00.0 Off |                    0 |\n| N/A   42C    P0            126W /  699W |     532MiB /  81559MiB |      0%      Default |\n|                                         |                        |             Disabled |\n+-----------------------------------------+------------------------+----------------------+\n|   1  NVIDIA H100 80GB HBM3          On  |   00000000:64:00.0 Off |                    0 |\n| N/A   42C    P0            123W /  699W |     532MiB /  81559MiB |      0%      Default |\n|                                         |                        |             Disabled |\n+-----------------------------------------+------------------------+----------------------+\n|   2  NVIDIA H100 80GB HBM3          On  |   00000000:84:00.0 Off |                    0 |\n| N/A   42C    P0            123W /  699W |     532MiB /  81559MiB |      0%      Default |\n|                                         |                        |             Disabled |\n+-----------------------------------------+------------------------+----------------------+\n|   3  NVIDIA H100 80GB HBM3          On  |   00000000:E4:00.0 Off |                    0 |\n| N/A   43C    P0            119W /  699W |     532MiB /  81559MiB |      0%      Default |\n|                                         |                        |             Disabled |\n+-----------------------------------------+------------------------+----------------------+\n\n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|    0   N/A  N/A   3658480      C   ...try2/h100/mpi/openacc/cray/./jacobi        522MiB |\n|    1   N/A  N/A   3658481      C   ...try2/h100/mpi/openacc/cray/./jacobi        522MiB |\n|    2   N/A  N/A   3658482      C   ...try2/h100/mpi/openacc/cray/./jacobi        522MiB |\n|    3   N/A  N/A   3658483      C   ...try2/h100/mpi/openacc/cray/./jacobi        522MiB |\n+-----------------------------------------------------------------------------------------+\n[tkaiser2@kl6 h100]$\n</code></pre> <p>There is a function module_restore defined in /nopt/nrel/apps/env.sh. Sourcing /nopt/nrel/apps/env.sh sets modules back to the original state. module_restore also modifies $PATH and $LD_LIBRARY_PATH putting paths with your  home directory at the beginning.</p> <pre><code>. /nopt/nrel/apps/env.sh\nmodule_restore\n</code></pre> <p>As of October 2024, /nopt/nrel/apps/env.sh is sourced automatically when you login so the function module_restore should be in you path.</p>"},{"location":"Documentation/Systems/Kestrel/Environments/gpubuildandrun/#just-mpi","title":"Just MPI","text":"<p>There is an extra slurm script quick which does a build of C and Fortran MPI hello world.  The script shows that to get a MPI program to build with the standard Programing environments PrgEnv-{cray,intel,gnu} you must module unload nvhpc and module load cuda.  These environments build using Cray's MPI and the various back end compilers.  There is a conflict between the default version of nvhpc and these environments.  Also, Cray MPI wants cuda even if the program being built does not require it.</p>"},{"location":"Documentation/Systems/Kestrel/Environments/gpubuildandrun/#programming-paradigms-discussed","title":"Programming paradigms Discussed","text":"<ul> <li>Multiple GPUs &amp; multiple nodes <ul> <li>Pure Cuda programs</li> <li>Cuda aware MPI programs</li> <li>MPI programs without Cuda,</li> <li>MPI programs with Cuda</li> <li>MPI programs with Openacc</li> <li>Pure Openacc programs.   </li> <li>Library routines</li> </ul> </li> <li>We\u2019ll build with: <ul> <li>Cray\u2019s standard programming environment </li> <li>NVIDIA\u2019s environment</li> <li>Gcc</li> <li>A few examples with Intel MPI</li> </ul> </li> </ul>"},{"location":"Documentation/Systems/Kestrel/Environments/gpubuildandrun/#common-issues-addressed-with-these-examples","title":"Common issues addressed with these examples","text":"<ul> <li>Can\u2019t find library at run time<ul> <li>Need to set LD_LIBRARY_PATH to point to directory containing the library. Try to load modules at run time.</li> </ul> </li> <li>Module xxx is not compatible with your cray-libsci.<ul> <li>Load an different version:  cray-libsci/22.10.1.2 or cray-libsci/22.12.1.1  or   cray-libsci/23.05.1.4</li> </ul> </li> <li>Can\u2019t find some function in the c++ library.<ul> <li>Load a newer version of gcc</li> </ul> </li> <li>At link time libgcc_s.so.1: file not recognized: File format not recognized.<ul> <li>Linker is missing after some combinations of loads.</li> <li>module load binutils</li> </ul> </li> <li>Examples shown here don\u2019t work.<ul> <li>Make sure you are running and or launching from a GPU node</li> </ul> </li> <li>cc1: error: bad value \u2018znver4\u2019 for \u2018-march=\u2019 switch.<ul> <li>-march=skylake</li> </ul> </li> <li>Package 'nccl', required by 'virtual:world', not found\u2026<ul> <li>module unload nvhpc</li> </ul> </li> <li>lib/libmpi_gtl_cuda.so: undefined reference to\u2026<ul> <li>module load cuda</li> </ul> </li> </ul>"},{"location":"Documentation/Systems/Kestrel/Environments/gpubuildandrun/#the-examples","title":"The examples","text":"<p>The examples are spread across a number of directories.  All examples can be run with a single sbatch command.</p> <ul> <li>Our driver script is just \"script\".</li> <li>Each example directory contains a file \"doit\".</li> <li>Our driver looks for each directory with an example; Goes there and sources doit.</li> <li> <p>We can select the default gcc compiler to use by setting the environmental variable MYGCC; This can be done outside of the script before sbatch. The possible versions of gcc are set as shown below:</p> <ul> <li>export MYGCC=gcc-native/12.1</li> <li>export MYGCC=gcc-stdalone/10.1.0</li> <li>export MYGCC=gcc-stdalone/12.3.0 </li> <li>export MYGCC=gcc-stdalone/13.1.0</li> </ul> </li> <li> <p>If we know that an example will not run with the chosen version of gcc \"doit\" will substitute on the fly</p> </li> <li>You can run a subset of the tests by setting the variable doits.  For example:</li> </ul> <pre><code>export doits=\"./cudalib/factor/doit ./cudalib/fft/doit ./mpi/cudaaware/doit\"\n</code></pre>"},{"location":"Documentation/Systems/Kestrel/Environments/gpubuildandrun/#general-notes-for-all-examples","title":"General notes for all examples","text":"<ul> <li>All examples run module_restore to set the environment to a know state.  See above.</li> <li>Many of the examples unload  PrgEnv-cray/8.5.0 and nvhpc/24.1 to prevent conflicts with other modules.</li> <li>There is a compile and run of one or more programs.  </li> <li>MPI programs are run with srun or mpirun on one or two nodes.  Mpirun is used with some versions of NVIDIA's environment because srun is not supported.</li> <li>GPU programs that use a single GPU are run on each GPU in turn.</li> </ul>"},{"location":"Documentation/Systems/Kestrel/Environments/gpubuildandrun/#the-script","title":"The script","text":"<p>Our script, shown below does the following:</p> <ol> <li>Test to make sure we are starting from a GPU node.</li> <li>Define a simple timer.</li> <li>Save our environment and a copy of the script.</li> <li>Bring the function module_restore into our environment (see above).</li> <li>Set our default version of gcc.</li> <li>Find our examples if the user has not set a list beforehand and echo our list.</li> <li>Go into each directory and run the test.</li> </ol> script <pre><code>#!/bin/bash\n#SBATCH --time=0:30:00\n#SBATCH --partition=gpu-h100\n#SBATCH --nodes=2\n#SBATCH --gres=gpu:h100:4\n#SBATCH --exclusive\n#SBATCH --output=output-%j.out\n#SBATCH --error=infor-%j.out\n\nfunc () { typeset -f $1 || alias $1; }\nfunc module &gt; module.$SLURM_JOB_ID\n\n# we \"overload\" srun here to not allow any one subjob has \n# a problem and to runs too long.  this should not happen.\nalias srun=\"/nopt/slurm/current/bin/srun --time=00:05:00 $@\"\nif echo $SLURM_SUBMIT_HOST | egrep \"kl5|kl6\" &gt;&gt; /dev/null  ; then : ; else echo Run script from a GPU node; exit ; fi\n# a simple timer\ndt ()\n{\n    now=`date +\"%s.%N\"`;\n    if (( $# &gt; 0 )); then\n        rtn=$(printf \"%0.3f\" `echo $now - $1 | bc`);\n    else\n        rtn=$(printf \"%0.3f\" `echo $now`);\n    fi;\n    echo $rtn\n}\n\nprintenv &gt; env-$SLURM_JOB_ID.out\ncat $0 &gt; script-$SLURM_JOB_ID.out\n\n#runs script to put our restore function in our environment\n. whack.sh\nmodule_restore\n\n#some possible values for gcc module\n#export MYGCC=gcc-native/12.1\n#export MYGCC=gcc-stdalone/10.1.0\n#export MYGCC=gcc-stdalone/12.3.0 \n#export MYGCC=gcc-stdalone/13.1.0\n\nif [ -z ${MYGCC+x} ]; then export MYGCC=gcc-native/12.1  ; else echo MYGCC already set ; fi\necho MYGCC=$MYGCC\n\nif [ -z ${doits+x} ]; then \n    doits=`find . -name doit | sort -t/ -k2,2`\nelse \n    echo doits already set \nfi\n\nfor x in $doits ; do\n    echo running example in `dirname $x`\ndone\n\nstartdir=`pwd`\nt1=`dt`\nfor x in $doits ; do\n dir=`dirname $x`\n echo ++++++++ $dir | tee &gt;(cat 1&gt;&amp;2)\n echo $dir\n cd $dir\n tbegin=`dt`\n . doit | tee  $SLURM_JOB_ID\n echo Runtime `dt $tbegin` $dir `dt $t1` total | tee &gt;(cat 1&gt;&amp;2)\n cd $startdir\ndone\necho FINISHED `dt $t1`\n\n# post  (this is optional)\nmkdir -p /scratch/$USER/gputest/$SLURM_JOB_ID\ncp *out  /scratch/$USER/gputest/$SLURM_JOB_ID\n# . cleanup\n\n    ```\n\n\n## cuda/cray\nHere we build and run a single GPU code stream.cu.  This code is a standard benchmark that measures the floating point performance for a GPU.\n\nIn this case we are loading PrgEnv-nvhpc/8.4.0 which requires cray-libsci/23.05.1.4.  We compile with the \"wrapper\" compiler CC which, in this case builds with NVIDIA's backend compiler.  CC would \"pull in\" Cray's MPI it it was required.\n\nWe run on each GPU of each Node in our allocation.\n\n??? example \"cuda/cray\"\n    ```bash\n    : Start from a known module state, the default\n    module_restore\n\n    : Load modules\n    #module unload PrgEnv-cray/8.5.0\n    #module unload nvhpc/24.1\n\n\n    if [ -z ${MYGCC+x} ]; then module load gcc ; else module load $MYGCC ; fi\n    ml PrgEnv-nvhpc/8.4.0\n    ml cray-libsci/23.05.1.4\n    ml binutils\n    : &lt;&lt; ++++ \n     Compile our program\n     CC as well as cc, and ftn are wrapper compilers. Because\n     we have PrgEnv-nvidia loaded they map to Nvidia's compilers\n     but use would use Cray MPI if this was an MPI program.\n     Note we can also use nvcc since this is not an MPI program.\n    ++++\n\n    rm -rf ./stream.sm_90\n    CC -gpu=cc90  -cuda -target-accel=nvidia90  stream.cu  -o stream.sm_90\n    # nvcc -std=c++11 -ccbin=g++ stream.cu -arch=sm_90 -o stream.sm_90\n\n    : Run on all of our nodes\n    nlist=`scontrol show hostnames | sort -u`\n    for l in $nlist ; do   \n      echo $l\n      for GPU in 0 1 2 3 ; do\n    : stream.cu will read the GPU on which to run from the command line\n          srun -n 1 --nodes=1 -w $l ./stream.sm_90 -g $GPU\n      done\n      echo\n    done\n</code></pre>"},{"location":"Documentation/Systems/Kestrel/Environments/gpubuildandrun/#cudagccalso","title":"cuda/gccalso","text":"<p>Here we build and run a single GPU code stream.cu. This code is a standard benchmark that measures the floating point performance for a GPU.  In this case we break the compile into two parts; compiling the \"normal\" C portions with gcc and the Cuda portions with compilers enabled via the load nvhpc-nompi.  This is NVIDIA's compilers without MPI.</p> cuda/gccalso <pre><code>: Start from a known module state, the default\nmodule_restore\n\n: Load modules\n#module unload  PrgEnv-cray/8.5.0\n#module unload nvhpc/24.1\n\nif [ -z ${MYGCC+x} ]; then module load gcc ; else module load $MYGCC ; fi\nml nvhpc-nompi/24.1\nml binutils\n\nml 2&gt;&amp;1 | grep gcc-stdalone/13.1.0 ; if [ $? -eq 0 ]  ; then echo REPLACING gcc-stdalone/13.1.0 ; ml gcc-stdalone/12.3.0  ; fi\n\n: &lt;&lt; ++++ \n Compile our program\n The module nvhpc-nompi gives us access to Nvidia's compilers\n nvc, nvc++, nvcc, nvfortran as well as the Portland Group \n compilers which are actually links to these.  We do not\n have direct access to MPI with this set of modules loaded.\n Here we compile routines that do not containe cuda with g++.\n++++\n\n\ng++ -c normal.c \nnvcc -std=c++11 -arch=sm_90 cuda.cu normal.o -o stream.sm_90\n\n: Run on all of our nodes\nnlist=`scontrol show hostnames | sort -u`\nfor l in $nlist ; do   \n  echo $l\n  for GPU in 0 1 2 3 ; do\n: stream.cu will read the GPU on which to run from the command line\n      srun -n 1 --nodes=1 -w $l ./stream.sm_90 -g $GPU\n  done\n  echo\ndone\n</code></pre>"},{"location":"Documentation/Systems/Kestrel/Environments/gpubuildandrun/#cudanvidia","title":"cuda/nvidia","text":"<p>Steam.cu runs a standard benchmark showing the computational speed of the gpu for simple math operations.</p> <p>We use nvhpc-nompi which is a NREL written environment that builds cuda programs without MPI and run on each of the GPUs one at a time.</p> cuda/nvidia <pre><code>: Start from a known module state, the default\nmodule_restore\n\n: Load modules\n#module unload  PrgEnv-cray/8.5.0\n#module unload nvhpc/24.1\n\nml nvhpc-nompi/24.1\n: &lt;&lt; ++++ \n Compile our program\n The module nvhpc-nompi gives us access to Nvidia's compilers\n nvc, nvc++, nvcc, nvfortran as well as the Portland Group \n compilers which are actually links to these.  We do not\n have direct access to MPI with this set of modules loaded.\n++++\n\n\nnvcc -std=c++11 -arch=sm_90 stream.cu -o stream.sm_90\n\n: Run on all of our nodes\nnlist=`scontrol show hostnames | sort -u`\nfor l in $nlist ; do   \n  echo $l\n  for GPU in 0 1 2 3 ; do\n: stream.cu will read the GPU on which to run from the command line\n      srun -n 1 --nodes=1 -w $l ./stream.sm_90 -g $GPU\n  done\n  echo\ndone\n</code></pre>"},{"location":"Documentation/Systems/Kestrel/Environments/gpubuildandrun/#mpinormalcray","title":"mpi/normal/cray","text":"<p>We are building MPI programs that do not contain Cuda.  We unload nvhpc and load an older version to prevent compile issues.  We need to load cuda because Cray's MPI expects it, even for nonCuda programs.  We compile with ftn and cc which are \"replacements\" for the more traditional mpicc and mpifort.  These will pull in MPI as needed.  These should be used for codes even if they don't contain MPI.  Parallel programs built with PrgEnv-* should be launched with srun as shown here.</p> mpi/normal/cray <pre><code>: Start from a known module state, the default\nmodule_restore\n\n\n: Load modules\n#module unload nvhpc/24.1\nml PrgEnv-cray/8.4.0 \n\nml cuda\n: &lt;&lt; ++++ \n Compile our program.\n\n Here we use cc and ftn.  These are wrappers\n that point to Cray C (clang) Cray Fortran\n and Cray MPI. cc and ftn are part of PrgEnv-cray\n with is part of the default setup.\n++++\n\ncc helloc.c -o helloc\nftn hellof.f90 -o hellof\n\n: We run with two tasks per nodes an two tasks on one node.\nfor arg in \"--tasks-per-node=2\" \"-n 2 --nodes=1\" ; do \n   echo running Fortran version\n   srun $arg hellof\n   echo\n   echo running C version\n   srun $arg helloc\n   echo\ndone\n\n: With PrgEnv-intel we get the Intel backend compilers\nml PrgEnv-intel\nml cray-libsci/23.05.1.4\n#ml gcc-stdalone/13.1.0\nml binutils\n\ncc helloc.c -o helloc.i\nftn hellof.f90 -o hellof.i\n\n: We run with two tasks per nodes an two tasks on one node.\nfor arg in \"--tasks-per-node=2\" \"-n 2 --nodes=1\" ; do \n   echo running Fortran version with Intel backend\n   srun $arg hellof.i\n   echo\n   echo running C version with Intel backend\n   srun $arg helloc.i\n   echo\ndone\n\n: With PrgEnv-gnu we get the gnu backend compilers\n: As of 04/04/24 the -march=znver3 flag is required\n: because the default version of gcc does not support the\n: current CPU on the GPU nodes.  Or you could\n: ml craype-x86-milan\nml PrgEnv-gnu\nml cray-libsci/23.05.1.4\ncc  -march=znver3 helloc.c -o helloc.g\nftn -march=znver3 hellof.f90 -o hellof.g\n\n: We run with two tasks per nodes an two tasks on one node.\nfor arg in \"--tasks-per-node=2\" \"-n 2 --nodes=1\" ; do \n   echo running Fortran version with gnu backend\n   srun $arg hellof.g\n   echo\n   echo running C version with gnu backend\n   srun $arg helloc.g\n   echo\ndone\n</code></pre>"},{"location":"Documentation/Systems/Kestrel/Environments/gpubuildandrun/#mpinormalintelabi","title":"mpi/normal/intel+abi","text":"<p>In this case we are building hello world using Intel's compilers and Intel's MPI.  We load the intel-onapi-{mpi,compilers} modules and build with mpiicx and and mpifc.</p> <p>These hello world programs will report the version of the MPI library used.  The report they are using Intel MPI.</p> <p>However, if we load the modules craype and cray-mpich-abi the Intel MPI library gets replaced with Cray MPI at runtime.  This is reported in the program output.  the advantage is better performance for off node communication.  Cray-mpich-abi will not work if the program contains C++ MPI calls but will work if C++ calls normal C MPI routines as dictated by the standard.</p> mpi/normal/intel+abi <pre><code>: Start from a known module state, the default\nmodule_restore\n\n: Load modules\n#module unload PrgEnv-cray/8.5.0\n#module unload nvhpc/24.1\n\nif [ -z ${MYGCC+x} ]; then module load gcc ; else module load $MYGCC ; fi\nml intel-oneapi-mpi\nml intel-oneapi-compilers\nml binutils\n\n: &lt;&lt; ++++ \n Compile our program.\n\n There are many ways to compile using Intel MPI.\n Here we use the \"Intel Suggested\" way using mpiicx\n and mpifc.  This gives us new Intel backend compilers\n with Intel MPI. mpif90 and mpicc would give us gcc\n and gfortan instead\n++++\n\nmpiicx helloc.c -o helloc\nmpifc hellof.f90 -o hellof\n\n: We run with two tasks per nodes an two tasks on one node.\nfor arg in \"--tasks-per-node=2\" \"-n 2 --nodes=1\" ; do \n   echo running Fortran version\n   srun $arg hellof\n   echo\n   echo running C version\n   srun $arg helloc\n   echo\ndone\n\n: Finally we module load cray-mpich-abi.  With this module\n: loaded Intel MPI is replaced with Cray MPI without needing\n: to recompile. After the load we rerun and see Cray MPI\n: in the output\n\nml craype\nml cray-mpich-abi\n\nfor arg in \"--tasks-per-node=2\" \"-n 2 --nodes=1\" ; do \n   echo running Fortran version\n   srun $arg hellof\n   echo\n   echo running C version\n   srun $arg helloc\n   echo\ndone\n</code></pre>"},{"location":"Documentation/Systems/Kestrel/Environments/gpubuildandrun/#mpinormalnvidianrelopenmpi","title":"mpi/normal/nvidia/nrelopenmpi","text":"<p>In this case we are building normal MPI programs but using a NREL built OpenMPI and a NREL installed version of NVIDIA's environment.  This particular OpenMPI was built using NVIDIA's compilers and thus is more compatible with other NVIDIA packages.  NREL's MPI versions are built with slurm support so these programs are launched with srun.</p> mpi/normal/nvidia/nrelopenmpi <pre><code>: Start from a known module state, the default\nmodule_restore\n\n: Load modules\n#module unload PrgEnv-cray/8.5.0\n#module unload nvhpc/24.1\n\nif [ -z ${MYGCC+x} ]; then module load gcc ; else module load $MYGCC ; fi\nml openmpi/4.1.6-nvhpc\nml nvhpc-nompi/24.1\nml binutils\n\n: &lt;&lt; ++++ \n Compile our program\n Here we use mpicc and mpif90.  There is support for Cuda\n but we are not using it in this case.\n++++\n\nmpicc helloc.c -o helloc\nmpif90 hellof.f90 -o hellof\n\n: We run with two tasks per nodes an two tasks on one node.\nfor arg in \"--tasks-per-node=2\" \"-n 2 --nodes=1\" ; do \n   echo running Fortran version\n   srun $arg hellof\n   echo\n   echo running C version\n   srun $arg helloc\n   echo\ndone\n</code></pre>"},{"location":"Documentation/Systems/Kestrel/Environments/gpubuildandrun/#mpinormalnvidianvidiaopenmpi","title":"mpi/normal/nvidia/nvidiaopenmpi","text":"<p>In this case we are building normal MPI programs but using a nvhpc/24.1.  This particular MPI was built using NVIDIA's compilers and thus is more compatible with other NVIDIA packages.  This version of MPI does not support slurm's srun command so we launch with mpirun.</p> mpi/normal/nvidia/nvidiaopenmpi <pre><code>: Start from a known module state, the default\nmodule_restore\n\n: Load modules\n#module unload PrgEnv-cray/8.5.0\n#module unload nvhpc/24.1\n\nif [ -z ${MYGCC+x} ]; then module load gcc ; else module load $MYGCC ; fi\nml  nvhpc-stdalone/24.1\n\n: &lt;&lt; ++++ \n Compile our program\n Here we use mpicc and mpif90.  There is support for Cuda\n but we are not using it in this case.\n++++\n\nmpicc helloc.c -o helloc\nmpif90 hellof.f90 -o hellof\n\n: This version of MPI does not support srun so we use mpirun\n: We run with two tasks per nodes an two tasks on one node.\nfor arg in \"-N 2\" \"-n 2\" ; do \n   echo running Fortran version\n   mpirun $arg hellof\n   echo\n   echo running C version\n   mpirun $arg helloc\n   echo\ndone\n</code></pre>"},{"location":"Documentation/Systems/Kestrel/Environments/gpubuildandrun/#mpiwithcudacray","title":"mpi/withcuda/cray","text":"<p>This example is a MPI ping-pong test where the data starts and ends up on a GPU but passes through CPU memory. Here are the Cuda copy routines and MPI routines. d_A is a GPU (device) array.  It is copied to/from A a CPU array using cudaMemcpy.  A is sent/received via the MPI calls.</p> <pre><code>for(int i=1; i&lt;=loop_count; i++){\n    if(rank == 0){\n        cudaMemcpy(A, d_A, N*sizeof(double), cudaMemcpyDeviceToHost) ;\n        MPI_Send(A, N, MPI_DOUBLE, 1, tag1, MPI_COMM_WORLD);\n        MPI_Recv(A, N, MPI_DOUBLE, 1, tag2, MPI_COMM_WORLD, &amp;stat);\n        cudaMemcpy(d_A, A, N*sizeof(double), cudaMemcpyHostToDevice) ;\n    }\n    else if(rank == 1){\n        MPI_Recv(A, N, MPI_DOUBLE, 0, tag1, MPI_COMM_WORLD, &amp;stat);\n        cudaMemcpy(d_A, A, N*sizeof(double), cudaMemcpyHostToDevice) ;\n        cudaMemcpy(A, d_A, N*sizeof(double), cudaMemcpyDeviceToHost) ;\n        MPI_Send(A, N, MPI_DOUBLE, 0, tag2, MPI_COMM_WORLD);\n    }\n}\n</code></pre> <p>We are using PrgEnv-nvhpc which combines Cray MPI and NVIDIA's back end compilers.  As of the date of this writing this version of with NVIDIA's compilers are not compatible with GCC 13.  So we test to see if it is loaded and replace it as needed.</p> <p>Here we use CC. If we were compiling Fortran then ftn instead of CC.  These are wrappers that point to Cray MPI.</p> <p>We also build and run a multi-GPU version of stream which measures numerical performance of the GPU Stream is run simultaneously on all GPUs.  This code can test that can be run to test if a GPU is running properly.  </p> <p>Since PrgEnv-* is compatible with slurm we launch using srun. We do a on-node and off-node test.</p> mpi/withcuda/cray <pre><code>: Start from a known module state, the default\nmodule_restore\n\n: Load modules\n#module unload PrgEnv-cray/8.5.0\n#module unload nvhpc/24.1\n\nml &gt;&amp;2\nif [ -z ${MYGCC+x} ]; then module load gcc ; else module load $MYGCC ; fi\n#######\nml 2&gt;&amp;1 | grep gcc-native/12.1 ; if [ $? -eq 0 ]  ; then echo REPLACING gcc-native/12.1 ; ml gcc-stdalone/13.1.0 ; fi\n#######\nml &gt;&amp;2\n\nml PrgEnv-nvhpc\nml cray-libsci/23.05.1.4\nml binutils\n: &lt;&lt; ++++ \n Compile our program.\n\n Here we use CC. If we were compiling Fortran\n then ftn instead of CC.  These are wrappers\n that point to Cray MPI and with PrgEnv-nvhpc\n we get Nvidia's back end compilers.  \n++++\n\nCC -gpu=cc90   ping_pong_cuda_staged.cu -o staged\n\n\n: We run with 2 tasks total. One 1 and two nodes\necho running staged on node\nsrun  --nodes=1 --tasks-per-node=2 ./staged\n\necho running staged off node\nsrun  --nodes=2 --tasks-per-node=1 ./staged\n\necho running multi-gpu stream\nCC -gpu=cc90  -DNTIMES=1000  mstream.cu -o mstream\nexport VSIZE=3300000000\nexport VSIZE=330000000\nsrun --tasks-per-node=4  ./mstream -n $VSIZE\n</code></pre>"},{"location":"Documentation/Systems/Kestrel/Environments/gpubuildandrun/#mpiwithcudanvidianrelopenmpi","title":"mpi/withcuda/nvidia/nrelopenmpi","text":"<p>This example is a MPI ping-pong test where the data starts and ends up on a GPU but passes through CPU memory.  See the explanation in the previous example.</p> <p>We are using ml openmpi/4.1.6-nvhpc and ml nvhpc-nompi/24.1.  These supply a NREL built version of OpenMPI with NVIDIA's backend compilers.</p> <p>Here we use mpiCC. If we were compiling Fortran then ftn instead of CC.  These are wrappers that point to Cray MPI.</p> <p>We also build and run a multi-GPU version of stream which measures numerical performance of the GPU Stream is run simultaneously on all GPUs.  This code cone test that can be run to test if a GPU is running properly.  </p> <p>Since PrgEnv-* is compatible with slurm we launch using srun. We do a on-node and off-node test.</p> mpi/withcuda/nvidia/nrelopenmpi <pre><code>: Start from a known module state, the default\nmodule_restore\n\n: Load modules\n#module unload PrgEnv-cray/8.5.0\n#module unload nvhpc/24.1\n\nif [ -z ${MYGCC+x} ]; then module load gcc ; else module load $MYGCC ; fi\nml openmpi/4.1.6-nvhpc\nml nvhpc-nompi/24.1\nml binutils\n\n: &lt;&lt; ++++ \n Compile our program\n Here we use mpiCC which uses, in this case a NREL built  version\n of MPI and Nvidia's backend compiler. \n++++\n\nmpiCC ping_pong_cuda_staged.cu -o staged\n\n: We run with 2 tasks total.\n: This version of MPI does not support srun so we use mpirun\n\necho Run on a single node\nsrun --tasks-per-node=2 --nodes=1 ./staged\n\necho Run on two nodes \nsrun --tasks-per-node=1 --nodes=2 ./staged\n\necho running multi-gpu stream\nmpiCC -gpu=cc90  -DNTIMES=1000  mstream.cu -o mstream\nexport VSIZE=3300000000\nexport VSIZE=330000000\nsrun --tasks-per-node=4  ./mstream -n $VSIZE\n</code></pre>"},{"location":"Documentation/Systems/Kestrel/Environments/gpubuildandrun/#mpiwithcudanvidianvidiaopenmpi","title":"mpi/withcuda/nvidia/nvidiaopenmpi","text":"<p>This example is a MPI ping-pong test where the data starts and ends up on a GPU but passes through CPU memory.  See the explanation two examples previous.</p> <p>Here we use nvhpc/24.1.  (Note we actually unload this module and then reload it.  This is not actually necessary but is here for historical reasons.  In this case we could have just left it loaded.)</p> <p>We compile with mpiCC.  Since NVIDIA's MPI does not support srun we launch with mpirun.  </p> mpi/withcuda/nvidia/nvidiaopenmpi <pre><code>: Start from a known module state, the default\nmodule_restore\n\n: Load modules\n#module unload PrgEnv-cray/8.5.0\n#module unload nvhpc/24.1\n\nif [ -z ${MYGCC+x} ]; then module load gcc ; else module load $MYGCC ; fi\nml 2&gt;&amp;1 | grep gcc-stdalone/13.1.0 ; if [ $? -eq 0 ]  ; then echo REPLACING gcc-stdalone/13.1.0 ; ml gcc-stdalone/12.3.0 ; fi\n\nml nvhpc-stdalone/24.1\n\n: &lt;&lt; ++++ \n Compile our program\n Here we use mpiCC which uses Nvidia's version of MPI and\n their backend compiler. The \"hpcx\" has a few more optimizations.\n++++\n\nmpiCC ping_pong_cuda_staged.cu -o staged\n\n: We run with 2 tasks total.\n: This version of MPI does not support srun so we use mpirun\n\necho Run on a single node\nmpirun -n 2 -N 2 ./staged\n\necho Run on two nodes \nmpirun -n 2 -N 1 ./staged\n\n\necho running multi-gpu stream\nmpiCC -gpu=cc80  -DNTIMES=1000  mstream.cu -o mstream\nexport VSIZE=3300000000\nexport VSIZE=330000000\nmpirun -n 8 -N 4  ./mstream -n $VSIZE\n</code></pre>"},{"location":"Documentation/Systems/Kestrel/Environments/gpubuildandrun/#mpicudaaware","title":"mpi/cudaaware","text":"<p>This example is a ping-pong test where the dat starts and ends up on a GPU. Unlike the previous three examples There is no explicit copy to/from the GPU and CPU.  Data is sent directly between GPUs.  The array d_A is a device array and is not defined on the CPU.  This is much faster than doing an explicit copy.  </p> <p><pre><code>for(int i=1; i&lt;=5; i++){\n    if(rank == 0){\n        MPI_Send(d_A, N, MPI_DOUBLE, 1, tag1, MPI_COMM_WORLD);\n        MPI_Recv(d_A, N, MPI_DOUBLE, 1, tag2, MPI_COMM_WORLD, &amp;stat);\n    }\n    else if(rank == 1){\n        MPI_Recv(d_A, N, MPI_DOUBLE, 0, tag1, MPI_COMM_WORLD, &amp;stat);\n        MPI_Send(d_A, N, MPI_DOUBLE, 0, tag2, MPI_COMM_WORLD);\n    }\n}\n</code></pre> Here we use PrgEnv-nvhpc and also need to load a specific version cray-libsci/23.05.1.4.</p> <p>We need to  MPICH_GPU_SUPPORT_ENABLED=1 to make this work.  Depending on the code setting MPICH_OFI_NIC_POLICY=GPU may improve performance.</p> <pre><code>??? example \"mpi/cudaaware\"\n```bash\n: Start from a known module state, the default\nmodule_restore\n\n: Load modules\n#module unload nvhpc/24.1\n#module unload PrgEnv-cray/8.5.0\n\nif [ -z ${MYGCC+x} ]; then module load gcc ; else module load $MYGCC ; fi\nml PrgEnv-nvhpc\nml cray-libsci/23.05.1.4  \nml binutils\n\n: &lt;&lt; ++++ \n Compile our program.\n\n Here we use cc and CC.  These are wrappers\n that point to Cray MPI but use Nvidia backend \n comilers.\n++++\n\nCC -gpu=cc90  -cuda -target-accel=nvidia90  -c ping_pong_cuda_aware.cu\ncc -gpu=cc90  -cuda -target-accel=nvidia90 -lcudart -lcuda ping_pong_cuda_aware.o -o pp_cuda_aware\n\nexport MPICH_GPU_SUPPORT_ENABLED=1\nexport MPICH_OFI_NIC_POLICY=GPU\nsrun -n 2 --nodes=1 ./pp_cuda_aware\nsrun --tasks-per-node=1 --nodes=2 ./pp_cuda_aware\nunset MPICH_GPU_SUPPORT_ENABLED\nunset MPICH_OFI_NIC_POLICY\n\n```\n</code></pre> <p>Here is a plot comparing the bandwidth using Staged and Cuda aware MPI. </p>"},{"location":"Documentation/Systems/Kestrel/Environments/gpubuildandrun/#openacccray","title":"openacc/cray","text":"<p>Here we run one of NVIDIA's examples.  It is a single GPU version nbody calculation.  It runs the same calculation on the CPU and the GPU and reports the difference in performance.</p> <p>We are using PrgEnv-nvhpc/8.5.0 which gives us access to NVIDIA's compilers and Cray's MPI.  However, we don't use MPI.</p> <p>We run on each GPU of each node in turn.  The variable CUDA_VISIBLE_DEVICES sets the GPU number.  </p> <p>Since this is not a MPI program we don't actually need srun.  However, we use it in this case with the -w option to select the node on which we will launch the application.</p> openacc/cray <pre><code>: Start from a known module state, the default\nmodule_restore\n\n: Load modules\n#module unload PrgEnv-cray/8.5.0\n#module unload nvhpc/24.1\nmodule load binutils\nif [ -z ${MYGCC+x} ]; then module load gcc ; else module load $MYGCC ; fi\nml PrgEnv-nvhpc/8.5.0\n\n\n: &lt;&lt; ++++ \n Compile our program\n The module PrgEnv-nvhpc/8.5.0 gives us access to Nvidia's \n compilers nvc, nvc++, nvcc, nvfortran as well as the Portland \n Group compilers which are actually links to these.  Since we \n are not using MPI we could have also used nvhpc-nompi/24.1 or\n even nvhpc-native/24.1.\n++++\n\n\nnvc -fast -Minline -Minfo -acc -DFP64 nbodyacc2.c -o nbody\n\n\n\n: Run on all of our nodes\nnlist=`scontrol show hostnames | sort -u`\nfor l in $nlist ; do   \n  echo $l\n  for GPU in 0 1 2 3 ; do\n: This is one way to set the GPU on which a openacc program runs.\n      export CUDA_VISIBLE_DEVICES=$GPU\n      echo running on gpu $CUDA_VISIBLE_DEVICES\n: Since we are not running MPI we actaully do not need srun here.\n      srun -n 1 --nodes=1 -w $l ./nbody\n  done\n  echo\ndone\n\nunset CUDA_VISIBLE_DEVICES\n</code></pre>"},{"location":"Documentation/Systems/Kestrel/Environments/gpubuildandrun/#openaccnvidia","title":"openacc/nvidia","text":"<p>Here we run one of NVIDIA's examples.  It is a single GPU version nbody calculation.  It runs the same calculation on the CPU and the GPU and reports the difference in performance.</p> <p>Here we use nvhpc which gives us access to NVIDIA's compilers.  We don't use MPI.</p> <p>We run on each GPU of each node in turn.  The variable CUDA_VISIBLE_DEVICES sets the GPU number.  </p> <p>Since this is not a MPI program we don't actually need srun.  However, we use it in this case with the -w option to select the node on which we will run.</p> openacc/nvidia <pre><code>: Start from a known module state, the default\nmodule_restore\n\n: Load modules\n#module unload PrgEnv-cray/8.5.0\n#module unload nvhpc/24.1\n\nif [ -z ${MYGCC+x} ]; then module load gcc ; else module load $MYGCC ; fi\nml nvhpc-stdalone/24.1\n\n\n: &lt;&lt; ++++ \n Compile our program\n The module nvhpc-stdalone gives us access to Nvidia's compilers\n nvc, nvc++, nvcc, nvfortran as well as the Portland Group \n compilers which are actually links to these.  Since we are\n not using MPI we could have also used nvhpc-nompi/24.1 or\n even PrgEnv-nvhpc/8.5.0.\n++++\n\n\nnvc -fast -Minline -Minfo -acc -DFP64 nbodyacc2.c -o nbody\n\n\n: Run on all of our nodes\nnlist=`scontrol show hostnames | sort -u`\nfor l in $nlist ; do   \n  echo $l\n  for GPU in 0 1 2 3 ; do\n: This is one way to set the GPU on which a openacc program runs.\n      export CUDA_VISIBLE_DEVICES=$GPU\n      echo running on gpu $CUDA_VISIBLE_DEVICES\n: Since we are not running MPI we actaully do not need srun here.\n      srun -n 1 --nodes=1 -w $l ./nbody\n  done\n  echo\ndone\n\nunset CUDA_VISIBLE_DEVICES\n</code></pre>"},{"location":"Documentation/Systems/Kestrel/Environments/gpubuildandrun/#mpiopenacccray","title":"mpi/openacc/cray","text":"<p>This is a somewhat contrived example.  If does, in fact combine MPI and OpenACC but the MPI does almost nothing.  At the MPI level it is embarrassingly parallel and each MPI task does the same calculation which is enhanced via OpenACC.  MPI starts the tasks and reports a summary of timings.  However, MPI combined with OpenACC is a important paradigm.  The GPU version of VASP can combine MPI and OpenACC.</p> <p>Here we load PrgEnv-nvhpc which requires cray-libsci/23.05.1.4 giving us Cray MPI and nvidia backend compilers.  Again recall that cc and ftn are wrappers that will build MPI and non-MPI programs.  </p> <p>We launch with srun since PrgEnv-* supports the slurm scheduler.</p> mpi/openacc/cray <pre><code>: Start from a known module state, the default\nmodule_restore\n\n: Load modules\n#module unload PrgEnv-cray/8.5.0\n#module unload nvhpc/24.1\n\nif [ -z ${MYGCC+x} ]; then module load gcc ; else module load $MYGCC ; fi\nml PrgEnv-nvhpc\nml cray-libsci/23.05.1.4\nml binutils\n\n: &lt;&lt; ++++ \n Compile our program.\n\n Here we use cc and ftn.  These are wrappers\n that point to Cray C (clang) Cray Fortran\n and Cray MPI. cc and ftn are part of PrgEnv-cray\n which is part of the default setup.\n++++\n\ncc -acc -Minfo=accel -fast acc_c3.c  -o jacobi\n\n: We run with 4 tasks per nodes.\nsrun --tasks-per-node=4 ./jacobi 46000 46000 5 nvidia\n</code></pre>"},{"location":"Documentation/Systems/Kestrel/Environments/gpubuildandrun/#mpiopenaccnvidianrelopenmpi","title":"mpi/openacc/nvidia/nrelopenmpi","text":"<p>As discussed above this is a somewhat contrived example.  If does, in fact combine MPI and OpenACC but the MPI does almost nothing.  At the MPI level it is embarrassingly parallel and each MPI task does the same calculation which is enhanced via OpenACC.  MPI starts the tasks and reports a summary of timings.  However, MPI combined with OpenACC is a important paradigm.  The GPU version of VASP can combine MPI and OpenACC.</p> <p>Here we load openmpi/4.1.6-nvhpc and  nvhpc-nompi/24.1 which together give us a Cuda aware MPI with NVIDIA's OpenACC compile capability.</p> <p>We launch with srun since NREL's OpenMPI supports the slurm scheduler.</p> mpi/openacc/nvidia/nrelopenmpi <pre><code>cat doit\n: Start from a known module state, the default\nmodule_restore\n\n: Load modules\n#module unload PrgEnv-cray/8.5.0\n#module unload nvhpc/24.1\n\nif [ -z ${MYGCC+x} ]; then module load gcc ; else module load $MYGCC ; fi\nml openmpi/4.1.6-nvhpc\nml nvhpc-nompi/24.1\nml binutils\n\n: &lt;&lt; ++++ \n Compile our program\n Here we use mpicc and mpif90.  There is support for Cuda\n but we are not directly using it in this case, just openacc.\n++++\n\nmpicc -acc -Minfo=accel -fast acc_c3.c -o jacobi\n\n: We run with 4 tasks per nodes.\nsrun --tasks-per-node=4 ./jacobi 46000 46000 5 nvidia\n</code></pre>"},{"location":"Documentation/Systems/Kestrel/Environments/gpubuildandrun/#mpiopenaccnvidianvidiaopenmpi","title":"mpi/openacc/nvidia/nvidiaopenmpi","text":"<p>As discussed above this is a somewhat contrived example.  If does, in fact combine MPI and OpenACC but the MPI does almost nothing.  At the MPI level it is embarrassingly parallel and each MPI task does the same calculation which is enhanced via OpenACC.  MPI starts the tasks and reports a summary of timings.  However, MPI combined with OpenACC is a important paradigm.  The GPU version of VASP can combine MPI and OpenACC.</p> <p>Here we load PrgEnv-cray/8.5.0.  We do not unload  and  nvhpc-nompi/24.1 so we have NVIDIA's version of MPI in our path.  </p> <p>We launch with mpirun since NVIDIA's MPI lacks support for the slurm scheduler.</p> mpi/openacc/nvidia/nvidiaopenmpi <pre><code>: Start from a known module state, the default\nmodule_restore\n\n: Load modules\nif [ -z ${MYGCC+x} ]; then module load gcc ; else module load $MYGCC ; fi\nml nvhpc-stdalone/24.1\n\n\n: &lt;&lt; ++++ \n Compile our program\n Here we use mpicc and mpif90.  There is support for Cuda\n but we are not using it in this case but we are using \n openacc.\n++++\n\nmpicc -acc -Minfo=accel -fast acc_c3.c -o jacobi\n\n: We run with 4 tasks per nodes.\n: This version of MPI does not support srun so we use mpirun\nmpirun -N 4 ./jacobi 46000 46000 5 nvidia\n</code></pre>"},{"location":"Documentation/Systems/Kestrel/Environments/gpubuildandrun/#cudalibfactor","title":"cudalib/factor","text":"<p>We have two programs here.  Both do a linear solve, actually a factorization.   cpu.C is a CPU program and it does its solve using a LAPACK routine dgetrs.  This program is built and run against Cray's libsci and Intel's MKL.  The program cusolver_getrf_example.cu calls a NVIDIA cuda linear solver cusolverDnDgetrf.</p> <p>We first build cpu.C using PrgEnv-gnu.  Note that we need to load cuda even though we don't use it because PrgEnv-gnu expects it.</p> <p>We next build the GPU version using nvhpc-stdalone which gives us access to Cuda and the libraries. For our compile/link line we need to provide the path to the include files and library.</p> <p>Next we run these two examples.  For the CPU version we run using 32 threads.  For the GPU version we run on each of the GPUs in turn.  </p> <p>For the Intel version we</p> <pre><code>ml intel-oneapi-mkl\nml intel-oneapi-compilers\n</code></pre> <p>build and run.</p> cudalib/factor <pre><code>cat doit\n: Size of our matrix to solve\nexport MSIZE=4500\n\n: Start from a known module state, the default\n: We are going to Cray libsci version with the GPU\n: environment even though it does not use GPUs\n: Start from a known module state, the default\nmodule_restore\n\n: Load modules\n#module unload PrgEnv-cray/8.5.0\n#module unload nvhpc/24.1\n\nml PrgEnv-gnu/8.4.0 \nml cuda\n\n# Here we build the CPU version with libsci We don't actaully use Cuda but the compiler wants it\nCC  -DMINE=$MSIZE  -fopenmp -march=native cpu.C -o invert.libsci\n\n: &lt;&lt; ++++\n Compile our GPU programs.\n The module nvhpc-native gives us access to Nvidia's compilers\n nvc, nvc++, nvcc, nvfortran as well as the Portland Group \n compilers which are actually links to these.\n++++\n#ml nvhpc-native\nml nvhpc-stdalone\n: GPU version with libcusolver\nexport L1=$NVHPC_ROOT/math_libs/lib64\nexport L3=$NVHPC_ROOT/REDIST/cuda/12.3/targets/x86_64-linux/lib\nnvcc  -DMINE=$MSIZE -L$L1 -lcusolver -L$L3 -lnvJitLink cusolver_getrf_example.cu -o invert.gpu\n\n\nexport OMP_NUM_THREADS=32\necho \necho \necho ++++++++++++++++++++++\necho running libsci version \necho ++++++++++++++++++++++\n./invert.libsci\n\nfor GPU in 0 1 2 3 ; do\necho \necho \necho ++++++++++++++++++++++\necho running gpu version on GPU $GPU\necho ++++++++++++++++++++++\n: invert.gpu will read the GPU on which to run from the command line\n./invert.gpu $GPU\ndone\n\n: We are going to compile the Intel version using \n: the CPU environment\nmodule_restore\nml intel-oneapi-mkl\nml intel-oneapi-compilers\nicpx  -DMINE=$MSIZE -qopenmp -D__INTEL__ -march=native cpu.C -mkl -lmkl_rt -o invert.mkl\n\necho \necho \necho ++++++++++++++++++++++\necho running MKL version\necho ++++++++++++++++++++++\n\n./invert.mkl\n\nmodule unload  intel-oneapi-compilers\nmodule unload intel-oneapi-mkl\n\nunset L1\nunset L3\nunset OMP_NUM_THREADS\nunset MSIZE\n</code></pre>"},{"location":"Documentation/Systems/Kestrel/Environments/gpubuildandrun/#cudalibfft","title":"cudalib/fft","text":"<p>We are doing FFTs on a cube.  There are two versions.  3d_mgpu_c2c_example.cpp is a multi GPU program that will also run on a single GPU.  fftw3d.c calls fftw; in this case Cray's version.  fftw3d.c can also be compiled for 1d and 2d; see the source.</p> <p>For the GPU version we use nvhpc-stdalone which gives us access to NVIDIA's compilers and libraries.  We compile with nvcc and also link with nvcc specifying the path to the Cuda fft library.</p> <p>The first command line argument is the size of the cube.</p> <p>In a single invocation the program will run on both 1 and then 4 GPUs.  If the second command line option is 1 it will run on a single GPU version then on all 4 gpus.  When it is 2 it will run the 4 gpu version first.  </p> <p>We actually run the code 4 times and see different runtimes.</p> <p>For the FFTW version we load ml  PrgEnv-cray/8.4.0, cray-fftw, and cuda.  This program does not use cuda but the MPI compiler requires it for a proper link. </p> <p>Again we run on a cube of size 512.</p> cudalib/fft <pre><code>: Start from a known module state, the default\nmodule_restore\n\n\n\n: Load modules\n#module unload nvhpc/24.1\n#module unload PrgEnv-cray/8.5.0\n\nif [ -z ${MYGCC+x} ]; then module load gcc ; else module load $MYGCC ; fi\nml nvhpc-stdalone\nml binutils\n\nml 2&gt;&amp;1 | grep gcc-stdalone/13.1.0 ; if [ $? -eq 0 ]  ; then echo REPLACING gcc-stdalone/13.1.0 ; ml gcc-stdalone/12.3.0 ; fi\n\n: &lt;&lt; ++++ \n Compile our GPU programs.\n The module nvhpc-stdalone gives us access to Nvidia's compilers\n nvc, nvc++, nvcc, nvfortran as well as the Portland Group \n compilers which are actually links to these.\n++++\n\nnvcc -O3 -forward-unknown-to-host-compiler  --generate-code=arch=compute_90,code=[compute_90,sm_90] -std=c++11 -x cu 3d_mgpu_c2c_example.cpp -c\nexport L1=$NVHPC_ROOT/REDIST/math_libs/12.3/targets/x86_64-linux/lib\nnvcc  -o 3dfft 3d_mgpu_c2c_example.o -L$L1 -lcufft\n\n: Run our program on a cube. The first parameter gives our cube size.\n: 2048 should work on the H100s.\n: Second parameter determines which algorithm runs first 1 GPU version or 4 GPU version\necho\necho\nfor DOIT in `seq 1 4` ; do\n  echo set $DOIT\n  echo ++++++++++++++\n  echo RUN SINGLE GPU VERSION FIRST\n  ./3dfft 512 1\n  echo\n  echo\n  echo ++++++++++++++\n  echo RUN FOUR GPU VERSION FIRST\n  ./3dfft 512 2\n  echo\n  echo\ndone\n\n: Build and run a fftw version\nmodule_restore\n#module unload nvhpc/24.1\n#module unload PrgEnv-cray/8.5.0\nml  PrgEnv-cray/8.4.0 \n\nml cray-fftw\nml cuda\ncc -O3 fftw3d.c -o fftw3.exe\n\necho\necho\necho ++++++++++++++\necho run fftw libsci version\n./fftw3.exe 512\n</code></pre>"},{"location":"Documentation/Systems/Kestrel/Environments/gpubuildandrun/#source-credits","title":"Source credits","text":"<ol> <li>stream.cu - https://github.com/bcumming/cuda-stream with mods for MPI</li> <li>nbodyacc2.c - Nvidia, part of the nvhpc distribution</li> <li>acc_c3.c - Nvidia, part of the nvhpc distribution</li> <li>helloc.c, hellof.f90 - Tim Kaiser tkaiser2@nrel.gov</li> <li>ping_pong_cuda_aware.cu, ping_pong_cuda_staged.cu https://github.com/olcf-tutorials/MPI_ping_pong</li> <li>cpu.C - Multiple sources with significant mods </li> <li>cusolver_getrf_example.cu - https://github.com/NVIDIA/CUDALibrarySamples.git with significant mods</li> <li>3d_mgpu_c2c_example.cpp - https://github.com/NVIDIA/CUDALibrarySamples.git</li> <li>ftw3d.c - Tim Kaiser tkaiser2@nrel.gov</li> </ol>"},{"location":"Documentation/Systems/Kestrel/Environments/tutorial/","title":"Environments tutorial","text":"<p>In this tutorial, we will walk through how to build and run a basic MPI code using the four principal toolchains/software stacks on Kestrel. We will discuss common pitfalls in building and running within each of these toolchains, too.</p> <p>We summarize these toolchains in the below table:</p> PrgEnv-* Compiler MPI cray cray cce Cray MPICH intel intel Cray MPICH n/a intel Intel MPI n/a gcc Open MPI <p>Note: There is an option to compile with MPICH-based MPI (e.g., Intel MPI but not Open MPI) and then use the module <code>cray-mpich-abi</code> at run-time, which causes the code to use Cray MPICH instead of the MPI it was built with. More information on how to use this feature will be added soon.</p>"},{"location":"Documentation/Systems/Kestrel/Environments/tutorial/#introduction","title":"Introduction","text":"<p>Kestrel is a Cray machine whose nodes are connected by \"Cray Slingshot\" (contrast this to Eagle, which uses infiniband). We've found that packages that make use of Cray tools like Cray MPICH perform faster than when the same package is built and run without Cray tools (e.g. compiling and running with intel MPI), in part because these Cray tools are optimized to work well with Cray Slingshot.</p> <p>Most of us coming from Eagle are probably used to running our codes with Intel MPI or Open MPI, but not Cray MPICH.</p> <p>Using the cray-designed programming environments (\"PrgEnvs\") requires using special Cray compiler wrappers <code>cc</code> and <code>ftn</code>. These wrappers replace the MPI compiler wrappers you're used to, like <code>mpicc</code>, <code>mpiicc</code>, <code>mpiifort</code>, etc.  </p> <p>This guide will walk through how to utilize the Cray <code>PrgEnv-</code> environments with Cray MPICH, how to use \"NREL-built\" environments, and how to make sure your build is using the dependencies you expect.</p>"},{"location":"Documentation/Systems/Kestrel/Environments/tutorial/#what-is-prgenv-","title":"What is \"PrgEnv-\"?","text":"<p>Kestrel comes pre-packaged with several \"programming environments.\" You can see which programming environments are available by typing <code>module avail PrgEnv</code>. For CPU codes, we focus on <code>PrgEnv-cray</code> and <code>PrgEnv-intel</code>. These environments provide compilers (accessible with the <code>cc</code>, <code>CC</code>, and <code>ftn</code> wrappers), Cray MPICH, and some other necessary lower-level libraries.</p>"},{"location":"Documentation/Systems/Kestrel/Environments/tutorial/#the-tutorial","title":"The Tutorial","text":"<p>We're going to walk through building and running an MPI benchmarking code called IMB. This is a simple code that only requires a compiler and an MPI as dependencies (no scientific libraries, etc. are needed).</p> <p>First, log onto Kestrel with <code>ssh [your username]@kestrel.hpc.nrel.gov</code></p> <p>Let's grab an interactive node session:</p> <p><code>salloc -N 1 -n 104 --time=01:00:00 --account=&lt;your allocation handle&gt;</code></p>"},{"location":"Documentation/Systems/Kestrel/Environments/tutorial/#environment-1-prgenv-cray","title":"Environment 1: PrgEnv-cray","text":"<p>Make a new directory <pre><code>mkdir IMB-tutorial\ncd IMB-tutorial\nmkdir PrgEnv-cray\ncd PrgEnv-cray\n</code></pre></p> <p>Then download the code: <pre><code>git clone https://github.com/intel/mpi-benchmarks.git\ncd mpi-benchmarks\n</code></pre></p> <p>PrgEnv-cray was formally the default environment on Kestrel. Now, PrgEnv-gnu is the default. So, first we want to run the command <code>module swap PrgEnv-gnu PrgEnv-cray</code> to load the environment. To check, type <code>module list</code> and make sure you see <code>PrgEnv-cray</code> somewhere in the module list. Now, we can build the code. Run the command:</p> <p><code>CC=cc CXX=CC CXXFLAGS=\"-std=c++11\" make IMB-MPI1</code></p> <p>What does this do?</p> <p><code>CC=cc</code> : set the c compiler to be <code>cc</code>. Recall that <code>cc</code> is the Cray wrapper around a c-compiler. Because we're in PrgEnv-cray, we expect the c compiler to be Cray's. We can test this by typing <code>cc --version</code>, which outputs: <pre><code>[ohull@kl1 imb]$ cc --version\nNo supported cpu target is set, CRAY_CPU_TARGET=x86-64 will be used.\nLoad a valid targeting module or set CRAY_CPU_TARGET\nCray clang version 14.0.4  (3d8a48c51d4c92570b90f8f94df80601b08918b8)\nTarget: x86_64-unknown-linux-gnu\nThread model: posix\nInstalledDir: /opt/cray/pe/cce/14.0.4/cce-clang/x86_64/share/../bin\n</code></pre></p> <p>As expected, we are using Cray's C compiler.</p> <p><code>CXX=CC</code>: This sets the C++ compiler to be <code>CC</code>, in the same way as <code>CC=cc</code> for the C compiler above.</p> <p><code>CXXFLAGS=\"-std=c++11\"</code> tells the compiler to use the C++11 standard for compiling the C++ code, which is necessary because IMB has some code that is deprecated in C++17, which is the standard that Cray's C++ compiler defaults to.</p> <p>Finally,</p> <p><code>make IMB-MPI1</code> builds IMB-MPI1, the IMB executable that we want.</p> <p>Let's see what libraries we dynamically linked to in this build. Once the code is done building, type: <code>ldd ./IMB-MPI1</code></p> <p>This will show all libraries required by the program (on the lefthand side) and the specific implementation of those libraries that the build is currently pointing to (on the righthand side).</p> <p>Let's focus on MPI. Run:</p> <p><code>ldd ./IMB-MPI1 | grep mpi</code></p> <p>This should output something like:</p> <pre><code>[ohull@kl1 PrgEnv-cray]$ ldd IMB-MPI1 | grep mpi\n    libmpi_cray.so.12 =&gt; /opt/cray/pe/lib64/libmpi_cray.so.12 (0x00007fddee9ea000)\n</code></pre> <p>So, the MPI library we're using is Cray's MPI (Cray MPICH)</p> <p>Let's run the code:</p> <p><code>srun -N 1 -n 104 ./IMB-MPI1 AllReduce &gt; out</code></p> <p>When it completes, take a look at the out file:</p> <p><code>cat out</code></p> <p>IMB swept from 1 MPI task to 104 MPI tasks, performing a number of MPI_ALLREDUCE calls between the MPI tasks (ranging from 0 bytes to 4194304 bytes)</p> <p>Note -- very important: when you run IMB-MPI1, you MUST specify IMB-MPI1 as <code>./IMB-MPI1</code> or otherwise give a direct path to this specific version of <code>IMB-MPI1</code>. When we move to the NREL-built intel environment in this tutorial, we will have an <code>IMB-MPI1</code> already loaded into the path by default, and the command <code>srun IMB-MPI1</code> will execute the default <code>IMB-MPI1</code>, not the one you just built.</p> <p>If you'd like, you can also submit this as a slurm job. Make a file <code>submit-IMB.in</code>, and paste the following contents:</p> <pre><code>#!/bin/bash\n#SBATCH --time=00:40:00\n#SBATCH --nodes=1\n#SBATCH --tasks-per-node=104\n\n#!/bin/bash\n\nsrun -N 1 --tasks-per-node=104 your/path/to/IMB-tutorial/PrgEnv-cray/mpi-benchmarks/IMB-MPI1 Allreduce &gt; out\n</code></pre> <p>Don't forget to update <code>your/path/to/IMB-tutorial/PrgEnv-cray/mpi-benchmarks/IMB-MPI1</code> to the actual path to your IMB-MPI1 executable.  </p> <p>Then, <code>sbatch submit-IMB.in</code></p>"},{"location":"Documentation/Systems/Kestrel/Environments/tutorial/#environment-2-prgenv-intel","title":"Environment 2: PrgEnv-intel","text":"<p>We'll now repeat all the above steps, except now with PrgEnv-intel. Return to your <code>IMB-tutorial</code> directory, and <code>mkdir PrgEnv-intel</code></p> <p>Now, load the PrgEnv-intel environment:</p> <pre><code>module swap PrgEnv-cray PrgEnv-intel\nmodule load gcc-stdalone/12.3.0\n</code></pre> <p>Note that where possible, we want to avoid using <code>module purge</code> because it can unset some environment variables that we generally want to keep. We unload the <code>cray-libsci</code> package for the sake of simplicity (we are working through resolving a default versioning conflict between cray-libsci and PrgEnv-intel. If you need to use cray-libsci within PrgEnv-intel, please reach out to hpc-help@nrel.gov).</p> <p>Again, we can test which C compiler we're using with: <code>cc --version</code> Now, this should output something like: <pre><code>[ohull@x1000c0s0b0n0 mpi-benchmarks]$ cc --version\nIntel(R) oneAPI DPC++/C++ Compiler 2023.2.0 (2023.2.0.20230622)\nTarget: x86_64-unknown-linux-gnu\nThread model: posix\nInstalledDir: /nopt/nrel/apps/cpu_stack/compilers/02-24/spack/opt/spack/linux-rhel8-sapphirerapids/gcc-12.2.1/intel-oneapi-compilers-2023.2.0-hwdq5hei2obxznfjhtlav4mi5h5jd4zw/compiler/2023.2.0/linux/bin-llvm\nConfiguration file: /nopt/nrel/apps/cpu_stack/compilers/02-24/spack/opt/spack/linux-rhel8-sapphirerapids/gcc-12.2.1/intel-oneapi-compilers-2023.2.0-hwdq5hei2obxznfjhtlav4mi5h5jd4zw/compiler/2023.2.0/linux/bin-llvm/../bin/icx.cfg\n</code></pre></p> <p>Contrast this to when we ran <code>cc --version</code> in the PrgEnv-cray section. We're now using a different compiler (Intel oneAPI) under the hood.</p> <p>We can now repeat the steps we took in the PrgEnv-cray section. Move up two directories and re-download the code:</p> <pre><code>cd ../../\nmkdir PrgEnv-intel\ncd PrgEnv-intel\ngit clone https://github.com/intel/mpi-benchmarks.git\ncd mpi-benchmarks\n</code></pre> <p>and build it:</p> <p><code>CC=cc CXX=CC CXXFLAGS=\"-std=c++11\" make IMB-MPI1</code></p> <p>Note that we specify the same compiler wrapper, cc, to be the C compiler (the <code>CC=cc</code> part of the line above), as we did in the PrgEnv-cray section. But, <code>cc</code> now wraps around the intel-oneapi C compiler, instead of the Cray C compiler. So, we will be building with a different compiler, even though the build command is identical!</p> <p>Again, we can run with:</p> <p><code>srun -N 1 -n 104 ./IMB-MPI1 AllReduce &gt; out</code></p> <p>Or check which libraries are dynamically linked:</p> <p><code>ldd ./IMB-MPI1</code></p> <p>Or, for MPI specifically:</p> <pre><code>[ohull@kl1 PrgEnv-intel]$ ldd ./IMB-MPI1 | grep mpi\n    libmpi_intel.so.12 =&gt; /opt/cray/pe/lib64/libmpi_intel.so.12 (0x00007f13f8f8f000)\n</code></pre> <p>Note that this MPI library is indeed still Cray MPICH, the name is different than in the PrgEnv-cray section because it is specifically Cray MPICH built to be compatible with intel compilers, not cray compilers, as in the last example.</p> <p>You can also submit this inside a Slurm submit script:</p> <pre><code>#!/bin/bash\n#SBATCH --time=00:40:00\n#SBATCH --nodes=1\n#SBATCH --tasks-per-node=104\n#SBATCH --account=&lt;your allocation handle&gt;\n\n#!/bin/bash\n\nmodule restore\nmodule swap PrgEnv-cray PrgEnv-intel\nmodule unload cray-libsci\n\nsrun -N 1 --tasks-per-node=104 your/path/to/IMB-tutorial/PrgEnv-intel/mpi-benchmarks/IMB-MPI1 Allreduce &gt; out\n</code></pre> <p>Note that the only difference between this submit script and the one for Environment 1 is that we exchange <code>PrgEnv-cray</code> for <code>PrgEnv-intel</code>.</p>"},{"location":"Documentation/Systems/Kestrel/Environments/tutorial/#environment-3-intel-compilers-and-intel-mpi","title":"Environment 3: Intel Compilers and Intel MPI","text":"<p>We've now seen two examples using Cray's environments, <code>PrgEnv-cray</code> and <code>PrgEnv-intel</code>. Let's build IMB using one of NREL's environments, which are separate from Cray's.</p> <p>First, go back to your <code>IMB-tutorial</code> directory and re-clone the code:</p> <pre><code>cd ../../\nmkdir intel-intelMPI\ncd intel-intelMPI\ngit clone https://github.com/intel/mpi-benchmarks.git\ncd mpi-benchmarks \n</code></pre> <p>Then, load the NREL environment. To do this, first run: <pre><code>module unload PrgEnv-intel\n</code></pre></p> <p>Again, we want to avoid <code>module purge</code> where possible, so we unload the previous environment (<code>PrgEnv-intel</code>) in order to retain underlying environment variables.</p> <p>Let's check out our options for Intel compilers now:</p> <p><code>module avail intel</code></p> <p>We should see a number of modules. Some correspond to applications built with an intel toolchain (e.g. <code>amr-wind/main-intel-oneapi-mpi-intel</code>, whose name implies that amr-wind was built with the intel oneapi MPI and intel compilers). Others correspond to the MPI (e.g. <code>intel-oneapi-mpi/2021.8.0-intel</code>) or the compilers itself (e.g. <code>intel-oneapi-compilers/2022.1.0</code>)</p> <p>Let's load Intel MPI and Intel compilers:</p> <pre><code>module load intel-oneapi\nmodule load intel-oneapi-compilers\nmodule load intel-oneapi-mpi\n</code></pre> <p>Note that if we look back at <code>module avail intel</code> and look at the header above, e.g., <code>intel-oneapi</code>, we can see that these intel modules live in <code>/nopt/nrel/apps/cpu_stack/modules/default/compilers_mpi</code> -- this is different than the PrgEnvs, which can be found in <code>/opt/cray/pe/lmod/modulefiles/core</code>. This is one way to tell that you are using NREL's set of modules and not Cray's set of modules.</p> <p>Now, we can build IMB with the intel compilers and Intel MPI:</p> <p><code>CC=mpiicc CXX=mpiicpc CXXFLAGS=\"-std=c++11\" make IMB-MPI1</code></p> <p>Note that this command is different than the make commands we saw in the PrgEnv-cray and PrgEnv-intel sections.</p> <p>Instead of <code>CC=cc</code> and <code>CXX=CC</code> we have <code>CC=mpiicc</code> and <code>CXX=mpiicpc</code>. <code>mpiicc</code>, is the intel MPI wrapper around the intel C compiler, and <code>mpiicpc</code> is the same but for C++.</p> <p>Remember that warning about <code>IMB-MPI1</code> being in the default path? This is now true, so be careful that when you run the package, you're running the version you just built, NOT the default path version.</p> <p>If you're still inside <code>your/path/to/IMB-tutorial/intel-intelMPI/mpi-benchmarks</code> then we can run the command:</p> <p><code>ldd ./IMB-MPI1 | grep mpi</code></p> <p>This outputs something like:</p> <pre><code>[ohull@kl1 intel-intelMPI]$ ldd ./IMB-MPI1 | grep mpi\n    libmpicxx.so.12 =&gt; /nopt/nrel/apps/mpi/07-23/spack/opt/spack/linux-rhel8-icelake/intel-2021.6.0/intel-oneapi-mpi-2021.8.0-6pnag4mmmx6lvoczign5a4fslwvbgebb/mpi/2021.8.0/lib/libmpicxx.so.12 (0x00007f94e5e09000)\n    libmpifort.so.12 =&gt; /nopt/nrel/apps/mpi/07-23/spack/opt/spack/linux-rhel8-icelake/intel-2021.6.0/intel-oneapi-mpi-2021.8.0-6pnag4mmmx6lvoczign5a4fslwvbgebb/mpi/2021.8.0/lib/libmpifort.so.12 (0x00007f94e5a55000)\n    libmpi.so.12 =&gt; /nopt/nrel/apps/mpi/07-23/spack/opt/spack/linux-rhel8-icelake/intel-2021.6.0/intel-oneapi-mpi-2021.8.0-6pnag4mmmx6lvoczign5a4fslwvbgebb/mpi/2021.8.0/lib/release/libmpi.so.12 (0x00007f94e4138000)\n</code></pre> <p>We see a few more libraries than we saw with the PrgEnvs. For example, we now have <code>libmpicxx</code>, <code>libmpifort</code>, and <code>libmpi</code>, instead of just <code>libmpi_intel</code> or <code>libmpi_cray</code>, as was the case with the two PrgEnvs. We can see that our three MPI library dependencies are pointing to the corresponding library's in the NREL-built environments.</p> <p>We can submit an IMB job with the following slurm script:</p> <pre><code>#!/bin/bash\n#SBATCH --time=00:40:00\n#SBATCH --nodes=1\n#SBATCH --tasks-per-node=104\n\nmodule restore\nmodule unload PrgEnv-cray\n\nmodule load intel-oneapi\nmodule load intel-oneapi-compilers\nmodule load intel-oneapi-mpi\n\nsrun -N 1 --tasks-per-node=104  /your/path/to/IMB-tutorial/intel-intelMPI/mpi-benchmarks/IMB-MPI1 Allreduce &gt; out\n</code></pre> <p>don't forget to replace <code>/your/path/to/IMB-tutorial/intel-intelMPI/mpi-benchmarks/IMB-MPI1</code> with your actual path.</p>"},{"location":"Documentation/Systems/Kestrel/Environments/tutorial/#environment-4-gcc-and-openmpi","title":"Environment 4: GCC and OpenMPI","text":"<p>Environment 4 works similarly to Environment 3, except instead of using the NREL-built intel modules, we'll use GCC and OpenMPI instead. Note that OpenMPI is not ever recommended to use multi-node, because it is unstable on cray slingshot networks. You should only use OpenMPI for single-node jobs.</p> <p>Return to your <code>IMB-tutorial</code> directory and set up for gcc-openMPI:</p> <pre><code>cd ../../\nmkdir gcc-openMPI\ncd gcc-openMPI\ngit clone https://github.com/intel/mpi-benchmarks.git\ncd mpi-benchmarks \n</code></pre> <p>Run:</p> <pre><code>module unload intel-oneapi intel-oneapi-mpi intel-oneapi-compilers\nmodule load binutils/2.41\nmodule load gcc-stdalone/12.3.0\nmodule load openmpi/5.0.3-gcc\n</code></pre> <p>We unload the intel environment we set up in the previous step, and load <code>gcc</code> and <code>openmpi5</code> instead. Note that there are a number of versions of <code>gcc</code> available. the <code>-stdalone</code> tag denotes that it will not cause a forced unloading of other environment modules, unlike <code>gcc</code> with no <code>-stdalone</code> tag, which can force-switch the environment to <code>PrgEnv-gnu</code>.</p> <p>Now, we can <code>module avail openmpi</code> to find openmpi-related modules. Note the version of openmpi we use:</p> <p><code>module load openmpi/5.0.3-gcc</code></p> <p>OpenMPI5 is more compatible with Kestrel's Cray Slingshot network than older versions of OpenMPI. While we do not generally recommend using OpenMPI, if you must use it, it is best to use OpenMPI5.</p> <p>Now, we can build the code. Run the command:</p> <p><code>CC=mpicc CXX=mpic++ CXXFLAGS=\"-std=c++11\" make IMB-MPI1</code></p> <p>Similar to using mpiicc and mpiicpc in the intel section, now we use mpicc and mpic++, because these are the Open MPI wrappers around the GCC C and C++ compilers (respectively). We are not using the <code>cc</code> and <code>CC</code> wrappers now because we are not using a <code>PrgEnv</code>. </p> <p>Once the executable is built, check the mpi library it's using with ldd:</p> <p><code>ldd ./IMB-MPI1 | grep libmpi</code></p> <p>This command should return something like:</p> <pre><code>[ohull@x1007c7s7b0n0 mpi-benchmarks]$ ldd ./IMB-MPI1 | grep libmpi\n    libmpi.so.40 =&gt; /nopt/nrel/apps/mpi/07-23/spack/opt/spack/linux-rhel8-icelake/gcc-10.1.0/openmpi-4.1.5-s5tpzjd3y4scuw76cngwz44nuup6knjt/lib/libmpi.so.40 (0x00007f5e0c823000)\n</code></pre> <p>We see that libmpi is indeed pointing where we want it to: to the openmpi version of libmpi built with gcc-10.1.0.</p> <p>Finally, we can submit an IMB job with the following slurm script:</p> <pre><code>#!/bin/bash\n#SBATCH --time=00:40:00\n#SBATCH --nodes=1\n#SBATCH --tasks-per-node=104\n\nmodule restore\nmodule unload PrgEnv-cray\nmodule unload cce\n\nmodule load openmpi/4.1.5-gcc\nmodule load gcc/10.1.0\n\nsrun -N 1 --tasks-per-node=104 /your/path/to/IMB-tutorial/gcc-openMPI/mpi-benchmarks/IMB-MPI1 Allreduce &gt; out\n</code></pre> <p>don't forget to replace <code>/your/path/to/IMB-tutorial/gcc-openMPI/mpi-benchmarks/IMB-MPI1</code> with your actual path.</p>"},{"location":"Documentation/Systems/Kestrel/Environments/tutorial/#final-words","title":"Final Words","text":"<p>With all four environments built, you could now run a few benchmarks comparing how MPI performs between them. Try this using 1 node and using 2 nodes, and compare the results for each environment. You should see that performance between all four environments is competitive on 1 node, but the two <code>PrgEnv</code> builds run a bit faster for large message sizes on 2 nodes, and the gcc/openmpi build is liable to randomly fail in the 2 node case.</p> <p>Keeping track of the environments on Kestrel can be tricky at first. The key point to remember is that there are two separate \"realms\" of environments: the Cray <code>PrgEnv</code>s, which use Cray MPICH and best practices dictate the use of the <code>cc</code>, <code>CC</code>, and <code>ftn</code> compiler wrappers for C, C++, and Fortran, respectively, and the NREL-built environments that function similar to how the environments on Eagle function, and which use the more familiar compiler wrappers like <code>mpiicc</code> (for compiling C code with intel/intel MPI) or <code>mpicc</code> (for compiling C code with gcc/Open MPI.)</p> <p>Earlier in the article, we mentioned the existence of the <code>cray-mpich-abi</code>, which allows you to compile your code with a non-Cray MPICH-based MPI, like Intel MPI, and then run the code with Cray MPICH via use of the <code>cray-mpich-abi</code> module. We will include instructions for how to use this in an updated version of the tutorial.</p>"},{"location":"Documentation/Systems/Kestrel/Environments/Toolchains/intel/","title":"Compile and run: Intel1API compilers &amp; MPI","text":""},{"location":"Documentation/Systems/Kestrel/Environments/Toolchains/intel/#introduction","title":"Introduction","text":"<p>This page shows how to compile and run programs using Intel's 1API tool chain.  We'll look at building using their MPI and Fortran and C compilers.  It is possible to build programs using Intel's MPI libraries but actually compile using gfortran and gcc. This is also covered.  </p> <p>Intel's C compiler icc has been around for many years.  It is being retired and replaced with icx.  As of summer of 2023 you can still use icc but it is scheduled to be removed by the end of the year.  Building with icc produces a warning message.  We'll discuss how to surpress the warning and more importantly, build using icx.</p> <p>Our example programs are hybrid MPI/Openmp so we'll show commands for building hybrid programs.  If your program is pure MPI the only change you need to make to the build process is to remove the compile line option -fopenmp.  </p> <p>Sample makefile, source codes, and runscript for on Kestrel can be found in our Kestrel Repo  under the Toolchains folder.  There are individual directories for source,makefiles, and scripts or you can download the intel.tgz file containing all required files.  The source differs slightly from what is shown here.  There is an extra file triad.c that gets compiled along with the Fortran and C programs discussed below.  This file does some \"dummy\" work to allow the programs to run for a few seconds.  </p>"},{"location":"Documentation/Systems/Kestrel/Environments/Toolchains/intel/#module-loads-for-compile","title":"module loads for compile","text":"<p>These are the module you will need for compiles:</p> <pre><code>module load  intel-oneapi-compilers \nmodule load intel-oneapi-mpi        \nmodule load gcc                     \n</code></pre> <p>Intel compilers use some gcc functionality so  we load gcc to give a newer version of that compiler.</p>"},{"location":"Documentation/Systems/Kestrel/Environments/Toolchains/intel/#module-loads-for-run","title":"module loads for run","text":"<p>Normally, builds are static, meaning that an application \"knows\" where to find its libraries.  Thus, we don't need to load the Intel modules at runtime  Unless you have some other external libaries that require a module load the only module lines you will need are:</p> <pre><code>module purge\nmodule load libfabric\n</code></pre>"},{"location":"Documentation/Systems/Kestrel/Environments/Toolchains/intel/#building-programs","title":"Building programs","text":"<p>As discussed above we can build with Intel (ifort, icc, icx) or GNU (gcc, gfortran) underlying compilers.  The 5 options are:</p> <ol> <li>Fortran with: Intel MPI and Intel Fortran compiler</li> <li>C with: Intel MPI and Intel C compiler, older compiler (icc) </li> <li>C with: Intel MPI and Intel C compiler, newer compiler (icx)</li> <li>Fortran with: Intel MPI with gfortran Fortran compiler</li> <li>C with: Intel MPI with gcc C compiler</li> </ol> <p>Here's what the compile lines should be where we add the -fopenmp option for Opnemp and the optimization flag -O3.</p>"},{"location":"Documentation/Systems/Kestrel/Environments/Toolchains/intel/#1-fortran-with-intel-mpi-and-intel-fortran-compiler","title":"1. Fortran with: Intel MPI and Intel Fortran compiler","text":"<pre><code>mpiifort -O3 -g -fopenmp  ex1.f90  \n</code></pre>"},{"location":"Documentation/Systems/Kestrel/Environments/Toolchains/intel/#2a-c-with-intel-mpi-and-intel-c-compiler-older-compiler-icc","title":"2a. C with: Intel MPI and Intel C compiler, older compiler (icc)","text":"<pre><code>mpiicc -O3 -g -fopenmp  ex1.c  -o ex_c\n</code></pre> <p>This will produce the warning message icc: remark #10441: The Intel(R) C++ Compiler Classic (ICC) is deprecated and will be removed from product release in the second half of 2023. The Intel(R) oneAPI DPC++/C++ Compiler (ICX) is the recommended compiler moving forward. Please transition to use this compiler. Use '-diag-disable=10441' to disable this message</p> <p>We can compile with the extra flag.</p> <pre><code>mpiicc -diag-disable=10441 -O3 -g -fopenmp  ex1.c   -o gex_c\n</code></pre>"},{"location":"Documentation/Systems/Kestrel/Environments/Toolchains/intel/#2b-older-compiler-icc-might-not-be-available","title":"2b. Older compiler (icc) might not be available","text":"<p>Depending on the version of compilers loaded the message shown above might be replaced with one saying that the icx is no longer available.  In this case you MUST use icx.  There are two ways to do that shown below.  </p>"},{"location":"Documentation/Systems/Kestrel/Environments/Toolchains/intel/#3a-c-with-intel-mpi-and-intel-c-compiler-newer-compiler-icx","title":"3a. C with: Intel MPI and Intel C compiler, newer compiler (icx)","text":"<p><pre><code>export I_MPI_CC=icx\nmpiicc -O3 -g -fopenmp  ex1.c  -o ex_c\n</code></pre> Setting the environmental variable tells mpiicc to use icx (the newer Intel compiler) instead of icc.</p>"},{"location":"Documentation/Systems/Kestrel/Environments/Toolchains/intel/#3a-c-with-intel-mpi-and-intel-c-compiler-newer-compiler-icx_1","title":"3a. C with: Intel MPI and Intel C compiler, newer compiler (icx)","text":"<p><pre><code>mpiicx -O3 -g -fopenmp  ex1.c  -o ex_c\n</code></pre> Explictly running mpiicx will give you icx as the backend compiler.  </p>"},{"location":"Documentation/Systems/Kestrel/Environments/Toolchains/intel/#mpicc-and-mpif90-may-not-give-you-what-you-expect","title":"mpicc and mpif90 may not give you what you expect.","text":"<p>The commands mpicc and mpif90 actually call gcc and gfortran instead of the Intel compilers. If you consider these the default way to compile programs the \"by default\" Intel MPI does not use Intel compilers.  </p>"},{"location":"Documentation/Systems/Kestrel/Environments/Toolchains/intel/#4-fortran-with-intel-mpi-with-gfortran-fortran-compiler","title":"4. Fortran with: Intel MPI with gfortran Fortran compiler","text":"<pre><code>mpif90 -O3 -g -fopenmp  ex1.f90 \n</code></pre>"},{"location":"Documentation/Systems/Kestrel/Environments/Toolchains/intel/#5-c-with-intel-mpi-with-gcc-c-compiler","title":"5. C with: Intel MPI with gcc C compiler","text":"<pre><code>mpicc -O3 -g -fopenmp  ex1.f90 \n</code></pre> <p>Example programs We have two example MPI/OpenMP programs, ex1.c and ex1.f90.  They are more or less identical in function.  They first print MPI Library and compiler information.  For example the fortran example compiled with mpiifort reports:</p> <pre><code>  Fortran MPI TASKS            4\n Intel(R) MPI Library 2021.8 for Linux* OS\n\n Intel(R) Fortran Intel(R) 64 Compiler Classic for applications running on Intel\n</code></pre> <p>For mpif90 we get:</p> <pre><code>  Fortran MPI TASKS            4\n Intel(R) MPI Library 2021.8 for Linux* OS\n\n GCC version 13.1.0\n</code></pre> <p>Note in these cases we have the same MPI library but different compilers.</p> <p>The programs call a routine, triad. It keeps the cores busy for about 4 seconds.  This allows the OS to settle down.  Then for each MPI task and each openmp thread we get a line of the form:</p> <pre><code>task 0001 is running on x9000c3s2b0n0 thread=   2 of   3 is on core  054\n</code></pre> <p>This is saying that MPI task 1 is running on node x9000c3s2b0n0.  The task has 3 openmp threads and the second is running on core 54.</p>"},{"location":"Documentation/Systems/Kestrel/Environments/Toolchains/intel/#example-makefile","title":"Example makefile","text":"<p>The triad.c file containes the routines that keeps the cores busy for 4 seconds.  This is common to both the fortran and C versions of our codes. As discussed above our main codes are ex1.c and ex1.f90.  Our makefile will build for </p>"},{"location":"Documentation/Systems/Kestrel/Environments/Toolchains/intel/#1-fortran-with-intel-mpi-and-intel-fortran-compiler_1","title":"1. Fortran with: Intel MPI and Intel Fortran compiler","text":""},{"location":"Documentation/Systems/Kestrel/Environments/Toolchains/intel/#3-c-with-intel-mpi-and-intel-c-compiler-newer-compiler-icx","title":"3. C with: Intel MPI and Intel C compiler, newer compiler (icx)","text":""},{"location":"Documentation/Systems/Kestrel/Environments/Toolchains/intel/#4-fortran-with-intel-mpi-with-gfortran-fortran-compiler_1","title":"4. Fortran with: Intel MPI with gfortran Fortran compiler","text":""},{"location":"Documentation/Systems/Kestrel/Environments/Toolchains/intel/#5-c-with-intel-mpi-with-gcc-c-compiler_1","title":"5. C with: Intel MPI with gcc C compiler","text":"<p>There are comments in the makefile to show how to build with</p>"},{"location":"Documentation/Systems/Kestrel/Environments/Toolchains/intel/#2-c-with-intel-mpi-and-intel-c-compiler-older-compiler-icc","title":"2. C with: Intel MPI and Intel C compiler, older compiler (icc)","text":"<p>The makefile has an intresting \"trick\".  The default target is recurse.  This target loads the modules then calls make again using the same makefile but with the targets  intel and gnu.  By using this \"trick\" you don't have to load modules before the make.  </p> <p>The targets intel and gnu each have a dependency to compile triad with either Intel or gcc compilers.  Then the final applications are built with Intel MPI and again the either Intel or gnu.</p> <p>The final MPI codes are: </p> <ul> <li>gex_c : gcc</li> <li>gex_f : gfortran</li> <li>ex_c  : Intel C (icx)</li> <li>ex_f  : Intel Fortran (ifort)</li> </ul>"},{"location":"Documentation/Systems/Kestrel/Environments/Toolchains/intel/#run-script","title":"Run script","text":"<ol> <li>Makes a new directory, copies the requred files and goes there</li> <li>Does a make with output going into make.log</li> <li>Sets the number of MPI tasks and openmp threads</li> <li>Sets some environmental variables to control and report on threads (discussed below)</li> <li>module commands<ol> <li>module purge</li> <li>module load libfabric</li> </ol> </li> <li>Creates a string with all of our srun options (discussed below)</li> <li>Calls srun on each version of our program<ol> <li>output goes to *.out</li> <li>Report on thread placement goes to *.info</li> </ol> </li> </ol> <p>Our script sets these openmp related variables.  The first is familiar. KMP_AFFINITY is unique to Intel compilers.  In this case we are telling the OS to scatter (spread) out our threads.  OMP_PROC_BIND=spread does the same thing but it is not unique to Intel compilers. So in this case KMP_AFFINITY is actually redundent.  </p> <pre><code>  export OMP_NUM_THREADS=3\n  export KMP_AFFINITY=scatter\n  export OMP_PROC_BIND=spread\n</code></pre> <p>The next line </p> <pre><code>export BIND=\"--cpu-bind=v,cores\"\n</code></pre> <p>is not technically used as an environmental variable but it will be used to create the srun command line.  Passing --cpu-bind=v to srun will casue it to report threading information.  The \"cores\" option tells srun to \"Automatically generate masks binding tasks to cores.\"  There are many other binding options as described in the srun man page. This setting works well for many programs.</p> <p>Our srun command line options for 2 tasks per node and 3 threads per task are:</p> <pre><code>--mpi=pmi2 --cpu-bind=v,cores --threads-per-core=1 --tasks-per-node=2 --cpus-per-task=3\n</code></pre> <ul> <li>--mpi=pmi2 : tells srun to use a particular launcher (This is optional.)</li> <li>--cpu-bind=v,cores : discussed above</li> <li>--threads-per-core=1 : don't allow multiple threads to run on the same core.  Without this option it is possible for multiple threads to end up on the same core, decreasing performance.  </li> <li>--cpus-per-task=3 : The cpus-per-task should always be equal to OMP_NUM_THREADS.</li> </ul> <p>The final thing the script does is produce a results report.  This is just a list of mapping of mpi tasks and threads.  There should not be any repeats in the list.  There will be \"repeats\" of cores but on different nodes.   There will be \"repeats\" of nodes but with different cores.</p> <p>You can change the values for --cpu-bind, OMP_PROC_BIND, and threads-per-core to see if this list changes.</p>"},{"location":"Documentation/Systems/Kestrel/Environments/Toolchains/others/","title":"Compile and run: MPI","text":""},{"location":"Documentation/Systems/Kestrel/Environments/Toolchains/others/#introduction","title":"Introduction","text":"<p>The ToolChains Intel document goes into great detail on running with various settings and  with the old and new versions of the Intel compilers.</p> <p>The mpi/normal section of gpubuildandrun shows how to build and run using the more standard version of MPI.</p>"},{"location":"Documentation/Systems/Kestrel/Filesystems/","title":"Kestrel Filesystems","text":""},{"location":"Documentation/Systems/Kestrel/Filesystems/#home-file-system","title":"Home File System","text":"<p>The Home File System (HFS) on Kestrel is part of the ClusterStor used for the Parallel File System (PFS), providing highly reliable storage for user home directories and NREL-specific software. HFS has 1.2 petabytes (PB) of capacity.</p> <p>/home</p> <p>The /home directory on Kestrel is intended to hold small files. These include shell startup files, scripts, source code, executables, and data files. Each user has a quota of 50 GB.</p> To check your home quota usage, run the following command: <pre><code>[user@kl1 ~]$ lfs quota -uh $USER /home/$USER\n</code></pre> <p>/nopt</p> <p>The /nopt directory on Kestrel resides on HFS and is where NREL-specific software, module files, licenses, and licensed software are kept.</p>"},{"location":"Documentation/Systems/Kestrel/Filesystems/#parallel-file-system","title":"Parallel File System","text":"<p>The Parallel File System (PFS) ProjectFS and ScratchFS on Kestrel is a ClusterStor Lustre file system intended for high-performance I/O. </p> <p>Warning</p> <p>There are no backups of PFS data.  Users are responsible for ensuring that critical data is copied to Mass Storage or other alternate data storage location.</p>"},{"location":"Documentation/Systems/Kestrel/Filesystems/#projectfs","title":"ProjectFS","text":"<p>ProjectFS provides 68 PB of capacity with 200 GB/s of IOR bandwidth. It is intended for longer term data storage.</p> <p>/projects</p> <p>Each project/allocation has a directory in /projects intended to host data, configuration, and applications shared by the project.</p> <p>Directories in /projects have a quota assigned based on the project resource allocation for that fiscal year. </p> To check your quota usage, run the following commands: <pre><code># To determine your Project ID run:\n\n[user@kl1 ~]$ lfs project -d /projects/csc000\n110255 P /projects/csc000\n\n# In this case, 110255 is the Project ID for project csc000.\n\n# To see usage towards your quota, run:\n\n[user@kl1 ~]$ lfs quota -hp 110255 /projects/csc000\n\nDisk quotas for prj 110255 (pid 110255):\n    Filesystem    used   quota   limit   grace   files   quota   limit   grace \n/projects/csc000    \n                617.5G    100T    100T       -  636875       0       0       -\n# An asterisk(*) by the used value indicates the project has exceeded its quota of storage, and writes to the directory are not allowed.\n</code></pre> <p>/kfs2/shared-projects</p> <p>Projects may request a shared project directory to host data, configuration, and applications shared by multiple projects/allocations. </p> <p>To request a /shared-projects directory, please contact hpc-help@nrel.gov and include the following information: <pre><code>1. The name of the primary/\"host\" allocation that the /shared-projects directory will belong to. \n2. The name/email of a person who will authorize changes to the /shared-projects directory. \n3. How much space you would like to request (in TB). \n4. A list of other allocations that should have access to the /shared-projects directory. \n5. What you would like to call the directory. For example, \"/shared-projects/myproject-shared\" or other similar descriptive name, ideally between about 4-15 characters in length. \n6. A group name for the UNIX group ownership of the directory, the same or similar to the directory name provided in Step 5. \n</code></pre></p> <p>/kfs2/datasets</p> <p>The /kfs2/datasets directory on Kestrel hosts widely used data sets.</p> <p>There are multiple big data sets that are commonly used across various projects for computation and analysis on NREL's HPC Systems. We provide a common location on Kestrel's filesystem at /kfs2/datasets, where these data sets are available for global reading by all compute nodes. Each data set contains a readme file that covers background, references, explanation of the data structure, and Python examples.</p> <p>These datasets include: </p> <p>/kfs2/datasets/NSRDB</p> <p>The National Solar Radiation Database (NSRDB) is a serially complete collection of meteorological and solar irradiance data sets for the United States and a growing list of international locations for 1998-2017. The NSRDB provides foundational information to support U.S. Department of Energy programs, research, and the general public.</p> <p>/kfs2/datasets/WIND</p> <p>The Wind Integration National Data Set (WIND) Toolkit consists of wind resource data for North America and was produced using the Weather Research and Forecasting Model (WRF).</p> <p>For more information on the data sets hosted on Kestrel, please see the Github repository.</p>"},{"location":"Documentation/Systems/Kestrel/Filesystems/#scratchfs","title":"ScratchFS","text":"<p>ScratchFS is a Lustre file system in a hybrid flash-disk configuration providing a total of 27 petabytes (PB) of capacity with 354 gigabytes (GB)/s of IOR bandwidth. It is intended to support intensive I/O and we recommend running jobs out of ScratchFS for the best performance. </p> <p>/scratch</p> <p>Each user has their own directory in /scratch. </p> <p>Warning</p> <p>Data in /scratch is subject to deletion after 28 days of inactivity. It is recommended to store your important data, libraries, and programs on ProjectFS. </p>"},{"location":"Documentation/Systems/Kestrel/Filesystems/#node-file-system","title":"Node File System","text":"<p>Some Kestrel CPU compute nodes have an NVMe local solid-state drive (SSD) for use by compute jobs. They vary in size; 1.7TB on 256 of the standard compute nodes and 5.6TB on the bigmem nodes. The GPU nodes also all have local NVMe drives, with 3.4TB available per node. There are several possible scenarios in which a local disk may make your job run faster. For instance, you may have a job accessing or creating many small (temporary) files, you may have many parallel tasks accessing the same file, or your job may do many random reads/writes or memory mapping.</p> <p>/tmp/scratch</p> <p>The local disk on nodes that have one is mounted at <code>/tmp/scratch</code>. To write to the local disk, use the <code>$TMPDIR</code> environment variable, which is set to <code>/tmp/scratch/&lt;JOBID&gt;</code>. A node will not have read or write access to any other node's local scratch, only its own. Also, this directory will be cleaned once the job ends. You will need to transfer any files to be saved to another file system. Note that writing to <code>$TMPDIR</code> on a node without a real local disk will use RAM. </p> <p>To request nodes with local disk, use the <code>--tmp</code> option in your job submission script. (e.g. <code>--tmp=1600000</code>). For more information about requesting this feature, please see the Running on Kestrel page.</p>"},{"location":"Documentation/Systems/Kestrel/Filesystems/#backups-and-snapshots","title":"Backups and Snapshots","text":"<p>There are no backups nor snapshots of the Kestrel filesystems. Though the system is protected from hardware failure by multiple layers of redundancy, please regularly back up important data put on Kestrel, and consider using a Version Control System (such as Git) for important code.</p>"},{"location":"Documentation/Systems/Kestrel/Filesystems/lustre/","title":"Lustre Best Practices","text":"<p>In some cases special care must be taken while using Lustre so as not to affect the performance of the filesystem for yourself and other users. The below Do's and Don'ts are provided as guidance. </p>"},{"location":"Documentation/Systems/Kestrel/Filesystems/lustre/#do","title":"Do","text":"<ul> <li>Use the <code>lfs find</code><ul> <li>e.g.  <pre><code>lfs find /scratch/username -type f -name \"*.py\"\n</code></pre></li> </ul> </li> <li>Break up directories with many files into more directories if possible</li> <li>Store small files and directories of small files on a single OST (Object Storage Target) </li> <li>Limit the number of processes accessing a file. It may be better to read in a file once and then broadcast necessary information to other processes</li> <li>Change your stripecount based on the filesize</li> <li>Write many files to the node filesystem <code>$TMPDIR</code>: some compute nodes have local storage available, and it is not a part of the Lustre filesystem. Once your work is complete, the files can then be added to a tar archive and transferred to the <code>/project/project_name</code> for later use, or deleted from <code>$TMPDIR</code> if no longer needed.</li> <li>Store data in <code>/projects</code> and run jobs from <code>/scratch/$USER</code></li> <li>Storing your conda environments in <code>/projects</code> or <code>/scratch</code></li> </ul>"},{"location":"Documentation/Systems/Kestrel/Filesystems/lustre/#do-not","title":"Do Not","text":"<ul> <li>Use <code>ls -l</code></li> <li>Have a file accessed by multiple processes</li> <li>In Python, avoid using <code>os.walk</code> or <code>os.scandir</code></li> <li>List files instead of using wildcards<ul> <li>e.g. don't use <code>cp * dir/</code></li> <li>If you need to tar/rm/cp a large number of files use xargs or similar: <pre><code>lfs find /scratch/username/old_data/ -t f -print0 | xargs -0 rm\n</code></pre></li> </ul> </li> <li>Have many small files in a single directory</li> <li>Store important files in <code>/scratch</code><ul> <li>e.g. don't keep data, libraries or programs in <code>/scratch/username</code>, as <code>/scratch</code> directories are subject to automated purging based on the Data Retention Policy</li> </ul> </li> </ul>"},{"location":"Documentation/Systems/Kestrel/Filesystems/lustre/#useful-lustre-commands","title":"Useful Lustre commands","text":"<ul> <li>Check your storage usage:<ul> <li><code>lfs quota -h -u &lt;username&gt; /scratch</code></li> </ul> </li> <li>See which MDT a directory is located on<ul> <li><code>lfs getstripe --mdt-index /scratch/&lt;username&gt;</code></li> <li>This will return an index 0-2 indicating the MDT</li> </ul> </li> <li>Create a folder on a specific MDT (admin only)<ul> <li><code>lfs mkdir \u2013i &lt;mdt_index&gt; /dir_path</code></li> </ul> </li> </ul>"},{"location":"Documentation/Systems/Kestrel/Filesystems/lustre/#striping","title":"Striping","text":"<p>Lustre provides a way to stripe files, this spreads them across multiple OSTs. Striping a large file being accessed by many processes can greatly improve the performace. See Lustre file striping for more details. </p> <p><pre><code>lfs setstripe &lt;file&gt; -c &lt;count&gt; -s &lt;size&gt;\n</code></pre> * The stripecount determines how many OST the data is spread across * The stripe size is how large each of the stripes are in KB, MB, GB</p>"},{"location":"Documentation/Systems/Kestrel/Filesystems/lustre/#references","title":"References","text":"<ul> <li>Lustre manual</li> <li>CU Boulder - Lustre Do's and Don'ts</li> <li>NASA - Lustre Best Practices</li> <li>NASA - Lustre basics</li> <li>UMBC - Lustre Best Practices</li> <li>NICS - I/O and Lustre Usage</li> <li>NERSC - Lustre</li> </ul>"},{"location":"Documentation/Systems/Kestrel/Running/","title":"Kestrel Job Partitions and Scheduling Policies","text":"<p>Learn about job partitions and policies for scheduling jobs on Kestrel.</p>"},{"location":"Documentation/Systems/Kestrel/Running/#kestrel-compute-nodes","title":"Kestrel Compute Nodes","text":"<p>There are two general types of compute nodes on Kestrel: CPU nodes and GPU nodes.</p>"},{"location":"Documentation/Systems/Kestrel/Running/#cpu-nodes","title":"CPU Nodes","text":"<p>Standard CPU-based compute nodes on Kestrel have 104 cores and 240G of usable RAM. Additional node types are as follows:</p> <ul> <li>512 nodes have dual network interface cards (NICs) which may increase performance for certain types of multi-node jobs. 32 of these nodes have 1TB of RAM. (hbw partition)</li> <li>32 single NIC nodes have 1TB of RAM. (medmem partition)</li> <li>10 nodes with 2TB of RAM and 5.6TB NVMe local disk. (bigmem partition)</li> <li>256 nodes have 1.7TB NVMe local disk. (nvme partition)</li> </ul>"},{"location":"Documentation/Systems/Kestrel/Running/#gpu-nodes","title":"GPU Nodes","text":"<p>Kestrel has 156 GPU nodes with 4 NVIDIA H100 GPUs, each with 80GB memory. These have Dual socket AMD Genoa 64-core processors (128 cores total). The breakdown of GPU node architecture is as follows:</p> <ul> <li>108 nodes having about 384G of usable system RAM, 3.4TB of NVMe local disk.</li> <li>24 nodes having about 768G of system RAM, 3.4TB of NVMe local disk.</li> <li>24 have 1.5T of system RAM, and 14T of NVMe local disk.</li> </ul> <p>Warning</p> <p>You should use a login node that matches the architecture of the compute nodes that your jobs will be running on for compiling software and submitting jobs. </p>"},{"location":"Documentation/Systems/Kestrel/Running/#using-node-local-storage","title":"Using Node Local Storage","text":"<p>The majority of CPU nodes do not have local disk storage, but there are 256 nodes with fast local NVMe drives for temporary storage by jobs with high disk I/O requirements. To request the standard CPU nodes with local disk, specify the <code>nvme</code> partition (<code>-p nvme</code> or <code>--partition=nvme</code>) in your job submission script. When your job is allocated nodes with local disk, the storage may then be accessed inside the job by using the <code>$TMPDIR</code> environment variable as the path. Be aware that on nodes without local disk, writing to <code>$TMPDIR</code> will consume RAM, reducing the available memory for running processes.  </p> <p>Note that all of the Bigmem and H100 GPU nodes have real local disk. </p>"},{"location":"Documentation/Systems/Kestrel/Running/#partitions","title":"Partitions","text":"<p>Kestrel nodes are associated with one or more partitions. Each partition is associated with one or more job characteristics, which include run time, per-node memory requirements, and per-node local scratch disk requirements.</p> <p>Excluding the shared and debug partitions, jobs will be automatically routed to the appropriate partitions by Slurm based on node quantity, walltime, hardware features, and other aspects specified in the submission. Jobs will have access to the largest number of nodes, thus shortest wait, if the partition is not specified during job submission..</p> <p>The following table summarizes the partitions on Kestrel:</p> Partition Name Description Limits Placement Condition <code>debug</code> Nodes dedicated to developing  and troubleshooting jobs. Debug nodes with each of the non-standard hardware configurations are available. - 1 job with a max of 2 nodes per user.  - 2 GPUs per user. - 1/2 GPU node resources per user (Across 1-2 nodes).  - 01:00:00 max walltime. <code>-p debug</code>    or <code>--partition=debug</code> <code>short</code> Nodes that prefer jobs with walltimes  &lt;= 4 hours. 2240 nodes total. <code>--time &lt;= 4:00:00</code><code>--mem &lt;= 984256</code> <code>--tmp &lt;= 1700000 (256 nodes)</code> <code>standard</code> Nodes that prefer jobs with walltimes  &lt;= 2 days. 2240 nodes total.  1050 nodes per user. <code>--mem &lt;= 984256</code> <code>--tmp &lt;= 1700000</code> <code>long</code> Nodes that prefer jobs with walltimes &gt; 2 days.Maximum walltime of any job is 10 days. 430 nodes total. 215 nodes per user. <code>--time &lt;= 10-00</code><code>--mem &lt;= 984256</code><code>--tmp &lt;= 1700000  (256 nodes)</code> <code>medmem</code> Nodes that have 1TB of RAM. 64 nodes total. 32 nodes per user. <code>--time &lt;= 10-00</code><code>246064 &lt; mem &lt;= 984256</code> <code>bigmem</code> Nodes that have 2 TB of RAM and 5.6 TB NVMe local disk. 10 nodes total. 4 nodes per user. <code>--mem &gt; 984256</code> <code>--time &lt;= 2-00</code><code>--tmp &gt; 1700000</code> <code>bigmeml</code> Bigmem nodes that prefer jobs with walltimes &gt; 2 days.Maximum walltime of any job is 10 days. 4 nodes total. 2 nodes per user. <code>--mem &gt; 984256</code><code>--time &gt; 2-00</code><code>--tmp &gt; 1700000</code> <code>hbw</code> CPU compute nodes with dual network interface cards. 512 nodes total. 256 nodes per user.  Minimum 2 nodes per job. <code>-p hbw</code> <code>--time &lt;= 2-00</code> <code>--nodes &gt;= 2</code> <code>--mem &lt;= 984256</code> <code>hbwl</code> HBW nodes that prefer jobs with walltimes &gt; 2 days.Maximum walltime of any job is 10 days. 128 nodes total. 64 nodes per user.  Minimum 2 nodes per job. <code>-p hbw</code> <code>--time &gt; 2-00</code> <code>--nodes &gt;= 2</code> <code>--mem &lt;= 984256</code> <code>nvme</code> CPU compute nodes with 1.7TB NVMe local drives. 256 nodes total. 128 nodes per user. <code>-p nvme</code> <code>--time &lt;= 2-00</code> <code>shared</code> Nodes that can be shared by multiple users and jobs. 64 nodes total.  Half of partition per user.  2 days max walltime. <code>-p shared</code>    or <code>--partition=shared</code> <code>sharedl</code> Nodes that can be shared by multiple users and prefer jobs with walltimes &gt; 2 days. 16 nodes total.  8 nodes per user. <code>-p sharedl</code>    or <code>--partition=sharedl</code> <code>gpu-h100</code> Shareable GPU nodes with 4 NVIDIA H100 SXM 80GB Computational Accelerators. 156 nodes total. <code>1 &lt;= --gpus &lt;= 4</code> <code>--time &lt;= 2-00</code> <code>gpu-h100s</code> Shareable GPU nodes that prefer jobs with walltimes &lt;= 4 hours. 156 nodes total. <code>1 &lt;= --gpus &lt;= 4</code> <code>--time &lt;= 4:00:00</code> <code>gpu-h100l</code> Shareable GPU nodes that prefer jobs with walltimes &gt; 2 days. 39 GPU nodes total. <code>1 &lt;= --gpus &lt;= 4</code> <code>--time &gt; 2-00</code> <p>Use the option listed above on the <code>srun</code>, <code>sbatch</code>, or <code>salloc</code> command or in your job script to specify what resources your job requires.  </p> <p>For more information on running jobs and Slurm job scheduling, please see the Slurm documentation section.</p>"},{"location":"Documentation/Systems/Kestrel/Running/#shared-node-partition","title":"Shared Node Partition","text":"<p>Nodes in the shared partition can be shared by multiple users or jobs. This partition is intended for jobs that do not require a whole node.</p> <p>Tip</p> <p>Testing at NREL has been done to evaluate the performance of VASP using shared nodes. Please see the VASP page for specific recommendations. </p>"},{"location":"Documentation/Systems/Kestrel/Running/#usage","title":"Usage","text":"<p>Currently, there are 64 standard compute nodes available in the shared partition. These nodes have about 240G of usable RAM and 104 cores. By default, your job will be allocated about 1G of RAM per core requested. To change this amount, you can use the <code>--mem</code> or <code>--mem-per-cpu</code> flag in your job submission. </p> Sample batch script for a job in the shared partition <pre><code>#!/bin/bash\n#SBATCH --nodes=1 \n#SBATCH --partition=shared         \n#SBATCH --time=2:00:00    \n#SBATCH --ntasks=26 # CPUs requested for job \n#SBATCH --mem-per-cpu=2000 # Request 2GB per core.\n#SBATCH --account=&lt;allocation handle&gt;\n\ncd /scratch/$USER \nsrun ./my_progam # Use your application's commands here  \n</code></pre>"},{"location":"Documentation/Systems/Kestrel/Running/#high-bandwidth-partition","title":"High Bandwidth Partition","text":"<p>In December 2024, Kestrel had two racks of CPU nodes reconfigured with an extra network interface card, which can greatly benefit communication-bound HPC software. 32 of these nodes have 1TB of memory.  A NIC is a hardware component that enables inter-node (i.e., network) communication as multi-node jobs run.  On Kestrel, most CPU nodes include a single NIC. Although having one NIC per node is acceptable for the majority of workflows run on Kestrel, it can lead to communication congestion  when running multi-node applications that send significant amounts of data over Kestrel's network. When this issue is encountered, increasing the number of available NICs  can alleviate such congestion during runtime. Some common examples of communication-bound HPC software are AMRWind and LAMMPS.</p> <p>To request nodes with two NICs, specify <code>--partition=hbw</code> in your job submissions. Because the purpose of the high bandwidth nodes is to optimize communication in multi-node jobs, it is not permitted to submit single-node jobs to the <code>hbw</code> partition. If you would like assistance with determining whether your workflow could benefit from running in the <code>hbw</code> partition, please reach out to HPC-Help@nrel.gov.</p>"},{"location":"Documentation/Systems/Kestrel/Running/#gpu-jobs","title":"GPU Jobs","text":"<p>Each GPU node has 4 NVIDIA H100 GPUs (80 GB), 128 CPU cores, and at least 350GB of useable RAM. 24 of the GPU nodes have about 700G of RAM and 24 have 1.5TB of RAM (1440000M usable). All of the GPU nodes are shared. We highly recommend considering the use of partial GPU nodes if possible in order to efficiently use the GPU nodes and your AUs. </p> <p>To request use of a GPU, use the flag <code>--gpus=&lt;quantity&gt;</code> with sbatch, srun, or salloc, or add it as an <code>#SBATCH</code> directive in your sbatch submit script, where <code>&lt;quantity&gt;</code> is a number from 1 to 4. All of the GPU memory for each GPU allocated will be available to the job (80 GB per GPU).</p> <p>If your job will require more than the default 1 CPU core and 1G of system RAM per core allocated, you must request the quantity of cores and/or system RAM that you will need, by using additional flags such as <code>--ntasks=</code> or <code>--mem=</code>.</p> <p>Most of the GPU nodes also have 3.4 TB of local disk space, and 24 of them have 14TB of space (13000000M useable). Note that other jobs running on the same GPU node could also be using this space. Slurm is unable to divide this space to separate jobs on the same node like it does for memory or CPUs. If you need to ensure that your job has exclusive access to all of the disk space, you'll need to use the <code>--exclusive</code> flag to prevent the node from being shared with other jobs.</p> <p>Warning</p> <p>A job with the <code>--exclusive</code> flag will be allocated all of the CPUs and GPUs, and GPU memory on a node, but is only allocated as much system RAM as requested. Use the flag <code>--mem=&lt;RAM amount&gt;</code> to request system RAM. The GPU nodes have up to 720000M of RAM, but please request only as much RAM as you need in order to efficiently use the nodes and minimize your jobs' wait times. </p>"},{"location":"Documentation/Systems/Kestrel/Running/#gpu-debug-jobs","title":"GPU Debug Jobs","text":"<p>To run GPU debug jobs, specify <code>--partition=debug</code> in your job script. In addition to the limits for the <code>debug</code> partition, 1 job per user, up to 2 nodes per user, and up to 1 hour of walltime, a single GPU job is also limited to half of a total GPU node's resources. This is equivalent to 64 CPU cores, 2 GPUs, and 180G of RAM, which can be spread across 1 or 2 nodes. Unlike the other GPU nodes, the GPU debug nodes can't be used exclusively, so the <code>--exclusive</code> flag can't be used for debug GPU jobs. </p>"},{"location":"Documentation/Systems/Kestrel/Running/#allocation-unit-au-charges","title":"Allocation Unit (AU) Charges","text":"<p>The equation for calculating the AU cost of a job is:</p> <p><code>AU cost = (Walltime in hours * Number of Nodes * QoS Factor * Charge Factor)</code></p> <p>The CPU node charge factor is 10, and the GPU node charge factor is 100. </p> <p>On shared nodes (nodes in the <code>shared</code> partition and GPU nodes), the value for <code>Number of Nodes</code> can be a fraction of a node. This value will be calculated based on either the number of cores, amount of memory, or the number of GPUs (on GPU nodes), whichever is a greater percentage of the total of that resource available on the node.</p> Example Job Cost Calculation - CPU shared  <p>For example, if you request 123032M of RAM (half of the available RAM on the node), and 26 cores, you will be billed 5 AUs per node hour.</p> <pre><code># To determine the Number of Nodes value: \n123032/246064 = 0.5\n\n26/104 = 0.25 \n\nNumber of Nodes = 0.5\n\n# Final calculation\n\n1 hour walltime * 0.5 nodes * 1 QoS Factor * 10 Charge Factor = 5 AUs\n</code></pre> Example Job Cost Calculation - GPU  <p>For example, if you request 270000M of RAM, 32 cores, and 2 GPUs you will be billed 75 AUs per node hour.</p> <pre><code># To determine the Number of Nodes value: \n\n# CPU RAM\n270000/360000  0.75\n\n# CPU Cores \n32/128 = 0.25 \n\n# GPUs\n2/4 = 0.5\n\n\nNumber of Nodes = 0.75\n\n# Final calculation\n\n1 hour walltime * 0.75 nodes * 1 QoS Factor * 100 Charge Factor = 75 AUs\n</code></pre> <p>If a job requests the maximum amount of any resource type available on the node (CPUs, GPUs, RAM), it will be charged with the full charge factor (10 or 100).</p>"},{"location":"Documentation/Systems/Kestrel/Running/#aus_report-command-line-utility","title":"aus_report Command Line Utility","text":"<p>There is a CLI utility <code>aus_report</code> available on Kestrel to track your AU usage. This utility uses the data from Lex to output AU usage information on a per-allocation and per-user basis. Please refer to Lex in the case of an discrepancies. Run <code>aus_report --help</code> for more information. </p>"},{"location":"Documentation/Systems/Kestrel/Running/#performance-recommendations","title":"Performance Recommendations","text":"<p>Please see this page for our most up-to-date performance recommendations on Kestrel.</p>"},{"location":"Documentation/Systems/Kestrel/Running/example_sbatch/","title":"Sample Batch Scripts for Running Jobs on the Kestrel System","text":"<p>For a walkthrough of the elements of an sbatch script, please see Submitting Batch Jobs. For application specific recommendations and examples, please check the Application pages. </p> Sample batch script for a CPU job in the debug queue <pre><code>#!/bin/bash \n#SBATCH --account=&lt;allocation handle&gt;   # Required\n#SBATCH --ntasks=104                    # Tasks to be run \n#SBATCH --nodes=1                       # Run the tasks on the same node \n#SBATCH --time=5                        # Required, maximum job duration \n#SBATCH --partition=debug \n\ncd /scratch/$USER \n\nsrun ./my_program.sh\n</code></pre> Sample batch script with memory request <p>Standard Kestrel CPU nodes have about 250G of usable RAM. There are 10 bigmem nodes with 2TB of ram.  <pre><code>#!/bin/bash \n#SBATCH --account=&lt;allocation handle&gt;   # Required \n#SBATCH --ntasks=104                    # CPU cores requested for job \n#SBATCH --time=01-00                    # Required, maximum job duration\n#SBATCH --mem=500G                      # Memory request\n\n\ncd /scratch/$USER \nsrun ./my_program.sh\n</code></pre></p> Sample batch script for a job in the shared partition <p>If your job doesn't need a full CPU node (104 cores), you can run your job in the shared partition. When running on a shared node, the default memory per CPU is 1G. To change this amount, use the <code>--mem-per-cpu=&lt;MEM_REQUEST&gt;</code> flag.</p> <pre><code>#!/bin/bash\n#SBATCH --nodes=1 \n#SBATCH --partition=shared         \n#SBATCH --time=2:00:00                  # Required, maximum job duration\n#SBATCH --ntasks=26                     # CPUs requested for job \n#SBATCH --mem-per-cpu=2000              # Requesting 2G per core.\n#SBATCH --account=&lt;allocation handle&gt;   # Required \n\ncd /scratch/$USER \nsrun ./my_progam # Use your application's commands here  \n</code></pre> Sample batch script to utilize local disk <p>On Kestrel, 256 of the standard compute nodes have 1.7TB of NVMe node local storage. Use the flag <code>SBATCH --tmp=&lt;LOCAL_DISK_REQUEST&gt;</code> to request a node with local disk space. The storage may then be accessed inside the job by using the <code>$TMPDIR</code> environment variable.</p> <pre><code>#!/bin/bash \n#SBATCH --account=&lt;allocation handle&gt;      # Required \n#SBATCH --ntasks=104                       # CPU cores requested for job \n#SBATCH --nodes=1                  \n#SBATCH --time=01-00                       # Required, maximum job duration\n#SBATCH --partition=nvme                   # Request node with local disk\n\n# Copy files into $TMPDIR \ncp /scratch/&lt;userid&gt;/myfiles* $TMPDIR \n\nsrun ./my_parallel_readwrite_program -input-options $TMPDIR/myfiles  # use your application's commands  \n</code></pre> Sample batch script for high-priority job <p>A job may request high priority using <code>--qos=high</code>, which will give a small priority bump in the queue. This will charge your allocation at 2x the normal rate. </p> <pre><code>#!/bin/bash\n#SBATCH --job-name=job_monitor\n#SBATCH --account=&lt;allocation handle&gt;      # Required     \n#SBATCH --time=00:05:00                    # Required, maximum job duration\n#SBATCH --qos=high                         # Request high priority\n#SBATCH --ntasks=104\n#SBATCH -N 2 \n#SBATCH --output=job_monitor.out \n\ncd /scratch/$USER \nsrun ./my_program.sh\n</code></pre> Sample batch script for a GPU job in the debug queue <p>All GPU nodes in the debug queue are shared.  You are limited to two GPUs per job, across 1 or 2 nodes.  <pre><code>#!/bin/bash \n#SBATCH --account=&lt;allocation handle&gt;   # Required\n#SBATCH --nodes=2  \n#SBATCH --gpus-per-node=1\n#SBATCH --mem=50G                       # Request system RAM per node. \n#SBATCH --ntasks-per-node=2             # Request CPU cores per node\n#SBATCH --time=01:00:00                 # Required, maximum job duration \n#SBATCH --partition=debug \n\ncd /scratch/$USER \n\nsrun ./my_program.sh\n</code></pre></p> Sample batch script for a full GPU node <p>Kestrel GPU nodes have 4 H100 GPUs. To run jobs on GPUs, your script should contain the <code>--gpus=&lt;NUM_GPUS&gt;</code> flag in the SBATCH directives. Even when requesting all GPUs, you must request the system memory with <code>--mem</code> or <code>--mem-per-cpu</code>.  Submit GPU jobs from the GPU login nodes.</p> <pre><code>#!/bin/bash\n#SBATCH --nodes=1\n#SBATCH --account=&lt;allocation handle&gt;   # Required \n#SBATCH --time=02:00:00                 # Required, maximum job duration\n#SBATCH --ntasks-per-node=128           # Maximum CPU cores for job \n#SBATCH --gpus=4                        # GPU request \n#SBATCH --exclusive                     # Request exclusive access to node. Allocates all CPU cores and GPUs by default.  \n#SBATCH --mem=360000                    # Request system RAM. If you need more memory, request up to 720000 to use the larger mem GPU nodes.\n\n# Load modules\nmodule load vasp\n\n# Run program\ncd /scratch/$USER \nsrun my_graphics_intensive_scripting \n</code></pre> Sample batch script for a partial GPU node <p>GPU nodes can be shared so you may request fewer than all 4 GPUs on a node. When doing so, you must also request appropriate CPU cores and memory with the <code>--ntasks-per-node=&lt;NUM_CPUS&gt;</code> and <code>--mem=&lt;MEMORY_REQUEST&gt;</code> flags, respectively. Submit GPU jobs from the GPU login nodes.</p> <pre><code>#!/bin/bash\n#SBATCH --nodes=1\n#SBATCH --account=&lt;allocation handle&gt;   # Required \n#SBATCH --time=2:00:00                  # Required, maximum job duration\n#SBATCH --ntasks-per-node=20            # Request CPU cores \n#SBATCH --gpus=2                        # GPU request \n#SBATCH --mem=170G                      # Request CPU memory\n\n# Load modules\n\n# Run program\ncd /scratch/$USER \nsrun my_graphics_intensive_scripting \n</code></pre>"},{"location":"Documentation/Systems/Kestrel/Running/kestrel_job_priorities/","title":"Job Priorities on Kestrel","text":"<p>Job priority on Kestrel is determined by a number of factors including time the job is eligible to run in the queue (age), the size of the job (jobsize), resources requested and their partition (partition), quality of service and the  associated priority (qos), and the relative fair-share of the individual allocation.</p> <p>Learn about job partitions and scheduling policies.</p>"},{"location":"Documentation/Systems/Kestrel/Running/kestrel_job_priorities/#job-priority-scheduling","title":"Job Priority &amp; Scheduling","text":"<p>The Slurm scheduler has two scheduling loops: </p> <ol> <li>Main scheduling loop, which schedules jobs in strict priority order, </li> <li>Backfill scheduling loop, that allows lower priority jobs to be scheduled (as long as the expected start time of higher priority jobs is not affected).  </li> </ol> <p>In both cases, Slurm schedules in strict priority, with higher priority jobs being considered first for scheduling; however, due to the resources requested or other configuration options, there may be availability for backfill to schedule lower priority jobs (with the same caveat as before, that lower priority jobs can not affect the expected start time of higher priority jobs).  The backfill scheduler uses the user defined wallclock during submission for scheduling, which can result in scheduler inefficiencies as the estimates diverge from actual wallclock.</p> <p>An individual job's priority is a combination of multiple factors: (1) age, (2) nodes requested or jobsize, (3) partition factor, (4) quality of service (qos), and (5) the relative fair-share of the individual allocation.  There is a weighting factor associated with each of these components (shown below) that determines the relative contribution of each factor:</p> Component Weighting Factor Nominal Weight Note age 30,589,200 4% Jobs accumulate AGE priority while in the queue and eligible to run (up to a maximum of 14 days) jobsize 221,771,700 29% Jobs receive increasing size priority as number of nodes requested increases. partition 38,236,500 5% Additional boost for certain partitions (e.g., nvme), so that jobs requesting specific resources receive priority. qos 76,473,000 10% A job may request high-priority using --qos=high and receive the full qos priority.  Jobs without this flag receive no qos priority. fairshare 397,659,600 52% A project is under-served (and receives a higher fair-share priority) if the project's usage is low relative to the size of its allocation.  There is additional complexity discussed below."},{"location":"Documentation/Systems/Kestrel/Running/kestrel_job_priorities/#fairshare","title":"Fairshare","text":"<p>Fairshare is a scheduling system where the size of a project's allocation relative to the size of the machine determines their relative fairshare.  A project's fairshare priority would be elevated if the utilization is low relative to their fairshare, where utilization is a function of sibling projects (same office).  Similarily, a project's fairshare priority would be lower if the utilization is high relative to the allocation.  </p> <p>A fairtree with a hypothetical allocation is illustrated below:</p> <p></p> <p>In this hypothetical scenario, fairshare values would be calculated at each point in the fairtree.  The fairshare calculations are a function of: (1) the allocation, (2) the sum of the sibling allocations, and (3) recent usage of both the allocation and the siblings.  For example, the utilization and allocations of all projects in EERE / Office 2 would be used to calculate the individual level fairshare value for the individual projects contained in that office.  Similarly, the fairshare values for all offices within EERE would be used to calculate the fairshare values for EERE / Office 1 and EERE / Office 2.</p> <p>The level fairshare values are calculated as follows:</p> \\[Level Fairshare = \\frac{S}{U}\\] <p>where </p> \\[S = \\frac{Sraw_{self}}{Sraw_{self+siblings}}, \\quad U = \\frac{Uraw_{self}}{Uraw_{self+siblings}}\\] <p>This is repeated at each level of the fairtree, and a ranked list is built using a depth first traversal of the fairshare tree.  A project's fairshare priority is proportional to its position on this list.  </p> <p>The list is descended depth first in part to prioritize the higher level assigned percentages (e.g.,  the EERE and NREL utilization is balanced first, then individual offices within EERE and NREL, and so on).  Due to the depth first traversal it is hypothetically possible that an underserved allocation exists with a high level fairshare value, but is lower on the ranked list as the order of traversal is determined from its siblings and parents' usage.   </p> <p>As additional complexity, the above usage calculations are modified by a half-decay system that emphasizes more recent usage and de-emphasizes historical usage:</p> \\[ U = U_{currentperiod} + ( D * U_{lastperiod}) + (D * D * U_{period-2}) + ...\\] <p>The decay factor, D, is a number between 0 and 1 that achieves the half-decay rate specified by the Slurm configruation files (14 day on Kestrel).</p>"},{"location":"Documentation/Systems/Kestrel/Running/kestrel_job_priorities/#how-to-view-slurm-job-priority","title":"How to View Slurm Job Priority","text":"<p>Once jobs are submitted to Kestrel, Slurm will periodically evaluate and assign jobs priority. As a user, you can view the current priority assigned by Slurm with the use of the <code>sprio</code> command. Please see here for instructions on how to view the priority of Slurm-submitted jobs.</p>"},{"location":"Documentation/Systems/Kestrel/Running/kestrel_job_priorities/#how-to-get-high-priority-for-a-job","title":"How to Get High Priority for a Job","text":"<p>You can submit your job to run at high priority or you can request a node reservation.</p>"},{"location":"Documentation/Systems/Kestrel/Running/kestrel_job_priorities/#running-a-job-at-high-priority","title":"Running a Job at High Priority","text":"<p>Jobs that are run at high priority will be charged against the project's allocation at twice the normal rate. If your job would have taken 60 hours to complete at normal priority, it will be charged 120 hours against your allocation when run with high priority.</p> <p>If you've got a deadline coming up and you want to reduce the queue wait time for your jobs, you can run your jobs at high priority by submitting them with the <code>--qos=high</code> option. This will provide a small priority boost.</p>"},{"location":"Documentation/Systems/Kestrel/Running/kestrel_job_priorities/#requesting-a-node-reservation","title":"Requesting a Node Reservation","text":"<p>If you are doing work that requires real-time Kestrel access in conjunction with other ESIF user facility laboratory resources, you may request that nodes be reserved for specific time periods.</p> <p>Your project allocation will be charged for the entire time you have the nodes reserved, whether you use them or not.</p> <p>To request a reservation, contact HPC Help.</p>"},{"location":"Documentation/Systems/Kestrel/Running/kestrel_job_priorities/#how-to-get-standby-priority-for-a-job","title":"How to Get Standby Priority for a Job","text":"<p>All partitions have a matching <code>-standby</code> partition, which has lower priority. You can always opt to run jobs in standby at no cost towards your project\u2019s AU consumption. To submit a standby job to any partition, simply add <code>#SBATCH --qos=standby</code> to your job submission script. Standby jobs only run when nodes are otherwise idle (i.e., regular AU-charged jobs will always take priority over standby jobs). Submitting jobs with <code>--qos=standby</code> can be a good option if:      1) Wait time is not a concern for your jobs, and/or     2) Your desired Slurm partition is relatively open, and you want to save AUs for other jobs. Please see here for instructions on how to estimate a partition's availability.</p> <p>Note that <code>standby</code> is the default QoS for allocations which have already consumed all awarded AUs for the year.</p>"},{"location":"Documentation/Systems/Kestrel/Running/performancerecs/","title":"Performance Recommendations","text":"<p>Please note that all of these recommendations are subject to change as we continue to improve the system.</p>"},{"location":"Documentation/Systems/Kestrel/Running/performancerecs/#mpi","title":"MPI","text":"<p>Applications running across multiple CPU nodes on Kestrel might experience performance problems. Following your scaling tests, if your application underperforms, consider these performance improvement suggestions.</p> <ul> <li> <p>For any application, use <code>Cray-MPICH</code> instead of <code>OpenMPI</code> because <code>Cray-MPICH</code> is highly optimized to leverge the interconnect used on Kestrel. For OpenMPI to Cray MPICH code rebuilding assistance, please contact hpc-help@nrel.gov. If your application was built with an MPICH ABI-compatible MPI library, use <code>cray-mpich-abi</code> for optimal performance. The cray-mpich-abi usage involves these steps: load the programming environment module (<code>PrgEnv-*</code>) that matches your application's compiler; load the <code>cray-mpich-abi</code> module; and run your application with <code>Slurm</code> (see example below).    <pre><code>$ module load PrgEnv-gnu or module load PrgEnv-intel\n$ module swap cray-mpich/8.1.28 cray-mpich-abi/8.1.28\n$ srun -N $SLURM_NNODES -n $SLURM_NTASKS --distribution=block:block --cpu_bind=rank_ldom ./&lt;application executable&gt;\n</code></pre></p> </li> <li> <p>The performance of latency-sensitive applications, such as AMR-Wind, Nalu-Wind, and LAMMPS with medium-size input, is impacted by message communication congestion in the interconnect when running jobs on over 8 CPU-nodes using <code>cray-mpich/8.1.28</code>. Higher congestion is observed using the standard CPU nodes which each have one NIC, than using the nodes in the <code>hbw</code> partition, which each have two NICs per node. The stall feature in <code>CRAY MPICH version 8.1.30.1</code> boosts application performance by regulating message injection into the interconnect. To utilize this library, applications must be built using either the <code>PrgEnv-gnu</code>, <code>PrgEnv-cray</code>, or <code>PrgEnv-intel</code> programming environment modules, or an MPICH ABI-compatible MPI library. Your job script needs one of these script snippets: use the first for programming environment builds; the second for MPICH-ABI-compatible builds.    <pre><code># Set the number of NICs to 2 for `hbw`\nexport NIC=1\n\n# Load the shared libraries\nexport LD_LIBRARY_PATH=/nopt/nrel/apps/cray-mpich-stall/libs_mpich_nrel_*:$LD_LIBRARY_PATH\n\n# Enable the stall mechanism\nexport MPICH_OFI_CQ_STALL=1\n\n# Activate the stall library\nexport MPICH_OFI_CQ_MIN_PPN_PER_NIC=($SLURM_NTASKS_PER_NODE/NIC)\n\n# Tune the stall value\nexport MPICH_OFI_CQ_STALL_USECS=26\n</code></pre> <pre><code># Set the number of NIC to 2 for `hbw`\nexport NIC=1\nexport LD_LIBRARY_PATH=/nopt/nrel/apps/cray-mpich-stall/libs_mpich_nrel_*_adj:$LD_LIBRARY_PATH\nexport MPICH_OFI_CQ_STALL=1\nexport MPICH_OFI_CQ_MIN_PPN_PER_NIC=($SLURM_NTASKS_PER_NODE/NIC)\nexport MPICH_OFI_CQ_STALL_USECS=1\n</code></pre> Substitute <code>*</code> with the compiler name (e.g., <code>cray</code>, <code>intel</code>, or <code>gnu</code>) used to compile your application. For best performance, experiment with stall (<code>MPICH_OFI_CQ_STALL_USECS</code>) values of between 1 and 26 microseconds; the default is 12 microseconds. For example, you may run your application using a stall value from this list: [1, 3, 6, 9, 12, 16, 20, 26]. If you need assistance in using this stall library, please email hpc-help@nrel.gov.</p> </li> </ul> <p>Note</p> <p><code>Spack</code>-built applications have hardcoded runtime paths in their executables, necessitating the use of <code>LD_PRELOAD</code>. For example, the PrgEnv-intel shared libraries can be loaded as follows: <code>export LD_PRELOAD=/nopt/nrel/apps/cray-mpich-stall/libs_mpich_nrel_intel/libmpi_intel.so.12:/nopt/nrel/apps/cray-mpich-stall/libs_mpich_nrel_intel/libmpifort_intel.so.12</code></p> <ul> <li>For hybrid MPI/OpenMP codes, requesting more threads per task than you tend to request on Eagle. This may yield performance improvements.</li> </ul>"},{"location":"Documentation/Systems/Kestrel/Running/performancerecs/#openmp","title":"OpenMP","text":"<p>If you are running a code with OpenMP enabled, we recommend manually setting one of the following environment variables:</p> <pre><code>export OMP_PROC_BIND=spread # for non-intel built codes\n\nexport KMP_AFFINITY=balanced # for codes built with intel compilers\n</code></pre> <p>You may need to export these variables even if you are not running your job with threading, i.e., with <code>OMP_NUM_THREADS=1</code></p>"},{"location":"Documentation/Systems/Swift/","title":"About the Swift Cluster","text":"<p>Swift is an AMD-based HPC cluster with AMD EPYC 7532 (Rome) CPU's that supports EERE Vehicles Technologies Office (VTO) projects. Any VTO funded EERE project is eligible for an allocation on Swift. Allocation decisions are made by EERE through the annual allocation cycle. Swift is well suited for parallel jobs up to 64 nodes and offers better queue wait times for projects that are eligible.</p> <p>Please see the System Configurations page for more information about hardware, storage, and networking.</p>"},{"location":"Documentation/Systems/Swift/#accessing-swift","title":"Accessing Swift","text":"<p>Access to Swift requires an NREL HPC account and permission to join an existing allocation. Please see the System Access page for more information on accounts and allocations.</p>"},{"location":"Documentation/Systems/Swift/#login-nodes","title":"Login Nodes:","text":"<pre><code>swift.hpc.nrel.gov\nswift-login-1.hpc.nrel.gov\n</code></pre>"},{"location":"Documentation/Systems/Swift/#for-nrel-employees","title":"For NREL Employees:","text":"<p>Swift can be reached from the NREL VPN via ssh to the login nodes as above.</p>"},{"location":"Documentation/Systems/Swift/#for-external-collaborators","title":"For External Collaborators:","text":"<p>There are currently no external-facing login nodes for Swift. There are two options to connect:</p> <ol> <li>Connect to the SSH gateway host and log in with your username, password, and OTP code. Once connected, ssh to the login nodes as above.</li> <li>Connect to the HPC VPN and ssh to the login nodes as above.</li> </ol>"},{"location":"Documentation/Systems/Swift/#get-help-with-swift","title":"Get Help With Swift","text":"<p>Please see the Help and Support Page for further information on how to seek assistance with Swift or your NREL HPC account. </p>"},{"location":"Documentation/Systems/Swift/applications/","title":"Swift applications","text":"<p>Some optimized versions of common applications are provided for the Swift cluster. Below is a list of how to utilize these applications and the optimizations for Swift. </p>"},{"location":"Documentation/Systems/Swift/applications/#modules","title":"Modules","text":"<p>Many are available as part of the Modules setup.</p>"},{"location":"Documentation/Systems/Swift/applications/#tensorflow","title":"TensorFlow","text":"<p>TensorFlow has been built for the AMD architecture on Swift. This was done by using the following two build flags. </p> <pre><code>-march=znver2\n-mtune=znver2\n</code></pre> <p>This version of TensorFlow can be installed from a wheel file:  <pre><code>pip install --upgrade --no-deps --force-reinstall /nopt/nrel/apps/wheels/tensorflow-2.4.2-cp38-cp38-linux_x86_64-cpu.whl\n</code></pre></p> <p>Currently, this wheel is not built with NVIDIA CUDA support for running on GPU. </p> <p>TensorFlow installed on Swift with Conda may be significantly slower than the optimized version </p>"},{"location":"Documentation/Systems/Swift/filesystems/","title":"Swift Filesystem Architecture Overview","text":"<p>Swift's central storage currently has a capacity of approximately 3PB, served over NFS (Network File System). It is a performant system with  multiple read and write cache layers and redundancies for data protection, but it is not a parallel filesystem, unlike Kestrel's Lustre configuration.</p> <p>The underlying filesystem and volume management is via ZFS. Data is protected in ZFS RAID arrangements (raidz3) of 8 storage disks and 3 parity disks. </p> <p>Each Swift fileserver serves a single storage chassis (JBOD, \"just a bunch of disks\") consisting of multiple spinning disks plus SSD drives for read and write caches. </p> <p>Each fileserver is also connected to a second storage chassis to serve as a redundant backup in case the primary fileserver for that storage chassis fails, allowing continued access to the data on the storage chassis until the primary fileserver for that chassis is restored to service.</p>"},{"location":"Documentation/Systems/Swift/filesystems/#project-storage-projects","title":"Project Storage: /projects","text":"<p>Each active project is granted a subdirectory under <code>/projects/&lt;projectname&gt;</code>. This is where the bulk of data is expected to be, and where jobs should generally be run from. Storage quotas are based on the allocation award.</p> <p>Quota usage can be viewed at any time by issuing a <code>cd</code> command into the project directory, and using the <code>df -h</code> command to view total, used, and remaining available space for the mounted project directory.</p>"},{"location":"Documentation/Systems/Swift/filesystems/#nfs-automount-system","title":"NFS Automount System","text":"<p>Project directories are automatically mounted or unmounted via NFS on an \"as-needed\" basis. /projects directories that have not been accessed for a period of time will be umounted and not immediately visible via a command such as <code>ls /projects</code>, but will become immediately available if a file or path is accessed with an <code>ls</code>, <code>cd</code>, or other file access is made in that path. </p>"},{"location":"Documentation/Systems/Swift/filesystems/#home-directories-home","title":"Home Directories: /home","text":"<p>/home directories are mounted as <code>/home/&lt;username&gt;</code>. Home directories are hosted under the user's initial /project directory. Quotas in /home are included as a part of the quota of that project's storage allocation. </p>"},{"location":"Documentation/Systems/Swift/filesystems/#scratch-space-scratchusername-and-scratchusernamejobid","title":"Scratch Space: /scratch/username and /scratch/username/jobid","text":"<p>For users who also have Kestrel allocations, please be aware that scratch space on Swift behaves differently, so adjustments to job scripts may be necessary. </p> <p>The scratch directory on each Swift compute node is a 1.8TB spinning disk, and is accessible only on that node. The default writable path for scratch use is <code>/scratch/&lt;username&gt;</code>. There is no global, network-accessible <code>/scratch</code> space. <code>/projects</code> and <code>/home</code> are both network-accessible, and may be used as /scratch-style working space instead.</p>"},{"location":"Documentation/Systems/Swift/filesystems/#temporary-space-tmpdir","title":"Temporary space: $TMPDIR","text":"<p>When a job starts, the environment variable <code>$TMPDIR</code> is set to <code>/scratch/&lt;username&gt;/&lt;jobid&gt;</code> for the duration of the job. This is temporary space only, and should be purged when your job is complete. Please be sure to use this path instead of /tmp for your tempfiles.</p> <p>There is no expectation of data longevity in scratch space, and it is subject to purging once the node is idle. If desired data is stored here during the job, please be sure to copy it to a /projects directory as part of the job script before the job finishes.</p>"},{"location":"Documentation/Systems/Swift/filesystems/#mass-storage-system","title":"Mass Storage System","text":"<p>There is no Mass Storage System for deep archive storage on Swift.</p>"},{"location":"Documentation/Systems/Swift/filesystems/#backups-and-snapshots","title":"Backups and Snapshots","text":"<p>There are no backups or snapshots of data on Swift. Though the system is protected from hardware failure by multiple layers of redundancy, please keep regular backups of important data on Swift, and consider using a Version Control System (such as Git) for important code. </p>"},{"location":"Documentation/Systems/Swift/modules/","title":"Swift Modules","text":"<p>This describes how to activate and use the modules available on Swift. </p>"},{"location":"Documentation/Systems/Swift/modules/#source","title":"Source","text":"<p>Environments are provided with a number of commonly used modules including compilers, common build tools, specific AMD optimized libraries, and some analysis tools. When you first login there is a default set of modules available.  These can be seen by running the command:</p> <pre><code>module avail \n</code></pre> <p>Since Swift is a new machine we are experimenting with additional environments. The environments are in date stamped subdirectory under in the directory /nopt/nrel/apps.  Each environemnt directory has a file myenv.*.   If the myenv.*. is missing from a directory then that environment is a work in progress.   Sourcing myenv.* file will enable the environment and give you a new set of modules.  </p> <p>For example to enable the environment /nopt/nrel/apps/210728a source the provided environment file. </p> <pre><code>source /nopt/nrel/apps/210728a/myenv.2107290127\n</code></pre> <p>You will now have access to the modules provided. These can be listed using the following: </p> <pre><code>ml avail \n</code></pre> <p>If you want to build applications you can then module load compilers and the like; for example</p> <pre><code>ml gcc openmpi\n</code></pre> <p>will load gnu 9.4 and openmpi.</p> <p>Software is installed using a spack hierarchy. It is possible to add software to the hierarchy.  This should be only done by people responsible for installing software for all users.  It is also possible to do a spack install creating a new level of the hierarchy in your personal space.  These procedures are documented in https://github.nrel.gov/tkaiser2/spackit.git in the file <code>Notes03.md</code> under the sections Building on the hierarchy and Building outside the hierarchy.  If you want to try this please contact Tim Kaiser to walk through the procedure.</p> <p>Most environments have an example directory.  You can copy this directory to you own space and compile and run the examples.  The files runintel and runopenmp are  simple batch scripts.  These also have \"module load\" lines that you need to run before building with either compiler set.</p>"},{"location":"Documentation/Systems/Swift/running/","title":"Running on Swift","text":"<p>Please see the Modules page for information about setting up your environment and loading modules. </p>"},{"location":"Documentation/Systems/Swift/running/#login-nodes","title":"Login nodes","text":"<pre><code>swift.hpc.nrel.gov\nswift-login-1.hpc.nrel.gov\n</code></pre> <p><code>swift.hpc.nrel.gov</code> is a round-robin alias that will connect you to any available login node.</p>"},{"location":"Documentation/Systems/Swift/running/#ssh-keys","title":"SSH Keys","text":"<p>User accounts have a default set of keys <code>cluster</code> and <code>cluster.pub</code>. The <code>config</code> file will use these even if you generate a new keypair using <code>ssh-keygen</code>. If you are adding your keys to Github or elsewhere you should either use <code>cluster.pub</code> or will have to modify the <code>config</code> file.</p>"},{"location":"Documentation/Systems/Swift/running/#slurm-and-partitions","title":"Slurm and Partitions","text":"<p>The most up to date list of partitions can always be found by running the <code>sinfo</code> command on the cluster.</p> Partition Description long jobs up to ten days of walltime standard jobs up to two days of walltime gpu Nodes with four NVIDIA A100 40 GB Computational Accelerators, up to two days of walltime debug two nodes reserved for short tests, up to four hours of walltime <p>Each partition also has a matching <code>-standby</code> partition. Allocations which have consumed all awarded AUs for the year may only submit jobs to these partitions, and their default QoS will be set to <code>standby</code>. Jobs in standby partitions will be scheduled when there are otherwise idle cycles and no other non-standby jobs are available. Jobs that run in the standby queue will not be charged any AUs. </p> <p>Any allocation may submit a job to a standby QoS, even if there are unspent AUs.</p> <p>By default, nodes can be shared between users.  To get exclusive access to a node use the <code>--exclusive</code> flag in your sbatch script or on the sbatch command line.</p> <p>Important</p> <p>Use <code>--cpus-per-task</code> with srun/sbatch otherwise some applications may only utilize a single core.</p>"},{"location":"Documentation/Systems/Swift/running/#gpu-nodes","title":"GPU Nodes","text":"<p>Swift now has ten GPU nodes. Each GPU node has 4 NVIDIA A100 40GB GPUs, 96 CPU cores, and 1TB RAM. </p> <p>GPU nodes are also shared, meaning that less than a full node may be requested for a job, leaving the remainder of the node for use by other jobs concurrently. (See the section below on AU Charges for how this affects the AU usage rate.) </p> <p>To request use of a GPU, use the flag <code>--gres=gpu:&lt;quantity&gt;</code> with sbatch, srun, or salloc, or add it as an <code>#SBATCH</code> directive in your sbatch submit script, where <code>&lt;quantity&gt;</code> is a number from 1 to 4. </p>"},{"location":"Documentation/Systems/Swift/running/#cpu-core-and-ram-defaults-on-gpu-nodes","title":"CPU Core and RAM Defaults on GPU Nodes","text":"<p>If your job will require more than the default 1 CPU core and 1.5GB RAM you must request the quantity of cores and/or RAM that you will need, by using additional flags such as <code>--ntasks=</code> or <code>--mem=</code>. See the Slurm Job Scheduling section for details on requesting additional resources.</p>"},{"location":"Documentation/Systems/Swift/running/#allocation-unit-au-charges","title":"Allocation Unit (AU) Charges","text":"<p>The equation for calculating the AU cost of a job on Swift is:</p> <p><code>AU cost = (Walltime in hours * Number of Nodes * QoS Factor * Charge Factor)</code></p> <p>The Walltime is the actual length of time that the job runs, in hours or fractions thereof.</p> <p>The Number of nodes can be whole nodes or fractions of a node. See below for more information.</p> <p>The Charge Factor for Swift CPU nodes is 5. </p> <p>The Charge Factor for Swift GPU nodes is 50, or 12.5 per GPU.</p> <p>The QoS Factor for normal priority jobs is 1. </p> <p>The QoS Factor for high-priority jobs is 2.</p> <p>The QoS Factor for standby priority jobs is 0. There is no AU cost for standby jobs.</p> <p>One CPU node for one hour of walltime at normal priority costs 5 AU total.</p> <p>One CPU node for one hour of walltime at high priority costs 10 AU total.</p> <p>One GPU for one hour of walltime at normal priority costs 12.5 AU total.</p> <p>Four GPUs for one hour of walltime at normal priority costs 50 AU total.</p>"},{"location":"Documentation/Systems/Swift/running/#sharedfractional-cpu-nodes","title":"Shared/Fractional CPU Nodes","text":"<p>Swift allows jobs to share nodes, meaning fractional allocations are possible. </p> <p>Standard (CPU) compute nodes have 128 CPU cores and 256GB RAM.</p> <p>When a job only requests part of a node, usage is tracked on the basis of: </p> <p>1 core = 2GB RAM = 1/128th of a node</p> <p>Using all resources on a single node, whether CPU, RAM, or both, will max out at 128/128 per node = 1.</p> <p>The highest quantity of resource requested will determine the total AU charge.</p> <p>For example, a job that requests 64 cores and 128GB RAM (one half of a node) would be: </p> <p>1 hour walltime * 0.5 nodes * 1 QoS Factor * 5 Charge Factor = 2.5 AU per node-hour.</p>"},{"location":"Documentation/Systems/Swift/running/#sharedfractional-gpu-nodes","title":"Shared/Fractional GPU Nodes","text":"<p>Jobs on Swift may also share GPU nodes.</p> <p>Standard GPU nodes have 96 CPU cores, four NVIDIA A100 40GB GPUs, and 1TB RAM.</p> <p>You may request 1, 2, 3, or 4 GPUs per GPU node, as well as any additional CPU and RAM required. </p> <p>Usage is tracked on the basis of: </p> <p>1 GPU = 25% of total cores (24/96) = 25% of total RAM (256GB/1TB) = 25% of a node </p> <p>The highest quantity of resource requested will determine the total AU charge.</p>"},{"location":"Documentation/Systems/Swift/running/#au-calculation-examples","title":"AU Calculation Examples","text":"<p>AU calculations are performed automatically between the Slurm scheduler and Lex(NREL's web-based allocation tracking/management software). The following calculations are approximations to help illustrate how your AU will be consumed based on your job resource requests and are approximations only:</p> <p>A request of 1 GPU, up to 24 CPU cores, and up to 256GB RAM will be charged at 12.5 AU/hr:</p> <ul> <li>1/4 GPUs = 25% total GPUs = 50 AU * 0.25 = 12.5 AU (this is what will be charged)</li> <li>1 core = 1% total cores = 50 AU * 0.01 = 0.50 AU   (ignored) </li> <li>1GB/1TB = 0.1% total RAM = 50 AU * 0.001 = 0.05 AU (ignored)</li> </ul> <p>A request of 1 GPU, 48 CPU cores, and 100GB RAM will be charged at 25 AU/hr:</p> <ul> <li>1/4 GPUs = 25% total GPUs = 50 AU * 0.25 = 12.5 AU (ignored)</li> <li>48/96 cores = 50% total cores = 50 AU * 0.5 = 25 AU (this is what will be charged)</li> <li>100GB/1TB = 10% total RAM = 50 AU * 0.10 = 5 AU (ignored)</li> </ul> <p>A request of 2 GPUs, 55 CPU cores, and 200GB RAM will be charged at approximately 28.7 AU/hr:</p> <ul> <li>2/4 GPUs = 50% total GPUS = 50 AU * 0.5 = 25 AU (ignored)</li> <li>55/96 cores = 57.3% of total cores = 50 AU * .573 = 28.65 AU (this is what will be charged)</li> <li>200GB/1TB = 20% total RAM = 50 AU * 0.2 = 10 AU (ignored)</li> </ul> <p>A request of 1 GPU, 1 CPU core, and 1TB RAM will be charged at 50 AU/hr:</p> <ul> <li>1/4 GPUs = 25% total GPUS = 50 AU * 0.25 = 12.5 AU (ignored)</li> <li>1/96 cores = 1% total cores = 50 AU * 0.01 = 0.50 AU (ignored)</li> <li>1TB/1TB = 100% total RAM = 50 AU * 1 = 50 AU (this is what will be charged)</li> </ul>"},{"location":"Documentation/Systems/Swift/running/#software-environments-and-example-files","title":"Software Environments and Example Files","text":"<p>Multiple software environments are available on Swift, with a number of commonly used modules including compilers, common build tools, specific AMD optimized libraries, and some analysis tools. The environments are in date stamped subdirectories, in the directory /nopt/nrel/apps. Each environment directory has a file myenv.*.  Sourcing that file will enable the environment.</p> <p>When you login you will have access to the default environments and the myenv file will have been sourced for you. You can see the directory containing the environment by running the <code>module avail</code> command.  </p> <p>In the directory for an environment you will see a subdirectory example. This contains a makefile for a simple hello world program written in both Fortran and C. The README.md file contains additional information, most of which is replicated here. It is suggested that you copy the example directory to your own /home for experimentation:</p> <pre><code>cp -r example ~/example\ncd ~/example\n</code></pre>"},{"location":"Documentation/Systems/Swift/running/#conda","title":"Conda","text":"<p>There is a very basic version of conda in the \"anaconda\" directory in each  /nopt/nrel/apps/YYMMDDa directory. However, there is a more complete environment pointed to by the module under /nopt/nrel/apps/modules. Please see our Conda Documentation for more information.</p>"},{"location":"Documentation/Systems/Swift/running/#simple-batch-script","title":"Simple batch script","text":"<p>Here is a sample batch script for running the 'hello world' example program, runopenmpi. </p> <pre><code>#!/bin/bash\n#SBATCH --job-name=\"install\"\n#SBATCH --nodes=2\n#SBATCH --tasks-per-node=2\n#SBATCH --exclusive\n#SBATCH --account=&lt;myaccount&gt;\n#SBATCH --partition=debug\n#SBATCH --time=00:01:00\n\n\ncat $0\n\n#These should be loaded before doing a make\nmodule load gcc  openmpi \n\nexport OMP_NUM_THREADS=2\nsrun  -n 4 ./fhostone -F\nsrun  -n 4 ./phostone -F\n</code></pre> <p>To run this you need to replace <code>&lt;myaccount&gt;</code> with the appropriate account and ensure that slurm is in your path by running:</p> <pre><code>module load slurm\n</code></pre> <p>Then submit the sbatch script with: </p> <pre><code>sbatch --partition=test runopenmpi\n</code></pre>"},{"location":"Documentation/Systems/Swift/running/#building-the-hello-world-example","title":"Building the 'hello world' example","text":"<p>Obviously for the script given above to work you must first build the application. You need to:</p> <ol> <li>Load the modules</li> <li>make</li> </ol>"},{"location":"Documentation/Systems/Swift/running/#loading-the-modules","title":"Loading the modules.","text":"<p>We are going to use gnu compilers with OpenMPI.</p> <pre><code>ml gcc openmpi\n</code></pre>"},{"location":"Documentation/Systems/Swift/running/#run-make","title":"Run make","text":"<pre><code>make\n</code></pre>"},{"location":"Documentation/Systems/Swift/running/#full-procedure","title":"Full procedure","text":"<pre><code>[nrmc2l@swift-login-1 ~]$ cd ~\n[nrmc2l@swift-login-1 ~]$ mkdir example\n[nrmc2l@swift-login-1 ~]$ cd ~/example\n[nrmc2l@swift-login-1 ~]$ cp -r /nopt/nrel/apps/210928a/example/* .\n\n[nrmc2l@swift-login-1 ~ example]$ cat runopenmpi \n#!/bin/bash\n#SBATCH --job-name=\"install\"\n#SBATCH --nodes=2\n#SBATCH --tasks-per-node=2\n#SBATCH --exclusive\n#SBATCH --account=&lt;myaccount&gt;\n#SBATCH --partition=debug\n#SBATCH --time=00:01:00\n\n\ncat $0\n\n#These should be loaded before doing a make:\nmodule load gcc  openmpi \n\nexport OMP_NUM_THREADS=2\nsrun  -n 4 ./fhostone -F\nsrun  -n 4 ./phostone -F\n\n\n[nrmc2l@swift-login-1 ~ example]$ module load gcc  openmpi\n[nrmc2l@swift-login-1 ~ example]$ make\nmpif90 -fopenmp fhostone.f90 -o fhostone\nrm getit.mod  mympi.mod  numz.mod\nmpicc -fopenmp phostone.c -o phostone\n[nrmc2l@swift-login-1 ~ example]$ sbatch runopenmpi\nSubmitted batch job 187\n[nrmc2l@swift-login-1 ~ example]$ \n</code></pre>"},{"location":"Documentation/Systems/Swift/running/#results","title":"Results","text":"<pre><code>[nrmc2l@swift-login-1 example]$ cat *312985*\n#!/bin/bash\n#SBATCH --job-name=\"install\"\n#SBATCH --nodes=2\n#SBATCH --tasks-per-node=2\n#SBATCH --exclusive\n#SBATCH --partition=debug\n#SBATCH --time=00:01:00\n\n\ncat $0\n\n#These should be loaded before doing a make\nmodule load gcc  openmpi \n\nexport OMP_NUM_THREADS=2\nsrun  -n 4 ./fhostone -F\nsrun  -n 4 ./phostone -F\n\nMPI Version:Open MPI v4.1.1, package: Open MPI nrmc2l@swift-login-1.swift.hpc.nrel.gov Distribution, ident: 4.1.1, repo rev: v4.1.1, Apr 24, 2021\ntask    thread             node name  first task    # on node  core\n0002      0000                 c1-31        0002         0000   018\n0000      0000                 c1-30        0000         0000   072\n0000      0001                 c1-30        0000         0000   095\n0001      0000                 c1-30        0000         0001   096\n0001      0001                 c1-30        0000         0001   099\n0002      0001                 c1-31        0002         0000   085\n0003      0000                 c1-31        0002         0001   063\n0003      0001                 c1-31        0002         0001   099\n0001      0000                 c1-30        0000         0001  0097\n0001      0001                 c1-30        0000         0001  0103\n0003      0000                 c1-31        0002         0001  0062\n0003      0001                 c1-31        0002         0001  0103\nMPI VERSION Open MPI v4.1.1, package: Open MPI nrmc2l@swift-login-1.swift.hpc.nrel.gov Distribution, ident: 4.1.1, repo rev: v4.1.1, Apr 24, 2021\ntask    thread             node name  first task    # on node  core\n0000      0000                 c1-30        0000         0000  0072\n0000      0001                 c1-30        0000         0000  0020\n0002      0000                 c1-31        0002         0000  0000\n0002      0001                 c1-31        0002         0000  0067\n[nrmc2l@swift-login-1 example]$ \n</code></pre>"},{"location":"Documentation/Systems/Swift/running/#building-with-intel-fortran-or-intel-c-and-openmpi","title":"Building with Intel Fortran or Intel C and OpenMPI","text":"<p>You can build parallel programs using OpenMPI and the Intel Fortran ifort and Intel C icc compilers.</p> <p>We have the example programs build with gnu compilers and OpenMP using the lines:</p> <pre><code>[nrmc2l@swift-login-1 ~ example]$ mpif90 -fopenmp fhostone.f90 -o fhostone\n[nrmc2l@swift-login-1 ~ example]$ mpicc -fopenmp phostone.c -o phostone\n</code></pre> <p>This gives us:</p> <p><pre><code>[nrmc2l@swift-login-1 ~ example]$ ls -l fhostone\n-rwxrwxr-x. 1 nrmc2l nrmc2l 42128 Jul 30 13:36 fhostone\n[nrmc2l@swift-login-1 ~ example]$ ls -l phostone\n-rwxrwxr-x. 1 nrmc2l nrmc2l 32784 Jul 30 13:36 phostone\n</code></pre> Note the size of the executable files.  </p> <p>If you want to use the Intel compilers, first load the appropriate modules:</p> <pre><code>module load openmpi intel-oneapi-compilers gcc\n</code></pre> <p>Then we can set the variables OMPI_FC=ifort and OMPI_CC=icc, and recompile:</p> <pre><code>[nrmc2l@swift-login-1 ~ example]$ export OMPI_FC=ifort\n[nrmc2l@swift-login-1 ~ example]$ export OMPI_CC=icc\n[nrmc2l@swift-login-1 ~ example]$ mpif90 -fopenmp fhostone.f90 -o fhostone\n[nrmc2l@swift-login-1 ~ example]$ mpicc -fopenmp phostone.c -o phostone\n\n\n[nrmc2l@swift-login-1 ~ example]$ ls -lt fhostone\n-rwxrwxr-x. 1 nrmc2l nrmc2l 41376 Jul 30 13:37 fhostone\n[nrmc2l@swift-login-1 ~ example]$ ls -lt phostone\n-rwxrwxr-x. 1 nrmc2l nrmc2l 32200 Jul 30 13:37 phostone\n[nrmc2l@swift-login-1 ~ example]$ \n</code></pre> <p>Note the size of the executable files have changed. You can also see the difference by running the commands:</p> <pre><code>nm fhostone | grep intel | wc\nnm phostone | grep intel | wc\n</code></pre> <p>on the two versions of the program. It will show how many calls to Intel routines are in each, 51 and 36 compared to 0 for the gnu versions.</p>"},{"location":"Documentation/Systems/Swift/running/#building-and-running-with-intel-mpi","title":"Building and Running with Intel MPI","text":"<p>We can build with the Intel versions of MPI. We assume we will want to build with icc and ifort as the backend compilers. We load the modules:</p> <pre><code>ml gcc\nml intel-oneapi-compilers\nml intel-oneapi-mpi\n</code></pre> <p>Then, build and run the same example as above:</p> <pre><code>make clean\nmake PFC=mpiifort PCC=mpiicc \n</code></pre> <p>Giving us:</p> <pre><code>[nrmc2l@swift-login-1 example]$ ls -lt fhostone phostone\n-rwxrwxr-x. 1 nrmc2l hpcapps 160944 Aug  5 16:14 phostone\n-rwxrwxr-x. 1 nrmc2l hpcapps 952352 Aug  5 16:14 fhostone\n[nrmc2l@swift-login-1 example]$ \n</code></pre> <p>We need to make some changes to our batch script.  Replace the module load line with:</p> <pre><code>module load intel-oneapi-mpi intel-oneapi-compilers gcc\n</code></pre> <p>Our IntelMPI batch script, runintel under /example, is:</p> <pre><code>#!/bin/bash\n#SBATCH --job-name=\"install\"\n#SBATCH --nodes=2\n#SBATCH --tasks-per-node=2\n#SBATCH --exclusive\n#SBATCH --account=&lt;myaccount&gt;\n#SBATCH --partition=debug\n#SBATCH --time=00:01:00\n\n\ncat $0\n\n#These should be loaded before doing a make\nmodule load intel-oneapi-mpi intel-oneapi-compilers gcc\n\nexport OMP_NUM_THREADS=2\nsrun  -n 4 ./fhostone -F\nsrun  -n 4 ./phostone -F\n</code></pre> <p>Which produces the following output:</p> <pre><code>MPI Version:Intel(R) MPI Library 2021.3 for Linux* OS\n\ntask    thread             node name  first task    # on node  core\n0000      0000                 c1-32        0000         0000   127\n0000      0001                 c1-32        0000         0000   097\n0001      0000                 c1-32        0000         0001   062\n0001      0001                 c1-32        0000         0001   099\n\nMPI VERSION Intel(R) MPI Library 2021.3 for Linux* OS\n\ntask    thread             node name  first task    # on node  core\n0000      0000                 c1-32        0000         0000  0127\n0000      0001                 c1-32        0000         0000  0097\n0001      0000                 c1-32        0000         0001  0127\n0001      0001                 c1-32        0000         0001  0099\n</code></pre>"},{"location":"Documentation/Systems/Swift/running/#vasp-jupyter-julia-and-other-applications-on-swift","title":"VASP, Jupyter, Julia, and Other Applications on Swift","text":"<p>Please see the relevant page in the Applications section for more information on using applications on Swift and other NREL clusters.</p>"},{"location":"Documentation/Systems/Vermilion/","title":"About Vermilion","text":"<p>Vermilion is an OpenHPC-based cluster running on Dual AMD EPYC 7532 Rome CPUs and nVidia A100 GPUs. The nodes run as virtual machines in a local virtual private cloud (OpenStack). Vermilion is allocated for NREL workloads and intended for LDRD, SPP or Office of Science workloads. Allocation decisions are made by the IACAC through the annual allocation request process. Check back regularly as the configuration and capabilities for Vermilion are augmented over time.</p>"},{"location":"Documentation/Systems/Vermilion/#accessing-vermilion","title":"Accessing Vermilion","text":"<p>Access to Vermilion requires an NREL HPC account and permission to join an existing allocation. Please see the System Access page for more information on accounts and allocations.</p>"},{"location":"Documentation/Systems/Vermilion/#for-nrel-employees","title":"For NREL Employees:","text":"<p>To access vermilion, log into the NREL network and connect via ssh:</p> <pre><code>ssh vs.hpc.nrel.gov\nssh vermilion.hpc.nrel.gov\n</code></pre>"},{"location":"Documentation/Systems/Vermilion/#for-external-collaborators","title":"For External Collaborators:","text":"<p>There are currently no external-facing login nodes for Vermilion. There are two options to connect:</p> <ol> <li>Connect to the SSH gateway host and log in with your username, password, and OTP code. Once connected, ssh to the login nodes as above.</li> <li>Connect to the HPC VPN and ssh to the login nodes as above.</li> </ol> <p>There are currently two login nodes. They share the same home directory so work done on one will appear on the other. They are:</p> <pre><code>vs-login-1\nvs-login-2\n</code></pre> <p>You may connect directly to a login node, but they may be cycled in and out of the pool. If a node is unavailable, try connecting to another login node or the <code>vs.hpc.nrel.gov</code> round-robin option.</p>"},{"location":"Documentation/Systems/Vermilion/#get-help-with-vermilion","title":"Get Help with Vermilion","text":"<p>Please see the Help and Support Page for further information on how to seek assistance with Vermilion or your NREL HPC account. </p>"},{"location":"Documentation/Systems/Vermilion/#building-code","title":"Building code","text":"<p>Don't build or run code on a login node. Login nodes have limited CPU and memory available. Use a compute or GPU node instead. Simply start an interactive job on an appropriately provisioned node and partition for your work and do your builds there. Similarly, build your projects under <code>/projects/your_project_name/</code> as home directories are limited to 5GB per user.</p>"},{"location":"Documentation/Systems/Vermilion/help/","title":"Vermilion Technical Support Contacts","text":"<p>For assistance with accounts or allocations, software installation requests, or technical questions, please email HPC-Help@nrel.gov with \"Vermilion\" in the subject line.</p>"},{"location":"Documentation/Systems/Vermilion/help/#microsoft-teams","title":"Microsoft Teams","text":"<p>There is a Microsoft Teams Vermilion channel that is one of the primary ways we communicate with Vermilion users about system updates and known problems.  </p> <p>Under the General Channel in the chat, you can post questions and collaborate with other users. We update the members annually from the project team listed. </p> <p>For internal users (NREL), please follow these instructions if we missed you and you would like to join: 1. In Teams click on the \u201cTeams\u201d icon in far left nav 1. Click \u201cJoin or create a team\u201d in lower left corner 1. In in the \u201cSearch teams\u201d field in the upper far right, type \u201cVermilion\u201d and hit return 1. Click Join </p> <p>For external users, please follow the instructions listed in the CSC Tutorial Team - External Users announcement. </p>"},{"location":"Documentation/Systems/Vermilion/help/#additional-support","title":"Additional Support","text":"<p>Additional HPC help and contact information can be found on the NREL HPC Help main page.</p>"},{"location":"Documentation/Systems/Vermilion/running/","title":"Running on Vermilion","text":"<p>This page discusses the compute nodes, partitions, and gives some examples of building and running applications.</p>"},{"location":"Documentation/Systems/Vermilion/running/#about-vermilion","title":"About Vermilion","text":""},{"location":"Documentation/Systems/Vermilion/running/#compute-hosts","title":"Compute hosts","text":"<p>Vermilion is a collection of physical nodes with each regular node containing Dual AMD EPYC 7532 Rome CPUs.  However, each node is virtualized.  That is it is split up into virtual nodes with each virtual node having a portion of the cores and memory of the physical node.  Similar virtual nodes are then assigned slurm partitions as shown below.  </p>"},{"location":"Documentation/Systems/Vermilion/running/#shared-file-systems","title":"Shared file systems","text":"<p>Vermilion's home directories are shared across all nodes. Each user has a quota of 5 GB. There is also /scratch/$USER and /projects spaces seen across all nodes.</p>"},{"location":"Documentation/Systems/Vermilion/running/#partitions","title":"Partitions","text":"<p>Partitions are flexible and fluid on Vermilion.  A list of partitions can be found by running the <code>sinfo</code> command.  Here are the partitions as of 3/27/2025.</p> Partition Name Qty RAM Cores/node /var/scratch 1K-blocks AU Charge Factor gpu1 x NVIDIA Tesla A100 16 114 GB 30 6,240,805,336 12 lg 39 229 GB 60 1,031,070,000 7 std 60 114 GB 30 515,010,816 3.5 sm 28 61 GB 16 256,981,000 0.875 t 15 16 GB 4 61,665,000 0.4375"},{"location":"Documentation/Systems/Vermilion/running/#allocation-unit-au-charges","title":"Allocation Unit (AU) Charges","text":"<p>The equation for calculating the AU cost of a job on Vermilion is: </p> <p><code>AU cost = (Walltime in hours * Number of Nodes * Charge Factor)</code></p> <p>The Walltime is the actual length of time that the job runs, in hours or fractions thereof.</p> <p>The Charge Factor for each partition is listed in the table above. </p>"},{"location":"Documentation/Systems/Vermilion/running/#operating-software","title":"Operating Software","text":"<p>The Vermilion HPC cluster runs fairly current versions of OpenHPC and SLURM on top of OpenStack.</p>"},{"location":"Documentation/Systems/Vermilion/running/#examples-build-and-run-simple-applications","title":"Examples: Build and run simple applications","text":"<p>This section discusses how to compile and run a simple MPI application, as well as how to link against the Intel MKL library.</p> <p>In the directory /nopt/nrel/apps/210929a you will see a subdirectory example.  This contains a makefile for a simple hello world program written in both Fortran and C and several run scripts. The README.md file contains additional information, some of which is replicated here. </p> <p>We will begin by creating a new directory and copying the source for a simple MPI test program.  More details about the test program are available in the README.md file that accompanies it.  Run the following commands to create a new directory and make a copy of the source code:</p> <pre><code>mkdir example\ncd example\ncp /nopt/nrel/apps/210929a/example/phostone.c .\n</code></pre>"},{"location":"Documentation/Systems/Vermilion/running/#compile-and-run-with-intel-mpi","title":"Compile and run with Intel MPI","text":"<p>First we will look at how to compile and run the application using Intel MPI.  To build the application, we load the necessary Intel modules.  Execute the following commands to load the modules and build the application, naming the output <code>phost.intelmpi</code>.  Note that this application uses OpenMP as well as MPI, so we provide the <code>-fopenmp</code> flag to link against the OpenMP libraries.</p> <pre><code>ml intel-oneapi-mpi intel-oneapi-compilers\nmpiicc -fopenmp phostone.c -o phost.intelmpi\n</code></pre> <p>The following batch script is an example that runs the job using two MPI ranks on a single node with two threads per rank.  Save this script to a file such as <code>submit_intel.sh</code>, replace <code>&lt;myaccount&gt;</code> with the appropriate account, and submit using <code>sbatch submit_intel.sh</code>.  Feel free to experiment with different numbers of tasks and threads.  Note that multi-node jobs on Vermilion can be finicky, and applications may not scale as well as they do on other systems.  At this time, it is not expected that multi-node jobs will always run successfully.</p> Intel MPI submission script <pre><code>#!/bin/bash\n#SBATCH --nodes=1\n#SBATCH --exclusive\n#SBATCH --time=00:01:00\n#SBATCH --account=&lt;myaccount&gt;\n\nml intel-oneapi-mpi intel-oneapi-compilers\n\nexport OMP_NUM_THREADS=2\nexport I_MPI_OFI_PROVIDER=tcp\nsrun --mpi=pmi2 --cpus-per-task 2 -n 2 ./phost.intelmpi -F\n</code></pre> <p>Your output should look similar to the following:</p> <pre><code>MPI VERSION Intel(R) MPI Library 2021.9 for Linux* OS\n\ntask    thread             node name  first task    # on node  core\n0000      0000           vs-std-0044        0000         0000  0001\n0000      0001           vs-std-0044        0000         0000  0000\n0001      0000           vs-std-0044        0000         0001  0003\n0001      0001           vs-std-0044        0000         0001  0002\n</code></pre>"},{"location":"Documentation/Systems/Vermilion/running/#link-intels-mkl-library","title":"Link Intel's MKL library","text":"<p>The <code>intel-oneapi-mkl</code> module is available for linking against Intel's MKL library.  Then to build against MKL using the Intel compilers icc or ifort, you normally just need to add the flag <code>-qmkl</code>. There are examples in the directory <code>/nopt/nrel/apps/210929a/example/mkl</code>, and there is a Readme.md file that explains in a bit more detail.</p> <p>To compile a simple test program that links against MKL, run:</p> <pre><code>cp /nopt/nrel/apps/210929a/example/mkl/mkl.c .\n\nml intel-oneapi-mkl intel-oneapi-compilers\nicc -O3 -qmkl mkl.c -o mkl\n</code></pre> <p>An example submission script is:</p> Intel MKL submission script <pre><code>#!/bin/bash\n#SBATCH --nodes=1\n#SBATCH --exclusive\n#SBATCH --time=00:01:00\n#SBATCH --account=&lt;myaccount&gt;\n\nsource /nopt/nrel/apps/210929a/myenv.2110041605\nml intel-oneapi-mkl intel-oneapi-compilers gcc\n\n./mkl\n</code></pre>"},{"location":"Documentation/Systems/Vermilion/running/#compile-and-run-with-open-mpi","title":"Compile and run with Open MPI","text":"<p>Warning</p> <p>Please note that multi-node jobs are not currently supported with Open MPI.</p> <p>Use the following commands to load the Open MPI modules and compile the test program into an executable named <code>phost.openmpi</code>:</p> <pre><code>ml gcc openmpi\nmpicc -fopenmp phostone.c -o phost.openmpi\n</code></pre> <p>The following is an example script that runs two tasks on a single node, with two threads per task:</p> Open MPI submission script <pre><code>#!/bin/bash\n#SBATCH --nodes=1\n#SBATCH --exclusive\n#SBATCH --time=00:01:00\n#SBATCH --account=&lt;myaccount&gt;\n\nml gcc openmpi\n\nexport OMP_NUM_THREADS=2\nmpirun -np 2 --map-by socket:PE=2 ./phost.openmpi -F\n</code></pre>"},{"location":"Documentation/Systems/Vermilion/running/#running-vasp-on-vermilion","title":"Running VASP on Vermilion","text":"<p>Please see the VASP page for detailed information and recommendations for running VASP on Vermilion.</p>"},{"location":"Documentation/Viz_Analytics/","title":"Visualization and Analytics Software Tools","text":"<p>Learn about the available visualization and analytics software tools</p>"},{"location":"Documentation/Viz_Analytics/#virtualglfastx","title":"VirtualGL/FastX","text":"<p>Provides remote visualization for OpenGL-based applications. For more information, see using VirtualGL and FastX .</p>"},{"location":"Documentation/Viz_Analytics/#paraview","title":"ParaView","text":"<p>An open-source, multi-platform data analysis and visualization application.  For information, see using ParaView.</p>"},{"location":"Documentation/Viz_Analytics/#r-statistical-computing-environment","title":"R Statistical Computing Environment","text":"<p>R is a language and environment for statistical computing and graphics. For more information, see running R.</p>"},{"location":"Documentation/Viz_Analytics/#matlab","title":"MATLAB","text":"<p>MATLAB is a high-performance language for technical computing. It integrates computation, visualization and programming in an easy-to-use environment where problems and solutions are expressed in familiar mathematical notation.</p> <p>The name MATLAB stands for Matrix Laboratory. MATLAB was originally written to provide easy access to matrix software developed by the LINPACK and EISPACK projects. Today, MATLAB engines incorporate the LAPACK and BLAS libraries, embedding the state of the art in software for matrix computation.</p> <p>For more information, see using MATLAB software. </p>"},{"location":"Documentation/Viz_Analytics/#visit","title":"VisIt","text":"<p>VisIt is a free interactive parallel visualization and graphical analysis tool for viewing scientific data on Unix and PC platforms.  VisIt features a robust remote visualization capability. VisIt can be started on a local machine and used to visualize data on a remote compute cluster. </p> <p>For more information, see using VisIt. </p>"},{"location":"Documentation/Viz_Analytics/paraview/","title":"ParaView","text":"<p>ParaView is an open-source, multi-platform data analysis and visualization application. ParaView users can quickly build visualizations to analyze their data using qualitative and quantitative techniques. The data exploration can be done interactively in 3D or programmatically using ParaView's batch processing capabilities. ParaView was developed to analyze extremely large data sets using distributed memory computing resources. It can be run on supercomputers to analyze data sets of terascale as well as on laptops for smaller data.</p> <p>The following tutorials are meant for Kestrel supercomputer. </p>"},{"location":"Documentation/Viz_Analytics/paraview/#using-paraview-in-client-server-mode","title":"Using ParaView in Client-Server Mode","text":"<p>Running ParaView interactively in client-server mode is a convenient worflow for researchers who have a large amount of remotely-stored data that they'd like to visualize using a locally-installed copy of ParaView. In this model, the HPC does the heavy lifting of reading file data and applying filters, taking advantage of parallel processing when possible, then \"serves\" the rendered data to the ParaView client running locally on your desktop. This allows you to interact with ParaView as you normally would (i.e., locally) with all your preferences and shortcuts intact without the time consuming step of transferring data from the supercomputer to your desktop or relying on a remote desktop environment.</p> <p>The first step is to install ParaView.  It is recommended that you use the binaries provided by Kitware on your workstation matching the NREL installed version.  This ensures client-server compatibility.  The version number that you install must identically match the version installed at NREL.  To determine which version of ParaView is installed on the cluster, connect to Kestrel as you normally would, load the ParaView module with <code>module load paraview</code>, then check the version with <code>pvserver --version</code>. The version number, e.g., 5.11.0, will then be displayed to your terminal. To download the correct ParaView client binary version for your desktop environment, visit the ParaView website.</p> <ol> <li> <p>Reserve Compute Nodes</p> <p>The first step is to reserve the computational resources on Kestrel that will be running the ParaView server.</p> <p>This requires using the Slurm <code>salloc</code> directive and specifying an allocation name and time limit for the reservation.</p> <p>Note that this is one of the few times where <code>salloc</code> is used instead of srun to launch the job, since we'll be launching multiple instances of pvserver using srun inside the job allocation in a later step.  In previous versions of Slurm (prior to 20.11) you would use srun instead of salloc, but that behavior has been deprecated due to changes in the way Slurm handles job steps inside an allocation.  The old \"srun-then-srun\" behavior may be replicated using the <code>srun --overlap</code> flag (see <code>man srun</code> and Slurm documentation for details), but the 'salloc-then-srun' construct works quite well and is what we'd recommend in this case for ease of use.</p> <p>(Otherwise, for interactive jobs that just require one process on one node, the \"salloc-then-srun\" construct isn't necessary at all; for that type of job you may just use <code>srun -A &lt;account&gt; -t &lt;time&gt; --pty $SHELL</code> to land on a compute node and run your software as per normal, without needing an srun in front.)</p> <p>To reserve the computational resources on Kestrel:</p> <pre><code>salloc -A &lt;alloc_name&gt; -t &lt;time_limit&gt;\n</code></pre> <p>where <code>&lt;alloc_name&gt;</code> will be replaced with the allocation name you wish to charge your time to and <code>&lt;time_limit&gt;</code> is the amount of time you're reserving the nodes for.  At this point, you may want to copy the name of the node that the Slurm scheduler assigns you (it will look something like r1i0n10, r4i3n3, etc., and follow immediately after the \"@\" symbol at the command prompt ) as we'll need it in Step 3.</p> <p>In the example above, we default to requesting only a single node which limits the maximum number of ParaView server processes we can launch to the maximum number of cores on a single Kestrel node (on Kestrel, this is 104). If you intend to launch more ParaView server processes than this, you'll need to request multiple nodes with your <code>salloc</code> command.</p> <pre><code>salloc -A &lt;alloc_name&gt; -t &lt;time_limit&gt; -N 2\n</code></pre> <p>where the <code>-N 2</code> option specifies that two nodes be reserved, which means the maximum number of ParaView servers that can be launched in Step 2 is  104 x 2 = 208 (Kestrel). Although this means you'll be granted multiple nodes with multiple names, the one to copy for Step 3 is still the one immediately following the \"@\" symbol. See the table of recommended workload distributions in Step 2 for more insight regarding the number of nodes to request.</p> </li> <li> <p>Start ParaView Server</p> <p>After reserving the compute nodes, load the ParaView module with</p> <pre><code>module load paraview\n</code></pre> <p>Next, start the ParaView server with another call to the Slrum <code>srun</code> directive</p> <pre><code>srun -n 8 pvserver --force-offscreen-rendering\n</code></pre> <p>In this example, the ParaView server will be started on 8 processes. The <code>--force-offscreen-rendering</code> option is present to ensure that, where possible, CPU-intensive filters and rendering calculations will be performed server-side (i.e., on the Kestrel compute nodes) and not on your local machine. Remember that the maximum number of ParaView server processes that can be launched is limited by the amount of nodes reserved in Step 1. Although every dataset may be different, ParaView offers the following recommendations for balancing grid cells to processors.</p> Grid Type Target Cells/Process Max Cells/Process Structured Data 5-10 M 20 M Unstructured Data 250-500 K 1 M <p>So for example, if you have data stored in an unstructured mesh with 6 M cells, you'd want to aim for between 12 and 24 ParaView server processes, which easily fits on a single Kestrel node. If the number of unstructured mesh cells was instead around 60 M, you'd want to aim for 120 to 240 processes, which means requesting a minimum of 2 Kestrel nodes. Note, this 2-nodes request may remain in the queue longer while the scheduler looks for resources, so depending on your needs, it may be necessary to factor queue times into your optimal cells-per-process calculation.</p> <p>Note: The <code>--server-port=&lt;port&gt;</code> option may be used with pvserver if you wish to use a port other than 11111 for Paraview.  You'll need to adjust the port in the SSH tunnel and tell your Paraview client which port to use, as well.  See the following sections for details.</p> </li> <li> <p>Create SSH Tunnel</p> <p>Next, we'll create what's called an SSH tunnel to connect your local desktop to the compute node(s) you reserved in Step 1. This will allow your local installation of ParaView to interact with files stored remotely on Kestrel. In a new terminal window, execute the following line of code on your own computer:</p> <p>For Kestrel: <pre><code>ssh -L 11111:&lt;node_name&gt;:11111 &lt;user_name&gt;@kestrel.hpc.nrel.gov\n</code></pre></p> <p>where <code>&lt;node_name&gt;</code> is the node name you copied in Step 1 and <code>&lt;user_name&gt;</code> is your HPC username.</p> <p>Note that if you changed the default port to something other than 11111 (see the previous section) you'll need to change the port settings in your SSH tunnel, as well.  The SSH command construct above follows the format of <code>&lt;local_port&gt;:&lt;node_name&gt;:&lt;remote_port&gt;</code>.  The <code>&lt;local_port&gt;</code> is the \"beginning\" of the tunnel on your computer, and is often the same as the \"end\" port of the tunnel, though this is not required.  You may set this to anything convenient to you, but you will need to tell your Paraview client the right port if you change it (see the next section for details.)  is the port on the Kestrel compute node where pvserver is running.  The default for pvserver is 11111, but if you changed this with pvserver <code>--server-port=</code> flag, you'll need to change  in your ssh command to match. <li> <p>Connect ParaView Client</p> <p>Now that the ParaView server is running on a compute node and your desktop is connected via the SSH tunnel, you can open ParaView as usual. From here, click the \"Connect\" icon or <code>File &gt; Connect</code>. Next, click the \"Add Server\" button and enter the following information.</p> Name Value Name Kestrel HPC Server Type Client/Server Host localhost Port 11111 <p>Only the last three fields, Server Type, Host, and Port, are strictly necessary (and many of them will appear by default) while the Name field can be any recognizable string you wish to associate with this connection. When these 4 fields have been entered, click \"Configure\" to move to the next screen, where we'll leave the Startup Type set to \"Manual\". Note that these setup steps only need to be completed the first time you connect to the ParaView server, future post-processing sessions will require only that you double click on this saved connection to launch it.</p> <p>When finished, select the server just created and click \"Connect\". The simplest way to confirm that the ParaView server is running as expected is to view the Memory Inspector toolbar (<code>View &gt; Memory Inspector</code>) where you should see a ParaView server for each process started in Step 2 (e.g., if <code>-n 8</code> was specified, processes <code>0-7</code> should be visible).</p> <p>That's it!  You can now <code>File &gt; Open</code> your data files as you normally would, but instead of your local hard drive you'll be presented with a list of the files stored on Kestrel.</p> </li>"},{"location":"Documentation/Viz_Analytics/paraview/#general-tips","title":"General Tips","text":"<ul> <li>The amount of time you can spend in a post-processing session is limited by the time limit specified when reserving the compute nodes in Step 1.  If saving a large time series to a video file, your reservation time may expire before the video is finished.  Keep this in mind and make sure you reserve the nodes long enough to complete your job.</li> <li>Adding more parallel processes in Step 2, e.g., <code>-n 36</code>, doesn't necessarily mean you'll be splitting the data into 36 blocks for each operation.  ParaView has the capability to use 36 parallel processes, but may use many fewer as it calculates the right balance between computational power and the additional overhead of communication between processors.</li> </ul>"},{"location":"Documentation/Viz_Analytics/paraview/#high-quality-rendering-with-paraview","title":"High-quality Rendering With ParaView","text":"<p>How to use ParaView in batch mode to generate single frames and animations on Kestrel</p> <p></p>"},{"location":"Documentation/Viz_Analytics/paraview/#building-pvbatch-scripts-in-interactive-environments","title":"Building PvBatch Scripts in Interactive Environments","text":"<ol> <li> <p>Begin by connecting to a Kestrel login node:</p> <pre><code>ssh {username}@kestrel.hpc.nrel.gov\n</code></pre> </li> <li> <p>Request an interactive compute session for 60 minutes):</p> <pre><code>salloc -A {allocation} -t 60\n</code></pre> <p>Note: Slurm changes in January 2022 resulted in the need to use salloc to start your interactive session, since we'll be running pvbatch on the compute node using srun in a later step. This \"srun-inside-an-salloc\" supercedes the previous Slurm behavior of \"srun-inside-an-srun\", which will no longer work.</p> </li> <li> <p>Once the session starts, load the appropriate modules:</p> <pre><code>module purge\nmodule load paraview/osmesa\n</code></pre> <p>Note: In this case, we select the <code>paraview/server</code> module as opposed to the default ParaView build, as the server version is built for rendering using offscreen methods suitable for compute nodes.</p> </li> <li> <p>and start your render job:</p> <pre><code>srun -n 1 pvbatch --force-offscreen-rendering render_sphere.py\n</code></pre> <p>where <code>render_sphere.py</code> is a simple ParaView Python script to add a sphere source and save an image.</p> </li> </ol>"},{"location":"Documentation/Viz_Analytics/paraview/#transitioning-to-batch-post-processing","title":"Transitioning to Batch Post-Processing","text":"<p>Tweaking the visualization options contained in the <code>pvrender.py</code> file inevitably requires some amount of trial and error and is most easily accomplished in an interactive compute session like the one outlined above.  Once you feel that your script is sufficiently automated, you can start submitting batch jobs that require no user interaction.</p> <ol> <li> <p>Prepare your script for <code>sbatch</code>. A minimal example of a batch script named <code>batch_render.sh</code> could look like:</p> <pre><code>#!/bin/bash\n\n#SBATCH --account={allocation}\n#SBATCH --time=60:00\n#SBATCH --job-name=pvrender\n#SBATCH --nodes=2\n\nmodule purge\nmodule load paraview/$version-server\n\nsrun -n 1 pvbatch --force-offscreen-rendering render_sphere.py 1 &amp;\nsrun -n 1 pvbatch --force-offscreen-rendering render_sphere.py 2 &amp;\nsrun -n 1 pvbatch --force-offscreen-rendering render_sphere.py 3 &amp;\n\nwait\n</code></pre> <p>where we run multiple instances of our dummy sphere example, highlighting that different options can be passed to each to post-process a large batch of simulated results on a single node.  Note also that for more computationally intensize rendering or larger file sizes (e.g., tens of millions of cells) the option <code>-n 1</code> option can be set as suggested in the client-server guide.</p> </li> <li> <p>Submit the job and wait:</p> <pre><code>sbatch batch_render.sh\n</code></pre> </li> </ol>"},{"location":"Documentation/Viz_Analytics/paraview/#tips-on-creating-the-pvbatch-python-script","title":"Tips on Creating the PvBatch Python Script","text":"<p>Your ParaView python script can be made in a number of ways. The easiest is to run a fresh session of ParaView (use version 5.x on your local machine) and select \"Tools\u2192Start Trace,\" then \"OK\". Perform all the actions you need to set your scene and save a screenshot. Then select \"Tools\u00a0\u2192 Stop Trace\" and save the resulting python script (we will use <code>render_sphere.py</code> in these examples). \u00a0</p> <p>Here are some useful components to add to your ParaView Python script.</p> <ul> <li> <p>Read the first command-line argument and use it to select a data     file to operate on.</p> <pre><code>import sys\ndoframe = 0\nif len(sys.argv) &gt; 1:\n    doframe = int(sys.argv[1])\ninfile = \"output%05d.dat\" % doframe\n</code></pre> <p>Note that <code>pvbatch</code> will pass any arguments after the script name to the script itself. So you can do the following to render frame 45:</p> <pre><code>srun -n 1 pvbatch --force-offscreen-rendering render_sphere.py 45\n</code></pre> <p>You could programmatically change this value inside the <code>batch_render.sh</code> script, your script would need to iterate using something like:</p> <pre><code>for frame in 45 46 47 48\ndo\n    srun -n 1 pvbatch --force-offscreen-rendering render_sphere.py $frame\ndone\n</code></pre> </li> </ul> <ul> <li> <p>Set the output image size to match FHD or UHD standards:</p> <pre><code>renderView1.ViewSize = [3840, 2160]\nrenderView1.ViewSize = [1920, 1080]\n</code></pre> </li> <li> <p>Don't forget to actually render the image!</p> <pre><code>pngname = \"image%05d.png\" % doframe\nSaveScreenshot(pngname, renderView1)\n</code></pre> </li> </ul>"},{"location":"Documentation/Viz_Analytics/paraview/#insight-center","title":"Insight Center","text":"<p>ParaView is supported in the Insight Center's immersive virtual environment.  Learn about the Insight Center. </p> <p>For assistance, contact Kenny Gruchalla.</p>"},{"location":"Documentation/Viz_Analytics/virtualgl_fastx/","title":"Using VirtualGL and FastX","text":"<p>VirtualGL and FastX provide remote desktop and visualization capabilities for graphical applications.</p>"},{"location":"Documentation/Viz_Analytics/virtualgl_fastx/#remote-visualization-on-kestrel","title":"Remote Visualization on Kestrel","text":"<p>In addition to standard ssh-only login nodes, Kestrel is also equipped with several specialized Data Analysis and Visualization (DAV) login nodes, intended for HPC applications on Kestrel that require a graphical user interface. </p> <p>Note</p> <p>DAV FastX nodes are a limited resource and not intended as a general-purpose remote desktop. We ask that you please restrict your usage to only HPC allocation-related work and/or visualization software that requires an HPC system.</p> <p>There are seven internal DAV nodes on Kestrel available only to NREL users on the NREL VPN, on campus, or via the HPC VPN that are accessible via round-robin at kestrel-dav.hpc.nrel.gov. The individual nodes are named kd1 through kd7.hpc.nrel.gov.</p> <p>There is also one node that is ONLY accessible by external (non-NREL) users available at kestrel-dav.nrel.gov. This address will connect to the node kd8, and requires both password and OTP for login. </p> <p>All Kestrel DAV nodes have 104 CPU cores (2x 52-core Intel Xeon Sapphire Rapids CPUs), 256GB RAM, 2x 48GB NVIDIA A40 GPUs, and offer a Linux desktop (via FastX) with visualization capabilities, optional VirtualGL, and standard Linux terminal applications.</p> <p>DAV nodes are shared resources that support multiple simultaneous users. CPU and RAM usage is monitored by automated software called Arbiter, and high usage may result in temporary throttling of processes. </p>"},{"location":"Documentation/Viz_Analytics/virtualgl_fastx/#virtualgl","title":"VirtualGL","text":"<p>VirtualGL is an open-source package that gives any Linux remote display software the ability to run OpenGL applications with full 3D hardware acceleration. </p> <p>The traditional method of displaying graphics applications to a remote X server (indirect rendering) supports 3D hardware acceleration, but this approach causes all of the OpenGL commands and 3D data to be sent over the network to be rendered on the client machine. With VirtualGL, the OpenGL commands and 3D data are redirected to a 3D graphics accelerator on the application server, and only the rendered 3D images are sent to the client machine. VirtualGL \"virtualizes\" 3D graphics hardware, allowing users to access and share large-memory visualization nodes with high-end graphics processing units (GPUs) from their energy-efficient desktops. </p>"},{"location":"Documentation/Viz_Analytics/virtualgl_fastx/#fastx","title":"FastX","text":"<p>FastX provides a means to use a graphical desktop remotely. By connecting to a FastX session on a DAV node, users can run graphical applications with a similar experience to running on their workstation.  Another benefit is that you can disconnect from a FastX connection, go to another location and reconnect to that same session, picking up where you left off.</p>"},{"location":"Documentation/Viz_Analytics/virtualgl_fastx/#connecting-to-dav-nodes-using-fastx","title":"Connecting to DAV Nodes Using FastX","text":"<p>NREL users may use the web browser or the FastX desktop client. External users must use the FastX desktop client, or connect to the HPC VPN for the web client.</p> NREL On-Site and VPN Users"},{"location":"Documentation/Viz_Analytics/virtualgl_fastx/#using-a-web-browser","title":"Using a Web Browser","text":"<p>Launch a web browser on your local machine and connect to https://kestrel-dav.hpc.nrel.gov. After logging in with your HPC username/password you will be able to launch a FastX session by choosing a desktop environment of your choice. Either GNOME or XFCE are available for use.</p>"},{"location":"Documentation/Viz_Analytics/virtualgl_fastx/#using-the-desktop-client","title":"Using the Desktop Client","text":"<p>Download the Desktop Client and install it on your local machine, then follow these instructions to connect to one of the DAV nodes.</p> <p>Step 1:</p> <p>Launch the FastX Desktop Client.</p> <p>Step 2:</p> <p>Add a profile using the + button on the right end corner of the tool using the SSH protocol. </p> <p>Step 3:</p> <p>Give your profile a name and enter the settings...</p> <p>Address/URL: kestrel-dav.hpc.nrel.gov</p> <p>OR you may use the address of an individual kd or ed node if you would like to resume a previous session.</p> <p>Username:  <p>...and then save the profile.</p> <p>Step 4:</p> <p>Once your profile is saved, you will be prompted for your password to connect.</p> <p>Step 5:</p> <p>If a previous session exists, click (double click if in \"List View\") on current session to reconnect.</p> <p>OR</p> <p>Step 5a:</p> <p>Click the PLUS (generally in the upper right corner of the session window) to add a session and continue to step 6.</p> <p>Step 6:</p> <p>Select a Desktop environment of your choice and click OK to launch. </p> Off-Site or Remote Users <p>Remote users must use the Desktop Client via SSH for access. NREL Multifactor token (OTP) required.</p> <p>Download the Desktop Client and install it on your local machine, then follow these instructions to connect to one of the DAV nodes.</p> <p>Step 1:</p> <p>Launch the FastX Desktop Client.</p> <p>Step 2:</p> <p>Add a profile using the + button on the right end corner of the tool using the SSH protocol. </p> <p>Step 3:</p> <p>Give your profile a name and enter the settings...</p> <p>Host: kestrel-dav.nrel.gov</p> <p>Port: 22</p> <p>Username:  <p>...and then save the profile.</p> <p></p> <p>Step 4:</p> <p>Once your profile is saved. You will be prompted for your password+OTP_token (your multifactor authentication code) to connect.</p> <p></p> <p>Step 5:</p> <p>Select a Desktop environment of your choice and click OK.</p> <p></p>"},{"location":"Documentation/Viz_Analytics/virtualgl_fastx/#launching-opengl-applications","title":"Launching OpenGL Applications","text":"<p>You can now run applications in the remote desktop. You can run X applications normally; however, to run hardware-accelerated OpenGL applications, you must run the application prefaced by the vglrun command.  <pre><code>$ module load matlab\n$ vglrun matlab\n</code></pre></p>"},{"location":"Documentation/Viz_Analytics/virtualgl_fastx/#choosing-a-gpu-on-kestrel","title":"Choosing a GPU on Kestrel","text":"<p>Kestrel DAV nodes have two NVIDIA A40 GPUs. Using vglrun will default to the first GPU available, which may leave one GPU overutilized while the second is underutilized. </p> <p>To run your OpenGL software with a GPU of your choosing, you may add the <code>-d &lt;gpu&gt;</code> flag to vglrun to pick a GPU. The first GPU is referred to as :0.0, the second as :0.1. For example, to run Matlab on the second GPU:</p> <p><code>vglrun -d :0.1 matlab</code></p> <p>to run Ansys on the first GPU:</p> <p><code>vglrun -d :0.0 ansys</code></p>"},{"location":"Documentation/Viz_Analytics/virtualgl_fastx/#download-fastx-desktop-client","title":"Download FastX Desktop Client","text":"Operating System Installer Mac Download Linux Download Windows Download"},{"location":"Documentation/Viz_Analytics/virtualgl_fastx/#multiple-fastx-sessions","title":"Multiple FastX Sessions","text":"<p>FastX sessions may be closed without terminating the session and resumed at a later time. However, since there is a  license-based limit to the number of concurrent users, please fully log out/terminate your remote desktop session when you are done working and no longer need to leave processes running. Avoid having remote desktop sessions open on multiple nodes that you are not using, or your sessions may be terminated by system administrators to make licenses available for active users. </p>"},{"location":"Documentation/Viz_Analytics/virtualgl_fastx/#reattaching-fastx-sessions","title":"Reattaching FastX Sessions","text":"<p>Connections to the DAV nodes via kestrel-dav.hpc.nrel.gov will connect you to a random node. To resume a session that you have suspended, take note of the node your session is running on (kd1, kd2, kd3, kd4, kd5, kd6, or kd7) before you close the FastX client or browser window, and you may directly access that node when you are ready to reconnect at e.g. <code>kd#.hpc.nrel.gov</code> in the FastX client or through your web browser at <code>https://kd#.hpc.nrel.gov</code>. </p>"},{"location":"Documentation/Viz_Analytics/virtualgl_fastx/#troubleshooting","title":"Troubleshooting","text":""},{"location":"Documentation/Viz_Analytics/virtualgl_fastx/#could-not-connect-to-session-bus-failed-to-connect-to-socket-tmpdbus-xxx-connection-refused","title":"Could not connect to session bus: Failed to connect to socket /tmp/dbus-XXX: Connection refused","text":"<p>This error is usually the result of a change to the default login environment, often by an alteration to <code>~/.bashrc</code> by  altering your $PATH, or by configuring Conda to launch into a (base) or other environment immediately upon login. </p> <p>For changes to your <code>$PATH</code>, be sure to prepend any changes with <code>$PATH</code> so that the default system paths are included before  any custom changes that you make. For example: <code>$PATH=$PATH:/home/username/bin</code> instead of <code>$PATH=/home/username/bin/:$PATH</code>.</p> <p>For conda users, the command <code>conda config --set auto_activate_base false</code> will prevent conda from launching into a base environment upon login. </p>"},{"location":"Documentation/Viz_Analytics/virtualgl_fastx/#no-free-licenses","title":"No Free Licenses","text":"<p>FastX has a limited number of licenses for concurrent usage, so please remember to log out of your X session AND out of FastX when you are done working. If you receive a \"no free licenses\" error when trying to start a new session, please contact hpc-help@nrel.gov for assistance.</p>"},{"location":"Documentation/Viz_Analytics/virtualgl_fastx/#how-to-get-help","title":"How to Get Help","text":"<p>Please contact the HPC Helpdesk at hpc-help@nrel.gov if you have any questions, technical issues, or receive a \"no free licenses\" error. </p>"},{"location":"Documentation/Viz_Analytics/visit/","title":"VisIT","text":"<p>VisIT is a free interactive parallel visualization and graphical analysis tool for viewing scientific data on Unix and PC platforms.</p> <p>With VisIt, users can quickly generate visualizations from their data, animate them through time, manipulate them, and save the resulting images for presentations. It contains a rich set of visualization features so that you can view your data in a variety of ways.  Also, it can be used to visualize scalar and vector fields defined on two- and three-dimensional (2D and 3D) structured and unstructured meshes.</p> <p>VisIt was designed to handle very large data set sizes in the terascale range, and yet can also handle small data sets in the kilobyte range.</p> <p>For more information on VisIt, see their Lawrence Livermore National Laboratory website.</p>"},{"location":"Documentation/Viz_Analytics/visit/#using-visit","title":"Using VisIT","text":"<p>VisIT can be used through the GUI on DAV nodes </p> <pre><code>module load visit \nvisit \n</code></pre> <p>VisIt features a robust remote visualization capability.  To enable remote visualization (client/server), follow these steps.</p> <ol> <li>On Kestrel, add:     <pre><code>module load visit\n</code></pre>     to your <code>.bashrc</code> file in the home directory</li> <li>On a local machine, download VisIt 3.3.3 for the appropriate platform from the Lawrence Livermore National Laboratory VisIt site.</li> <li>The installed profile can be viewed and edited by clicking on 'Options \u2192 Host profiles ... '. A remote host profile should appear.  </li> <li>Go to Launch Profiles. </li> <li>Go to the Parallel tab, set up the job parameters, select sbatch/srun for \u2018Parallel launch method\u2019 and then click Apply. </li> <li>To connect to VisIt, go to File \u2192 Open file </li> <li>In the Host option, click on the drop down menu and choose the host kestrel.hpc.nrel.gov </li> <li>It will display a window with an option to change the username, if the username is not correct, then click on change username. This is your HPC username</li> <li>Type your HPC username and click Confirm username.</li> <li>Enter your HPC password and click OK.</li> <li>Wait for visit client to connect to the server on Kestrel.</li> <li>Enter the directory where your data is located into Path. </li> <li>Once you choose your data file, VisIt will display the job information; you can change them and then click OK.</li> <li>Once the job is submitted, you can start applying visualization filters to your data. For the job information:<ul> <li>Bank / Account: enter the project name you are charging to.</li> <li>Time limit: enter the time you need for the job in the following format H:M:S.</li> </ul> </li> </ol>"},{"location":"blog/2020-12-01-numba/","title":"Speeding up Python Code with Numba","text":"<p>Numba is a just in time (JIT) compiler for Python and NumPy code. From their official website, \"Numba translates Python functions to optimized machine code at runtime using the industry-standard LLVM compiler library. Numba-compiled numerical algorithms in Python can approach the speeds of C or FORTRAN.\"</p> <pre><code>@jit(nopython=True)\ndef function_to_be_compiled():\n    # Standard numerical/NumPy code here\n    ...\n</code></pre> <p>Importantly, many functions require no changes or refactoring to gain this speedup.  In this getting-started guide, we build an example environment on Eagle, test the performance of a Numba-compiled function using the most common implementation of the <code>@jit</code> decorator, and discuss what sorts of functions will see performance improvements when compiled.</p>"},{"location":"blog/2021-05-06-tf/","title":"Faster Machine Learning with Custom Built TensorFlow on Eagle","text":"<p>TensorFlow is a widely used and powerful symbolic math library commonly used for a variety of machine learning techniques. TensorFlow has built in API support for regression, clustering, classification, hidden Markov models, neural networks, reinforcement learning, as well as, a variety of activation functions, loss function, and optimizers. TensorFlow has received growing adoption among scientists, researchers, and industry professionals for its broad applicability and flexibility.</p> <p>TensorFlow versions obtained from <code>pip</code> or <code>conda</code> installs may not be optimized for the CPU and GPU architectures found on Eagle. To address this, pre-compiled versions which are optimized both for the CPU and GPU architectures have been created and offer computational benefits compared to other installation approaches. These versions can easily be installed from the <code>wheels</code> provided in <code>/nopt/nrel/apps/wheels/</code> which contains different TensorFlow versions. </p> <p>Here is an example of how you can install an optimized version of TensorFlow to your environment.  <pre><code>pip install --upgrade --no-deps --force-reinstall /nopt/nrel/apps/wheels/tensorflow-2.4.0-cp38-cp38-linux_x86_64.whl\n</code></pre> These builds provide a significant advantage as illustrated below over the standard <code>conda</code> install of TensorFlow.  </p> <p>A recent tutorial was given on this topic, for more information see the recording or checkout the tutorial materials</p>"},{"location":"blog/2021-06-18-srun/","title":"Using srun to Launch Applications Under Slurm","text":""},{"location":"blog/2021-06-18-srun/#subjects-covered","title":"Subjects covered","text":"<ol> <li>Basics</li> <li>Pointers to Examples</li> <li>Why not just use mpiexec/mpirun?</li> <li>Simple runs</li> <li>Threaded (OpenMP) runs</li> <li>Hybrid MPI/OpenMPI</li> <li>MPMD - a simple distribution</li> <li>MPMD multinode</li> </ol>"},{"location":"blog/2021-06-18-srun/#1-basics","title":"1. Basics","text":"<p>Eagle uses the Slurm scheduler and applications run on a compute node must be run via the scheduler.  For batch runs users write a script and submit the script using the sbatch command.  The script tells the scheduler what resources are required including a limit on the time to run.  The script also normally contains \"charging\" or account information.</p> <p>Here is a very basic script that just runs hostname to list the nodes allocated for a job.</p> <pre><code>#!/bin/bash\n#SBATCH --nodes=2\n#SBATCH --ntasks-per-node=1\n#SBATCH --time=00:01:00\n#SBATCH --account=hpcapps \n\n\nsrun hostname\n</code></pre> <p>Note we used the srun command to launch multiple (parallel) instances of our application hostname.  </p> <p>This article primarily discusses options for the srun command to enable good parallel execution.  In the script above we have asked for two nodes --nodes=2 and each node will run a single instance of hostname --ntasks-per-node=1.  If srun is not given options on the command line it will determine the number of tasks to run from the arguments in the header.  Thus our output from the script given above will be two lines, a list of nodes allocated for the job.  </p>"},{"location":"blog/2021-06-18-srun/#2-pointers-to-examples","title":"2. Pointers to examples","text":"<p>The page https://www.nrel.gov/hpc/eagle-batch-jobs.html has information about running jobs under Slurm including a link to example batch scripts.  The page https://github.com/NREL/HPC/tree/master/slurm has many slurm examples ranging from simple to complex.  This article is based on the second page.  </p>"},{"location":"blog/2021-06-18-srun/#3-why-not-just-use-mpiexecmpirun","title":"3. Why not just use mpiexec/mpirun?","text":"<p>The srun command is an integral part of the Slurm scheduling system.  It \"knows\" the configuration of the machine and recognizes the environmental variables set by the scheduler, such as cores per nodes.  Mpiexec and mpirun come with the MPI compilers. The amount of integration with the scheduler is implementation and install methodology dependent.  They may not enable the best performance for your applications.  In some cases they flat out just don't work correctly on Eagle.  For example, when trying to run MPMD applications (different programs running on different cores) using the mpt version of mpiexec, the same programs gets launched on all cores. </p>"},{"location":"blog/2021-06-18-srun/#4-simple-runs","title":"4. Simple runs","text":"<p>For our srun examples we will use two glorified \"Hello World\" programs, one in Fortran and the other in C.  They are essentially the same program written in the two languages.  They can be compiled as MPI, OpenMP, or as hybrid MPI/OpenMP.  They are available from the NREL HPC repository https://github.com/NREL/HPC.git in the slurm/source directory or by running the wget commands shown below.  </p> <p>wget https://raw.githubusercontent.com/NREL/HPC/master/slurm/source/fhostone.f90 wget https://raw.githubusercontent.com/NREL/HPC/master/slurm/source/mympi.f90 wget https://raw.githubusercontent.com/NREL/HPC/master/slurm/source/phostone.c wget https://raw.githubusercontent.com/NREL/HPC/master/slurm/source/makehello -O makefile</p> <p>After the files are downloaded you can build the programs </p>"},{"location":"blog/2021-06-18-srun/#using-the-mpt-mpi-compilers","title":"using the mpt MPI compilers","text":"<pre><code>module purge\nmodule load mpt gcc/10.1.0\nmake\n</code></pre>"},{"location":"blog/2021-06-18-srun/#or-using-intel-mpi-compilers","title":"or using Intel MPI compilers","text":"<pre><code>module purge\nmodule load intel-mpi gcc/10.1.0\nmake\n</code></pre> <p>You will end up with the executables:</p> <pre><code>fomp         - Fortran Openmp program\nfhybrid      - Fortran hybrid MPI/Openmp program\nfmpi         - Fortran MPI program\ncomp         - C hybrid Openmp program\nchybrid      - C hybrid MPI/Openmp program\ncmpi         - C MPI program\n</code></pre> <p>These programs have many options.  Running with the command line option -h will show them. Not all options are applicable for all versions.  Run without options the programs just print the hostname on which they were run.</p> <p>We look at our simple example again.  Here we ask for 2 nodes, 4 tasks per node for a total of 8 tasks.</p> <pre><code>#!/bin/bash\n#SBATCH --job-name=\"hostname\"\n#SBATCH --nodes=2\n#SBATCH --ntasks-per-node=4\n#SBATCH --ntasks=8\n#SBATCH --time=00:10:00\n\nsrun ./cmpi\n</code></pre> <p>This will produce (sorted) output like:</p> <pre><code>r105u33\nr105u33\nr105u33\nr105u33\nr105u37\nr105u37\nr105u37\nr105u37\n</code></pre> <p>In the above script we have nodes,ntasks-per-node and ntasks.  You do not need to specify all three parameters but values that are specified must be consistent.</p> <ul> <li>If nodes is not specified it will default to 1.  </li> <li>If ntasks is not specified it will default to 1 tasks per node.  </li> <li>You can put --ntasks-per-node and/or --ntasks on the srun line.  For example, to run a total of 9 tasks, 5 on one node and 4 on the second:</li> </ul> <pre><code>#!/bin/bash\n#SBATCH --job-name=\"hostname\"\n#SBATCH --nodes=2\n#SBATCH --time=00:10:00\n\nsrun --ntasks=9 ./cmpi\n</code></pre>"},{"location":"blog/2021-06-18-srun/#5-threaded-openmp-runs","title":"5. Threaded (OpenMP) runs","text":"<p>The variable used to tell the operating system how many threads to use for an OpenMP program is OMP_NUM_THREADS.  In the ideal world you could just set OMP_NUM_THREADS to a value, say 36, the number of cores on each Eagle node, and each thread would be assigned to a core.  Unfortunately without setting additional variables you will get the requested number of threads but threads might not be spread across all cores.  This can result in a significant slowdown.  For a program that is computationally intensive if two threads get mapped to the same core the runtime will increase 100%.  If all threads end up on the same core, the slowdown could actually be greater than the number of cores.  </p> <p>Our example programs, phostone.c and fhostone.f90, have a nice feature.  If you add -F to the command line they will produce a report showing on which core each thread runs. We are going to look at the C version of the code and compile it with both the Intel version of C, icc and with the Gnu compiler gcc.  </p> <p><pre><code>ml comp-intel/2020.1.217 gcc/10.1.0\ngcc -fopenmp -DNOMPI phostone.c -o comp.gcc\nicc -fopenmp -DNOMPI phostone.c -o comp.icc\n</code></pre> Run the script...</p> <p><pre><code>#!/bin/bash\n#SBATCH --job-name=\"hostname\"\n#SBATCH --cpus-per-task=36\n## ask for 10 minutes\n#SBATCH --time=00:10:00\n#SBATCH --nodes=1\n#SBATCH --partition=debug\nexport OMP_NUM_THREADS=36\n\nsrun ./comp.gcc -F &gt; gcc.out\nsrun ./comp.gcc -F &gt; icc.out\n</code></pre> Note we have added the line #SBATCH --cpus-per-task=36.  cpus-per-task should match the value of OMP_NUM_THREADS.</p> <p>We now look at the sorted head of each of the output files</p> <pre><code>el3:nslurm&gt; cat icc.out | sort -k6,6\ntask    thread             node name  first task    # on node  core\n0000      0030               r5i7n35        0000         0000  0000\n0000      0001               r5i7n35        0000         0000  0001\n0000      0034               r5i7n35        0000         0000  0001\n0000      0002               r5i7n35        0000         0000  0002\n0000      0035               r5i7n35        0000         0000  0002\n0000      0032               r5i7n35        0000         0000  0003\n. . .\n\nel3:nslurm&gt; cat gcc.out | sort -k6,6\ntask    thread             node name  first task    # on node  core\n0000      0031               r5i7n35        0000         0000  0000\n0000      0001               r5i7n35        0000         0000  0001\n0000      0002               r5i7n35        0000         0000  0002\n0000      0034               r5i7n35        0000         0000  0002\n0000      0003               r5i7n35        0000         0000  0003\n0000      0004               r5i7n35        0000         0000  0004\n. . .\n</code></pre> <p>The last column shows the core on which a thread is run.  We see that there is duplication of cores, potentially leading to poor performance.  </p> <p>There are two sets of environmental variables that can be used to map threads to cores.  One variable is specific to the Intel compilers, KMP_AFFINITY.  The others are general for OpenMP compilers and should work for any OpenMP compiler, OMP_PLACES and OMP_PROC_BIND.  These are documented at: </p> <p>https://software.intel.com/content/www/us/en/develop/documentation/cpp-compiler-developer-guide-and-reference/top/optimization-and-programming-guide/openmp-support/openmp-library-support/thread-affinity-interface-linux-and-windows.html</p> <p>https://www.openmp.org/spec-html/5.0/openmpse52.html</p> <p>https://www.openmp.org/spec-html/5.0/openmpse53.html</p> <p>We ran each version of our code 100 times with 5 different settings.  The settings were:</p> <ol> <li>export KMP_AFFINITY=verbose,scatter</li> <li>export KMP_AFFINITY=verbose,compact</li> <li>export OMP_PLACES=coresexport OMP_PROC_BIND=spread</li> <li>export OMP_PLACES=coresexport OMP_PROC_BIND=close</li> <li>NONE</li> </ol> <p>The table below shows the results of our runs.  In particular, it shows the minimum number of cores used with the particular settings.  36 is the desired value.  We see that for gcc the following settings worked well:  </p> <p>export OMP_PLACES=coresexport OMP_PROC_BIND=spread </p> <p>or  export OMP_PLACES=coresexport OMP_PROC_BIND=clone</p> <p>Setting KMP_AFFINITY did not work for gcc but for the Intel compiler KMP_AFFINITY also gave good results.  </p> Compiler Setting Workedmincores meancores maxcores gcc cores, close yes 36 36 36 gcc cores, spread yes 36 36 36 gcc KMP_AFFINITY=compactno 25 34.18 36 gcc KMP_AFFINITY=scatter no 26 34.56 36 gcc none no 28 34.14 36 icc cores, close yes 36 36 36 icc cores, spread yes 36 36 36 icc KMP_AFFINITY=compactyes 36 36 36 icc KMP_AFFINITY=scatter yes 36 36 36 icc none no 19 23.56 29 <p>So our final working script for OpenMP programs could be:</p> <pre><code>#!/bin/bash\n#SBATCH --job-name=\"hostname\"\n#SBATCH --cpus-per-task=36\n## ask for 10 minutes\n#SBATCH --time=00:10:00\n#SBATCH --nodes=1\n#SBATCH --partition=debug\nexport OMP_NUM_THREADS=36\n\nexport OMP_PLACES=cores\nexport OMP_PROC_BIND=close\n#export OMP_PROC_BIND=spread\n\nsrun ./comp.gcc -F &gt; gcc.out\nsrun ./comp.gcc -F &gt; icc.out\n</code></pre> <p>When a job is run the SLURM_CPUS_PER_TASK is set to cpus-per-task so you may want to </p> <pre><code>export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK\n</code></pre> <p>More on this in the next section.</p>"},{"location":"blog/2021-06-18-srun/#6-hybrid-mpiopenmpi","title":"6. Hybrid MPI/OpenMPI","text":"<p>The next script is just an extension of the last.  We now run hybrid, a combination of MPI and OpenMP.  Our base example programs, fhostame.f90 and phostname.c can be compiled in hybrid mode as well as in pure MPI and pure OpenMP.</p> <p>First we look at the (sorted) output from our program run in hybrid mode with 4 tasks on two nodes and 4 threads.  </p> <pre><code>MPI VERSION Intel(R) MPI Library 2019 Update 7 for Linux* OS\ntask    thread             node name  first task    # on node  core\n0000      0000                r5i0n4        0000         0000  0000\n0000      0001                r5i0n4        0000         0000  0004\n0000      0002                r5i0n4        0000         0000  0009\n0000      0003                r5i0n4        0000         0000  0014\n0001      0000                r5i0n4        0000         0001  0018\n0001      0001                r5i0n4        0000         0001  0022\n0001      0002                r5i0n4        0000         0001  0027\n0001      0003                r5i0n4        0000         0001  0032\n0002      0000               r5i0n28        0002         0000  0000\n0002      0001               r5i0n28        0002         0000  0004\n0002      0002               r5i0n28        0002         0000  0009\n0002      0003               r5i0n28        0002         0000  0014\n0003      0000               r5i0n28        0002         0001  0018\n0003      0001               r5i0n28        0002         0001  0022\n0003      0002               r5i0n28        0002         0001  0027\n0003      0003               r5i0n28        0002         0001  0032\ntotal time      3.009\n</code></pre> <p>The first column is the MPI task number followed by the thread, then the node.  The last column is the core on which that give task/thread was run.  We can cat a list of unique combinations of nodes and cores by piping the file into </p> <pre><code>grep ^0 | awk '{print $3, $6}' | sort -u | wc -l`\n</code></pre> <p>We get 16 which is the number of tasks times the number of threads.  That is, we have each task/thread assigned to its own core.  This will give good performance.  The script below runs  on a fixed number of tasks (4 = 2 per node * 2 nodes) and using from 1 to cpus-per-task=18 threads.  </p> <p>The variable SLURM_CPUS_PER_TASK is set by slurm to be cpus-per-task.  After the srun line we post process the output to report core usage.  </p> <p><pre><code>#!/bin/bash\n#SBATCH --account=hpcapps \n#SBATCH --time=00:10:00 \n#SBATCH --nodes=2 \n#SBATCH --partition=short \n#SBATCH --cpus-per-task=18\n#SBATCH --ntasks=4\n\nmodule purge\nmodule load intel-mpi/2020.1.217 gcc/10.1.0\n\n\nexport OMP_PLACES=cores\nexport OMP_PROC_BIND=spread\n\necho \"CPT TASKS THREADS  cores\"\nfor n in `seq 1 $SLURM_CPUS_PER_TASK` ; do\n    request=`python -c \"print($n*$SLURM_NTASKS)\"`\n    have=72\n    if ((request &lt;= have)); then\n      export OMP_NUM_THREADS=$n\n      srun  --ntasks-per-core=1 -n $SLURM_NTASKS ./phostone.icc -F -t 3 &gt; out.$SLURM_NTASKS.$OMP_NUM_THREADS\n# post process\n      cores=`cat out.$SLURM_NTASKS.$OMP_NUM_THREADS | grep ^0 | awk '{print $3, $6}' | sort -u | wc -l`\n      echo $SLURM_CPUS_PER_TASK \"    \" $SLURM_NTASKS \"    \" $OMP_NUM_THREADS \"    \" $cores\n    fi\ndone\n</code></pre> Our final output from this script is:</p> <pre><code>el3:stuff&gt; cat slurm-7002718.out\nCPT TASKS THREADS cores\n18      4      1      4\n18      4      2      8\n18      4      3      12\n18      4      4      16\n18      4      5      20\n18      4      6      24\n18      4      7      28\n18      4      8      32\n18      4      9      36\n18      4      10      40\n18      4      11      44\n18      4      12      48\n18      4      13      52\n18      4      14      56\n18      4      15      60\n18      4      16      64\n18      4      17      68\n18      4      18      72\nel3:stuff&gt; \n</code></pre> <p>The important lines are:</p> <pre><code>#SBATCH --cpus-per-task=18\n. . .\nexport OMP_PLACES=cores\nexport OMP_PROC_BIND=spread\n. . .\nsrun  --ntasks-per-core=1 -n $SLURM_NTASKS ./phostone.icc \n</code></pre> <p>We need to set cpus-per-task to tell slurm we are going to run multithreaded and how many cores we are going to use for our threads.  This should be set to the maximum number of threads per task we expect to use.</p> <p>We use the OMP variables to map threads to cores.  IMPORTANT: using KMP_AFFINTY will not give the desired results.  It will cause all threads for a task to be mapped to a single core.</p> <p>We can run this script for hybrid MPI/OpenMP programs as is or set the number of cpus-per-task and tasks on the sbatch command line.  For example:  </p> <pre><code>sbatch --cpus-per-task=9 --ntasks=8 simple\n</code></pre> <p>gives us:</p> <pre><code>el3:stuff&gt; cat slurm-7002858.out\nCPT TASKS THREADS  cores\n9      8      1      8\n9      8      2      16\n9      8      3      24\n9      8      4      32\n9      8      5      40\n9      8      6      48\n9      8      7      56\n9      8      8      64\n9      8      9      72\nel3:stuff&gt; \n</code></pre>"},{"location":"blog/2021-06-18-srun/#7-mpmd-a-simple-distribution","title":"7. MPMD - a simple distribution","text":"<p>Here we look at launching Multi Program Multi Data runs.   We use a the --multi-prog option with srun.  This involves creating a config_file that lists the  programs we are going to run along with the task ID.  See: https://computing.llnl.gov/tutorials/linux_clusters/multi-prog.html for a quick description of the format for the config_file.</p> <p>Here we create the file on the fly but it could be done beforehand.</p> <p>We have two MPI programs to run together, phostone and fhostone.  They are actually the same program written in C and Fortran. In the real world MPMD applications would maybe run a GUI or a manager for one task and rest doing compute.  </p> <p>The syntax for running MPMD programs is</p> <p>srun --multi-prog mapfile</p> <p>where mapfile is a config_file that lists the programs to run.</p> <p>It is possible to pass different arguments to each program as discussed in the link above.  Here we just add command line arguments for task 0.</p> <p>Our mapfile has 8 programs listed.  The even tasks are running phostone and the odd fhostone.  Our script uses two for loops to add lines to the mapfile and then uses sed to append command line arguments to the first line.  </p> <pre><code>#!/bin/bash\n#SBATCH --account=hpcapps \n#SBATCH --time=00:10:00 \n#SBATCH --nodes=1\n#SBATCH --partition=debug \n#SBATCH --cpus-per-task=1\n\n# create our mapfile\napp1=./phostone\nfor n in 0 2 4 6 ; do\n  echo $n $app1 &gt;&gt; mapfile\ndone\napp2=./fhostone\nfor n in 1 3 5 7 ; do\n  echo $n $app2 &gt;&gt; mapfile\ndone\n\n# add a command line option to the first line\n# sed does an in-place change to the first line\n# of our mapfile adding *-F*\nsed -i \"1 s/$/ -F /\" mapfile\n\ncat mapfile\n\nexport OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK\nsrun -n8 --multi-prog mapfile\n</code></pre> <p>Here is the complete output including the mapfile and output from our two programs. Lines with three digits for core number were created by the Fortran version of the program.  </p> <pre><code>el3:stuff&gt; cat *7003104*\n0 ./phostone -F \n2 ./phostone\n4 ./phostone\n6 ./phostone\n1 ./fhostone\n3 ./fhostone\n5 ./fhostone\n7 ./fhostone\nMPI VERSION Intel(R) MPI Library 2019 Update 7 for Linux* OS\n\ntask    thread             node name  first task    # on node  core\n0000      0000               r1i7n35        0000         0000  0022\n0001      0000               r1i7n35        0001         0000   021\n0002      0000               r1i7n35        0000         0001  0027\n0003      0000               r1i7n35        0003         0000   023\n0004      0000               r1i7n35        0000         0002  0020\n0005      0000               r1i7n35        0005         0000   025\n0006      0000               r1i7n35        0000         0003  0026\n0007      0000               r1i7n35        0007         0000   019\nel3:stuff&gt; \n</code></pre>"},{"location":"blog/2021-06-18-srun/#8-mpmd-multinode","title":"8. MPMD multinode","text":"<p>Our final example again just extends the previous one.  We want to add the capability to launch different numbers of tasks on a set of nodes and at the same time have different programs on each of the nodes.  We create a mapfile to list the programs to run as was done above.  In this case for illustration  purposes we are running one copy of phostone and seven instances of fhostone.</p> <p>We add to that a hostfile that lists the nodes on which to run.  The hostfile has one host per MPI task.</p> <p><pre><code>#!/bin/bash\n#SBATCH --account=hpcapps \n#SBATCH --time=00:10:00 \n#SBATCH --nodes=2\n#SBATCH --partition=debug \n\nexport OMP_NUM_THREADS=1\n\n# Create our mapfile\nrm -rf mapfile\napp1=./phostone\nfor n in 0  ; do\n  echo $n $app1 &gt;&gt; mapfile\ndone\napp2=./fhostone\nfor n in 1 2 3 4 5 6 7 ; do\n  echo $n $app2 &gt;&gt; mapfile\ndone\n\n# Add a command line option to the first line\n# sed does an in-place change to the first line\n# of our mapfile adding *-F*\nsed -i \"1 s/$/ -F /\" mapfile\n\n# Count of each app to run on a node\ncounts=\"1 7\"\n\n# Get a list of nodes on a single line\nnodes=`scontrol show hostnames | tr '\\n' ' '`\n\n# Create our hostfile and tell slrum its name\nexport SLURM_HOSTFILE=hostlist\n\n# It is possible to do this in bash but\n# I think this is easier to understand\n# in python.  It uses the values for\n# counts and nodes set above.\npython -  &gt; $SLURM_HOSTFILE &lt;&lt; EOF\nc=\"$counts\".split()\nnodes=\"$nodes\".split()\nk=0\nfor i in c:\n  i=int(i)\n  node=nodes[k]\n  for j in range(0,i):\n      print(node)\n  k=k+1\nEOF\n\n\nsrun -n 8 --multi-prog mapfile\n</code></pre> Here is the output from our run including the mapfile and hostlist.  Notice that the first instance of the set of running programs is the C version.  It is the only thing running on the first nodes.  The rest of the MPI tasks are the Fortran version of the program running on the second node.</p> <pre><code>el3:stuff&gt; cat slurm-7003587.out | sort -k3,3 -k1,1\nMPI VERSION Intel(R) MPI Library 2019 Update 7 for Linux* OS\ntask    thread             node name  first task    # on node  core\n0000      0000               r102u34        0000         0000  0004\n0001      0000               r102u35        0001         0000   003\n0002      0000               r102u35        0002         0000   000\n0003      0000               r102u35        0001         0001   006\n0004      0000               r102u35        0004         0000   007\n0005      0000               r102u35        0001         0002   004\n0006      0000               r102u35        0002         0001   005\n0007      0000               r102u35        0001         0003   002\nel3:stuff&gt; cat mapfile\n0 ./phostone -F \n1 ./fhostone\n2 ./fhostone\n3 ./fhostone\n4 ./fhostone\n5 ./fhostone\n6 ./fhostone\n7 ./fhostone\nel3:stuff&gt; c\nel3:stuff&gt; cat hostlist\nr102u34\nr102u35\nr102u35\nr102u35\nr102u35\nr102u35\nr102u35\nr102u35\nel3:stuff&gt; \n</code></pre>"},{"location":"blog/2022-02-02-Changes_to_Slurm_srun_for_interactive_jobs/","title":"Changes to Slurm \"srun\" for Interactive Jobs","text":"<p>The Slurm job scheduler was upgraded during the recent system time on January 10, 2022.  One of the side effects of this was a change in the way Slurm handles job steps internally in certain cases.  This may affect the way some users run job steps with srun inside of interactive jobs <code>srun --pty $SHELL</code>, so we wanted to provide some guidance as we work on updating our documentation to reflect this change. </p> <p>When running an interactive job with <code>srun --pty $SHELL</code> and then launching job steps on a node, a second <code>srun</code> is often used \"inside\" the first srun to launch certain software.  For example, for users of Paraview, a Paraview server may be launched on an interactive node with <code>srun -n 8 pvserver --force-offscreen-rendering</code>.  (Certain GPU-enabled or MPI-enabled interactive software also functions in a similar manner.)</p> <p>This \"srun-inside-an-srun\" process will no longer function in the same way as in the past. Instead, the \"outer\" <code>srun</code> should be replaced with an <code>salloc</code> command. <code>salloc</code> will accept the same arguments as srun, but <code>--pty $SHELL</code> will no longer be required. <code>salloc</code> will automatically open a shell to the node once the job starts, and the \"inner\" <code>srun</code> can then be run successfully as normal.</p> <p>Other regular uses of srun and srun inside sbatch scripts should continue to behave as expected. </p> <p>For further technical details on this Slurm change, please see the Slurm 20.11 Release Notes regarding job steps, <code>srun</code>, and the new <code>--overlap</code> flag.</p>"},{"location":"blog/2022-03-05-local-io-performance/","title":"Eagle Local I/O Performance","text":"<p>We sometimes receive questions about disk types and I/O performance on compute nodes. Eagle has two network file systems. Qumulo provides /home and /nopt. It is NFS and is not considered a performance file system. /home has snapshots for restoration of lost data, but should not be used as a replacement for a source code repository like Git. Lustre is our performance file system, and it provides storage for the /scratch, /projects, /shared-projects and /datasets directories.</p> <p>Eagle also has two storage options on the compute nodes. /dev/shm is an in-memory space (shm: shared memory), which is fast, but you need to balance its usage with your job's memory usage as it is located directly in RAM. /tmp/scratch is physical storage. The type of storage and performance differ depending on the specific type of compute node. </p> <p>If we look under Eagle's Compute Node Hardware Details on the central NREL HPC website, there are nodes listed as having SATA drives, and nodes listed as having SSDs. Our SATA drives are still spinning disks, while SAS (serial attached SCSI) is how the SSDs are connected to the node. We would generally expect the nodes with SSDs to perform better. Let\u2019s test that out with a simple test. </p> <p>This following is a command we regularly use to verify Lustre OST (object storage target) performance. It\u2019s designed to write enough information so that you are seeing disk performance, and not just the performance of the storage controller of the disk: </p> <p><code>dd if=/dev/zero of=X bs=1M count=10k</code></p> <p>This is writing in file in chunks of 1M, 10k times, to X. It writes an 11GB file. The results:</p> <p>Lustre: 1.6 GB/s per OST</p> <p>Node /dev/shm: 2.8 GB/s</p> <p>Node SATA spinning disk: 2.4 GB/s</p> <p>Node SAS SSD: 2.4 GB/s</p> <p>Surprising! There is not a difference between the two local disks. Let\u2019s do the same test, but instead of writing in 1M chunks, we will write in 10M chunks which will write a 107GB file. For this case, Lustre and /dev/shm maintain performance, but here\u2019s what we get for the two local disk types:</p> <p>Node SATA spinning disk: 146 MB/s</p> <p>Node SAS SSD: 1.9 GB/s</p> <p>That is a rather drastic drop off in performance for the SATA disk. So how your data writes to disk can drastically affect performance. A lot of tiny files will look the same between the two disk types, one large continuous write would differ.</p>"},{"location":"blog/2022-10-04-python2to3/","title":"Running Legacy Python 2 Code on Eagle","text":""},{"location":"blog/2022-10-04-python2to3/#what-is-legacy-code","title":"What is Legacy Code?","text":"<p>One definition of \"legacy\" code or software is code was written in the past using currently outdated, obsolete, or otherwise deprecated, compilers, functions, methods, or methodology.</p> <p>While Python 2 was sunset on January 1, 2020 in favor of Python 3, there is still \"legacy\" Python 2 software that may need to be run on Eagle.  We always encourage Eagle users to upgrade their code to Python 3.x to continue receiving official updates, bug fixes, and security patches. But we do understand that there will always be code that is not worth porting to Python 3.  In those cases here are some options you have</p> <ol> <li>Set up a custom Python 2 environment</li> <li>Check for updates</li> <li>Find an alternative tool that fits your needs</li> <li>Convert Python 2 code to Python 3</li> </ol>"},{"location":"blog/2022-10-04-python2to3/#1-set-up-a-custom-python-2-environment-using-conda-or-containers","title":"1. Set up a custom Python 2 environment using Conda or containers","text":"<p>It is best to create this python environment within conda. For example,</p> <pre><code>conda create --name my_environment python=2\n</code></pre> <p>This conda environment can be made even more portable by using Docker or Apptainer. Currently, Eagle only supports Apptainer. Subsequently, the outputs of the Python 2 code can be incorporated, e.g., using a file-based approach.</p> <p>It is possible to run legacy Python 2 code in parallel within a container or a conda environment, but your mileage may vary.</p>"},{"location":"blog/2022-10-04-python2to3/#2-check-for-updates","title":"2. Check for updates","text":"<p>If the software or module is still under active development, it's highly likely that the authors have transitioned the software to Python 3.x.  If an updated version is available, you should strongly consider it over an outdated Python 2 version.  It will likely be more secure, have better performance, be more reliable, and have a longer shelf life for reproducibility in the future.</p>"},{"location":"blog/2022-10-04-python2to3/#3-find-an-alternative-tool-that-fits-your-needs","title":"3. Find an alternative tool that fits your needs","text":"<p>There are usually multiple alternatives to your software of concern that have the same or slightly different features. Some of the potential advantages and disadvantages include:</p> <p>Advantages</p> <ol> <li>If they are written in Python 3, they are newer and might offer better software support.</li> <li>They allow for efficient parallelism, enabling better use of Eagle.</li> <li>Leverages Python 3 features and packages, e.g., Abstract Base Classes.</li> </ol> <p>Disadvantages</p> <ol> <li>It may break your build environment.</li> <li>It may require a significant code rewrite to extract the best performance.</li> <li>Additional code may need to be written if the alternative does not have all the features as the Python 2 software.</li> </ol>"},{"location":"blog/2022-10-04-python2to3/#4-convert-python-2-code-to-python-3","title":"4. Convert Python 2 code to Python 3","text":"<p>This is an option when the Python 2 code under consideration does not have a lot of dependencies, you have the source code, and the software license allows you to make changes to the source code.  For scientific software, we do encourage developers to make this version jump as this enables code longevitiy and accessibility. Several online resource are available to enable porting your code to Python 3. Some of them include:</p> <ol> <li>Porting Python 2 Code to Python 3</li> <li>The Conservative Python 3 Porting Guide</li> <li>Supporting Python 3: An in-depth guide</li> <li>Python FAQ: How do I port to Python 3?</li> <li>How to Port Python 2 Code to Python 3</li> <li>2to3 \u2014 Automated Python 2 to 3 code translation</li> </ol> <p>Note that the above is not an exhasutive list and we encourage the user to look at other resources as well. Additionally, depending on the needs of your project, you can also reach out to HPC-Help@nrel.gov to help with this port.</p> <p>Remember, since Python 2 has been officially deprecated, more and more code is either being updated or rewritten entirely in Python 3 as time passes. Additionally, the community-maintained Python 2 packages in the default conda channels will likely disappear at some point in the future. Keeping your code modernized to the latest standards will help ensure both the longevity and reproducibility of your software and your results.</p>"},{"location":"blog/2022-12-19-windows_ssh/","title":"Workaround for Windows SSH \"Corrupted MAC on input\" Error","text":"<p>Some people who use Windows 10/11 computers to ssh to Eagle from a Windows command prompt, powershell, or via Visual Studio Code's SSH extension might receive an error message about a \"Corrupted MAC on input\" or \"message authentication code incorrect.\" This error is due to an outdated OpenSSL library included in Windows and a security-mandated change to ssh on Eagle. However, there is a functional workaround for this issue. (Note: If you are not experiencing the above error, you do not need and should not use the following workaround.)</p> <p>For command-line and Powershell ssh users, adding <code>-m hmac-sha2-512</code> to your ssh command will resolve the issue. For example: <code>ssh -m hmac-sha2-512 &lt;username&gt;@eagle.hpc.nrel.gov</code>.</p> <p>For VS Code SSH extension users, you will need to create an ssh config file on your local computer (~/.ssh/config), with a host entry for Eagle that specifies a new message authentication code:  <pre><code>Host eagle\n    HostName eagle.hpc.nrel.gov\n    MACs hmac-sha2-512\n</code></pre></p> <p>The configuration file will also apply to command-line ssh in Windows. This Visual Studio Blog post has further instructions on how to create the ssh configuration file for Windows and VS Code.</p>"},{"location":"blog/2023-01-10-using_specific_module_versions_on_hpc/","title":"Using Specific Module Versions on the HPC","text":"<p>Modules on NREL HPCs are updated to with newer versions with on a regular basis. Since Lmod, the underlying module system, sets the most recent version of a module as the default, a user's typical workflow may break if they are not specifying the exact module version in their scripts.</p> <p>For example, at the time of this writing, the current default module for Conda on Eagle is 4.9.2. If a user wishes to use conda v4.12.0, they must specify the version in the module command as</p> <pre><code>module load conda/4.12.0\n</code></pre> <p>The user can also create custom module files for their own use and point to them. For example, assuming a user has custom TCL or LUA module files in the following directory</p> <pre><code>/home/${USER}/private_modules/\n</code></pre> <p>They can use these module files by adding it to the module search path using the following command</p> <pre><code>module use -a /home/${USER}/private_modules/\n</code></pre> <p>Furthermore, if a user wishes to have these module paths available at all times, they can update their <code>.bash_profile</code> or <code>.bashrc</code> file in their home directory. For example by using the following command</p> <pre><code>echo 'module use -a /home/${USER}/private_modules/' &gt;&gt; /home/${USER}/.bash_profile\n</code></pre> <p>or a text editor. As a quick reminder, <code>.bash_profile</code> and <code>.bashrc</code> are simply configuration files that enable the user to customize your Linux or MacOS terminal experience, assuming they are using Bash as their shell (Similar to Command Prompt on Windows). The difference between <code>.bash_profile</code> and <code>.bashrc</code> is that the former is executed for login shells while the latter in executed for non-login shells, e.g., <code>.bash_profile</code> is executed when a user SSHs into Eagle, whereas <code>.bashrc</code> is executed when one opens a new terminal.</p>"},{"location":"blog/2023-11-28_module_updates/","title":"Module Updates on Kestrel for November 2023","text":""},{"location":"blog/2023-11-28_module_updates/#openfoam","title":"OpenFOAM","text":"<ul> <li>OpenFOAM versions 10, 11, and 2306 are now available as modules. All are compiled with GCC and OpenMPI.</li> <li>The module <code>openfoam/v2306-openmpi-gcc</code> is an installation of OpenFOAM obtained from the .com OpenFOAM website.</li> <li>OpenFOAM 10 and 11 are from the .org website which is cloned from the OpenFOAM repo. If there is a need for the dev version of OpenFOAM on Github, please let us know via HPC-Help@nrel.gov and we can work on making it available. </li> </ul>"},{"location":"blog/2023-11-28_module_updates/#anaconda","title":"Anaconda","text":"<ul> <li><code>anaconda3/2022.05</code> was lately noticed to generate error message. This is now rectified, and the 2022 version should be working properly.</li> <li>The modules <code>anaconda3/2023</code> and <code>mamba/1.4.2</code> are also available.</li> </ul>"},{"location":"blog/2023-11-28_module_updates/#netcdf","title":"NetCDF","text":"<ul> <li><code>netcdf/4.9.2-intel-oneapi-mpi-intel</code> is now available as a module. The module contains netcdfc, netcdfcxx, and netcdf-fortran compiled with Intel in one single module as opposed to having them in separate modules.</li> </ul>"}]}