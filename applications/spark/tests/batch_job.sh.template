#!/bin/bash
#SBATCH --account=TEST_ACCOUNT
#SBATCH --job-name=spark_job
#SBATCH --time=00:10:00
#SBATCH --output=output_%j.o
#SBATCH --error=output_%j.e
#SBATCH --nodes=2
#SBATCH --partition=debug
#SBATCH --qos=standby

module load apptainer
SCRIPT_DIR=../../spark_scripts
${SCRIPT_DIR}/create_config.sh
${SCRIPT_DIR}/configure_and_start_spark.sh \
    -c /kfs2/pdatasets/images/apache_spark/spark350_py311.sif \
    -H \
    -m 2 \
    --driver-memory-gb 2
ret=$?
if [ ${ret} -ne 0 ]; then
    echo "Error: Failed to run configure_spark.sh: ${ret}"
    exit 1
fi

grep "spark.sql.shuffle.partitions 412" conf/spark-defaults.conf
if [ ${?} -ne 0 ]; then
    echo "Number of shuffle partitions should have been (104 cores * 2 nodes - 2) * 2"
    exit 1
fi

grep "spark.driver.maxResultSize 2g" conf/spark-defaults.conf
if [ ${?} -ne 0 ]; then
    echo "driver maxResultSize was not 2g"
    exit 1
fi

grep "spark.eventLog.enabled true" conf/spark-defaults.conf
if [ ${?} -ne 0 ]; then
    echo "eventLog was not enabled"
    exit 1
fi

grep "spark.dynamicAllocation.enabled true" conf/spark-defaults.conf
if [ ${?} -eq 0 ]; then
    echo "dynamic allocation should not have been enabled"
    exit 1
fi
echo "Verified Spark configuration settings"

apptainer run instance://spark spark-submit --master spark://$(hostname):7077 ../test_job.py
ret=$?
if [ ${ret} -ne 0 ]; then
    echo "Error: Failed to run test_job.py: ${ret}"
    exit 1
fi

${SCRIPT_DIR}/stop_spark_cluster.sh
ret=$?
if [ ${ret} -ne 0 ]; then
    echo "Error: Failed to run stop_spark_cluster.sh: ${ret}"
    exit 1
fi

echo "Tests passed"
