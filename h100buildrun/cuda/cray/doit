: Start from a known module state, the default
module_restore

: Load modules
#module unload PrgEnv-cray/8.5.0
#module unload nvhpc/24.1


if [ -z ${MYGCC+x} ]; then module load gcc ; else module load $MYGCC ; fi
ml PrgEnv-nvhpc/8.4.0
ml cray-libsci/23.05.1.4
ml binutils
: << ++++ 
 Compile our program
 CC as well as cc, and ftn are wrapper compilers. Because
 we have PrgEnv-nvidia loaded they map to Nvidia's compilers
 but use would use Cray MPI if this was an MPI program.
 Note we can also use nvcc since this is not an MPI program.
++++

rm -rf ./stream.sm_90
CC -gpu=cc90  -cuda -target-accel=nvidia90  stream.cu  -o stream.sm_90
# nvcc -std=c++11 -ccbin=g++ stream.cu -arch=sm_90 -o stream.sm_90

: Run on all of our nodes
nlist=`scontrol show hostnames | sort -u`
for l in $nlist ; do   
  echo $l
  for GPU in 0 1 2 3 ; do
: stream.cu will read the GPU on which to run from the command line
      srun -n 1 --nodes=1 -w $l ./stream.sm_90 -g $GPU
  done
  echo
done
