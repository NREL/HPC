{"0": {
    "doc": "Beginner",
    "title": "Beginner",
    "content": " ",
    "url": "/General/beginner/beginner.html",
    "relUrl": "/General/beginner/beginner.html"
  },"1": {
    "doc": "Blog",
    "title": "Blog",
    "content": "def main(): print (\"hello\") . ",
    "url": "/blog/",
    "relUrl": "/blog/"
  },"2": {
    "doc": "Building Packages",
    "title": "Building packages on Peregrine for individual or project use.",
    "content": "This training module will walk through how to build a reasonably complex package, OpenMPI, and deploy it for use by yourself or members of a project. | Acquire the package and set up for build . | Configure, build, and install the package . | Setting up your own environment module . | . ",
    "url": "/General/intermediate/building-packages.html#building-packages-on-peregrine-for-individual-or-project-use",
    "relUrl": "/General/intermediate/building-packages.html#building-packages-on-peregrine-for-individual-or-project-use"
  },"3": {
    "doc": "Building Packages",
    "title": "Why build your own application?",
    "content": ". | Sometimes, the package version that you need, or the capabilities you want, are only available as source code. | Other times, a package has dependencies on other ones with application programming interfaces that change rapidly. A source code build might have code to adapt to the (older, newer) libraries you have available, whereas a binary distribution will likely not. In other cases, a binary distribution may be associated with a particular Linux distribution and version different from Peregrine’s. One example is a package for Linux version X+1 (with a shiny new libc). If you try to run this on Linux version X, you will almost certainly get errors associated with the GLIBC version required. If you build the application against your own, older libc version, those dependencies are not created. | Performance; for example, if a more performant numerical library is available, you may be able to link against it. A pre-built binary may have been built against a more universally available but lower performance library. The same holds for optimizing compilers. | Curiosity to know more about the tools you use. | Pride of building one’s tools oneself. | For the sheer thrill of building packages. :smile: | . ",
    "url": "/General/intermediate/building-packages.html#why-build-your-own-application",
    "relUrl": "/General/intermediate/building-packages.html#why-build-your-own-application"
  },"4": {
    "doc": "Building Packages",
    "title": "Building Packages",
    "content": " ",
    "url": "/General/intermediate/building-packages.html",
    "relUrl": "/General/intermediate/building-packages.html"
  },"5": {
    "doc": "Dask-MPI",
    "title": "Dask-MPI",
    "content": " ",
    "url": "/python/dask/dask-mpi.html",
    "relUrl": "/python/dask/dask-mpi.html"
  },"6": {
    "doc": "Dask",
    "title": "Dask",
    "content": "Dask provides a way to parallelize Python code either on a single node or across the cluster. It is similar to the functionality provided by Apache Spark, with easier setup. It provides a similar API to other common Python packages such as NumPY, Pandas, and others. ",
    "url": "/python/dask/dask.html",
    "relUrl": "/python/dask/dask.html"
  },"7": {
    "doc": "Dask",
    "title": "Dask single node",
    "content": "Dask can be used locally on your laptop or an individual node. Additionally, it provides wrappers for multiprocessing and threadpools. The advantage of using LocalCluster though is you can easily drop in another cluster configuration to further parallelize. import socket from distributed import Client, LocalCluster import dask from collections import Counter def test(): return socket.gethostname() def main(): cluster = LocalCluster(n_workers=2) client = Client(cluster) result = [] for i in range (0,20): result.append(client.submit(test).result()) print (Counter(result)) if __name__ == '__main__': main() . ",
    "url": "/python/dask/dask.html#dask-single-node",
    "relUrl": "/python/dask/dask.html#dask-single-node"
  },"8": {
    "doc": "Dask",
    "title": "Dask MPI",
    "content": "Dask-MPI can be used to parallelize calculations across a number of nodes as part of a batch job submitted to slurm. Dask will automatically create a scheduler on rank 0 and workers will be created on all other ranks. Install . Note: The version of dask-mpi installed via Conda may be incompatible with the MPI libaries on Eagle. Use the pip install instead. conda create -n daskmpi python=3.7 conda activate daskmpi pip install dask-mpi . Python script: This script holds the calculation to be performed in the test function. The script relies on the Dask cluster setup on MPI which is created in the . from dask_mpi import initialize from dask.distributed import Client, wait import socket import time from collections import Counter def test(): return socket.gethostname() def main(): initialize(interface='ib0') client = Client() time.sleep(15) result = [] for i in range (0,100): result.append(client.submit(test).result()) time.sleep(1) out = str(Counter(result)) print (f'nodes: {out}') main() . sbatch script: This runs the above python script using MPI. #!/bin/bash #SBATCH --nodes=2 #SBATCH --time=01:00:00 #SBATCH --account=&lt;hpc account&gt; #SBATCH --partition=&lt;Eagle partition&gt; module purge ml intel-mpi/2018.0.3 mpiexec -np 4 \\ python mpi_dask.py \\ --scheduler-file scheduler.json \\ --interface ib0 \\ --no-nanny \\ --nthreads 5 . ",
    "url": "/python/dask/dask.html#dask-mpi",
    "relUrl": "/python/dask/dask.html#dask-mpi"
  },"9": {
    "doc": "Dask",
    "title": "Dask jobqueue",
    "content": "Dask can also run using the Slurm scheduler already installed on Eagle. The Jobqueue library can handle submission of a computation to the cluster. This is particularly useful when running an interactive notebook or similar and you need to scale workers. from dask_jobqueue import SLURMCluster import socket from distributed import Client from collections import Counter cluster = SLURMCluster( cores=18, memory='24GB', queue='short', project='&lt;hpc account&gt;', walltime='00:30:00', interface='ib0', processes=17, ) client = Client(cluster) def test(): return socket.gethostname() result = [] cluster.scale(jobs=2) for i in range (0,2000): result.append(client.submit(test).result()) print (Counter(result)) print (cluster.job_script()) . ",
    "url": "/python/dask/dask.html#dask-jobqueue",
    "relUrl": "/python/dask/dask.html#dask-jobqueue"
  },"10": {
    "doc": "Dask",
    "title": "References",
    "content": "Dask documentation . Dask Jobqueue . Dask MPI . ",
    "url": "/python/dask/dask.html#references",
    "relUrl": "/python/dask/dask.html#references"
  },"11": {
    "doc": "File Transfers",
    "title": "how-to-transfer-files",
    "content": "Learn how to transfer data within, to and from NREL’s high-performance computing (HPC) systems. A supported set of instructions for data transfer using NREL HPC systems is provided on the HPC NREL Website. ",
    "url": "/General/beginner/file-transfers.html#how-to-transfer-files",
    "relUrl": "/General/beginner/file-transfers.html#how-to-transfer-files"
  },"12": {
    "doc": "File Transfers",
    "title": "Checking Usage and Quota",
    "content": "The below command is used to check your quota from a Peregrine login node. alloc_tracker will display your usage and quota for each filesystem. $ alloc_tracker . ",
    "url": "/General/beginner/file-transfers.html#checking-usage-and-quota",
    "relUrl": "/General/beginner/file-transfers.html#checking-usage-and-quota"
  },"13": {
    "doc": "File Transfers",
    "title": "Best Practices for Transfering Files",
    "content": "File Transfers Between Filesystems on the NREL network . rsync is the recommended tool for transferring data between NREL systems. It allows you to easily restart transfers if they fail, and also provides more consistency when dealing with symbolic links, hard links, and sparse files than either scp or cp. It is recommended you do not use compression for transfers within NREL systems. An example command is: . $ rsync -aP --no-g /scratch/username/dataset1/ /mss/users/username/dataset1/ . Mass Storage has quotas that limit the number of individual files you can store. If you are copying hundreds of thousands of files then it is best to archive these files prior to copying to Mass Storage. See the guide on how to archive files. Mass Storage quotas rely on the group of the file and not the directory path. It is best to use the --no-g option when rsyncing to MSS so you use the destination group rather than the group permissions of your source. You can also chgrp your files to the appropriate group prior to rsyncing to MSS. Small Transfers (&lt;100GB) outside of the NREL network . rsync, scp, and curl will be your best option for small transfers (&lt;100GB) outside of the NREL network. If your rsync/scp/curl transfers are taking hours to complete then you should consider using Globus. If you’re transferring many files then you should use rsync: . $ rsync -azP --no-g /mss/users/username/dataset1/ user@desthost:/home/username/dataset1/ . If you’re transferring an individual file then use scp: . $ scp /home/username/example.tar.gz user@desthost:/home/username/ . You can use curl or wget to download individual files: . $ curl -O https://URL $ wget https://URL . Large Transfers (&gt;100GB) outside of the NREL network . Globus is optimized for file transfers between data centers and anything outside of the NREL network. It will be several times faster than any other tools you will have available. Documentation about requesting a HPC Globus account is available on the Globus Services page on the HPC website. See Transfering files using Globus for instructions on transfering files with Globus. Transfering files using Windows . For Windows you will need to download WinSCP to transfer files to and from HPC systems over SCP. See Transfering using WinSCP. ",
    "url": "/General/beginner/file-transfers.html#best-practices-for-transfering-files",
    "relUrl": "/General/beginner/file-transfers.html#best-practices-for-transfering-files"
  },"14": {
    "doc": "File Transfers",
    "title": "File Transfers",
    "content": " ",
    "url": "/General/beginner/file-transfers.html",
    "relUrl": "/General/beginner/file-transfers.html"
  },"15": {
    "doc": "Filesystems",
    "title": "File systems",
    "content": "Eagle has three primary file systems available for compute nodes. Understanding the usage of these is important for achieving the best performance. ",
    "url": "/General/beginner/filesystems.html#file-systems",
    "relUrl": "/General/beginner/filesystems.html#file-systems"
  },"16": {
    "doc": "Filesystems",
    "title": "NREL file systems",
    "content": ". | Home File System . | Quota of 50 GB | Used to hold scripts, source code, executables | . | Lustre parallel file system: Accessiblle across all nodes. When using this file system please familiarize yourself with the best practices section . | /scratch/username | /projects | /shared-projects | /datasets | . | Node file system: The local drive on each node, these are accessible only on a given node. | /tmp/scratch | . | . For more information on the file systems available on Eagle please see: Eagle System Configuration . ",
    "url": "/General/beginner/filesystems.html#nrel-file-systems",
    "relUrl": "/General/beginner/filesystems.html#nrel-file-systems"
  },"17": {
    "doc": "Filesystems",
    "title": "Lustre best practices",
    "content": "In some cases special care must be taken while using Lustre so as not to affect the performance of the filesystem for yourself and other users. The below Do’s and Don’ts are provided as guidance. | Do . | Use the lfs find . | e.g. lfs find /scratch/username -type f -name \"*.py\" . | . | Break up directories with many files into more directories if possible | Store small files and directories of small files on a single OST. | Limit the number of processes accessing a file. It may be better to read in a file once and broadcast necessary information to other processes. | Change your stripecount based on the filesize | Write many files to the node filesystem /tmp/scratch/ this is not a Lustre filesystem. The files can then be added to a tar archive and transferred to the /project/project_name | . | Don’t . | Use ls -l | Have a file accessed by multiple processes | In Python avoid using os.walk or os.scandir | List files instead of using wildcards . | e.g. don’t use cp * dir/ | If you need to tar/rm/cp a large number of files use xargs or similar. lfs find /scratch/username/old_data/ -t f -print0 | xargs -0 rm . | . | Have many small files in a single directory | Run binary executables from the Lustre filesystem . | e.g. don’t keep libraries or programs in /scratch/username ",
    "url": "/General/beginner/filesystems.html#lustre-best-practices",
    "relUrl": "/General/beginner/filesystems.html#lustre-best-practices"
  },"18": {
    "doc": "Filesystems",
    "title": "Useful Lustre commands",
    "content": "| . | . | Check your storage usage: . | lfs quota -h -u &lt;username&gt; /scratch | . | See which MDT a directory is located on . | lfs getstripe --mdt-index /scratch/&lt;username&gt; | This will return an index 0-2 indicating the MDT | . | Create a folder on a specific MDT (admin only) . | lfs mkdir –i &lt;mdt_index&gt; /dir_path | . | . ",
    "url": "/General/beginner/filesystems.html#useful-lustre-commands",
    "relUrl": "/General/beginner/filesystems.html#useful-lustre-commands"
  },"19": {
    "doc": "Filesystems",
    "title": "Striping",
    "content": "Lustre provides a way to stripe files, this spreads them across multiple OSTs. Striping a large file being accessed by many processes can greatly improve the performace. See Lustre file stripingfor more details. lfs setstripe &lt;file&gt; -c &lt;count&gt; -s &lt;size&gt; . | The stripecount determines how many OST the data is spread across | The stripe size is how large each of the stripes are in KB, MB, GB | . ",
    "url": "/General/beginner/filesystems.html#striping",
    "relUrl": "/General/beginner/filesystems.html#striping"
  },"20": {
    "doc": "Filesystems",
    "title": "References",
    "content": ". | Lustre manual | CU Boulder - Lustre Do’s and Don’ts | NASA - Lustre Best Practices | NASA - Lustre basics | UMBC - Lustre Best Practices | NICS - I/O and Lustre Usage | NERSC - Lustre | . ",
    "url": "/General/beginner/filesystems.html#references",
    "relUrl": "/General/beginner/filesystems.html#references"
  },"21": {
    "doc": "Filesystems",
    "title": "Filesystems",
    "content": " ",
    "url": "/General/beginner/filesystems.html",
    "relUrl": "/General/beginner/filesystems.html"
  },"22": {
    "doc": "Python Additional Resources",
    "title": "Some additional Python resources",
    "content": ". | Resource 1 | Resource 2 | . ",
    "url": "/python/further-resources.html#some-additional-python-resources",
    "relUrl": "/python/further-resources.html#some-additional-python-resources"
  },"23": {
    "doc": "Python Additional Resources",
    "title": "Python Additional Resources",
    "content": " ",
    "url": "/python/further-resources.html",
    "relUrl": "/python/further-resources.html"
  },"24": {
    "doc": "General",
    "title": "General HPC information",
    "content": " ",
    "url": "/General/general.html#general-hpc-information",
    "relUrl": "/General/general.html#general-hpc-information"
  },"25": {
    "doc": "General",
    "title": "General",
    "content": " ",
    "url": "/General/general.html",
    "relUrl": "/General/general.html"
  },"26": {
    "doc": "Home",
    "title": "NREL HPC resources",
    "content": " ",
    "url": "/#nrel-hpc-resources",
    "relUrl": "/#nrel-hpc-resources"
  },"27": {
    "doc": "Home",
    "title": "Top Documentation",
    "content": ". | Filesystems | Python | . ",
    "url": "/#top-documentation",
    "relUrl": "/#top-documentation"
  },"28": {
    "doc": "Home",
    "title": "Additional NREL resources",
    "content": ". | User Basics | Eagle Slurm partitions | . ",
    "url": "/#additional-nrel-resources",
    "relUrl": "/#additional-nrel-resources"
  },"29": {
    "doc": "Home",
    "title": "Home",
    "content": " ",
    "url": "/",
    "relUrl": "/"
  },"30": {
    "doc": "Intermediate",
    "title": "Intermediate",
    "content": " ",
    "url": "/General/intermediate/intermediate.html",
    "relUrl": "/General/intermediate/intermediate.html"
  },"31": {
    "doc": "Python",
    "title": "Python",
    "content": " ",
    "url": "/python/python.html",
    "relUrl": "/python/python.html"
  },"32": {
    "doc": "Python",
    "title": "Eagle tutorials",
    "content": ". | Python environments : Utilize a specific version of Python on Eagle and install packages | Dask : Parallelize your Python code | Jupyter notebooks : Run interactive notebooks on Eagle | . ",
    "url": "/python/python.html#eagle-tutorials",
    "relUrl": "/python/python.html#eagle-tutorials"
  },"33": {
    "doc": "Python",
    "title": "HPC Python",
    "content": "When running Python . | MPI4PY Python bindings to use MPI to distribute computations across cluster nodes | Dask Easily launch Dask workers on one node or across nodes | Numba Optimize your Python code to run faster | PyCUDA Utilize GPUs to accelerate computations | . ",
    "url": "/python/python.html#hpc-python",
    "relUrl": "/python/python.html#hpc-python"
  }
}
