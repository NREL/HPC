{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "028d59ad",
   "metadata": {},
   "source": [
    "# CuPy GPU Computing Demo\n",
    "\n",
    "This notebook demonstrates how to use CuPy for GPU-accelerated computing in Python. We'll compare CPU performance with GPU performance to show the dramatic speedups possible with GPU computing.\n",
    "\n",
    "## What is CuPy?\n",
    "\n",
    "CuPy is a NumPy-compatible library for GPU-accelerated computing with Python. It provides:\n",
    "- **NumPy compatibility**: Drop-in replacement for NumPy arrays on GPU\n",
    "- **CUDA acceleration**: Leverages NVIDIA GPUs for massive parallel processing\n",
    "- **Easy data transfer**: Seamless movement between CPU and GPU memory\n",
    "- **Extensive library support**: GPU versions of many NumPy and SciPy functions\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- NVIDIA GPU with CUDA support\n",
    "- CuPy installed with appropriate CUDA version\n",
    "- Sufficient GPU memory for the arrays we'll create"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca8fdec",
   "metadata": {},
   "source": [
    "## Import Required Libraries\n",
    "\n",
    "Let's start by importing the necessary libraries:\n",
    "- `numpy`: Traditional CPU-based array operations for comparison\n",
    "- `cupy`: GPU-accelerated array operations (NumPy-compatible API)\n",
    "- `cupyx.profiler.benchmark`: CuPy's benchmarking tool for accurate GPU performance measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b173f1c1-467b-415b-8ee4-99257830d74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cupy as cp\n",
    "from cupyx.profiler import benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1dacc61",
   "metadata": {},
   "source": [
    "## Define a Custom GPU Function\n",
    "\n",
    "Let's create a mathematical function that will benefit from GPU parallelization:\n",
    "\n",
    "This function combines several mathematical operations:\n",
    "- Trigonometric functions (`sin`)\n",
    "- Exponential functions (`exp`) \n",
    "- Arithmetic operations with constants (`π`)\n",
    "\n",
    "These element-wise operations on large arrays are perfect candidates for GPU acceleration since they can be performed in parallel across thousands of GPU cores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9282ca52-03a7-41f9-8074-15fb457b6894",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_func(a, b):\n",
    "    return cp.pi * cp.sin(-a) * cp.exp(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcacac9f",
   "metadata": {},
   "source": [
    "## Benchmark GPU Performance\n",
    "\n",
    "Now let's create large GPU arrays and benchmark our function's performance:\n",
    "\n",
    "- **Array size**: 2560 × 1024 ≈ 2.6 million elements per array\n",
    "- **Data type**: 64-bit floating point numbers  \n",
    "- **Memory usage**: ~20 MB per array on GPU\n",
    "- **Benchmark**: 100,000 iterations for statistical accuracy\n",
    "\n",
    "The benchmark function handles GPU timing correctly, accounting for:\n",
    "- GPU kernel launch overhead\n",
    "- Asynchronous execution\n",
    "- Memory transfer considerations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be231d9e-90f5-4a5a-b49c-58de8c9271c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = cp.random.random((2560, 1024))\n",
    "b = cp.random.random((2560, 1024))\n",
    "print(benchmark(my_func, (a,b), n_repeat=100000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e1fff1",
   "metadata": {},
   "source": [
    "## CPU vs GPU Comparison: Linear Algebra\n",
    "\n",
    "Let's compare CPU and GPU performance for a common scientific computing task: solving a system of linear equations.\n",
    "\n",
    "### Problem Setup\n",
    "We'll solve the equation **Ax = b** where:\n",
    "- **A**: 1000×1000 coefficient matrix (~8 MB)\n",
    "- **b**: 1000-element vector\n",
    "- **Solution method**: LU decomposition with partial pivoting\n",
    "\n",
    "### Data Preparation\n",
    "1. Create arrays on CPU using NumPy\n",
    "2. Transfer data to GPU using CuPy\n",
    "3. Compare identical operations on both platforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798d1913-b406-4629-aecf-988134d3b650",
   "metadata": {},
   "outputs": [],
   "source": [
    "np_rng = np.random.default_rng()\n",
    "\n",
    "A_cpu = np_rng.random((1000, 1000))\n",
    "b_cpu = np_rng.random(1000)\n",
    "\n",
    "A_gpu = cp.array(A_cpu)\n",
    "b_gpu = cp.array(b_cpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57e4109",
   "metadata": {},
   "source": [
    "### CPU Performance (NumPy)\n",
    "\n",
    "First, let's measure how long it takes to solve the linear system using CPU-based NumPy:\n",
    "\n",
    "- **Method**: `numpy.linalg.solve()` \n",
    "- **Implementation**: Uses optimized BLAS/LAPACK libraries (Intel MKL, OpenBLAS, etc.)\n",
    "- **Parallelization**: Limited to available CPU cores\n",
    "- **Expected time**: Several milliseconds to tens of milliseconds depending on CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad205a2-9ba1-4740-9182-57b82af9baf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "np.linalg.solve(A_cpu, b_cpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc30d3e",
   "metadata": {},
   "source": [
    "### GPU Performance (CuPy)\n",
    "\n",
    "Now let's measure the same operation using GPU-accelerated CuPy:\n",
    "\n",
    "- **Method**: `cupy.linalg.solve()` \n",
    "- **Implementation**: Uses cuSOLVER (CUDA's dense linear algebra library)\n",
    "- **Parallelization**: Leverages thousands of GPU cores\n",
    "- **Expected speedup**: 5-50x faster depending on GPU model and problem size\n",
    "\n",
    "**Note**: The speedup will be more dramatic for larger matrices, as GPU overhead becomes negligible compared to computation time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d96215-9b1f-4999-ab3e-cc926bb3284c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "cp.linalg.solve(A_gpu, b_gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dbaa580",
   "metadata": {},
   "source": [
    "## Data Transfer Between GPU and CPU\n",
    "\n",
    "An important aspect of GPU computing is managing data transfer between GPU and CPU memory:\n",
    "\n",
    "### GPU to CPU Transfer\n",
    "- **Method**: `.get()` method converts CuPy arrays to NumPy arrays\n",
    "- **Memory copy**: Data is copied from GPU memory to CPU memory\n",
    "- **Performance consideration**: Minimize transfers as they can be a bottleneck\n",
    "- **Use case**: When you need results on CPU for further processing or saving\n",
    "\n",
    "Let's demonstrate creating data on GPU and transferring it back to CPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042864d2-c29c-4be0-8779-935d25759072",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_gpu = cp.ones((1000, 1000))\n",
    "x_cpu = x_gpu.get()\n",
    "print(f'Type of CPU array is: {type(x_cpu)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5beba59f-8d17-437d-a2f6-f8bbab28c6fd",
   "metadata": {},
   "source": [
    "## Key Takeaways and Best Practices\n",
    "\n",
    "This demo illustrates several important concepts about GPU computing with CuPy:\n",
    "\n",
    "### 1. **Massive Parallelism**\n",
    "- GPUs excel at element-wise operations on large arrays\n",
    "- Thousands of cores work simultaneously on different data elements\n",
    "- Best suited for problems that can be parallelized across data\n",
    "\n",
    "### 2. **Memory Management**\n",
    "- Keep data on GPU as long as possible to avoid transfer overhead\n",
    "- Use `.get()` only when you need data back on CPU\n",
    "- Consider GPU memory limitations when working with large datasets\n",
    "\n",
    "### 3. **When to Use GPU Computing**\n",
    "**Good candidates:**\n",
    "- Large matrix operations (linear algebra, FFTs)\n",
    "- Element-wise mathematical functions\n",
    "- Image processing and computer vision\n",
    "- Machine learning training and inference\n",
    "\n",
    "**Poor candidates:**\n",
    "- Small arrays (overhead dominates)\n",
    "- Highly sequential algorithms\n",
    "- Frequent CPU-GPU data transfers\n",
    "\n",
    "### 4. **Performance Optimization Tips**\n",
    "- **Batch operations**: Combine multiple small operations into larger ones\n",
    "- **Memory coalescing**: Access memory in patterns that optimize GPU bandwidth\n",
    "- **Persistent kernels**: Keep data on GPU across multiple operations\n",
    "- **Mixed precision**: Use float32 instead of float64 when precision allows\n",
    "\n",
    "### 5. **CuPy Advantages**\n",
    "- **Easy adoption**: NumPy-compatible API requires minimal code changes\n",
    "- **Comprehensive library**: Most NumPy functions have CuPy equivalents\n",
    "- **Interoperability**: Works well with other GPU libraries (PyTorch, TensorFlow)\n",
    "- **Custom kernels**: Allows writing custom CUDA code when needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a566fc83",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myEnvJupyter",
   "language": "python",
   "name": "myenvjupyter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
