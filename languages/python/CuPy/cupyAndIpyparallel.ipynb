{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f98f0d74",
   "metadata": {},
   "source": [
    "# CuPy + IPyParallel + MPI Distributed Computing Demo\n",
    "\n",
    "This notebook demonstrates how to combine multiple powerful technologies for distributed, parallel computing:\n",
    "- **IPyParallel**: For managing parallel computing clusters\n",
    "- **MPI**: For inter-process communication across nodes\n",
    "- **CuPy**: For GPU-accelerated computing (optional)\n",
    "\n",
    "## What This Demo Shows\n",
    "\n",
    "1. **Cluster Setup**: Creating and managing a parallel computing cluster\n",
    "2. **Multi-node Execution**: Running code across multiple compute engines\n",
    "3. **MPI Communication**: Passing data between distributed processes\n",
    "4. **GPU Integration**: Optional GPU acceleration with CuPy\n",
    "\n",
    "## Technologies Overview\n",
    "\n",
    "### IPyParallel\n",
    "- **Purpose**: Interactive parallel computing in Jupyter\n",
    "- **Features**: Manages engines, load balancing, and result collection\n",
    "- **Use Cases**: Distributed computations, parameter sweeps, embarrassingly parallel tasks\n",
    "\n",
    "### MPI (Message Passing Interface)\n",
    "- **Purpose**: Standard for parallel computing communication\n",
    "- **Features**: Point-to-point and collective communication primitives\n",
    "- **Use Cases**: Scientific computing, distributed simulations, HPC applications\n",
    "\n",
    "### Integration Benefits\n",
    "- **Scalability**: From single machine to large clusters\n",
    "- **Flexibility**: Mix CPU and GPU computing as needed\n",
    "- **Productivity**: Interactive development with production-scale deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26a2437",
   "metadata": {},
   "source": [
    "## Configuration and Imports\n",
    "\n",
    "Let's start by configuring our parallel computing environment:\n",
    "\n",
    "### Cluster Configuration\n",
    "- **numberOfNodes**: Set to 2 for this demo (can be scaled up)\n",
    "- **Engine Type**: MPI engines for inter-process communication\n",
    "- **Libraries**: Import essential parallel computing libraries\n",
    "\n",
    "### Required Imports\n",
    "- `mpi4py`: Python bindings for MPI\n",
    "- `os`: Operating system interface for process management\n",
    "- `ipyparallel`: Interactive parallel computing framework\n",
    "\n",
    "**Note**: The number of nodes can be adjusted based on your available resources. For HPC systems, this could be scaled to hundreds or thousands of processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a880d8b0-d919-463e-b4dc-402c23493efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "numberOfNodes=2\n",
    "import mpi4py\n",
    "import os\n",
    "import ipyparallel as ipp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fac544b",
   "metadata": {},
   "source": [
    "## Create and Start the Parallel Cluster\n",
    "\n",
    "Now we'll create an IPyParallel cluster with MPI engines:\n",
    "\n",
    "### Cluster Parameters\n",
    "- **engines=\"mpi\"**: Use MPI for inter-engine communication\n",
    "- **n=numberOfNodes**: Create the specified number of engines\n",
    "- **controller_ip='*'**: Allow connections from any IP address\n",
    "\n",
    "### What Happens Here\n",
    "1. **Cluster Creation**: IPyParallel sets up a cluster manager\n",
    "2. **Engine Startup**: MPI processes are launched on available resources\n",
    "3. **Connection**: Client connects to the cluster for sending commands\n",
    "4. **Synchronization**: Wait for all engines to be ready\n",
    "\n",
    "**Note**: This step may take a few moments as engines are started and MPI communication is established."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127dc93e-1745-49d8-a9e1-2351ed0092b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = ipp.Cluster(engines=\"mpi\", n=numberOfNodes,controller_ip='*')\n",
    "rc = cluster.start_and_connect_sync()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5499ed8",
   "metadata": {},
   "source": [
    "## Verify Cluster Status\n",
    "\n",
    "Let's verify that our cluster is properly configured and all engines are running:\n",
    "\n",
    "### Cluster Verification Steps\n",
    "1. **Wait for Engines**: Ensure all requested engines are available\n",
    "2. **Create DirectView**: Get a view object for executing code on all engines\n",
    "3. **Check Engine IDs**: Display the unique identifiers for each engine\n",
    "\n",
    "### DirectView (`dview`)\n",
    "- **Purpose**: Execute code synchronously across all engines\n",
    "- **Usage**: Commands sent to `dview` run on all engines simultaneously\n",
    "- **Result**: Collects and returns results from all engines\n",
    "\n",
    "**Expected Output**: You should see engine IDs (typically 0, 1, etc.) indicating successful cluster setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee27e14-31e3-46ed-a083-f99ad32410da",
   "metadata": {},
   "outputs": [],
   "source": [
    "rc.wait_for_engines(n=numberOfNodes)\n",
    "dview=rc[:]\n",
    "rc.ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4305027a",
   "metadata": {},
   "source": [
    "## Test Parallel Execution\n",
    "\n",
    "Let's test our cluster by running code on specific engines and checking their hostnames:\n",
    "\n",
    "### IPyParallel Magic Commands\n",
    "- **`%%px`**: Execute cell code on all engines in parallel\n",
    "- **`--target 0:2`**: Run only on engines 0 and 1 (range 0 to 2, exclusive)\n",
    "- **Alternative**: `--target 0:1` would run only on engine 0\n",
    "\n",
    "### Information Gathering\n",
    "- **Hostname**: Shows which physical machines the engines are running on\n",
    "- **Process ID**: Unique identifier for each process (commented out)\n",
    "- **Distributed Verification**: Confirms code is actually running in parallel\n",
    "\n",
    "**Expected Output**: You'll see hostnames printed from each engine, potentially showing different machines if running on a cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1216bef-49eb-4c6f-8ef5-2ae3a6eb499d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px --target 0:2\n",
    "#%%px --target 0:1\n",
    "import os, socket\n",
    "#print(os.getpid())\n",
    "print(socket.gethostname())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d75f02",
   "metadata": {},
   "source": [
    "## MPI Communication Demo\n",
    "\n",
    "Now for the main demonstration: MPI communication between processes with optional GPU acceleration.\n",
    "\n",
    "### Key Components\n",
    "\n",
    "#### 1. **GPU/CPU Selection**\n",
    "- **`useGPU=False`**: Switch between NumPy (CPU) and CuPy (GPU)\n",
    "- **Flexibility**: Same code works with both backends\n",
    "- **Performance**: Set to `True` for GPU acceleration when available\n",
    "\n",
    "#### 2. **MPI Setup**\n",
    "- **`MPI.COMM_WORLD`**: Global communicator including all processes\n",
    "- **`size`**: Total number of processes in the communicator\n",
    "- **`rank`**: Unique identifier for each process (0, 1, 2, ...)\n",
    "\n",
    "#### 3. **Communication Pattern**\n",
    "- **Rank 0 (Sender)**: Creates random data and sends to rank 1\n",
    "- **Rank 1 (Receiver)**: Receives data from rank 0\n",
    "- **Point-to-Point**: Direct communication between specific processes\n",
    "- **Tagged Messages**: Use tag=42 to identify message type\n",
    "\n",
    "#### 4. **Data Details**\n",
    "- **Shape**: 4D tensor (N×N×N×N) where N=2\n",
    "- **Size**: 16 elements total for this demo\n",
    "- **Type**: Random numbers from normal distribution\n",
    "- **Scalability**: Can easily increase N for larger data transfers\n",
    "\n",
    "**Expected Output**: You'll see the sender and receiver processes print the data being transferred, demonstrating successful MPI communication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3247f8c5-dea9-4bfa-8ba7-e14f920b2fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "from mpi4py import MPI\n",
    "\n",
    "useGPU=False\n",
    "\n",
    "if useGPU:\n",
    "    import cupy as cp\n",
    "else:\n",
    "    import numpy as cp\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "size = comm.Get_size()\n",
    "rank = comm.Get_rank()\n",
    "\n",
    "if rank == 0:\n",
    "    N=2\n",
    "    data = cp.random.randn(N,N,N,N)\n",
    "    comm.send(data, dest=1, tag=42)\n",
    "    print('Process {} sent data:'.format(rank), data)\n",
    "    \n",
    "elif rank == 1:\n",
    "    data = comm.recv(source=0, tag=42)\n",
    "    print('Process {} received data:'.format(rank), data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47a47d6-b7f0-4e5d-a8ba-37649d805a10",
   "metadata": {},
   "source": [
    "## Key Takeaways and Real-World Applications\n",
    "\n",
    "This demo showcases the powerful combination of IPyParallel, MPI, and optional GPU computing:\n",
    "\n",
    "#### **IPyParallel Advantages**\n",
    "- **Interactive Development**: Test parallel code interactively in Jupyter\n",
    "- **Easy Scaling**: From laptop to supercomputer with minimal code changes\n",
    "- **Flexible Execution**: Target specific engines or broadcast to all\n",
    "- **Result Management**: Automatic collection and synchronization of results\n",
    "\n",
    "#### **MPI Communication**\n",
    "- **Standards-Based**: Industry standard for parallel computing\n",
    "- **Scalable**: From 2 processes to millions of processes\n",
    "- **Rich Communication**: Point-to-point, collective, and one-sided operations\n",
    "- **Performance**: Optimized for high-performance networks\n",
    "\n",
    "#### **GPU Integration**\n",
    "- **Optional Acceleration**: Same code works with/without GPUs\n",
    "- **Memory Efficiency**: Process GPU arrays directly without CPU transfer\n",
    "- **Hybrid Computing**: Mix CPU and GPU computation as needed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91412431",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myEnvJupyter",
   "language": "python",
   "name": "myenvjupyter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
